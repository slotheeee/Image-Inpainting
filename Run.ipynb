{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "root = '/datasets/COCO-2017'\n",
    "mode = 'train2017'\n",
    "#root = 'datasets-2/chalearn'\n",
    "outfile = './train2017.txt'\n",
    "if not os.path.exists(os.path.join(root, mode)):\n",
    "    print(\"Cannot access\")\n",
    "else:\n",
    "    print(\"Successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 777 train.py\n",
    "!chmod 777 dataset.py\n",
    "!chmod 777 model.py\n",
    "!chmod 777 utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./output’: File exists\n",
      "mkdir: cannot create directory ‘./checkpoints2’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./output\n",
    "!mkdir ./checkpoints2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training dataset: 118280, dataloader: 118280\n",
      "Size of the validation dataset: 5000, dataloader: 5000\n",
      "success in loading checkpoint\n",
      "success in loading checkpoint\n",
      "Train: step:     10, time: 0.249, loss: 1064.606689\n",
      "Train: step:     20, time: 0.265, loss: 1681.717041\n",
      "Train: step:     30, time: 0.279, loss: 736.677917\n",
      "Train: step:     40, time: 0.201, loss: 1415.625244\n",
      "Train: step:     50, time: 0.210, loss: 625.747253\n",
      "Train: step:     60, time: 0.250, loss: 2308.329834\n",
      "Train: step:     70, time: 0.252, loss: 3091.807617\n",
      "Train: step:     80, time: 0.198, loss: 1727.408691\n",
      "Train: step:     90, time: 0.201, loss: 643.831909\n",
      "Train: step:    100, time: 0.254, loss: 2720.898682\n",
      "Train: step:    110, time: 0.284, loss: 582.758789\n",
      "Train: step:    120, time: 0.199, loss: 2363.521484\n",
      "Train: step:    130, time: 0.247, loss: 1413.709351\n",
      "Train: step:    140, time: 0.225, loss: 2026.826294\n",
      "Train: step:    150, time: 0.276, loss: 1699.482544\n",
      "Train: step:    160, time: 0.256, loss: 2794.580566\n",
      "Train: step:    170, time: 0.199, loss: 239.695084\n",
      "Train: step:    180, time: 0.282, loss: 658.791504\n",
      "Train: step:    190, time: 0.259, loss: 2155.784180\n",
      "Train: step:    200, time: 0.272, loss: 1649.004517\n",
      "Train: step:    210, time: 0.219, loss: 922.848450\n",
      "Train: step:    220, time: 0.249, loss: 765.943176\n",
      "Train: step:    230, time: 0.284, loss: 1624.866211\n",
      "Train: step:    240, time: 0.280, loss: 2007.178101\n",
      "Train: step:    250, time: 0.242, loss: 1143.565552\n",
      "Train: step:    260, time: 0.278, loss: 2541.027588\n",
      "Train: step:    270, time: 0.255, loss: 1825.064087\n",
      "Train: step:    280, time: 0.257, loss: 1511.076538\n",
      "Train: step:    290, time: 0.279, loss: 4077.943115\n",
      "Train: step:    300, time: 0.274, loss: 1852.782837\n",
      "Train: step:    310, time: 0.286, loss: 2423.143311\n",
      "Train: step:    320, time: 0.237, loss: 997.300354\n",
      "Train: step:    330, time: 0.252, loss: 227.231522\n",
      "Train: step:    340, time: 0.248, loss: 611.831055\n",
      "Train: step:    350, time: 0.269, loss: 2741.406250\n",
      "Train: step:    360, time: 0.255, loss: 2801.413330\n",
      "Train: step:    370, time: 0.244, loss: 569.945007\n",
      "Train: step:    380, time: 0.252, loss: 1722.019165\n",
      "Train: step:    390, time: 0.249, loss: 143.526917\n",
      "Train: step:    400, time: 0.278, loss: 810.501953\n",
      "Train: step:    410, time: 0.238, loss: 2010.140747\n",
      "Train: step:    420, time: 0.270, loss: 1509.925537\n",
      "Train: step:    430, time: 0.249, loss: 1636.935913\n",
      "Train: step:    440, time: 0.260, loss: 641.815308\n",
      "Train: step:    450, time: 0.258, loss: 1942.369873\n",
      "Train: step:    460, time: 0.290, loss: 1898.552734\n",
      "Train: step:    470, time: 0.257, loss: 1937.655029\n",
      "Train: step:    480, time: 0.204, loss: 1070.509155\n",
      "Train: step:    490, time: 0.273, loss: 1996.983154\n",
      "Train: step:    500, time: 0.210, loss: 300.988678\n",
      "Train: step:    510, time: 0.202, loss: 541.463562\n",
      "Train: step:    520, time: 0.207, loss: 1533.800171\n",
      "Train: step:    530, time: 0.204, loss: 895.484375\n",
      "Train: step:    540, time: 0.205, loss: 3296.412354\n",
      "Train: step:    550, time: 0.245, loss: 1445.943848\n",
      "Train: step:    560, time: 0.257, loss: 2495.511963\n",
      "Train: step:    570, time: 0.290, loss: 1556.121338\n",
      "Train: step:    580, time: 0.245, loss: 2036.904541\n",
      "Train: step:    590, time: 0.253, loss: 2806.441895\n",
      "Train: step:    600, time: 0.246, loss: 3006.732666\n",
      "Train: step:    610, time: 0.250, loss: 2226.641602\n",
      "Train: step:    620, time: 0.253, loss: 2182.721436\n",
      "Train: step:    630, time: 0.304, loss: 1847.142456\n",
      "Train: step:    640, time: 0.260, loss: 2587.850586\n",
      "Train: step:    650, time: 0.232, loss: 2461.183350\n",
      "Train: step:    660, time: 0.270, loss: 1440.336304\n",
      "Train: step:    670, time: 0.209, loss: 1289.128662\n",
      "Train: step:    680, time: 0.261, loss: 2041.839111\n",
      "Train: step:    690, time: 0.241, loss: 1863.101196\n",
      "Train: step:    700, time: 0.241, loss: 2360.802734\n",
      "Train: step:    710, time: 0.271, loss: 908.650391\n",
      "Train: step:    720, time: 0.209, loss: 390.832306\n",
      "Train: step:    730, time: 0.210, loss: 2129.892822\n",
      "Train: step:    740, time: 0.282, loss: 275.001709\n",
      "Train: step:    750, time: 0.242, loss: 3210.612305\n",
      "Train: step:    760, time: 0.203, loss: 1781.571533\n",
      "Train: step:    770, time: 0.256, loss: 3388.270264\n",
      "Train: step:    780, time: 0.259, loss: 2337.461670\n",
      "Train: step:    790, time: 0.253, loss: 1688.224854\n",
      "Train: step:    800, time: 0.275, loss: 1830.837036\n",
      "Train: step:    810, time: 0.217, loss: 1511.786987\n",
      "Train: step:    820, time: 0.212, loss: 3310.959473\n",
      "Train: step:    830, time: 0.270, loss: 1911.488647\n",
      "Train: step:    840, time: 0.277, loss: 1068.085205\n",
      "Train: step:    850, time: 0.258, loss: 2139.831787\n",
      "Train: step:    860, time: 0.277, loss: 2622.842773\n",
      "Train: step:    870, time: 0.231, loss: 2355.388916\n",
      "Train: step:    880, time: 0.283, loss: 1919.640259\n",
      "Train: step:    890, time: 0.203, loss: 2252.536377\n",
      "Train: step:    900, time: 0.215, loss: 3528.540771\n",
      "Train: step:    910, time: 0.275, loss: 2803.338379\n",
      "Train: step:    920, time: 0.275, loss: 713.566223\n",
      "Train: step:    930, time: 0.275, loss: 856.546265\n",
      "Train: step:    940, time: 0.208, loss: 2511.503662\n",
      "Train: step:    950, time: 0.202, loss: 673.533325\n",
      "Train: step:    960, time: 0.203, loss: 1358.704590\n",
      "Train: step:    970, time: 0.202, loss: 1152.623291\n",
      "Train: step:    980, time: 0.247, loss: 872.724915\n",
      "Train: step:    990, time: 0.203, loss: 2138.573975\n",
      "Train: step:   1000, time: 0.209, loss: 2491.163574\n",
      "Train: step:   1010, time: 0.201, loss: 1967.523193\n",
      "Train: step:   1020, time: 0.275, loss: 1937.249390\n",
      "Train: step:   1030, time: 0.254, loss: 1647.626465\n",
      "Train: step:   1040, time: 0.255, loss: 3742.048584\n",
      "Train: step:   1050, time: 0.241, loss: 1356.699463\n",
      "Train: step:   1060, time: 0.274, loss: 1718.109253\n",
      "Train: step:   1070, time: 0.303, loss: 2178.130615\n",
      "Train: step:   1080, time: 0.249, loss: 3431.880615\n",
      "Train: step:   1090, time: 0.262, loss: 2496.462891\n",
      "Train: step:   1100, time: 0.249, loss: 1865.311523\n",
      "Train: step:   1110, time: 0.250, loss: 2115.770020\n",
      "Train: step:   1120, time: 0.249, loss: 1347.259155\n",
      "Train: step:   1130, time: 0.277, loss: 461.981934\n",
      "Train: step:   1140, time: 0.275, loss: 3450.629395\n",
      "Train: step:   1150, time: 0.258, loss: 1779.918213\n",
      "Train: step:   1160, time: 0.275, loss: 2207.305908\n",
      "Train: step:   1170, time: 0.253, loss: 841.097107\n",
      "Train: step:   1180, time: 0.204, loss: 1634.262207\n",
      "Train: step:   1190, time: 0.224, loss: 692.352966\n",
      "Train: step:   1200, time: 0.250, loss: 2353.150391\n",
      "Train: step:   1210, time: 0.205, loss: 968.835999\n",
      "Train: step:   1220, time: 0.202, loss: 1441.915527\n",
      "Train: step:   1230, time: 0.254, loss: 1981.036987\n",
      "Train: step:   1240, time: 0.285, loss: 2984.068359\n",
      "Train: step:   1250, time: 0.205, loss: 1202.559448\n",
      "Train: step:   1260, time: 0.252, loss: 1180.071777\n",
      "Train: step:   1270, time: 0.296, loss: 1623.816895\n",
      "Train: step:   1280, time: 0.264, loss: 2791.889160\n",
      "Train: step:   1290, time: 0.265, loss: 2179.722900\n",
      "Train: step:   1300, time: 0.204, loss: 2257.360596\n",
      "Train: step:   1310, time: 0.205, loss: 616.115051\n",
      "Train: step:   1320, time: 0.207, loss: 650.422241\n",
      "Train: step:   1330, time: 0.203, loss: 1774.687988\n",
      "Train: step:   1340, time: 0.266, loss: 1919.222412\n",
      "Train: step:   1350, time: 0.206, loss: 3992.983643\n",
      "Train: step:   1360, time: 0.287, loss: 1043.360962\n",
      "Train: step:   1370, time: 0.259, loss: 1134.820923\n",
      "Train: step:   1380, time: 0.207, loss: 2905.654785\n",
      "Train: step:   1390, time: 0.225, loss: 2640.436523\n",
      "Train: step:   1400, time: 0.254, loss: 3439.729980\n",
      "Train: step:   1410, time: 0.282, loss: 3311.252930\n",
      "Train: step:   1420, time: 0.256, loss: 1395.731812\n",
      "Train: step:   1430, time: 0.255, loss: 2068.570557\n",
      "Train: step:   1440, time: 0.250, loss: 726.718506\n",
      "Train: step:   1450, time: 0.201, loss: 1313.714478\n",
      "Train: step:   1460, time: 0.208, loss: 1116.349731\n",
      "Train: step:   1470, time: 0.204, loss: 2489.448730\n",
      "Train: step:   1480, time: 0.248, loss: 2192.220947\n",
      "Train: step:   1490, time: 0.276, loss: 2821.450684\n",
      "Train: step:   1500, time: 0.250, loss: 3078.894531\n",
      "Train: step:   1510, time: 0.209, loss: 3370.422607\n",
      "Train: step:   1520, time: 0.207, loss: 1424.943359\n",
      "Train: step:   1530, time: 0.204, loss: 1979.472290\n",
      "Train: step:   1540, time: 0.264, loss: 1859.402832\n",
      "Train: step:   1550, time: 0.204, loss: 2430.413818\n",
      "Train: step:   1560, time: 0.241, loss: 3144.678467\n",
      "Train: step:   1570, time: 0.246, loss: 1529.231323\n",
      "Train: step:   1580, time: 0.254, loss: 2108.577881\n",
      "Train: step:   1590, time: 0.244, loss: 1758.470581\n",
      "Train: step:   1600, time: 0.259, loss: 824.113831\n",
      "Train: step:   1610, time: 0.239, loss: 3600.005127\n",
      "Train: step:   1620, time: 0.203, loss: 2782.717529\n",
      "Train: step:   1630, time: 0.207, loss: 481.229797\n",
      "Train: step:   1640, time: 0.201, loss: 984.235352\n",
      "Train: step:   1650, time: 0.249, loss: 2008.535522\n",
      "Train: step:   1660, time: 0.251, loss: 2731.969971\n",
      "Train: step:   1670, time: 0.201, loss: 1861.438477\n",
      "Train: step:   1680, time: 0.208, loss: 810.786011\n",
      "Train: step:   1690, time: 0.287, loss: 3001.223877\n",
      "Train: step:   1700, time: 0.276, loss: 3531.023682\n",
      "Train: step:   1710, time: 0.261, loss: 1567.054443\n",
      "Train: step:   1720, time: 0.263, loss: 972.216858\n",
      "Train: step:   1730, time: 0.209, loss: 1995.664551\n",
      "Train: step:   1740, time: 0.274, loss: 3604.882080\n",
      "Train: step:   1750, time: 0.262, loss: 1735.923828\n",
      "Train: step:   1760, time: 0.250, loss: 2876.209961\n",
      "Train: step:   1770, time: 0.251, loss: 597.309448\n",
      "Train: step:   1780, time: 0.264, loss: 1154.333252\n",
      "Train: step:   1790, time: 0.204, loss: 744.169861\n",
      "Train: step:   1800, time: 0.250, loss: 2049.208984\n",
      "Train: step:   1810, time: 0.245, loss: 2469.396729\n",
      "Train: step:   1820, time: 0.287, loss: 593.072021\n",
      "Train: step:   1830, time: 0.240, loss: 563.273071\n",
      "Train: step:   1840, time: 0.296, loss: 2076.173096\n",
      "Train: step:   1850, time: 0.282, loss: 2878.030762\n",
      "Train: step:   1860, time: 0.286, loss: 1282.261719\n",
      "Train: step:   1870, time: 0.276, loss: 1941.649170\n",
      "Train: step:   1880, time: 0.268, loss: 2184.613037\n",
      "Train: step:   1890, time: 0.265, loss: 604.500488\n",
      "Train: step:   1900, time: 0.287, loss: 2077.857910\n",
      "Train: step:   1910, time: 0.214, loss: 2306.568604\n",
      "Train: step:   1920, time: 0.209, loss: 3744.717285\n",
      "Train: step:   1930, time: 0.209, loss: 1004.608582\n",
      "Train: step:   1940, time: 0.248, loss: 1702.359619\n",
      "Train: step:   1950, time: 0.213, loss: 294.568024\n",
      "Train: step:   1960, time: 0.278, loss: 2159.700684\n",
      "Train: step:   1970, time: 0.202, loss: 1860.017456\n",
      "Train: step:   1980, time: 0.211, loss: 1996.628662\n",
      "Train: step:   1990, time: 0.204, loss: 898.829651\n",
      "Train: step:   2000, time: 0.289, loss: 2139.915039\n",
      "Train: step:   2010, time: 0.252, loss: 2538.233398\n",
      "Train: step:   2020, time: 0.294, loss: 2170.837158\n",
      "Train: step:   2030, time: 0.205, loss: 641.044312\n",
      "Train: step:   2040, time: 0.288, loss: 1022.581299\n",
      "Train: step:   2050, time: 0.279, loss: 1553.427612\n",
      "Train: step:   2060, time: 0.209, loss: 2951.713135\n",
      "Train: step:   2070, time: 0.264, loss: 2430.384033\n",
      "Train: step:   2080, time: 0.281, loss: 1407.393555\n",
      "Train: step:   2090, time: 0.285, loss: 1659.443359\n",
      "Train: step:   2100, time: 0.207, loss: 2666.042480\n",
      "Train: step:   2110, time: 0.246, loss: 1827.369629\n",
      "Train: step:   2120, time: 0.302, loss: 465.965942\n",
      "Train: step:   2130, time: 0.212, loss: 1947.277954\n",
      "Train: step:   2140, time: 0.203, loss: 1887.888794\n",
      "Train: step:   2150, time: 0.277, loss: 572.731750\n",
      "Train: step:   2160, time: 0.251, loss: 1308.004883\n",
      "Train: step:   2170, time: 0.203, loss: 1251.210693\n",
      "Train: step:   2180, time: 0.206, loss: 1304.559326\n",
      "Train: step:   2190, time: 0.216, loss: 3484.900879\n",
      "Train: step:   2200, time: 0.260, loss: 2665.984863\n",
      "Train: step:   2210, time: 0.252, loss: 426.906860\n",
      "Train: step:   2220, time: 0.206, loss: 2186.646240\n",
      "Train: step:   2230, time: 0.248, loss: 837.215393\n",
      "Train: step:   2240, time: 0.225, loss: 2462.249512\n",
      "Train: step:   2250, time: 0.203, loss: 2397.947510\n",
      "Train: step:   2260, time: 0.205, loss: 2596.662354\n",
      "Train: step:   2270, time: 0.205, loss: 2049.245361\n",
      "Train: step:   2280, time: 0.252, loss: 2910.312500\n",
      "Train: step:   2290, time: 0.204, loss: 2240.523438\n",
      "Train: step:   2300, time: 0.258, loss: 748.563904\n",
      "Train: step:   2310, time: 0.273, loss: 2844.107910\n",
      "Train: step:   2320, time: 0.263, loss: 3116.668945\n",
      "Train: step:   2330, time: 0.252, loss: 2350.651123\n",
      "Train: step:   2340, time: 0.205, loss: 1425.843384\n",
      "Train: step:   2350, time: 0.250, loss: 1492.156738\n",
      "Train: step:   2360, time: 0.210, loss: 2157.367188\n",
      "Train: step:   2370, time: 0.280, loss: 2418.077148\n",
      "Train: step:   2380, time: 0.204, loss: 3468.304199\n",
      "Train: step:   2390, time: 0.207, loss: 2302.365234\n",
      "Train: step:   2400, time: 0.205, loss: 2605.143311\n",
      "Train: step:   2410, time: 0.208, loss: 2857.620361\n",
      "Train: step:   2420, time: 0.202, loss: 1411.903809\n",
      "Train: step:   2430, time: 0.204, loss: 1387.553223\n",
      "Train: step:   2440, time: 0.251, loss: 1563.634399\n",
      "Train: step:   2450, time: 0.211, loss: 1592.043213\n",
      "Train: step:   2460, time: 0.270, loss: 783.451965\n",
      "Train: step:   2470, time: 0.202, loss: 3188.854492\n",
      "Train: step:   2480, time: 0.282, loss: 1385.659546\n",
      "Train: step:   2490, time: 0.269, loss: 1591.791016\n",
      "Train: step:   2500, time: 0.235, loss: 1822.353394\n",
      "Train: step:   2510, time: 0.205, loss: 2695.725830\n",
      "Train: step:   2520, time: 0.204, loss: 2516.393555\n",
      "Train: step:   2530, time: 0.205, loss: 1679.256958\n",
      "Train: step:   2540, time: 0.207, loss: 2471.363281\n",
      "Train: step:   2550, time: 0.208, loss: 2725.973633\n",
      "Train: step:   2560, time: 0.203, loss: 704.712463\n",
      "Train: step:   2570, time: 0.264, loss: 1545.799072\n",
      "Train: step:   2580, time: 0.206, loss: 2112.346680\n",
      "Train: step:   2590, time: 0.212, loss: 1670.243652\n",
      "Train: step:   2600, time: 0.214, loss: 1902.213257\n",
      "Train: step:   2610, time: 0.209, loss: 2985.791016\n",
      "Train: step:   2620, time: 0.205, loss: 716.379822\n",
      "Train: step:   2630, time: 0.255, loss: 1341.726440\n",
      "Train: step:   2640, time: 0.252, loss: 3467.997314\n",
      "Train: step:   2650, time: 0.214, loss: 1396.221924\n",
      "Train: step:   2660, time: 0.288, loss: 1581.028442\n",
      "Train: step:   2670, time: 0.282, loss: 1662.501221\n",
      "Train: step:   2680, time: 0.276, loss: 847.246887\n",
      "Train: step:   2690, time: 0.252, loss: 2010.825928\n",
      "Train: step:   2700, time: 0.265, loss: 988.986450\n",
      "Train: step:   2710, time: 0.205, loss: 3452.607910\n",
      "Train: step:   2720, time: 0.248, loss: 2733.491211\n",
      "Train: step:   2730, time: 0.254, loss: 1058.596313\n",
      "Train: step:   2740, time: 0.253, loss: 1110.286011\n",
      "Train: step:   2750, time: 0.223, loss: 646.984863\n",
      "Train: step:   2760, time: 0.255, loss: 3259.351807\n",
      "Train: step:   2770, time: 0.253, loss: 948.496338\n",
      "Train: step:   2780, time: 0.271, loss: 1897.202271\n",
      "Train: step:   2790, time: 0.260, loss: 2056.814453\n",
      "Train: step:   2800, time: 0.251, loss: 2795.201904\n",
      "Train: step:   2810, time: 0.251, loss: 3079.985596\n",
      "Train: step:   2820, time: 0.295, loss: 1561.857910\n",
      "Train: step:   2830, time: 0.295, loss: 1890.060791\n",
      "Train: step:   2840, time: 0.289, loss: 4140.168945\n",
      "Train: step:   2850, time: 0.288, loss: 1779.534058\n",
      "Train: step:   2860, time: 0.209, loss: 1027.381470\n",
      "Train: step:   2870, time: 0.208, loss: 3162.586670\n",
      "Train: step:   2880, time: 0.211, loss: 1619.300781\n",
      "Train: step:   2890, time: 0.202, loss: 2521.340576\n",
      "Train: step:   2900, time: 0.260, loss: 1448.363037\n",
      "Train: step:   2910, time: 0.261, loss: 1429.002930\n",
      "Train: step:   2920, time: 0.251, loss: 985.745422\n",
      "Train: step:   2930, time: 0.212, loss: 2440.837646\n",
      "Train: step:   2940, time: 0.247, loss: 2883.752930\n",
      "Train: step:   2950, time: 0.224, loss: 3177.094238\n",
      "Train: step:   2960, time: 0.252, loss: 1763.304443\n",
      "Train: step:   2970, time: 0.281, loss: 4419.812500\n",
      "Train: step:   2980, time: 0.251, loss: 2500.561523\n",
      "Train: step:   2990, time: 0.203, loss: 2051.052979\n",
      "Train: step:   3000, time: 0.248, loss: 2052.223145\n",
      "Train: step:   3010, time: 0.256, loss: 966.471436\n",
      "Train: step:   3020, time: 0.290, loss: 586.017639\n",
      "Train: step:   3030, time: 0.279, loss: 2999.271729\n",
      "Train: step:   3040, time: 0.249, loss: 1600.081787\n",
      "Train: step:   3050, time: 0.243, loss: 1525.629883\n",
      "Train: step:   3060, time: 0.240, loss: 2591.399902\n",
      "Train: step:   3070, time: 0.284, loss: 1401.739746\n",
      "Train: step:   3080, time: 0.277, loss: 776.489746\n",
      "Train: step:   3090, time: 0.279, loss: 1951.420654\n",
      "Train: step:   3100, time: 0.247, loss: 2093.739502\n",
      "Train: step:   3110, time: 0.288, loss: 2214.987061\n",
      "Train: step:   3120, time: 0.209, loss: 2077.758057\n",
      "Train: step:   3130, time: 0.277, loss: 2382.378418\n",
      "Train: step:   3140, time: 0.283, loss: 1617.857666\n",
      "Train: step:   3150, time: 0.203, loss: 856.011475\n",
      "Train: step:   3160, time: 0.286, loss: 1807.907715\n",
      "Train: step:   3170, time: 0.281, loss: 2062.745605\n",
      "Train: step:   3180, time: 0.266, loss: 4200.100098\n",
      "Train: step:   3190, time: 0.248, loss: 2076.949707\n",
      "Train: step:   3200, time: 0.247, loss: 398.003143\n",
      "Train: step:   3210, time: 0.247, loss: 1745.446655\n",
      "Train: step:   3220, time: 0.261, loss: 906.626404\n",
      "Train: step:   3230, time: 0.251, loss: 2268.253906\n",
      "Train: step:   3240, time: 0.267, loss: 983.615662\n",
      "Train: step:   3250, time: 0.277, loss: 1905.364258\n",
      "Train: step:   3260, time: 0.247, loss: 3116.293213\n",
      "Train: step:   3270, time: 0.274, loss: 3550.949707\n",
      "Train: step:   3280, time: 0.289, loss: 1151.381348\n",
      "Train: step:   3290, time: 0.245, loss: 1884.562378\n",
      "Train: step:   3300, time: 0.206, loss: 2511.543457\n",
      "Train: step:   3310, time: 0.281, loss: 1972.885620\n",
      "Train: step:   3320, time: 0.249, loss: 3217.538330\n",
      "Train: step:   3330, time: 0.203, loss: 2403.160645\n",
      "Train: step:   3340, time: 0.250, loss: 1641.849609\n",
      "Train: step:   3350, time: 0.287, loss: 1410.810181\n",
      "Train: step:   3360, time: 0.275, loss: 907.027527\n",
      "Train: step:   3370, time: 0.248, loss: 748.445923\n",
      "Train: step:   3380, time: 0.244, loss: 1229.966553\n",
      "Train: step:   3390, time: 0.246, loss: 331.198792\n",
      "Train: step:   3400, time: 0.255, loss: 815.285645\n",
      "Train: step:   3410, time: 0.280, loss: 4105.584473\n",
      "Train: step:   3420, time: 0.211, loss: 1546.399048\n",
      "Train: step:   3430, time: 0.251, loss: 999.339539\n",
      "Train: step:   3440, time: 0.251, loss: 1214.505859\n",
      "Train: step:   3450, time: 0.204, loss: 2333.126953\n",
      "Train: step:   3460, time: 0.257, loss: 1381.572266\n",
      "Train: step:   3470, time: 0.252, loss: 2078.737305\n",
      "Train: step:   3480, time: 0.211, loss: 1091.451538\n",
      "Train: step:   3490, time: 0.205, loss: 1226.476562\n",
      "Train: step:   3500, time: 0.220, loss: 852.857910\n",
      "Train: step:   3510, time: 0.263, loss: 2413.744385\n",
      "Train: step:   3520, time: 0.261, loss: 636.088623\n",
      "Train: step:   3530, time: 0.283, loss: 1400.296509\n",
      "Train: step:   3540, time: 0.245, loss: 535.074707\n",
      "Train: step:   3550, time: 0.251, loss: 1143.537231\n",
      "Train: step:   3560, time: 0.256, loss: 1885.513306\n",
      "Train: step:   3570, time: 0.289, loss: 536.265137\n",
      "Train: step:   3580, time: 0.274, loss: 1646.335083\n",
      "Train: step:   3590, time: 0.206, loss: 2187.790527\n",
      "Train: step:   3600, time: 0.301, loss: 2803.084229\n",
      "Train: step:   3610, time: 0.248, loss: 871.120239\n",
      "Train: step:   3620, time: 0.250, loss: 3772.974365\n",
      "Train: step:   3630, time: 0.249, loss: 751.717590\n",
      "Train: step:   3640, time: 0.282, loss: 476.710541\n",
      "Train: step:   3650, time: 0.282, loss: 400.535553\n",
      "Train: step:   3660, time: 0.290, loss: 1211.280396\n",
      "Train: step:   3670, time: 0.285, loss: 1664.933228\n",
      "Train: step:   3680, time: 0.202, loss: 2222.677734\n",
      "Train: step:   3690, time: 0.273, loss: 3465.433350\n",
      "Train: step:   3700, time: 0.262, loss: 1213.128418\n",
      "Train: step:   3710, time: 0.250, loss: 2043.065063\n",
      "Train: step:   3720, time: 0.213, loss: 904.648743\n",
      "Train: step:   3730, time: 0.216, loss: 1620.949463\n",
      "Train: step:   3740, time: 0.209, loss: 1263.710571\n",
      "Train: step:   3750, time: 0.273, loss: 1031.889404\n",
      "Train: step:   3760, time: 0.256, loss: 1809.322266\n",
      "Train: step:   3770, time: 0.264, loss: 1006.167297\n",
      "Train: step:   3780, time: 0.280, loss: 2298.208740\n",
      "Train: step:   3790, time: 0.270, loss: 2216.437012\n",
      "Train: step:   3800, time: 0.290, loss: 1503.014771\n",
      "Train: step:   3810, time: 0.203, loss: 2171.362549\n",
      "Train: step:   3820, time: 0.277, loss: 3181.741211\n",
      "Train: step:   3830, time: 0.255, loss: 1728.838013\n",
      "Train: step:   3840, time: 0.281, loss: 1269.093384\n",
      "Train: step:   3850, time: 0.275, loss: 2342.562012\n",
      "Train: step:   3860, time: 0.248, loss: 1439.269653\n",
      "Train: step:   3870, time: 0.273, loss: 2529.955566\n",
      "Train: step:   3880, time: 0.272, loss: 1743.439575\n",
      "Train: step:   3890, time: 0.287, loss: 425.112579\n",
      "Train: step:   3900, time: 0.251, loss: 1890.367432\n",
      "Train: step:   3910, time: 0.285, loss: 1566.569702\n",
      "Train: step:   3920, time: 0.253, loss: 429.450287\n",
      "Train: step:   3930, time: 0.237, loss: 2334.715088\n",
      "Train: step:   3940, time: 0.244, loss: 1644.191284\n",
      "Train: step:   3950, time: 0.274, loss: 938.082825\n",
      "Train: step:   3960, time: 0.201, loss: 1081.852173\n",
      "Train: step:   3970, time: 0.254, loss: 629.616577\n",
      "Train: step:   3980, time: 0.259, loss: 1994.785034\n",
      "Train: step:   3990, time: 0.267, loss: 1524.370239\n",
      "Train: step:   4000, time: 0.219, loss: 2550.764404\n",
      "Train: step:   4010, time: 0.250, loss: 1606.979370\n",
      "Train: step:   4020, time: 0.250, loss: 2050.036377\n",
      "Train: step:   4030, time: 0.249, loss: 3356.156494\n",
      "Train: step:   4040, time: 0.254, loss: 2994.795654\n",
      "Train: step:   4050, time: 0.279, loss: 1863.727173\n",
      "Train: step:   4060, time: 0.243, loss: 1497.622192\n",
      "Train: step:   4070, time: 0.213, loss: 2747.088867\n",
      "Train: step:   4080, time: 0.258, loss: 4114.605469\n",
      "Train: step:   4090, time: 0.231, loss: 3700.914795\n",
      "Train: step:   4100, time: 0.259, loss: 457.021912\n",
      "Train: step:   4110, time: 0.267, loss: 832.324951\n",
      "Train: step:   4120, time: 0.272, loss: 3841.877930\n",
      "Train: step:   4130, time: 0.288, loss: 561.235596\n",
      "Train: step:   4140, time: 0.246, loss: 872.800293\n",
      "Train: step:   4150, time: 0.206, loss: 4041.986816\n",
      "Train: step:   4160, time: 0.203, loss: 3384.270508\n",
      "Train: step:   4170, time: 0.261, loss: 2555.515381\n",
      "Train: step:   4180, time: 0.272, loss: 414.736847\n",
      "Train: step:   4190, time: 0.202, loss: 1093.425903\n",
      "Train: step:   4200, time: 0.252, loss: 3310.508789\n",
      "Train: step:   4210, time: 0.250, loss: 334.503204\n",
      "Train: step:   4220, time: 0.241, loss: 1408.608276\n",
      "Train: step:   4230, time: 0.258, loss: 884.950073\n",
      "Train: step:   4240, time: 0.257, loss: 496.688141\n",
      "Train: step:   4250, time: 0.260, loss: 1901.589600\n",
      "Train: step:   4260, time: 0.313, loss: 2713.343750\n",
      "Train: step:   4270, time: 0.272, loss: 875.893616\n",
      "Train: step:   4280, time: 0.259, loss: 3055.835449\n",
      "Train: step:   4290, time: 0.263, loss: 991.484131\n",
      "Train: step:   4300, time: 0.243, loss: 1115.906494\n",
      "Train: step:   4310, time: 0.204, loss: 1953.333862\n",
      "Train: step:   4320, time: 0.252, loss: 1920.785889\n",
      "Train: step:   4330, time: 0.260, loss: 1080.561890\n",
      "Train: step:   4340, time: 0.252, loss: 1775.026367\n",
      "Train: step:   4350, time: 0.277, loss: 1541.820068\n",
      "Train: step:   4360, time: 0.256, loss: 3608.287598\n",
      "Train: step:   4370, time: 0.248, loss: 1045.635254\n",
      "Train: step:   4380, time: 0.205, loss: 2121.161377\n",
      "Train: step:   4390, time: 0.203, loss: 555.937439\n",
      "Train: step:   4400, time: 0.263, loss: 1480.192871\n",
      "Train: step:   4410, time: 0.210, loss: 1552.584839\n",
      "Train: step:   4420, time: 0.281, loss: 1507.338989\n",
      "Train: step:   4430, time: 0.250, loss: 2205.745605\n",
      "Train: step:   4440, time: 0.202, loss: 699.734192\n",
      "Train: step:   4450, time: 0.215, loss: 397.475342\n",
      "Train: step:   4460, time: 0.205, loss: 3552.692627\n",
      "Train: step:   4470, time: 0.253, loss: 1498.364258\n",
      "Train: step:   4480, time: 0.278, loss: 566.464783\n",
      "Train: step:   4490, time: 0.239, loss: 2604.486572\n",
      "Train: step:   4500, time: 0.272, loss: 1746.041138\n",
      "Train: step:   4510, time: 0.208, loss: 1620.998413\n",
      "Train: step:   4520, time: 0.256, loss: 2452.913086\n",
      "Train: step:   4530, time: 0.202, loss: 905.271484\n",
      "Train: step:   4540, time: 0.247, loss: 1484.027710\n",
      "Train: step:   4550, time: 0.206, loss: 2389.986572\n",
      "Train: step:   4560, time: 0.208, loss: 2442.783936\n",
      "Train: step:   4570, time: 0.273, loss: 2097.013672\n",
      "Train: step:   4580, time: 0.254, loss: 1028.459351\n",
      "Train: step:   4590, time: 0.212, loss: 1053.671509\n",
      "Train: step:   4600, time: 0.276, loss: 2218.581543\n",
      "Train: step:   4610, time: 0.271, loss: 2181.302979\n",
      "Train: step:   4620, time: 0.247, loss: 2000.261108\n",
      "Train: step:   4630, time: 0.214, loss: 593.941895\n",
      "Train: step:   4640, time: 0.204, loss: 2148.306396\n",
      "Train: step:   4650, time: 0.253, loss: 1873.286621\n",
      "Train: step:   4660, time: 0.252, loss: 2087.132324\n",
      "Train: step:   4670, time: 0.253, loss: 1988.492676\n",
      "Train: step:   4680, time: 0.278, loss: 1296.366211\n",
      "Train: step:   4690, time: 0.280, loss: 1588.494263\n",
      "Train: step:   4700, time: 0.259, loss: 1239.985352\n",
      "Train: step:   4710, time: 0.254, loss: 1076.143066\n",
      "Train: step:   4720, time: 0.254, loss: 1213.877441\n",
      "Train: step:   4730, time: 0.259, loss: 1911.946045\n",
      "Train: step:   4740, time: 0.275, loss: 2950.634521\n",
      "Train: step:   4750, time: 0.208, loss: 1565.206421\n",
      "Train: step:   4760, time: 0.249, loss: 1493.987915\n",
      "Train: step:   4770, time: 0.254, loss: 1595.267334\n",
      "Train: step:   4780, time: 0.278, loss: 1419.579712\n",
      "Train: step:   4790, time: 0.272, loss: 2682.192383\n",
      "Train: step:   4800, time: 0.202, loss: 3417.571045\n",
      "Train: step:   4810, time: 0.275, loss: 4582.076660\n",
      "Train: step:   4820, time: 0.262, loss: 1301.294434\n",
      "Train: step:   4830, time: 0.251, loss: 3204.252930\n",
      "Train: step:   4840, time: 0.253, loss: 2503.659424\n",
      "Train: step:   4850, time: 0.248, loss: 1343.606201\n",
      "Train: step:   4860, time: 0.251, loss: 462.158295\n",
      "Train: step:   4870, time: 0.267, loss: 942.629822\n",
      "Train: step:   4880, time: 0.285, loss: 1289.125244\n",
      "Train: step:   4890, time: 0.253, loss: 988.886536\n",
      "Train: step:   4900, time: 0.241, loss: 2752.886719\n",
      "Train: step:   4910, time: 0.250, loss: 1617.299927\n",
      "Train: step:   4920, time: 0.254, loss: 1972.391602\n",
      "Train: step:   4930, time: 0.209, loss: 1725.586670\n",
      "Train: step:   4940, time: 0.209, loss: 2014.968750\n",
      "Train: step:   4950, time: 0.271, loss: 1820.260986\n",
      "Train: step:   4960, time: 0.291, loss: 452.573273\n",
      "Train: step:   4970, time: 0.286, loss: 2075.285645\n",
      "Train: step:   4980, time: 0.248, loss: 1206.234985\n",
      "Train: step:   4990, time: 0.258, loss: 900.043945\n",
      "Train: step:   5000, time: 0.245, loss: 3513.153564\n",
      "Train: step:   5010, time: 0.274, loss: 2542.949951\n",
      "Train: step:   5020, time: 0.231, loss: 208.169235\n",
      "Train: step:   5030, time: 0.249, loss: 1936.829102\n",
      "Train: step:   5040, time: 0.256, loss: 1020.589478\n",
      "Train: step:   5050, time: 0.295, loss: 1042.633911\n",
      "Train: step:   5060, time: 0.248, loss: 918.617615\n",
      "Train: step:   5070, time: 0.292, loss: 581.332397\n",
      "Train: step:   5080, time: 0.265, loss: 800.605469\n",
      "Train: step:   5090, time: 0.255, loss: 1879.323975\n",
      "Train: step:   5100, time: 0.250, loss: 3710.943604\n",
      "Train: step:   5110, time: 0.258, loss: 2811.896973\n",
      "Train: step:   5120, time: 0.252, loss: 3166.306885\n",
      "Train: step:   5130, time: 0.251, loss: 1081.374146\n",
      "Train: step:   5140, time: 0.249, loss: 2943.552734\n",
      "Train: step:   5150, time: 0.248, loss: 1607.162109\n",
      "Train: step:   5160, time: 0.251, loss: 2160.916016\n",
      "Train: step:   5170, time: 0.264, loss: 2181.622314\n",
      "Train: step:   5180, time: 0.270, loss: 3072.608154\n",
      "Train: step:   5190, time: 0.273, loss: 3484.167480\n",
      "Train: step:   5200, time: 0.257, loss: 1889.917725\n",
      "Train: step:   5210, time: 0.261, loss: 1206.865967\n",
      "Train: step:   5220, time: 0.280, loss: 1897.546997\n",
      "Train: step:   5230, time: 0.271, loss: 1751.795776\n",
      "Train: step:   5240, time: 0.259, loss: 2958.249512\n",
      "Train: step:   5250, time: 0.252, loss: 2291.349121\n",
      "Train: step:   5260, time: 0.246, loss: 1081.135132\n",
      "Train: step:   5270, time: 0.257, loss: 2841.342041\n",
      "Train: step:   5280, time: 0.203, loss: 3079.718994\n",
      "Train: step:   5290, time: 0.251, loss: 2417.236328\n",
      "Train: step:   5300, time: 0.248, loss: 331.083130\n",
      "Train: step:   5310, time: 0.201, loss: 1779.921997\n",
      "Train: step:   5320, time: 0.257, loss: 1281.369019\n",
      "Train: step:   5330, time: 0.200, loss: 1679.827271\n",
      "Train: step:   5340, time: 0.260, loss: 380.963531\n",
      "Train: step:   5350, time: 0.307, loss: 709.231934\n",
      "Train: step:   5360, time: 0.269, loss: 2460.728760\n",
      "Train: step:   5370, time: 0.251, loss: 2760.477295\n",
      "Train: step:   5380, time: 0.279, loss: 528.366577\n",
      "Train: step:   5390, time: 0.252, loss: 1293.823608\n",
      "Train: step:   5400, time: 0.281, loss: 1591.940063\n",
      "Train: step:   5410, time: 0.254, loss: 2792.899902\n",
      "Train: step:   5420, time: 0.290, loss: 3645.228271\n",
      "Train: step:   5430, time: 0.256, loss: 1068.069336\n",
      "Train: step:   5440, time: 0.204, loss: 3318.934570\n",
      "Train: step:   5450, time: 0.205, loss: 1467.221924\n",
      "Train: step:   5460, time: 0.255, loss: 2683.523926\n",
      "Train: step:   5470, time: 0.276, loss: 1946.192993\n",
      "Train: step:   5480, time: 0.245, loss: 473.687012\n",
      "Train: step:   5490, time: 0.256, loss: 788.319763\n",
      "Train: step:   5500, time: 0.264, loss: 1788.011719\n",
      "Train: step:   5510, time: 0.281, loss: 542.761353\n",
      "Train: step:   5520, time: 0.273, loss: 208.597717\n",
      "Train: step:   5530, time: 0.217, loss: 1123.931152\n",
      "Train: step:   5540, time: 0.212, loss: 1587.728149\n",
      "Train: step:   5550, time: 0.260, loss: 3964.263428\n",
      "Train: step:   5560, time: 0.206, loss: 2369.285400\n",
      "Train: step:   5570, time: 0.211, loss: 3017.344727\n",
      "Train: step:   5580, time: 0.250, loss: 3118.662354\n",
      "Train: step:   5590, time: 0.266, loss: 1634.367798\n",
      "Train: step:   5600, time: 0.290, loss: 132.244324\n",
      "Train: step:   5610, time: 0.276, loss: 2028.192505\n",
      "Train: step:   5620, time: 0.205, loss: 2791.660400\n",
      "Train: step:   5630, time: 0.231, loss: 1488.366577\n",
      "Train: step:   5640, time: 0.311, loss: 2068.887451\n",
      "Train: step:   5650, time: 0.273, loss: 2930.106934\n",
      "Train: step:   5660, time: 0.250, loss: 1501.628906\n",
      "Train: step:   5670, time: 0.202, loss: 2590.885498\n",
      "Train: step:   5680, time: 0.202, loss: 861.886169\n",
      "Train: step:   5690, time: 0.223, loss: 3058.670654\n",
      "Train: step:   5700, time: 0.200, loss: 1046.195068\n",
      "Train: step:   5710, time: 0.202, loss: 1150.101929\n",
      "Train: step:   5720, time: 0.197, loss: 2540.709717\n",
      "Train: step:   5730, time: 0.250, loss: 173.484848\n",
      "Train: step:   5740, time: 0.205, loss: 1460.455078\n",
      "Train: step:   5750, time: 0.251, loss: 3074.370605\n",
      "Train: step:   5760, time: 0.209, loss: 1020.317688\n",
      "Train: step:   5770, time: 0.199, loss: 757.257202\n",
      "Train: step:   5780, time: 0.203, loss: 1825.418945\n",
      "Train: step:   5790, time: 0.211, loss: 1556.969727\n",
      "Train: step:   5800, time: 0.212, loss: 2238.301758\n",
      "Train: step:   5810, time: 0.251, loss: 1060.916992\n",
      "Train: step:   5820, time: 0.201, loss: 3237.615967\n",
      "Train: step:   5830, time: 0.207, loss: 1180.873535\n",
      "Train: step:   5840, time: 0.286, loss: 2746.640625\n",
      "Train: step:   5850, time: 0.276, loss: 2232.310059\n",
      "Train: step:   5860, time: 0.285, loss: 1540.022705\n",
      "Train: step:   5870, time: 0.252, loss: 1781.839844\n",
      "Train: step:   5880, time: 0.208, loss: 1851.994751\n",
      "Train: step:   5890, time: 0.279, loss: 2263.831543\n",
      "Train: step:   5900, time: 0.203, loss: 2922.959961\n",
      "Train: step:   5910, time: 0.207, loss: 996.199463\n",
      "Train: step:   5920, time: 0.202, loss: 2230.461182\n",
      "Train: step:   5930, time: 0.204, loss: 3172.605957\n",
      "Train: step:   5940, time: 0.210, loss: 1693.719849\n",
      "Train: step:   5950, time: 0.203, loss: 1336.422363\n",
      "Train: step:   5960, time: 0.287, loss: 1970.003418\n",
      "Train: step:   5970, time: 0.279, loss: 1153.341797\n",
      "Train: step:   5980, time: 0.250, loss: 1418.082642\n",
      "Train: step:   5990, time: 0.273, loss: 2409.625977\n",
      "Train: step:   6000, time: 0.277, loss: 1693.125122\n",
      "Train: step:   6010, time: 0.211, loss: 3458.457031\n",
      "Train: step:   6020, time: 0.205, loss: 2060.134277\n",
      "Train: step:   6030, time: 0.244, loss: 1428.991455\n",
      "Train: step:   6040, time: 0.256, loss: 2575.135986\n",
      "Train: step:   6050, time: 0.266, loss: 1238.127563\n",
      "Train: step:   6060, time: 0.244, loss: 1309.892700\n",
      "Train: step:   6070, time: 0.277, loss: 3053.350586\n",
      "Train: step:   6080, time: 0.239, loss: 1445.395142\n",
      "Train: step:   6090, time: 0.280, loss: 913.211975\n",
      "Train: step:   6100, time: 0.263, loss: 2604.193848\n",
      "Train: step:   6110, time: 0.265, loss: 734.168640\n",
      "Train: step:   6120, time: 0.282, loss: 1987.923340\n",
      "Train: step:   6130, time: 0.283, loss: 591.996094\n",
      "Train: step:   6140, time: 0.211, loss: 1791.921631\n",
      "Train: step:   6150, time: 0.250, loss: 4050.153076\n",
      "Train: step:   6160, time: 0.220, loss: 3079.188721\n",
      "Train: step:   6170, time: 0.202, loss: 1692.997437\n",
      "Train: step:   6180, time: 0.209, loss: 2891.652832\n",
      "Train: step:   6190, time: 0.276, loss: 1521.916260\n",
      "Train: step:   6200, time: 0.247, loss: 3515.719238\n",
      "Train: step:   6210, time: 0.271, loss: 1198.595093\n",
      "Train: step:   6220, time: 0.219, loss: 776.581787\n",
      "Train: step:   6230, time: 0.277, loss: 2929.732422\n",
      "Train: step:   6240, time: 0.284, loss: 1160.570801\n",
      "Train: step:   6250, time: 0.291, loss: 2062.674072\n",
      "Train: step:   6260, time: 0.266, loss: 2601.092773\n",
      "Train: step:   6270, time: 0.254, loss: 571.486023\n",
      "Train: step:   6280, time: 0.254, loss: 793.798157\n",
      "Train: step:   6290, time: 0.249, loss: 1460.283203\n",
      "Train: step:   6300, time: 0.203, loss: 1887.644287\n",
      "Train: step:   6310, time: 0.204, loss: 2944.309326\n",
      "Train: step:   6320, time: 0.254, loss: 1141.282959\n",
      "Train: step:   6330, time: 0.255, loss: 3058.778809\n",
      "Train: step:   6340, time: 0.255, loss: 1140.949829\n",
      "Train: step:   6350, time: 0.303, loss: 1721.424072\n",
      "Train: step:   6360, time: 0.250, loss: 3020.442383\n",
      "Train: step:   6370, time: 0.207, loss: 2870.726807\n",
      "Train: step:   6380, time: 0.209, loss: 307.646759\n",
      "Train: step:   6390, time: 0.248, loss: 2813.433105\n",
      "Train: step:   6400, time: 0.274, loss: 1549.197998\n",
      "Train: step:   6410, time: 0.294, loss: 3314.538574\n",
      "Train: step:   6420, time: 0.250, loss: 2375.751953\n",
      "Train: step:   6430, time: 0.260, loss: 2343.663818\n",
      "Train: step:   6440, time: 0.202, loss: 1383.951172\n",
      "Train: step:   6450, time: 0.210, loss: 1497.594604\n",
      "Train: step:   6460, time: 0.262, loss: 710.459290\n",
      "Train: step:   6470, time: 0.203, loss: 823.449463\n",
      "Train: step:   6480, time: 0.201, loss: 2247.889160\n",
      "Train: step:   6490, time: 0.277, loss: 1723.935059\n",
      "Train: step:   6500, time: 0.212, loss: 2753.176758\n",
      "Train: step:   6510, time: 0.280, loss: 2211.293701\n",
      "Train: step:   6520, time: 0.210, loss: 3002.788818\n",
      "Train: step:   6530, time: 0.250, loss: 719.134521\n",
      "Train: step:   6540, time: 0.247, loss: 2951.634033\n",
      "Train: step:   6550, time: 0.251, loss: 428.644318\n",
      "Train: step:   6560, time: 0.253, loss: 3000.806885\n",
      "Train: step:   6570, time: 0.250, loss: 3730.378662\n",
      "Train: step:   6580, time: 0.266, loss: 4478.452637\n",
      "Train: step:   6590, time: 0.281, loss: 2699.359375\n",
      "Train: step:   6600, time: 0.244, loss: 1836.576294\n",
      "Train: step:   6610, time: 0.274, loss: 1781.066772\n",
      "Train: step:   6620, time: 0.252, loss: 1643.719849\n",
      "Train: step:   6630, time: 0.269, loss: 2172.825439\n",
      "Train: step:   6640, time: 0.281, loss: 719.420105\n",
      "Train: step:   6650, time: 0.254, loss: 940.800415\n",
      "Train: step:   6660, time: 0.252, loss: 1310.212891\n",
      "Train: step:   6670, time: 0.203, loss: 417.115204\n",
      "Train: step:   6680, time: 0.292, loss: 924.761292\n",
      "Train: step:   6690, time: 0.250, loss: 2836.833740\n",
      "Train: step:   6700, time: 0.262, loss: 1938.061523\n",
      "Train: step:   6710, time: 0.260, loss: 1265.367798\n",
      "Train: step:   6720, time: 0.252, loss: 2315.939209\n",
      "Train: step:   6730, time: 0.253, loss: 394.051575\n",
      "Train: step:   6740, time: 0.261, loss: 1401.286011\n",
      "Train: step:   6750, time: 0.282, loss: 1430.669434\n",
      "Train: step:   6760, time: 0.247, loss: 2092.583740\n",
      "Train: step:   6770, time: 0.203, loss: 258.212524\n",
      "Train: step:   6780, time: 0.214, loss: 900.168945\n",
      "Train: step:   6790, time: 0.201, loss: 989.843811\n",
      "Train: step:   6800, time: 0.208, loss: 1133.859009\n",
      "Train: step:   6810, time: 0.246, loss: 2264.615234\n",
      "Train: step:   6820, time: 0.204, loss: 1661.552246\n",
      "Train: step:   6830, time: 0.246, loss: 3743.632812\n",
      "Train: step:   6840, time: 0.220, loss: 714.840210\n",
      "Train: step:   6850, time: 0.200, loss: 1937.066406\n",
      "Train: step:   6860, time: 0.256, loss: 3504.696533\n",
      "Train: step:   6870, time: 0.262, loss: 1828.892334\n",
      "Train: step:   6880, time: 0.248, loss: 2637.602783\n",
      "Train: step:   6890, time: 0.258, loss: 3628.752197\n",
      "Train: step:   6900, time: 0.203, loss: 749.700806\n",
      "Train: step:   6910, time: 0.205, loss: 2067.994141\n",
      "Train: step:   6920, time: 0.205, loss: 1604.539062\n",
      "Train: step:   6930, time: 0.250, loss: 1828.867554\n",
      "Train: step:   6940, time: 0.287, loss: 1623.428711\n",
      "Train: step:   6950, time: 0.253, loss: 2337.593750\n",
      "Train: step:   6960, time: 0.206, loss: 1092.591309\n",
      "Train: step:   6970, time: 0.205, loss: 2653.619629\n",
      "Train: step:   6980, time: 0.204, loss: 2884.017334\n",
      "Train: step:   6990, time: 0.207, loss: 1911.033325\n",
      "Train: step:   7000, time: 0.285, loss: 336.105713\n",
      "Train: step:   7010, time: 0.291, loss: 1934.311768\n",
      "Train: step:   7020, time: 0.211, loss: 2976.373779\n",
      "Train: step:   7030, time: 0.207, loss: 2070.808594\n",
      "Train: step:   7040, time: 0.210, loss: 3444.736084\n",
      "Train: step:   7050, time: 0.252, loss: 2344.441895\n",
      "Train: step:   7060, time: 0.213, loss: 459.934235\n",
      "Train: step:   7070, time: 0.289, loss: 1317.264404\n",
      "Train: step:   7080, time: 0.205, loss: 2076.080078\n",
      "Train: step:   7090, time: 0.251, loss: 2195.284668\n",
      "Train: step:   7100, time: 0.254, loss: 1939.086670\n",
      "Train: step:   7110, time: 0.285, loss: 2572.533447\n",
      "Train: step:   7120, time: 0.280, loss: 1612.004150\n",
      "Train: step:   7130, time: 0.254, loss: 977.518555\n",
      "Train: step:   7140, time: 0.308, loss: 1609.504150\n",
      "Train: step:   7150, time: 0.265, loss: 600.648987\n",
      "Train: step:   7160, time: 0.203, loss: 2115.977539\n",
      "Train: step:   7170, time: 0.265, loss: 810.110168\n",
      "Train: step:   7180, time: 0.249, loss: 1330.693848\n",
      "Train: step:   7190, time: 0.255, loss: 1781.495483\n",
      "Train: step:   7200, time: 0.211, loss: 1488.454712\n",
      "Train: step:   7210, time: 0.247, loss: 706.607117\n",
      "Train: step:   7220, time: 0.263, loss: 2569.601074\n",
      "Train: step:   7230, time: 0.277, loss: 1449.147583\n",
      "Train: step:   7240, time: 0.261, loss: 2183.811035\n",
      "Train: step:   7250, time: 0.253, loss: 1031.776733\n",
      "Train: step:   7260, time: 0.306, loss: 1551.139648\n",
      "Train: step:   7270, time: 0.296, loss: 1303.059692\n",
      "Train: step:   7280, time: 0.284, loss: 534.405457\n",
      "Train: step:   7290, time: 0.202, loss: 282.385071\n",
      "Train: step:   7300, time: 0.277, loss: 4107.744141\n",
      "Train: step:   7310, time: 0.208, loss: 3174.401611\n",
      "Train: step:   7320, time: 0.202, loss: 676.488708\n",
      "Train: step:   7330, time: 0.253, loss: 1262.797974\n",
      "Train: step:   7340, time: 0.234, loss: 1868.643188\n",
      "Train: step:   7350, time: 0.220, loss: 1116.256470\n",
      "Train: step:   7360, time: 0.205, loss: 1611.088257\n",
      "Train: step:   7370, time: 0.254, loss: 833.048401\n",
      "Train: step:   7380, time: 0.206, loss: 746.837708\n",
      "Train: step:   7390, time: 0.289, loss: 590.967468\n",
      "Train: step:   7400, time: 0.204, loss: 3521.227783\n",
      "Train: step:   7410, time: 0.207, loss: 1798.015991\n",
      "Train: step:   7420, time: 0.255, loss: 2759.460938\n",
      "Train: step:   7430, time: 0.247, loss: 739.729675\n",
      "Train: step:   7440, time: 0.210, loss: 1170.655029\n",
      "Train: step:   7450, time: 0.222, loss: 1213.984985\n",
      "Train: step:   7460, time: 0.203, loss: 1488.875244\n",
      "Train: step:   7470, time: 0.275, loss: 1133.026489\n",
      "Train: step:   7480, time: 0.215, loss: 2534.252686\n",
      "Train: step:   7490, time: 0.248, loss: 1321.457520\n",
      "Train: step:   7500, time: 0.211, loss: 1152.573975\n",
      "Train: step:   7510, time: 0.253, loss: 2583.223389\n",
      "Train: step:   7520, time: 0.263, loss: 2507.077637\n",
      "Train: step:   7530, time: 0.250, loss: 2185.867432\n",
      "Train: step:   7540, time: 0.274, loss: 2431.583252\n",
      "Train: step:   7550, time: 0.287, loss: 1516.800781\n",
      "Train: step:   7560, time: 0.265, loss: 1474.635132\n",
      "Train: step:   7570, time: 0.274, loss: 2231.285156\n",
      "Train: step:   7580, time: 0.253, loss: 998.435852\n",
      "Train: step:   7590, time: 0.249, loss: 1832.161499\n",
      "Train: step:   7600, time: 0.262, loss: 1939.980347\n",
      "Train: step:   7610, time: 0.288, loss: 2657.819336\n",
      "Train: step:   7620, time: 0.311, loss: 1074.015747\n",
      "Train: step:   7630, time: 0.254, loss: 1083.680542\n",
      "Train: step:   7640, time: 0.250, loss: 4453.483398\n",
      "Train: step:   7650, time: 0.265, loss: 1089.356445\n",
      "Train: step:   7660, time: 0.256, loss: 1244.635742\n",
      "Train: step:   7670, time: 0.255, loss: 3092.286133\n",
      "Train: step:   7680, time: 0.252, loss: 300.791992\n",
      "Train: step:   7690, time: 0.262, loss: 976.834900\n",
      "Train: step:   7700, time: 0.208, loss: 3473.833496\n",
      "Train: step:   7710, time: 0.264, loss: 2922.219482\n",
      "Train: step:   7720, time: 0.285, loss: 1261.963379\n",
      "Train: step:   7730, time: 0.207, loss: 1194.279053\n",
      "Train: step:   7740, time: 0.256, loss: 1662.159912\n",
      "Train: step:   7750, time: 0.237, loss: 1755.048340\n",
      "Train: step:   7760, time: 0.258, loss: 1888.440308\n",
      "Train: step:   7770, time: 0.251, loss: 1857.207153\n",
      "Train: step:   7780, time: 0.261, loss: 2040.874390\n",
      "Train: step:   7790, time: 0.314, loss: 800.870667\n",
      "Train: step:   7800, time: 0.271, loss: 1658.233154\n",
      "Train: step:   7810, time: 0.205, loss: 1502.949707\n",
      "Train: step:   7820, time: 0.285, loss: 1317.510498\n",
      "Train: step:   7830, time: 0.250, loss: 2076.191650\n",
      "Train: step:   7840, time: 0.212, loss: 2216.868652\n",
      "Train: step:   7850, time: 0.296, loss: 816.473511\n",
      "Train: step:   7860, time: 0.292, loss: 639.101135\n",
      "Train: step:   7870, time: 0.273, loss: 2438.763916\n",
      "Train: step:   7880, time: 0.266, loss: 2232.333740\n",
      "Train: step:   7890, time: 0.214, loss: 3290.149170\n",
      "Train: step:   7900, time: 0.279, loss: 1686.071899\n",
      "Train: step:   7910, time: 0.210, loss: 1257.855347\n",
      "Train: step:   7920, time: 0.289, loss: 1173.451782\n",
      "Train: step:   7930, time: 0.208, loss: 2894.795410\n",
      "Train: step:   7940, time: 0.253, loss: 346.561890\n",
      "Train: step:   7950, time: 0.273, loss: 2337.770996\n",
      "Train: step:   7960, time: 0.210, loss: 2193.931641\n",
      "Train: step:   7970, time: 0.251, loss: 1378.560181\n",
      "Train: step:   7980, time: 0.207, loss: 703.376343\n",
      "Train: step:   7990, time: 0.211, loss: 3318.448975\n",
      "Train: step:   8000, time: 0.249, loss: 736.206055\n",
      "Train: step:   8010, time: 0.291, loss: 1393.189941\n",
      "Train: step:   8020, time: 0.290, loss: 1069.987549\n",
      "Train: step:   8030, time: 0.219, loss: 384.789001\n",
      "Train: step:   8040, time: 0.281, loss: 883.835144\n",
      "Train: step:   8050, time: 0.212, loss: 1919.797607\n",
      "Train: step:   8060, time: 0.208, loss: 1629.463623\n",
      "Train: step:   8070, time: 0.276, loss: 1905.668701\n",
      "Train: step:   8080, time: 0.250, loss: 1910.631714\n",
      "Train: step:   8090, time: 0.205, loss: 1177.645752\n",
      "Train: step:   8100, time: 0.210, loss: 2027.538818\n",
      "Train: step:   8110, time: 0.204, loss: 3074.296631\n",
      "Train: step:   8120, time: 0.211, loss: 2401.404541\n",
      "Train: step:   8130, time: 0.252, loss: 1562.879028\n",
      "Train: step:   8140, time: 0.254, loss: 1125.015137\n",
      "Train: step:   8150, time: 0.274, loss: 859.278015\n",
      "Train: step:   8160, time: 0.246, loss: 1664.467285\n",
      "Train: step:   8170, time: 0.264, loss: 889.953369\n",
      "Train: step:   8180, time: 0.254, loss: 2393.618896\n",
      "Train: step:   8190, time: 0.289, loss: 607.606934\n",
      "Train: step:   8200, time: 0.205, loss: 1085.164429\n",
      "Train: step:   8210, time: 0.215, loss: 1263.726318\n",
      "Train: step:   8220, time: 0.257, loss: 1549.758057\n",
      "Train: step:   8230, time: 0.212, loss: 1140.296997\n",
      "Train: step:   8240, time: 0.259, loss: 1288.456787\n",
      "Train: step:   8250, time: 0.276, loss: 282.678314\n",
      "Train: step:   8260, time: 0.213, loss: 1458.196411\n",
      "Train: step:   8270, time: 0.207, loss: 4041.931396\n",
      "Train: step:   8280, time: 0.214, loss: 3760.381348\n",
      "Train: step:   8290, time: 0.219, loss: 1445.962036\n",
      "Train: step:   8300, time: 0.205, loss: 1559.706177\n",
      "Train: step:   8310, time: 0.261, loss: 1105.308228\n",
      "Train: step:   8320, time: 0.205, loss: 1598.924561\n",
      "Train: step:   8330, time: 0.250, loss: 1564.171875\n",
      "Train: step:   8340, time: 0.264, loss: 396.088165\n",
      "Train: step:   8350, time: 0.208, loss: 1247.090454\n",
      "Train: step:   8360, time: 0.258, loss: 563.383911\n",
      "Train: step:   8370, time: 0.263, loss: 2336.921875\n",
      "Train: step:   8380, time: 0.259, loss: 4597.955078\n",
      "Train: step:   8390, time: 0.243, loss: 736.093933\n",
      "Train: step:   8400, time: 0.244, loss: 2347.804199\n",
      "Train: step:   8410, time: 0.235, loss: 825.344177\n",
      "Train: step:   8420, time: 0.254, loss: 939.210266\n",
      "Train: step:   8430, time: 0.266, loss: 1132.416382\n",
      "Train: step:   8440, time: 0.279, loss: 2968.564941\n",
      "Train: step:   8450, time: 0.241, loss: 930.525879\n",
      "Train: step:   8460, time: 0.258, loss: 1675.461670\n",
      "Train: step:   8470, time: 0.253, loss: 432.143219\n",
      "Train: step:   8480, time: 0.258, loss: 2323.702637\n",
      "Train: step:   8490, time: 0.267, loss: 1805.871826\n",
      "Train: step:   8500, time: 0.238, loss: 1075.847046\n",
      "Train: step:   8510, time: 0.251, loss: 1272.313232\n",
      "Train: step:   8520, time: 0.208, loss: 820.748718\n",
      "Train: step:   8530, time: 0.210, loss: 1398.244995\n",
      "Train: step:   8540, time: 0.298, loss: 1637.640991\n",
      "Train: step:   8550, time: 0.245, loss: 3659.287842\n",
      "Train: step:   8560, time: 0.253, loss: 451.628326\n",
      "Train: step:   8570, time: 0.258, loss: 2283.518311\n",
      "Train: step:   8580, time: 0.256, loss: 2018.509277\n",
      "Train: step:   8590, time: 0.258, loss: 2817.830566\n",
      "Train: step:   8600, time: 0.209, loss: 519.582947\n",
      "Train: step:   8610, time: 0.265, loss: 2506.008789\n",
      "Train: step:   8620, time: 0.213, loss: 997.586304\n",
      "Train: step:   8630, time: 0.209, loss: 2127.739014\n",
      "Train: step:   8640, time: 0.292, loss: 1470.714844\n",
      "Train: step:   8650, time: 0.262, loss: 2132.644287\n",
      "Train: step:   8660, time: 0.252, loss: 2422.549805\n",
      "Train: step:   8670, time: 0.256, loss: 2455.595703\n",
      "Train: step:   8680, time: 0.253, loss: 2074.936279\n",
      "Train: step:   8690, time: 0.294, loss: 513.203735\n",
      "Train: step:   8700, time: 0.255, loss: 1953.943115\n",
      "Train: step:   8710, time: 0.273, loss: 1942.491455\n",
      "Train: step:   8720, time: 0.254, loss: 3107.089111\n",
      "Train: step:   8730, time: 0.266, loss: 1357.437012\n",
      "Train: step:   8740, time: 0.199, loss: 852.711548\n",
      "Train: step:   8750, time: 0.262, loss: 2273.407715\n",
      "Train: step:   8760, time: 0.278, loss: 1651.933105\n",
      "Train: step:   8770, time: 0.260, loss: 2363.261475\n",
      "Train: step:   8780, time: 0.257, loss: 568.369995\n",
      "Train: step:   8790, time: 0.293, loss: 1534.176147\n",
      "Train: step:   8800, time: 0.261, loss: 2345.145996\n",
      "Train: step:   8810, time: 0.226, loss: 958.668823\n",
      "Train: step:   8820, time: 0.257, loss: 1546.257935\n",
      "Train: step:   8830, time: 0.299, loss: 1859.437378\n",
      "Train: step:   8840, time: 0.272, loss: 2064.969482\n",
      "Train: step:   8850, time: 0.241, loss: 1934.964478\n",
      "Train: step:   8860, time: 0.253, loss: 321.350494\n",
      "Train: step:   8870, time: 0.202, loss: 2712.368652\n",
      "Train: step:   8880, time: 0.266, loss: 2576.247070\n",
      "Train: step:   8890, time: 0.265, loss: 3004.053467\n",
      "Train: step:   8900, time: 0.273, loss: 540.819763\n",
      "Train: step:   8910, time: 0.247, loss: 2879.754883\n",
      "Train: step:   8920, time: 0.206, loss: 1519.170044\n",
      "Train: step:   8930, time: 0.218, loss: 4352.517090\n",
      "Train: step:   8940, time: 0.291, loss: 1290.048828\n",
      "Train: step:   8950, time: 0.281, loss: 533.413757\n",
      "Train: step:   8960, time: 0.260, loss: 1963.318115\n",
      "Train: step:   8970, time: 0.213, loss: 1218.797607\n",
      "Train: step:   8980, time: 0.243, loss: 455.842163\n",
      "Train: step:   8990, time: 0.268, loss: 2477.162598\n",
      "Train: step:   9000, time: 0.272, loss: 2151.301758\n",
      "Train: step:   9010, time: 0.285, loss: 3544.451172\n",
      "Train: step:   9020, time: 0.273, loss: 2032.010010\n",
      "Train: step:   9030, time: 0.260, loss: 1353.668335\n",
      "Train: step:   9040, time: 0.268, loss: 1245.841064\n",
      "Train: step:   9050, time: 0.235, loss: 1769.812012\n",
      "Train: step:   9060, time: 0.255, loss: 2318.325684\n",
      "Train: step:   9070, time: 0.254, loss: 2344.181396\n",
      "Train: step:   9080, time: 0.279, loss: 2255.493896\n",
      "Train: step:   9090, time: 0.287, loss: 1618.591919\n",
      "Train: step:   9100, time: 0.219, loss: 1424.648193\n",
      "Train: step:   9110, time: 0.211, loss: 572.760437\n",
      "Train: step:   9120, time: 0.250, loss: 2202.812500\n",
      "Train: step:   9130, time: 0.243, loss: 3649.072510\n",
      "Train: step:   9140, time: 0.266, loss: 2774.335938\n",
      "Train: step:   9150, time: 0.279, loss: 1506.419556\n",
      "Train: step:   9160, time: 0.297, loss: 2703.562744\n",
      "Train: step:   9170, time: 0.244, loss: 1738.438232\n",
      "Train: step:   9180, time: 0.286, loss: 409.436127\n",
      "Train: step:   9190, time: 0.306, loss: 1615.862183\n",
      "Train: step:   9200, time: 0.203, loss: 3952.924072\n",
      "Train: step:   9210, time: 0.248, loss: 748.424011\n",
      "Train: step:   9220, time: 0.242, loss: 1613.854614\n",
      "Train: step:   9230, time: 0.286, loss: 1384.722168\n",
      "Train: step:   9240, time: 0.209, loss: 2680.395020\n",
      "Train: step:   9250, time: 0.285, loss: 2881.781494\n",
      "Train: step:   9260, time: 0.325, loss: 3907.057373\n",
      "Train: step:   9270, time: 0.211, loss: 2276.174072\n",
      "Train: step:   9280, time: 0.261, loss: 1432.623901\n",
      "Train: step:   9290, time: 0.246, loss: 1959.145142\n",
      "Train: step:   9300, time: 0.252, loss: 1410.386963\n",
      "Train: step:   9310, time: 0.280, loss: 1009.235229\n",
      "Train: step:   9320, time: 0.234, loss: 2531.207031\n",
      "Train: step:   9330, time: 0.293, loss: 2328.407959\n",
      "Train: step:   9340, time: 0.250, loss: 2228.521729\n",
      "Train: step:   9350, time: 0.250, loss: 1962.469727\n",
      "Train: step:   9360, time: 0.282, loss: 621.628357\n",
      "Train: step:   9370, time: 0.213, loss: 2281.185303\n",
      "Train: step:   9380, time: 0.252, loss: 2500.492432\n",
      "Train: step:   9390, time: 0.258, loss: 2446.561279\n",
      "Train: step:   9400, time: 0.280, loss: 943.119507\n",
      "Train: step:   9410, time: 0.279, loss: 2616.806641\n",
      "Train: step:   9420, time: 0.255, loss: 1470.574829\n",
      "Train: step:   9430, time: 0.204, loss: 1199.971680\n",
      "Train: step:   9440, time: 0.291, loss: 751.967651\n",
      "Train: step:   9450, time: 0.205, loss: 1296.213867\n",
      "Train: step:   9460, time: 0.263, loss: 1003.526123\n",
      "Train: step:   9470, time: 0.210, loss: 2354.139160\n",
      "Train: step:   9480, time: 0.208, loss: 3998.441406\n",
      "Train: step:   9490, time: 0.254, loss: 343.627289\n",
      "Train: step:   9500, time: 0.213, loss: 2224.459473\n",
      "Train: step:   9510, time: 0.210, loss: 2084.941895\n",
      "Train: step:   9520, time: 0.253, loss: 2391.161865\n",
      "Train: step:   9530, time: 0.291, loss: 1908.581665\n",
      "Train: step:   9540, time: 0.204, loss: 847.041016\n",
      "Train: step:   9550, time: 0.202, loss: 2198.999023\n",
      "Train: step:   9560, time: 0.203, loss: 2633.120850\n",
      "Train: step:   9570, time: 0.201, loss: 1938.479858\n",
      "Train: step:   9580, time: 0.287, loss: 2297.779785\n",
      "Train: step:   9590, time: 0.277, loss: 2584.687988\n",
      "Train: step:   9600, time: 0.264, loss: 547.985413\n",
      "Train: step:   9610, time: 0.301, loss: 926.339417\n",
      "Train: step:   9620, time: 0.249, loss: 939.914246\n",
      "Train: step:   9630, time: 0.282, loss: 2797.822754\n",
      "Train: step:   9640, time: 0.297, loss: 2535.565674\n",
      "Train: step:   9650, time: 0.204, loss: 1348.334106\n",
      "Train: step:   9660, time: 0.271, loss: 1176.663086\n",
      "Train: step:   9670, time: 0.220, loss: 2268.083984\n",
      "Train: step:   9680, time: 0.209, loss: 621.366089\n",
      "Train: step:   9690, time: 0.208, loss: 1064.689087\n",
      "Train: step:   9700, time: 0.289, loss: 2624.983643\n",
      "Train: step:   9710, time: 0.240, loss: 797.989807\n",
      "Train: step:   9720, time: 0.254, loss: 1982.485718\n",
      "Train: step:   9730, time: 0.258, loss: 1627.868774\n",
      "Train: step:   9740, time: 0.253, loss: 1995.020264\n",
      "Train: step:   9750, time: 0.263, loss: 3300.886719\n",
      "Train: step:   9760, time: 0.213, loss: 1881.160645\n",
      "Train: step:   9770, time: 0.253, loss: 1787.348755\n",
      "Train: step:   9780, time: 0.251, loss: 2906.718262\n",
      "Train: step:   9790, time: 0.261, loss: 1539.378296\n",
      "Train: step:   9800, time: 0.283, loss: 3908.101562\n",
      "Train: step:   9810, time: 0.280, loss: 2842.300781\n",
      "Train: step:   9820, time: 0.275, loss: 1064.105591\n",
      "Train: step:   9830, time: 0.254, loss: 925.981262\n",
      "Train: step:   9840, time: 0.206, loss: 1932.678467\n",
      "Train: step:   9850, time: 0.307, loss: 1058.436279\n",
      "Train: step:   9860, time: 0.243, loss: 1398.394897\n",
      "Train: step:   9870, time: 0.242, loss: 536.747131\n",
      "Train: step:   9880, time: 0.202, loss: 2735.037354\n",
      "Train: step:   9890, time: 0.204, loss: 1326.436523\n",
      "Train: step:   9900, time: 0.206, loss: 1011.262634\n",
      "Train: step:   9910, time: 0.208, loss: 1930.082153\n",
      "Train: step:   9920, time: 0.204, loss: 4543.474121\n",
      "Train: step:   9930, time: 0.278, loss: 419.226990\n",
      "Train: step:   9940, time: 0.211, loss: 1894.163696\n",
      "Train: step:   9950, time: 0.262, loss: 1006.650330\n",
      "Train: step:   9960, time: 0.266, loss: 1562.896484\n",
      "Train: step:   9970, time: 0.295, loss: 3481.618164\n",
      "Train: step:   9980, time: 0.282, loss: 202.607361\n",
      "Train: step:   9990, time: 0.288, loss: 2372.174072\n",
      "Train: step:  10000, time: 0.249, loss: 2769.710938\n",
      "Train: step:  10010, time: 0.248, loss: 1372.876099\n",
      "Train: step:  10020, time: 0.253, loss: 1447.770874\n",
      "Train: step:  10030, time: 0.264, loss: 2148.105225\n",
      "Train: step:  10040, time: 0.297, loss: 2710.278809\n",
      "Train: step:  10050, time: 0.290, loss: 2538.299316\n",
      "Train: step:  10060, time: 0.256, loss: 3086.305908\n",
      "Train: step:  10070, time: 0.207, loss: 703.686523\n",
      "Train: step:  10080, time: 0.217, loss: 870.650574\n",
      "Train: step:  10090, time: 0.201, loss: 1722.547607\n",
      "Train: step:  10100, time: 0.265, loss: 2482.897217\n",
      "Train: step:  10110, time: 0.296, loss: 805.063477\n",
      "Train: step:  10120, time: 0.271, loss: 1115.901855\n",
      "Train: step:  10130, time: 0.253, loss: 2687.281006\n",
      "Train: step:  10140, time: 0.253, loss: 2233.245117\n",
      "Train: step:  10150, time: 0.207, loss: 540.366699\n",
      "Train: step:  10160, time: 0.266, loss: 587.341675\n",
      "Train: step:  10170, time: 0.212, loss: 1199.725342\n",
      "Train: step:  10180, time: 0.207, loss: 583.321411\n",
      "Train: step:  10190, time: 0.212, loss: 2534.524658\n",
      "Train: step:  10200, time: 0.255, loss: 2813.103516\n",
      "Train: step:  10210, time: 0.210, loss: 3323.289307\n",
      "Train: step:  10220, time: 0.250, loss: 3114.204834\n",
      "Train: step:  10230, time: 0.254, loss: 2714.411133\n",
      "Train: step:  10240, time: 0.252, loss: 928.008301\n",
      "Train: step:  10250, time: 0.203, loss: 723.692566\n",
      "Train: step:  10260, time: 0.213, loss: 1862.193604\n",
      "Train: step:  10270, time: 0.286, loss: 2606.429688\n",
      "Train: step:  10280, time: 0.255, loss: 3605.197510\n",
      "Train: step:  10290, time: 0.265, loss: 2349.678955\n",
      "Train: step:  10300, time: 0.209, loss: 539.538086\n",
      "Train: step:  10310, time: 0.255, loss: 1599.722778\n",
      "Train: step:  10320, time: 0.212, loss: 1102.202515\n",
      "Train: step:  10330, time: 0.256, loss: 532.769592\n",
      "Train: step:  10340, time: 0.261, loss: 1237.777954\n",
      "Train: step:  10350, time: 0.256, loss: 550.316284\n",
      "Train: step:  10360, time: 0.289, loss: 1880.123413\n",
      "Train: step:  10370, time: 0.211, loss: 2802.119629\n",
      "Train: step:  10380, time: 0.285, loss: 1356.874146\n",
      "Train: step:  10390, time: 0.253, loss: 2331.745117\n",
      "Train: step:  10400, time: 0.269, loss: 234.221176\n",
      "Train: step:  10410, time: 0.202, loss: 1928.080933\n",
      "Train: step:  10420, time: 0.267, loss: 1956.420654\n",
      "Train: step:  10430, time: 0.208, loss: 1848.358521\n",
      "Train: step:  10440, time: 0.283, loss: 1361.708862\n",
      "Train: step:  10450, time: 0.208, loss: 1442.844849\n",
      "Train: step:  10460, time: 0.263, loss: 1381.601196\n",
      "Train: step:  10470, time: 0.203, loss: 1180.910889\n",
      "Train: step:  10480, time: 0.208, loss: 1384.195435\n",
      "Train: step:  10490, time: 0.209, loss: 1969.059692\n",
      "Train: step:  10500, time: 0.213, loss: 1798.562744\n",
      "Train: step:  10510, time: 0.244, loss: 2121.407959\n",
      "Train: step:  10520, time: 0.207, loss: 3723.756348\n",
      "Train: step:  10530, time: 0.276, loss: 2917.146240\n",
      "Train: step:  10540, time: 0.254, loss: 832.414001\n",
      "Train: step:  10550, time: 0.234, loss: 1534.823853\n",
      "Train: step:  10560, time: 0.245, loss: 2388.736328\n",
      "Train: step:  10570, time: 0.404, loss: 1315.020630\n",
      "Train: step:  10580, time: 0.247, loss: 1804.648926\n",
      "Train: step:  10590, time: 0.275, loss: 1610.277344\n",
      "Train: step:  10600, time: 0.207, loss: 2614.924072\n",
      "Train: step:  10610, time: 0.435, loss: 600.212646\n",
      "Train: step:  10620, time: 0.253, loss: 2413.255615\n",
      "Train: step:  10630, time: 0.210, loss: 2051.259033\n",
      "Train: step:  10640, time: 0.272, loss: 1975.956665\n",
      "Train: step:  10650, time: 0.255, loss: 4089.375244\n",
      "Train: step:  10660, time: 0.268, loss: 555.978149\n",
      "Train: step:  10670, time: 0.348, loss: 2566.829590\n",
      "Train: step:  10680, time: 0.264, loss: 395.696716\n",
      "Train: step:  10690, time: 0.278, loss: 2762.500244\n",
      "Train: step:  10700, time: 0.288, loss: 2467.868896\n",
      "Train: step:  10710, time: 0.214, loss: 1023.112122\n",
      "Train: step:  10720, time: 0.254, loss: 3290.819824\n",
      "Train: step:  10730, time: 0.256, loss: 1332.116211\n",
      "Train: step:  10740, time: 0.211, loss: 466.441711\n",
      "Train: step:  10750, time: 0.258, loss: 1119.635132\n",
      "Train: step:  10760, time: 0.215, loss: 442.179352\n",
      "Train: step:  10770, time: 0.299, loss: 2326.892578\n",
      "Train: step:  10780, time: 0.259, loss: 2458.252197\n",
      "Train: step:  10790, time: 0.251, loss: 3795.934570\n",
      "Train: step:  10800, time: 0.259, loss: 1800.110596\n",
      "Train: step:  10810, time: 0.252, loss: 4481.409668\n",
      "Train: step:  10820, time: 0.257, loss: 2776.825195\n",
      "Train: step:  10830, time: 0.269, loss: 2771.123779\n",
      "Train: step:  10840, time: 0.253, loss: 2540.592285\n",
      "Train: step:  10850, time: 0.246, loss: 2216.312256\n",
      "Train: step:  10860, time: 0.237, loss: 2359.897949\n",
      "Train: step:  10870, time: 0.221, loss: 2574.490723\n",
      "Train: step:  10880, time: 0.266, loss: 1910.746948\n",
      "Train: step:  10890, time: 0.268, loss: 1523.258423\n",
      "Train: step:  10900, time: 0.282, loss: 2834.485596\n",
      "Train: step:  10910, time: 0.245, loss: 3067.811768\n",
      "Train: step:  10920, time: 0.290, loss: 3695.327393\n",
      "Train: step:  10930, time: 0.262, loss: 569.307251\n",
      "Train: step:  10940, time: 0.245, loss: 1300.068115\n",
      "Train: step:  10950, time: 0.367, loss: 2703.532715\n",
      "Train: step:  10960, time: 0.251, loss: 970.056702\n",
      "Train: step:  10970, time: 0.206, loss: 2380.555908\n",
      "Train: step:  10980, time: 0.275, loss: 2170.337402\n",
      "Train: step:  10990, time: 0.252, loss: 3855.603271\n",
      "Train: step:  11000, time: 0.275, loss: 510.392944\n",
      "Train: step:  11010, time: 0.259, loss: 1688.311401\n",
      "Train: step:  11020, time: 0.234, loss: 2512.530518\n",
      "Train: step:  11030, time: 0.246, loss: 798.655823\n",
      "Train: step:  11040, time: 0.275, loss: 1629.684326\n",
      "Train: step:  11050, time: 0.236, loss: 558.608887\n",
      "Train: step:  11060, time: 0.254, loss: 902.042725\n",
      "Train: step:  11070, time: 0.237, loss: 2881.627930\n",
      "Train: step:  11080, time: 0.208, loss: 991.619202\n",
      "Train: step:  11090, time: 0.278, loss: 3092.132324\n",
      "Train: step:  11100, time: 0.279, loss: 2536.373535\n",
      "Train: step:  11110, time: 0.256, loss: 2504.541748\n",
      "Train: step:  11120, time: 0.274, loss: 2769.616699\n",
      "Train: step:  11130, time: 0.276, loss: 3512.210449\n",
      "Train: step:  11140, time: 0.251, loss: 2326.765137\n",
      "Train: step:  11150, time: 0.266, loss: 714.501831\n",
      "Train: step:  11160, time: 0.286, loss: 2447.250732\n",
      "Train: step:  11170, time: 0.268, loss: 1324.608887\n",
      "Train: step:  11180, time: 0.264, loss: 2535.733643\n",
      "Train: step:  11190, time: 0.258, loss: 2261.857178\n",
      "Train: step:  11200, time: 0.300, loss: 253.804001\n",
      "Train: step:  11210, time: 0.257, loss: 1077.116211\n",
      "Train: step:  11220, time: 0.256, loss: 618.445374\n",
      "Train: step:  11230, time: 0.255, loss: 673.896729\n",
      "Train: step:  11240, time: 0.257, loss: 1795.472412\n",
      "Train: step:  11250, time: 0.258, loss: 1779.260986\n",
      "Train: step:  11260, time: 0.261, loss: 2198.988770\n",
      "Train: step:  11270, time: 0.248, loss: 1690.410522\n",
      "Train: step:  11280, time: 0.253, loss: 3168.925049\n",
      "Train: step:  11290, time: 0.231, loss: 1979.971313\n",
      "Train: step:  11300, time: 0.237, loss: 452.870087\n",
      "Train: step:  11310, time: 0.203, loss: 1295.479370\n",
      "Train: step:  11320, time: 0.253, loss: 627.707458\n",
      "Train: step:  11330, time: 0.269, loss: 2108.487305\n",
      "Train: step:  11340, time: 0.280, loss: 1057.546631\n",
      "Train: step:  11350, time: 0.274, loss: 1718.659302\n",
      "Train: step:  11360, time: 0.275, loss: 2099.656006\n",
      "Train: step:  11370, time: 0.248, loss: 2292.311035\n",
      "Train: step:  11380, time: 0.267, loss: 633.235046\n",
      "Train: step:  11390, time: 0.254, loss: 738.814026\n",
      "Train: step:  11400, time: 0.205, loss: 994.810852\n",
      "Train: step:  11410, time: 0.251, loss: 1735.693970\n",
      "Train: step:  11420, time: 0.274, loss: 2444.726807\n",
      "Train: step:  11430, time: 0.242, loss: 366.492096\n",
      "Train: step:  11440, time: 0.202, loss: 2382.792725\n",
      "Train: step:  11450, time: 0.322, loss: 1772.442017\n",
      "Train: step:  11460, time: 0.271, loss: 2593.534668\n",
      "Train: step:  11470, time: 0.205, loss: 733.621338\n",
      "Train: step:  11480, time: 0.271, loss: 1463.239502\n",
      "Train: step:  11490, time: 0.248, loss: 2873.284424\n",
      "Train: step:  11500, time: 0.241, loss: 2421.004395\n",
      "Train: step:  11510, time: 0.273, loss: 1550.329224\n",
      "Train: step:  11520, time: 0.264, loss: 1696.969238\n",
      "Train: step:  11530, time: 0.256, loss: 643.962769\n",
      "Train: step:  11540, time: 0.270, loss: 2219.585938\n",
      "Train: step:  11550, time: 0.285, loss: 1713.084106\n",
      "Train: step:  11560, time: 0.205, loss: 1556.460815\n",
      "Train: step:  11570, time: 0.250, loss: 932.234009\n",
      "Train: step:  11580, time: 0.210, loss: 1506.716675\n",
      "Train: step:  11590, time: 0.201, loss: 1190.700317\n",
      "Train: step:  11600, time: 0.262, loss: 1835.615601\n",
      "Train: step:  11610, time: 0.257, loss: 1444.326294\n",
      "Train: step:  11620, time: 0.257, loss: 2886.863770\n",
      "Train: step:  11630, time: 0.212, loss: 1598.968262\n",
      "Train: step:  11640, time: 0.251, loss: 2030.549194\n",
      "Train: step:  11650, time: 0.294, loss: 2660.639893\n",
      "Train: step:  11660, time: 0.276, loss: 3155.831787\n",
      "Train: step:  11670, time: 0.284, loss: 1012.303345\n",
      "Train: step:  11680, time: 0.244, loss: 996.168396\n",
      "Train: step:  11690, time: 0.278, loss: 1824.916504\n",
      "Train: step:  11700, time: 0.274, loss: 2965.591064\n",
      "Train: step:  11710, time: 0.298, loss: 2395.843506\n",
      "Train: step:  11720, time: 0.203, loss: 1669.781494\n",
      "Train: step:  11730, time: 0.261, loss: 2761.148682\n",
      "Train: step:  11740, time: 0.235, loss: 1125.114868\n",
      "Train: step:  11750, time: 0.200, loss: 1459.129883\n",
      "Train: step:  11760, time: 0.277, loss: 1644.003052\n",
      "Train: step:  11770, time: 0.244, loss: 1014.638489\n",
      "Train: step:  11780, time: 0.252, loss: 920.450806\n",
      "Train: step:  11790, time: 0.200, loss: 1199.786377\n",
      "Train: step:  11800, time: 0.273, loss: 3800.571289\n",
      "Train: step:  11810, time: 0.259, loss: 2516.486084\n",
      "Train: step:  11820, time: 0.278, loss: 1453.959595\n",
      "Train: step:  11830, time: 0.255, loss: 2895.916748\n",
      "Train: step:  11840, time: 0.277, loss: 538.763611\n",
      "Train: step:  11850, time: 0.207, loss: 1562.083618\n",
      "Train: step:  11860, time: 0.288, loss: 1829.749268\n",
      "Train: step:  11870, time: 0.290, loss: 2731.361328\n",
      "Train: step:  11880, time: 0.275, loss: 2080.268066\n",
      "Train: step:  11890, time: 0.291, loss: 4241.387207\n",
      "Train: step:  11900, time: 0.230, loss: 2269.558105\n",
      "Train: step:  11910, time: 0.278, loss: 1876.245361\n",
      "Train: step:  11920, time: 0.219, loss: 3395.482422\n",
      "Train: step:  11930, time: 0.279, loss: 1343.956421\n",
      "Train: step:  11940, time: 0.279, loss: 400.134613\n",
      "Train: step:  11950, time: 0.257, loss: 515.083191\n",
      "Train: step:  11960, time: 0.284, loss: 1125.409058\n",
      "Train: step:  11970, time: 0.261, loss: 669.230042\n",
      "Train: step:  11980, time: 0.265, loss: 2101.845459\n",
      "Train: step:  11990, time: 0.275, loss: 2005.509155\n",
      "Train: step:  12000, time: 0.257, loss: 3206.983887\n",
      "Train: step:  12010, time: 0.244, loss: 423.992706\n",
      "Train: step:  12020, time: 0.253, loss: 1869.370728\n",
      "Train: step:  12030, time: 0.201, loss: 2556.155518\n",
      "Train: step:  12040, time: 0.251, loss: 2510.707275\n",
      "Train: step:  12050, time: 0.274, loss: 2344.009766\n",
      "Train: step:  12060, time: 0.250, loss: 1402.673218\n",
      "Train: step:  12070, time: 0.279, loss: 802.075256\n",
      "Train: step:  12080, time: 0.250, loss: 911.739014\n",
      "Train: step:  12090, time: 0.250, loss: 1746.154419\n",
      "Train: step:  12100, time: 0.247, loss: 2962.833252\n",
      "Train: step:  12110, time: 0.276, loss: 1223.216064\n",
      "Train: step:  12120, time: 0.273, loss: 579.868469\n",
      "Train: step:  12130, time: 0.250, loss: 1386.177856\n",
      "Train: step:  12140, time: 0.251, loss: 2392.646484\n",
      "Train: step:  12150, time: 0.241, loss: 2794.749268\n",
      "Train: step:  12160, time: 0.245, loss: 1273.614746\n",
      "Train: step:  12170, time: 0.216, loss: 2386.338379\n",
      "Train: step:  12180, time: 0.252, loss: 3216.700439\n",
      "Train: step:  12190, time: 0.286, loss: 1359.286133\n",
      "Train: step:  12200, time: 0.267, loss: 1037.268188\n",
      "Train: step:  12210, time: 0.207, loss: 1652.217651\n",
      "Train: step:  12220, time: 0.208, loss: 1254.070557\n",
      "Train: step:  12230, time: 0.268, loss: 1928.075684\n",
      "Train: step:  12240, time: 0.211, loss: 2925.618652\n",
      "Train: step:  12250, time: 0.206, loss: 181.095490\n",
      "Train: step:  12260, time: 0.247, loss: 2410.003906\n",
      "Train: step:  12270, time: 0.252, loss: 1198.972290\n",
      "Train: step:  12280, time: 0.251, loss: 2379.919922\n",
      "Train: step:  12290, time: 0.266, loss: 2425.870605\n",
      "Train: step:  12300, time: 0.270, loss: 3710.727539\n",
      "Train: step:  12310, time: 0.264, loss: 2334.205322\n",
      "Train: step:  12320, time: 0.261, loss: 2149.199219\n",
      "Train: step:  12330, time: 0.289, loss: 2869.801270\n",
      "Train: step:  12340, time: 0.257, loss: 1571.303589\n",
      "Train: step:  12350, time: 0.259, loss: 2416.175049\n",
      "Train: step:  12360, time: 0.279, loss: 757.909973\n",
      "Train: step:  12370, time: 0.262, loss: 1343.864258\n",
      "Train: step:  12380, time: 0.295, loss: 984.278198\n",
      "Train: step:  12390, time: 0.272, loss: 1246.949463\n",
      "Train: step:  12400, time: 0.280, loss: 2273.066406\n",
      "Train: step:  12410, time: 0.208, loss: 1750.548828\n",
      "Train: step:  12420, time: 0.256, loss: 2797.801025\n",
      "Train: step:  12430, time: 0.249, loss: 3363.664307\n",
      "Train: step:  12440, time: 0.261, loss: 1190.014526\n",
      "Train: step:  12450, time: 0.209, loss: 1923.822998\n",
      "Train: step:  12460, time: 0.264, loss: 3115.407227\n",
      "Train: step:  12470, time: 0.279, loss: 1588.444946\n",
      "Train: step:  12480, time: 0.269, loss: 1641.110962\n",
      "Train: step:  12490, time: 0.260, loss: 1789.495483\n",
      "Train: step:  12500, time: 0.248, loss: 2724.502930\n",
      "Train: step:  12510, time: 0.250, loss: 2229.127686\n",
      "Train: step:  12520, time: 0.277, loss: 1493.964844\n",
      "Train: step:  12530, time: 0.327, loss: 679.725952\n",
      "Train: step:  12540, time: 0.212, loss: 609.782349\n",
      "Train: step:  12550, time: 0.256, loss: 873.547852\n",
      "Train: step:  12560, time: 0.208, loss: 3086.262695\n",
      "Train: step:  12570, time: 0.251, loss: 2094.190430\n",
      "Train: step:  12580, time: 0.202, loss: 655.638855\n",
      "Train: step:  12590, time: 0.253, loss: 1553.551392\n",
      "Train: step:  12600, time: 0.256, loss: 994.491089\n",
      "Train: step:  12610, time: 0.264, loss: 1581.609863\n",
      "Train: step:  12620, time: 0.295, loss: 1860.367676\n",
      "Train: step:  12630, time: 0.253, loss: 2617.574707\n",
      "Train: step:  12640, time: 0.259, loss: 3419.487305\n",
      "Train: step:  12650, time: 0.245, loss: 1810.887573\n",
      "Train: step:  12660, time: 0.253, loss: 198.354004\n",
      "Train: step:  12670, time: 0.278, loss: 2433.177979\n",
      "Train: step:  12680, time: 0.211, loss: 2849.757080\n",
      "Train: step:  12690, time: 0.203, loss: 3080.838867\n",
      "Train: step:  12700, time: 0.411, loss: 3246.512939\n",
      "Train: step:  12710, time: 0.337, loss: 552.428650\n",
      "Train: step:  12720, time: 0.307, loss: 1341.098755\n",
      "Train: step:  12730, time: 0.276, loss: 1238.531738\n",
      "Train: step:  12740, time: 0.210, loss: 597.431885\n",
      "Train: step:  12750, time: 0.252, loss: 687.617920\n",
      "Train: step:  12760, time: 0.255, loss: 1622.675293\n",
      "Train: step:  12770, time: 0.248, loss: 632.310852\n",
      "Train: step:  12780, time: 0.206, loss: 2670.084473\n",
      "Train: step:  12790, time: 0.203, loss: 1664.913574\n",
      "Train: step:  12800, time: 0.251, loss: 3839.932129\n",
      "Train: step:  12810, time: 0.258, loss: 1599.783691\n",
      "Train: step:  12820, time: 0.262, loss: 1040.515747\n",
      "Train: step:  12830, time: 0.274, loss: 1977.612915\n",
      "Train: step:  12840, time: 0.250, loss: 1257.841675\n",
      "Train: step:  12850, time: 0.273, loss: 555.436646\n",
      "Train: step:  12860, time: 0.279, loss: 749.895142\n",
      "Train: step:  12870, time: 0.271, loss: 312.881958\n",
      "Train: step:  12880, time: 0.279, loss: 1491.470947\n",
      "Train: step:  12890, time: 0.271, loss: 1133.652832\n",
      "Train: step:  12900, time: 0.238, loss: 614.017029\n",
      "Train: step:  12910, time: 0.274, loss: 920.791931\n",
      "Train: step:  12920, time: 0.285, loss: 4138.750488\n",
      "Train: step:  12930, time: 0.369, loss: 1521.323853\n",
      "Train: step:  12940, time: 0.248, loss: 2043.249268\n",
      "Train: step:  12950, time: 0.254, loss: 1454.216187\n",
      "Train: step:  12960, time: 0.252, loss: 2268.747803\n",
      "Train: step:  12970, time: 0.253, loss: 281.570007\n",
      "Train: step:  12980, time: 0.407, loss: 1005.696228\n",
      "Train: step:  12990, time: 0.305, loss: 1810.608398\n",
      "Train: step:  13000, time: 0.259, loss: 1568.508057\n",
      "Train: step:  13010, time: 0.202, loss: 1119.503906\n",
      "Train: step:  13020, time: 0.248, loss: 1217.844849\n",
      "Train: step:  13030, time: 0.269, loss: 1077.066162\n",
      "Train: step:  13040, time: 0.297, loss: 2886.065186\n",
      "Train: step:  13050, time: 0.277, loss: 1834.575562\n",
      "Train: step:  13060, time: 0.276, loss: 1974.844727\n",
      "Train: step:  13070, time: 0.253, loss: 1764.022583\n",
      "Train: step:  13080, time: 0.252, loss: 2546.206055\n",
      "Train: step:  13090, time: 0.243, loss: 1334.759644\n",
      "Train: step:  13100, time: 0.295, loss: 3233.804932\n",
      "Train: step:  13110, time: 0.252, loss: 294.678192\n",
      "Train: step:  13120, time: 0.246, loss: 1499.483765\n",
      "Train: step:  13130, time: 0.256, loss: 1664.613770\n",
      "Train: step:  13140, time: 0.261, loss: 3574.244385\n",
      "Train: step:  13150, time: 0.271, loss: 1748.416626\n",
      "Train: step:  13160, time: 0.245, loss: 1415.615112\n",
      "Train: step:  13170, time: 0.252, loss: 585.562744\n",
      "Train: step:  13180, time: 0.255, loss: 1766.115967\n",
      "Train: step:  13190, time: 0.206, loss: 1316.222778\n",
      "Train: step:  13200, time: 0.274, loss: 1278.236450\n",
      "Train: step:  13210, time: 0.293, loss: 2086.160156\n",
      "Train: step:  13220, time: 0.257, loss: 1628.073730\n",
      "Train: step:  13230, time: 0.286, loss: 3441.142578\n",
      "Train: step:  13240, time: 0.266, loss: 3821.610352\n",
      "Train: step:  13250, time: 0.237, loss: 2345.430664\n",
      "Train: step:  13260, time: 0.264, loss: 2750.661133\n",
      "Train: step:  13270, time: 0.247, loss: 3277.850342\n",
      "Train: step:  13280, time: 0.277, loss: 1515.483887\n",
      "Train: step:  13290, time: 0.221, loss: 1035.512573\n",
      "Train: step:  13300, time: 0.261, loss: 2617.654785\n",
      "Train: step:  13310, time: 0.284, loss: 1866.508057\n",
      "Train: step:  13320, time: 0.207, loss: 1308.882812\n",
      "Train: step:  13330, time: 0.249, loss: 263.458374\n",
      "Train: step:  13340, time: 0.272, loss: 877.373657\n",
      "Train: step:  13350, time: 0.249, loss: 1914.310059\n",
      "Train: step:  13360, time: 0.217, loss: 2198.967285\n",
      "Train: step:  13370, time: 0.203, loss: 1039.324341\n",
      "Train: step:  13380, time: 0.253, loss: 1616.196655\n",
      "Train: step:  13390, time: 0.206, loss: 1035.543091\n",
      "Train: step:  13400, time: 0.200, loss: 1806.864502\n",
      "Train: step:  13410, time: 0.202, loss: 2935.519531\n",
      "Train: step:  13420, time: 0.284, loss: 646.201233\n",
      "Train: step:  13430, time: 0.251, loss: 3706.100342\n",
      "Train: step:  13440, time: 0.201, loss: 1744.232056\n",
      "Train: step:  13450, time: 0.200, loss: 535.926514\n",
      "Train: step:  13460, time: 0.257, loss: 1585.901245\n",
      "Train: step:  13470, time: 0.247, loss: 3250.790771\n",
      "Train: step:  13480, time: 0.250, loss: 3776.580322\n",
      "Train: step:  13490, time: 0.200, loss: 711.656006\n",
      "Train: step:  13500, time: 0.260, loss: 2699.513184\n",
      "Train: step:  13510, time: 0.245, loss: 2794.495117\n",
      "Train: step:  13520, time: 0.228, loss: 2453.478027\n",
      "Train: step:  13530, time: 0.200, loss: 2941.338379\n",
      "Train: step:  13540, time: 0.244, loss: 704.919373\n",
      "Train: step:  13550, time: 0.254, loss: 1329.889526\n",
      "Train: step:  13560, time: 0.205, loss: 374.555176\n",
      "Train: step:  13570, time: 0.210, loss: 2742.405273\n",
      "Train: step:  13580, time: 0.254, loss: 1592.533569\n",
      "Train: step:  13590, time: 0.259, loss: 1504.817627\n",
      "Train: step:  13600, time: 0.257, loss: 740.037170\n",
      "Train: step:  13610, time: 0.227, loss: 1289.200073\n",
      "Train: step:  13620, time: 0.251, loss: 2475.163818\n",
      "Train: step:  13630, time: 0.270, loss: 901.852051\n",
      "Train: step:  13640, time: 0.250, loss: 2657.566406\n",
      "Train: step:  13650, time: 0.276, loss: 2211.974609\n",
      "Train: step:  13660, time: 0.285, loss: 2614.049561\n",
      "Train: step:  13670, time: 0.242, loss: 1388.388794\n",
      "Train: step:  13680, time: 0.253, loss: 1666.086670\n",
      "Train: step:  13690, time: 0.209, loss: 2799.121094\n",
      "Train: step:  13700, time: 0.251, loss: 1332.757935\n",
      "Train: step:  13710, time: 0.201, loss: 957.703796\n",
      "Train: step:  13720, time: 0.267, loss: 1269.398560\n",
      "Train: step:  13730, time: 0.284, loss: 2933.776855\n",
      "Train: step:  13740, time: 0.253, loss: 1472.227051\n",
      "Train: step:  13750, time: 0.264, loss: 2126.000244\n",
      "Train: step:  13760, time: 0.283, loss: 1229.416870\n",
      "Train: step:  13770, time: 0.208, loss: 2509.798340\n",
      "Train: step:  13780, time: 0.207, loss: 2443.077637\n",
      "Train: step:  13790, time: 0.204, loss: 1956.813232\n",
      "Train: step:  13800, time: 0.220, loss: 2848.077393\n",
      "Train: step:  13810, time: 0.253, loss: 741.051086\n",
      "Train: step:  13820, time: 0.252, loss: 1186.003540\n",
      "Train: step:  13830, time: 0.252, loss: 2402.191406\n",
      "Train: step:  13840, time: 0.261, loss: 2788.145264\n",
      "Train: step:  13850, time: 0.204, loss: 2048.417725\n",
      "Train: step:  13860, time: 0.273, loss: 1407.340210\n",
      "Train: step:  13870, time: 0.290, loss: 2353.252441\n",
      "Train: step:  13880, time: 0.245, loss: 3197.837402\n",
      "Train: step:  13890, time: 0.254, loss: 2845.110596\n",
      "Train: step:  13900, time: 0.300, loss: 706.098083\n",
      "Train: step:  13910, time: 0.263, loss: 3079.755615\n",
      "Train: step:  13920, time: 0.279, loss: 2116.224121\n",
      "Train: step:  13930, time: 0.202, loss: 1762.702026\n",
      "Train: step:  13940, time: 0.281, loss: 4834.834961\n",
      "Train: step:  13950, time: 0.284, loss: 2818.296143\n",
      "Train: step:  13960, time: 0.266, loss: 1420.638916\n",
      "Train: step:  13970, time: 0.258, loss: 755.749207\n",
      "Train: step:  13980, time: 0.245, loss: 2575.201904\n",
      "Train: step:  13990, time: 0.249, loss: 2213.332031\n",
      "Train: step:  14000, time: 0.273, loss: 3647.658203\n",
      "Train: step:  14010, time: 0.244, loss: 2140.663818\n",
      "Train: step:  14020, time: 0.286, loss: 3462.102295\n",
      "Train: step:  14030, time: 0.251, loss: 2172.327881\n",
      "Train: step:  14040, time: 0.273, loss: 1923.441406\n",
      "Train: step:  14050, time: 0.205, loss: 1244.516235\n",
      "Train: step:  14060, time: 0.276, loss: 2507.720703\n",
      "Train: step:  14070, time: 0.292, loss: 2241.782471\n",
      "Train: step:  14080, time: 0.251, loss: 3247.483643\n",
      "Train: step:  14090, time: 0.284, loss: 671.689270\n",
      "Train: step:  14100, time: 0.298, loss: 1884.892578\n",
      "Train: step:  14110, time: 0.289, loss: 985.544006\n",
      "Train: step:  14120, time: 0.253, loss: 3489.724609\n",
      "Train: step:  14130, time: 0.281, loss: 1695.933105\n",
      "Train: step:  14140, time: 0.272, loss: 2928.703613\n",
      "Train: step:  14150, time: 0.250, loss: 2401.021484\n",
      "Train: step:  14160, time: 0.263, loss: 2176.107422\n",
      "Train: step:  14170, time: 0.279, loss: 1944.572510\n",
      "Train: step:  14180, time: 0.252, loss: 2906.511475\n",
      "Train: step:  14190, time: 0.210, loss: 366.819916\n",
      "Train: step:  14200, time: 0.266, loss: 941.045227\n",
      "Train: step:  14210, time: 0.285, loss: 2081.660400\n",
      "Train: step:  14220, time: 0.202, loss: 1903.977905\n",
      "Train: step:  14230, time: 0.201, loss: 1714.247681\n",
      "Train: step:  14240, time: 0.267, loss: 993.735779\n",
      "Train: step:  14250, time: 0.252, loss: 504.278534\n",
      "Train: step:  14260, time: 0.202, loss: 1415.159302\n",
      "Train: step:  14270, time: 0.240, loss: 530.653442\n",
      "Train: step:  14280, time: 0.216, loss: 2773.295166\n",
      "Train: step:  14290, time: 0.300, loss: 1570.862671\n",
      "Train: step:  14300, time: 0.259, loss: 1869.617432\n",
      "Train: step:  14310, time: 0.251, loss: 328.719940\n",
      "Train: step:  14320, time: 0.253, loss: 1490.310059\n",
      "Train: step:  14330, time: 0.270, loss: 924.707092\n",
      "Train: step:  14340, time: 0.255, loss: 1886.195312\n",
      "Train: step:  14350, time: 0.250, loss: 1758.592163\n",
      "Train: step:  14360, time: 0.275, loss: 1405.765259\n",
      "Train: step:  14370, time: 0.295, loss: 414.525177\n",
      "Train: step:  14380, time: 0.255, loss: 2435.180908\n",
      "Train: step:  14390, time: 0.303, loss: 1678.863403\n",
      "Train: step:  14400, time: 0.228, loss: 858.577759\n",
      "Train: step:  14410, time: 0.253, loss: 3816.481934\n",
      "Train: step:  14420, time: 0.255, loss: 1722.428345\n",
      "Train: step:  14430, time: 0.277, loss: 1927.308594\n",
      "Train: step:  14440, time: 0.278, loss: 1840.318604\n",
      "Train: step:  14450, time: 0.297, loss: 820.656311\n",
      "Train: step:  14460, time: 0.280, loss: 1508.100098\n",
      "Train: step:  14470, time: 0.259, loss: 1685.213989\n",
      "Train: step:  14480, time: 0.201, loss: 2810.340576\n",
      "Train: step:  14490, time: 0.255, loss: 2758.745850\n",
      "Train: step:  14500, time: 0.292, loss: 1130.915161\n",
      "Train: step:  14510, time: 0.208, loss: 2174.842285\n",
      "Train: step:  14520, time: 0.294, loss: 588.525391\n",
      "Train: step:  14530, time: 0.250, loss: 2638.789062\n",
      "Train: step:  14540, time: 0.211, loss: 1616.637573\n",
      "Train: step:  14550, time: 0.234, loss: 1278.072021\n",
      "Train: step:  14560, time: 0.204, loss: 1726.336914\n",
      "Train: step:  14570, time: 0.255, loss: 2614.652100\n",
      "Train: step:  14580, time: 0.277, loss: 3169.060059\n",
      "Train: step:  14590, time: 0.276, loss: 2431.974365\n",
      "Train: step:  14600, time: 0.241, loss: 1027.937500\n",
      "Train: step:  14610, time: 0.207, loss: 2768.670898\n",
      "Train: step:  14620, time: 0.239, loss: 1107.052368\n",
      "Train: step:  14630, time: 0.252, loss: 1566.027588\n",
      "Train: step:  14640, time: 0.245, loss: 2440.338623\n",
      "Train: step:  14650, time: 0.254, loss: 845.665833\n",
      "Train: step:  14660, time: 0.201, loss: 2935.055908\n",
      "Train: step:  14670, time: 0.249, loss: 160.903687\n",
      "Train: step:  14680, time: 0.263, loss: 3031.287842\n",
      "Train: step:  14690, time: 0.248, loss: 3138.264404\n",
      "Train: step:  14700, time: 0.207, loss: 2819.071045\n",
      "Train: step:  14710, time: 0.268, loss: 3289.271973\n",
      "Train: step:  14720, time: 0.310, loss: 2129.833984\n",
      "Train: step:  14730, time: 0.286, loss: 299.173828\n",
      "Train: step:  14740, time: 0.201, loss: 1934.857056\n",
      "Train: step:  14750, time: 0.252, loss: 3070.861572\n",
      "Train: step:  14760, time: 0.253, loss: 951.828186\n",
      "Train: step:  14770, time: 0.252, loss: 1410.987305\n",
      "Train: step:  14780, time: 0.266, loss: 848.845093\n",
      "Train: step:  14790, time: 0.261, loss: 877.230530\n",
      "Train: step:  14800, time: 0.286, loss: 1239.567017\n",
      "Train: step:  14810, time: 0.281, loss: 2051.976807\n",
      "Train: step:  14820, time: 0.272, loss: 302.487701\n",
      "Train: step:  14830, time: 0.281, loss: 866.230286\n",
      "Train: step:  14840, time: 0.249, loss: 2135.702637\n",
      "Train: step:  14850, time: 0.281, loss: 3480.181152\n",
      "Train: step:  14860, time: 0.255, loss: 1912.302856\n",
      "Train: step:  14870, time: 0.273, loss: 360.368805\n",
      "Train: step:  14880, time: 0.292, loss: 1850.233887\n",
      "Train: step:  14890, time: 0.274, loss: 1856.539551\n",
      "Train: step:  14900, time: 0.204, loss: 2623.743896\n",
      "Train: step:  14910, time: 0.235, loss: 1510.667847\n",
      "Train: step:  14920, time: 0.245, loss: 3931.109619\n",
      "Train: step:  14930, time: 0.261, loss: 2306.286621\n",
      "Train: step:  14940, time: 0.223, loss: 645.793457\n",
      "Train: step:  14950, time: 0.289, loss: 1290.324585\n",
      "Train: step:  14960, time: 0.215, loss: 928.937805\n",
      "Train: step:  14970, time: 0.204, loss: 614.212769\n",
      "Train: step:  14980, time: 0.211, loss: 1295.085083\n",
      "Train: step:  14990, time: 0.206, loss: 2246.849854\n",
      "Train: step:  15000, time: 0.214, loss: 2008.109253\n",
      "Train: step:  15010, time: 0.256, loss: 1308.874634\n",
      "Train: step:  15020, time: 0.251, loss: 1512.303589\n",
      "Train: step:  15030, time: 0.209, loss: 995.916748\n",
      "Train: step:  15040, time: 0.254, loss: 2675.908203\n",
      "Train: step:  15050, time: 0.238, loss: 1062.779419\n",
      "Train: step:  15060, time: 0.273, loss: 482.486938\n",
      "Train: step:  15070, time: 0.247, loss: 3524.704834\n",
      "Train: step:  15080, time: 0.203, loss: 1091.339111\n",
      "Train: step:  15090, time: 0.212, loss: 1095.115112\n",
      "Train: step:  15100, time: 0.207, loss: 1543.098389\n",
      "Train: step:  15110, time: 0.248, loss: 2393.681885\n",
      "Train: step:  15120, time: 0.211, loss: 1645.700806\n",
      "Train: step:  15130, time: 0.203, loss: 2686.741699\n",
      "Train: step:  15140, time: 0.206, loss: 1591.726440\n",
      "Train: step:  15150, time: 0.288, loss: 365.840179\n",
      "Train: step:  15160, time: 0.205, loss: 2015.288574\n",
      "Train: step:  15170, time: 0.254, loss: 3234.028320\n",
      "Train: step:  15180, time: 0.248, loss: 2294.430908\n",
      "Train: step:  15190, time: 0.204, loss: 2702.188965\n",
      "Train: step:  15200, time: 0.249, loss: 407.139862\n",
      "Train: step:  15210, time: 0.205, loss: 1001.691162\n",
      "Train: step:  15220, time: 0.271, loss: 1396.171509\n",
      "Train: step:  15230, time: 0.206, loss: 1828.326294\n",
      "Train: step:  15240, time: 0.247, loss: 2168.439209\n",
      "Train: step:  15250, time: 0.250, loss: 1271.674561\n",
      "Train: step:  15260, time: 0.198, loss: 3929.137451\n",
      "Train: step:  15270, time: 0.198, loss: 1261.266479\n",
      "Train: step:  15280, time: 0.204, loss: 2694.726562\n",
      "Train: step:  15290, time: 0.247, loss: 1223.538940\n",
      "Train: step:  15300, time: 0.252, loss: 1572.604126\n",
      "Train: step:  15310, time: 0.247, loss: 2673.334229\n",
      "Train: step:  15320, time: 0.268, loss: 489.768188\n",
      "Train: step:  15330, time: 0.247, loss: 1220.627930\n",
      "Train: step:  15340, time: 0.277, loss: 2262.621582\n",
      "Train: step:  15350, time: 0.257, loss: 1720.707886\n",
      "Train: step:  15360, time: 0.290, loss: 1947.689941\n",
      "Train: step:  15370, time: 0.273, loss: 2893.593994\n",
      "Train: step:  15380, time: 0.198, loss: 915.281860\n",
      "Train: step:  15390, time: 0.196, loss: 1173.392090\n",
      "Train: step:  15400, time: 0.198, loss: 2191.543701\n",
      "Train: step:  15410, time: 0.247, loss: 2239.510742\n",
      "Train: step:  15420, time: 0.250, loss: 3310.356689\n",
      "Train: step:  15430, time: 0.245, loss: 3002.667969\n",
      "Train: step:  15440, time: 0.198, loss: 1307.494507\n",
      "Train: step:  15450, time: 0.196, loss: 1103.430420\n",
      "Train: step:  15460, time: 0.274, loss: 1169.583618\n",
      "Train: step:  15470, time: 0.283, loss: 3030.719727\n",
      "Train: step:  15480, time: 0.281, loss: 1567.267090\n",
      "Train: step:  15490, time: 0.241, loss: 1223.964111\n",
      "Train: step:  15500, time: 0.202, loss: 1721.273926\n",
      "Train: step:  15510, time: 0.262, loss: 2106.112061\n",
      "Train: step:  15520, time: 0.271, loss: 1420.045898\n",
      "Train: step:  15530, time: 0.235, loss: 896.514404\n",
      "Train: step:  15540, time: 0.200, loss: 1817.312622\n",
      "Train: step:  15550, time: 0.265, loss: 557.487671\n",
      "Train: step:  15560, time: 0.262, loss: 2101.389648\n",
      "Train: step:  15570, time: 0.207, loss: 601.241211\n",
      "Train: step:  15580, time: 0.303, loss: 2090.314209\n",
      "Train: step:  15590, time: 0.200, loss: 3758.044678\n",
      "Train: step:  15600, time: 0.248, loss: 1921.689941\n",
      "Train: step:  15610, time: 0.277, loss: 1547.651489\n",
      "Train: step:  15620, time: 0.211, loss: 1962.953125\n",
      "Train: step:  15630, time: 0.254, loss: 1687.052368\n",
      "Train: step:  15640, time: 0.220, loss: 3437.717773\n",
      "Train: step:  15650, time: 0.273, loss: 1906.544922\n",
      "Train: step:  15660, time: 0.248, loss: 2587.788330\n",
      "Train: step:  15670, time: 0.290, loss: 3172.696533\n",
      "Train: step:  15680, time: 0.262, loss: 2641.522705\n",
      "Train: step:  15690, time: 0.284, loss: 592.960815\n",
      "Train: step:  15700, time: 0.264, loss: 2033.542847\n",
      "Train: step:  15710, time: 0.270, loss: 494.089935\n",
      "Train: step:  15720, time: 0.251, loss: 388.313751\n",
      "Train: step:  15730, time: 0.280, loss: 1389.836548\n",
      "Train: step:  15740, time: 0.254, loss: 587.855164\n",
      "Train: step:  15750, time: 0.245, loss: 479.693817\n",
      "Train: step:  15760, time: 0.252, loss: 2170.978271\n",
      "Train: step:  15770, time: 0.256, loss: 1677.028442\n",
      "Train: step:  15780, time: 0.205, loss: 4137.202637\n",
      "Train: step:  15790, time: 0.241, loss: 2814.671875\n",
      "Train: step:  15800, time: 0.276, loss: 510.316101\n",
      "Train: step:  15810, time: 0.246, loss: 1977.707275\n",
      "Train: step:  15820, time: 0.249, loss: 1887.491333\n",
      "Train: step:  15830, time: 0.198, loss: 1916.064087\n",
      "Train: step:  15840, time: 0.252, loss: 1846.448608\n",
      "Train: step:  15850, time: 0.251, loss: 1985.479004\n",
      "Train: step:  15860, time: 0.199, loss: 876.047119\n",
      "Train: step:  15870, time: 0.265, loss: 876.554993\n",
      "Train: step:  15880, time: 0.198, loss: 813.477661\n",
      "Train: step:  15890, time: 0.253, loss: 1748.961060\n",
      "Train: step:  15900, time: 0.274, loss: 1746.695923\n",
      "Train: step:  15910, time: 0.234, loss: 2354.123291\n",
      "Train: step:  15920, time: 0.199, loss: 789.818848\n",
      "Train: step:  15930, time: 0.202, loss: 3002.737793\n",
      "Train: step:  15940, time: 0.248, loss: 1341.552612\n",
      "Train: step:  15950, time: 0.258, loss: 643.177429\n",
      "Train: step:  15960, time: 0.213, loss: 1091.620361\n",
      "Train: step:  15970, time: 0.251, loss: 2540.789062\n",
      "Train: step:  15980, time: 0.202, loss: 3702.251953\n",
      "Train: step:  15990, time: 0.211, loss: 751.110291\n",
      "Train: step:  16000, time: 0.248, loss: 2537.880859\n",
      "Train: step:  16010, time: 0.292, loss: 423.976837\n",
      "Train: step:  16020, time: 0.252, loss: 1543.090576\n",
      "Train: step:  16030, time: 0.265, loss: 849.637329\n",
      "Train: step:  16040, time: 0.202, loss: 2486.438721\n",
      "Train: step:  16050, time: 0.266, loss: 435.144318\n",
      "Train: step:  16060, time: 0.204, loss: 948.478149\n",
      "Train: step:  16070, time: 0.254, loss: 1760.722046\n",
      "Train: step:  16080, time: 0.279, loss: 1715.736328\n",
      "Train: step:  16090, time: 0.287, loss: 1404.556152\n",
      "Train: step:  16100, time: 0.203, loss: 609.192322\n",
      "Train: step:  16110, time: 0.202, loss: 3298.033447\n",
      "Train: step:  16120, time: 0.202, loss: 382.429962\n",
      "Train: step:  16130, time: 0.275, loss: 588.862854\n",
      "Train: step:  16140, time: 0.250, loss: 2178.041504\n",
      "Train: step:  16150, time: 0.286, loss: 970.510681\n",
      "Train: step:  16160, time: 0.266, loss: 1390.938843\n",
      "Train: step:  16170, time: 0.202, loss: 2861.796875\n",
      "Train: step:  16180, time: 0.252, loss: 2119.101318\n",
      "Train: step:  16190, time: 0.284, loss: 1031.472656\n",
      "Train: step:  16200, time: 0.251, loss: 1553.273438\n",
      "Train: step:  16210, time: 0.248, loss: 1823.538330\n",
      "Train: step:  16220, time: 0.215, loss: 1474.071533\n",
      "Train: step:  16230, time: 0.283, loss: 1947.178711\n",
      "Train: step:  16240, time: 0.259, loss: 1986.094971\n",
      "Train: step:  16250, time: 0.291, loss: 429.439697\n",
      "Train: step:  16260, time: 0.294, loss: 2205.109375\n",
      "Train: step:  16270, time: 0.286, loss: 1902.692383\n",
      "Train: step:  16280, time: 0.282, loss: 276.318665\n",
      "Train: step:  16290, time: 0.252, loss: 274.740631\n",
      "Train: step:  16300, time: 0.350, loss: 350.501526\n",
      "Train: step:  16310, time: 0.200, loss: 1174.895386\n",
      "Train: step:  16320, time: 0.254, loss: 823.161743\n",
      "Train: step:  16330, time: 0.258, loss: 810.915527\n",
      "Train: step:  16340, time: 0.263, loss: 1767.520874\n",
      "Train: step:  16350, time: 0.245, loss: 2276.760254\n",
      "Train: step:  16360, time: 0.256, loss: 2728.256592\n",
      "Train: step:  16370, time: 0.223, loss: 2766.669434\n",
      "Train: step:  16380, time: 0.255, loss: 2336.506104\n",
      "Train: step:  16390, time: 0.246, loss: 653.418518\n",
      "Train: step:  16400, time: 0.214, loss: 855.567444\n",
      "Train: step:  16410, time: 0.270, loss: 1969.051025\n",
      "Train: step:  16420, time: 0.250, loss: 403.403809\n",
      "Train: step:  16430, time: 0.247, loss: 1380.178467\n",
      "Train: step:  16440, time: 0.245, loss: 2066.654297\n",
      "Train: step:  16450, time: 0.252, loss: 2173.268066\n",
      "Train: step:  16460, time: 0.248, loss: 2817.154785\n",
      "Train: step:  16470, time: 0.249, loss: 965.969360\n",
      "Train: step:  16480, time: 0.275, loss: 2045.205811\n",
      "Train: step:  16490, time: 0.252, loss: 1438.268433\n",
      "Train: step:  16500, time: 0.204, loss: 1906.644775\n",
      "Train: step:  16510, time: 0.247, loss: 1187.760010\n",
      "Train: step:  16520, time: 0.202, loss: 2099.687744\n",
      "Train: step:  16530, time: 0.245, loss: 3086.625488\n",
      "Train: step:  16540, time: 0.258, loss: 402.262665\n",
      "Train: step:  16550, time: 0.245, loss: 3127.296387\n",
      "Train: step:  16560, time: 0.274, loss: 2033.580444\n",
      "Train: step:  16570, time: 0.287, loss: 3239.599609\n",
      "Train: step:  16580, time: 0.280, loss: 1183.072998\n",
      "Train: step:  16590, time: 0.273, loss: 3148.437744\n",
      "Train: step:  16600, time: 0.210, loss: 2475.786133\n",
      "Train: step:  16610, time: 0.203, loss: 1556.919678\n",
      "Train: step:  16620, time: 0.284, loss: 1298.014282\n",
      "Train: step:  16630, time: 0.252, loss: 1531.100586\n",
      "Train: step:  16640, time: 0.270, loss: 2516.189209\n",
      "Train: step:  16650, time: 0.245, loss: 3164.808594\n",
      "Train: step:  16660, time: 0.208, loss: 1674.859985\n",
      "Train: step:  16670, time: 0.272, loss: 1489.811768\n",
      "Train: step:  16680, time: 0.258, loss: 733.818909\n",
      "Train: step:  16690, time: 0.240, loss: 1492.090942\n",
      "Train: step:  16700, time: 0.254, loss: 1820.727905\n",
      "Train: step:  16710, time: 0.206, loss: 700.544006\n",
      "Train: step:  16720, time: 0.277, loss: 1419.544189\n",
      "Train: step:  16730, time: 0.201, loss: 2566.766113\n",
      "Train: step:  16740, time: 0.207, loss: 1840.432983\n",
      "Train: step:  16750, time: 0.202, loss: 2566.675049\n",
      "Train: step:  16760, time: 0.221, loss: 3511.553467\n",
      "Train: step:  16770, time: 0.214, loss: 514.031982\n",
      "Train: step:  16780, time: 0.214, loss: 1100.360596\n",
      "Train: step:  16790, time: 0.249, loss: 1701.470947\n",
      "Train: step:  16800, time: 0.262, loss: 1544.120361\n",
      "Train: step:  16810, time: 0.256, loss: 1055.053589\n",
      "Train: step:  16820, time: 0.204, loss: 2904.351562\n",
      "Train: step:  16830, time: 0.202, loss: 2375.676025\n",
      "Train: step:  16840, time: 0.282, loss: 2355.751465\n",
      "Train: step:  16850, time: 0.252, loss: 3639.346680\n",
      "Train: step:  16860, time: 0.246, loss: 2318.295654\n",
      "Train: step:  16870, time: 0.204, loss: 3447.573242\n",
      "Train: step:  16880, time: 0.203, loss: 2478.432861\n",
      "Train: step:  16890, time: 0.245, loss: 1109.438477\n",
      "Train: step:  16900, time: 0.209, loss: 314.852875\n",
      "Train: step:  16910, time: 0.249, loss: 681.039429\n",
      "Train: step:  16920, time: 0.246, loss: 1766.002441\n",
      "Train: step:  16930, time: 0.283, loss: 1819.122437\n",
      "Train: step:  16940, time: 0.287, loss: 1708.632202\n",
      "Train: step:  16950, time: 0.206, loss: 624.828613\n",
      "Train: step:  16960, time: 0.200, loss: 1851.936646\n",
      "Train: step:  16970, time: 0.272, loss: 1035.716919\n",
      "Train: step:  16980, time: 0.219, loss: 2186.419189\n",
      "Train: step:  16990, time: 0.204, loss: 2272.446289\n",
      "Train: step:  17000, time: 0.207, loss: 1356.969482\n",
      "Train: step:  17010, time: 0.253, loss: 1744.458374\n",
      "Train: step:  17020, time: 0.214, loss: 3329.139160\n",
      "Train: step:  17030, time: 0.266, loss: 1805.658569\n",
      "Train: step:  17040, time: 0.289, loss: 1411.167236\n",
      "Train: step:  17050, time: 0.272, loss: 1919.868652\n",
      "Train: step:  17060, time: 0.249, loss: 1221.620483\n",
      "Train: step:  17070, time: 0.247, loss: 2750.745850\n",
      "Train: step:  17080, time: 0.244, loss: 420.489105\n",
      "Train: step:  17090, time: 0.241, loss: 1572.962524\n",
      "Train: step:  17100, time: 0.232, loss: 1862.180542\n",
      "Train: step:  17110, time: 0.203, loss: 529.600586\n",
      "Train: step:  17120, time: 0.255, loss: 1841.797485\n",
      "Train: step:  17130, time: 0.275, loss: 1626.957642\n",
      "Train: step:  17140, time: 0.270, loss: 1881.252563\n",
      "Train: step:  17150, time: 0.287, loss: 4323.847656\n",
      "Train: step:  17160, time: 0.269, loss: 1372.921997\n",
      "Train: step:  17170, time: 0.266, loss: 3409.110596\n",
      "Train: step:  17180, time: 0.273, loss: 1432.267944\n",
      "Train: step:  17190, time: 0.259, loss: 1551.084229\n",
      "Train: step:  17200, time: 0.231, loss: 1144.721558\n",
      "Train: step:  17210, time: 0.201, loss: 2767.217773\n",
      "Train: step:  17220, time: 0.276, loss: 2738.066162\n",
      "Train: step:  17230, time: 0.209, loss: 347.749542\n",
      "Train: step:  17240, time: 0.209, loss: 1502.789307\n",
      "Train: step:  17250, time: 0.204, loss: 3306.088379\n",
      "Train: step:  17260, time: 0.225, loss: 2540.219727\n",
      "Train: step:  17270, time: 0.258, loss: 1186.469971\n",
      "Train: step:  17280, time: 0.283, loss: 1706.538696\n",
      "Train: step:  17290, time: 0.205, loss: 853.389282\n",
      "Train: step:  17300, time: 0.207, loss: 1909.660156\n",
      "Train: step:  17310, time: 0.255, loss: 1793.763306\n",
      "Train: step:  17320, time: 0.257, loss: 2056.047852\n",
      "Train: step:  17330, time: 0.260, loss: 1394.210693\n",
      "Train: step:  17340, time: 0.202, loss: 1111.411255\n",
      "Train: step:  17350, time: 0.289, loss: 267.842651\n",
      "Train: step:  17360, time: 0.257, loss: 3036.524170\n",
      "Train: step:  17370, time: 0.263, loss: 2182.314697\n",
      "Train: step:  17380, time: 0.250, loss: 1036.174927\n",
      "Train: step:  17390, time: 0.272, loss: 2334.944092\n",
      "Train: step:  17400, time: 0.297, loss: 1824.806274\n",
      "Train: step:  17410, time: 0.288, loss: 2110.915039\n",
      "Train: step:  17420, time: 0.254, loss: 2114.764160\n",
      "Train: step:  17430, time: 0.300, loss: 2915.938477\n",
      "Train: step:  17440, time: 0.305, loss: 3365.989502\n",
      "Train: step:  17450, time: 0.277, loss: 2204.596924\n",
      "Train: step:  17460, time: 0.244, loss: 1112.259033\n",
      "Train: step:  17470, time: 0.259, loss: 928.122131\n",
      "Train: step:  17480, time: 0.294, loss: 1030.758301\n",
      "Train: step:  17490, time: 0.215, loss: 1242.127441\n",
      "Train: step:  17500, time: 0.271, loss: 746.439270\n",
      "Train: step:  17510, time: 0.200, loss: 1470.671021\n",
      "Train: step:  17520, time: 0.254, loss: 1184.378052\n",
      "Train: step:  17530, time: 0.255, loss: 1273.341187\n",
      "Train: step:  17540, time: 0.251, loss: 1544.515381\n",
      "Train: step:  17550, time: 0.201, loss: 2505.060059\n",
      "Train: step:  17560, time: 0.220, loss: 859.202637\n",
      "Train: step:  17570, time: 0.251, loss: 1121.660522\n",
      "Train: step:  17580, time: 0.250, loss: 2057.419678\n",
      "Train: step:  17590, time: 0.282, loss: 2021.508545\n",
      "Train: step:  17600, time: 0.274, loss: 3362.473389\n",
      "Train: step:  17610, time: 0.206, loss: 2191.760010\n",
      "Train: step:  17620, time: 0.280, loss: 1452.182739\n",
      "Train: step:  17630, time: 0.284, loss: 1350.082520\n",
      "Train: step:  17640, time: 0.216, loss: 3310.420898\n",
      "Train: step:  17650, time: 0.259, loss: 854.518494\n",
      "Train: step:  17660, time: 0.280, loss: 1533.730835\n",
      "Train: step:  17670, time: 0.250, loss: 2084.494873\n",
      "Train: step:  17680, time: 0.280, loss: 2474.916748\n",
      "Train: step:  17690, time: 0.208, loss: 3251.562012\n",
      "Train: step:  17700, time: 0.277, loss: 919.669128\n",
      "Train: step:  17710, time: 0.226, loss: 1647.380371\n",
      "Train: step:  17720, time: 0.258, loss: 2273.142578\n",
      "Train: step:  17730, time: 0.197, loss: 2659.992676\n",
      "Train: step:  17740, time: 0.269, loss: 2096.305176\n",
      "Train: step:  17750, time: 0.274, loss: 3661.980469\n",
      "Train: step:  17760, time: 0.253, loss: 1681.935059\n",
      "Train: step:  17770, time: 0.252, loss: 547.295044\n",
      "Train: step:  17780, time: 0.291, loss: 3502.250488\n",
      "Train: step:  17790, time: 0.239, loss: 342.549835\n",
      "Train: step:  17800, time: 0.204, loss: 2378.737061\n",
      "Train: step:  17810, time: 0.204, loss: 1178.551392\n",
      "Train: step:  17820, time: 0.205, loss: 2794.691650\n",
      "Train: step:  17830, time: 0.306, loss: 830.614197\n",
      "Train: step:  17840, time: 0.248, loss: 3959.744629\n",
      "Train: step:  17850, time: 0.279, loss: 1753.659180\n",
      "Train: step:  17860, time: 0.250, loss: 3324.998535\n",
      "Train: step:  17870, time: 0.274, loss: 2278.361816\n",
      "Train: step:  17880, time: 0.278, loss: 1697.432373\n",
      "Train: step:  17890, time: 0.216, loss: 1598.837769\n",
      "Train: step:  17900, time: 0.257, loss: 1149.811035\n",
      "Train: step:  17910, time: 0.259, loss: 2596.799561\n",
      "Train: step:  17920, time: 0.294, loss: 2398.780029\n",
      "Train: step:  17930, time: 0.278, loss: 3148.317627\n",
      "Train: step:  17940, time: 0.248, loss: 1049.453125\n",
      "Train: step:  17950, time: 0.253, loss: 2163.793457\n",
      "Train: step:  17960, time: 0.251, loss: 2176.032959\n",
      "Train: step:  17970, time: 0.248, loss: 3461.921143\n",
      "Train: step:  17980, time: 0.283, loss: 2640.387939\n",
      "Train: step:  17990, time: 0.205, loss: 867.076599\n",
      "Train: step:  18000, time: 0.204, loss: 1377.160522\n",
      "Train: step:  18010, time: 0.206, loss: 1946.025757\n",
      "Train: step:  18020, time: 0.206, loss: 747.636108\n",
      "Train: step:  18030, time: 0.256, loss: 3124.900879\n",
      "Train: step:  18040, time: 0.286, loss: 928.096558\n",
      "Train: step:  18050, time: 0.256, loss: 2645.962646\n",
      "Train: step:  18060, time: 0.254, loss: 1511.090210\n",
      "Train: step:  18070, time: 0.203, loss: 1926.365967\n",
      "Train: step:  18080, time: 0.213, loss: 2162.475342\n",
      "Train: step:  18090, time: 0.248, loss: 2732.322510\n",
      "Train: step:  18100, time: 0.251, loss: 1466.619019\n",
      "Train: step:  18110, time: 0.278, loss: 2411.595947\n",
      "Train: step:  18120, time: 0.260, loss: 2183.793457\n",
      "Train: step:  18130, time: 0.261, loss: 1804.254761\n",
      "Train: step:  18140, time: 0.249, loss: 2024.295166\n",
      "Train: step:  18150, time: 0.279, loss: 370.191528\n",
      "Train: step:  18160, time: 0.252, loss: 2240.985107\n",
      "Train: step:  18170, time: 0.276, loss: 2104.881348\n",
      "Train: step:  18180, time: 0.253, loss: 783.707947\n",
      "Train: step:  18190, time: 0.268, loss: 1440.607300\n",
      "Train: step:  18200, time: 0.260, loss: 808.138672\n",
      "Train: step:  18210, time: 0.243, loss: 2189.978760\n",
      "Train: step:  18220, time: 0.236, loss: 2384.728516\n",
      "Train: step:  18230, time: 0.270, loss: 1984.559082\n",
      "Train: step:  18240, time: 0.210, loss: 1612.426392\n",
      "Train: step:  18250, time: 0.265, loss: 1763.144287\n",
      "Train: step:  18260, time: 0.269, loss: 747.909302\n",
      "Train: step:  18270, time: 0.412, loss: 1370.107178\n",
      "Train: step:  18280, time: 0.225, loss: 1596.759644\n",
      "Train: step:  18290, time: 0.243, loss: 4316.278320\n",
      "Train: step:  18300, time: 0.228, loss: 1824.674194\n",
      "Train: step:  18310, time: 0.278, loss: 2318.825439\n",
      "Train: step:  18320, time: 0.250, loss: 4153.089844\n",
      "Train: step:  18330, time: 0.244, loss: 1491.384399\n",
      "Train: step:  18340, time: 0.207, loss: 516.497803\n",
      "Train: step:  18350, time: 0.247, loss: 2664.745361\n",
      "Train: step:  18360, time: 0.266, loss: 634.139709\n",
      "Train: step:  18370, time: 0.232, loss: 2316.754150\n",
      "Train: step:  18380, time: 0.201, loss: 4096.423828\n",
      "Train: step:  18390, time: 0.268, loss: 246.314682\n",
      "Train: step:  18400, time: 0.203, loss: 496.930786\n",
      "Train: step:  18410, time: 0.210, loss: 289.883179\n",
      "Train: step:  18420, time: 0.282, loss: 318.681152\n",
      "Train: step:  18430, time: 0.197, loss: 1925.717651\n",
      "Train: step:  18440, time: 0.290, loss: 1278.300537\n",
      "Train: step:  18450, time: 0.295, loss: 929.477783\n",
      "Train: step:  18460, time: 0.256, loss: 2009.761353\n",
      "Train: step:  18470, time: 0.245, loss: 3094.989746\n",
      "Train: step:  18480, time: 0.226, loss: 1029.813232\n",
      "Train: step:  18490, time: 0.207, loss: 721.961060\n",
      "Train: step:  18500, time: 0.250, loss: 2301.072510\n",
      "Train: step:  18510, time: 0.269, loss: 3678.799072\n",
      "Train: step:  18520, time: 0.245, loss: 637.724121\n",
      "Train: step:  18530, time: 0.267, loss: 1942.967651\n",
      "Train: step:  18540, time: 0.258, loss: 909.909851\n",
      "Train: step:  18550, time: 0.279, loss: 744.464661\n",
      "Train: step:  18560, time: 0.212, loss: 2442.227539\n",
      "Train: step:  18570, time: 0.246, loss: 584.201416\n",
      "Train: step:  18580, time: 0.241, loss: 1776.661987\n",
      "Train: step:  18590, time: 0.247, loss: 2149.766846\n",
      "Train: step:  18600, time: 0.260, loss: 2256.300781\n",
      "Train: step:  18610, time: 0.251, loss: 1723.455933\n",
      "Train: step:  18620, time: 0.275, loss: 943.544250\n",
      "Train: step:  18630, time: 0.248, loss: 1306.836670\n",
      "Train: step:  18640, time: 0.268, loss: 1302.746704\n",
      "Train: step:  18650, time: 0.273, loss: 1413.253784\n",
      "Train: step:  18660, time: 0.246, loss: 2357.206055\n",
      "Train: step:  18670, time: 0.237, loss: 2303.049561\n",
      "Train: step:  18680, time: 0.269, loss: 603.640259\n",
      "Train: step:  18690, time: 0.261, loss: 2213.738281\n",
      "Train: step:  18700, time: 0.206, loss: 2177.267822\n",
      "Train: step:  18710, time: 0.219, loss: 2752.223633\n",
      "Train: step:  18720, time: 0.262, loss: 1671.715698\n",
      "Train: step:  18730, time: 0.235, loss: 1631.588867\n",
      "Train: step:  18740, time: 0.228, loss: 1747.826172\n",
      "Train: step:  18750, time: 0.237, loss: 878.153870\n",
      "Train: step:  18760, time: 0.261, loss: 2312.480957\n",
      "Train: step:  18770, time: 0.239, loss: 1758.452759\n",
      "Train: step:  18780, time: 0.236, loss: 1104.085205\n",
      "Train: step:  18790, time: 0.244, loss: 2384.637451\n",
      "Train: step:  18800, time: 0.230, loss: 2338.185059\n",
      "Train: step:  18810, time: 0.238, loss: 1699.671875\n",
      "Train: step:  18820, time: 0.243, loss: 3354.125977\n",
      "Train: step:  18830, time: 0.251, loss: 1703.735352\n",
      "Train: step:  18840, time: 0.206, loss: 851.684692\n",
      "Train: step:  18850, time: 0.203, loss: 944.565552\n",
      "Train: step:  18860, time: 0.236, loss: 419.616730\n",
      "Train: step:  18870, time: 0.247, loss: 1703.346802\n",
      "Train: step:  18880, time: 0.249, loss: 2192.184326\n",
      "Train: step:  18890, time: 0.211, loss: 1028.275513\n",
      "Train: step:  18900, time: 0.264, loss: 2907.217285\n",
      "Train: step:  18910, time: 0.231, loss: 2114.418213\n",
      "Train: step:  18920, time: 0.263, loss: 1872.346558\n",
      "Train: step:  18930, time: 0.272, loss: 2544.682129\n",
      "Train: step:  18940, time: 0.235, loss: 1358.832153\n",
      "Train: step:  18950, time: 0.206, loss: 1663.554932\n",
      "Train: step:  18960, time: 0.208, loss: 2698.636963\n",
      "Train: step:  18970, time: 0.251, loss: 3936.364746\n",
      "Train: step:  18980, time: 0.252, loss: 1762.108032\n",
      "Train: step:  18990, time: 0.225, loss: 515.588379\n",
      "Train: step:  19000, time: 0.288, loss: 1337.812256\n",
      "Train: step:  19010, time: 0.243, loss: 618.367126\n",
      "Train: step:  19020, time: 0.244, loss: 908.208984\n",
      "Train: step:  19030, time: 0.238, loss: 1835.455933\n",
      "Train: step:  19040, time: 0.262, loss: 635.721558\n",
      "Train: step:  19050, time: 0.236, loss: 3095.197998\n",
      "Train: step:  19060, time: 0.250, loss: 2729.287842\n",
      "Train: step:  19070, time: 0.249, loss: 1852.653198\n",
      "Train: step:  19080, time: 0.277, loss: 3180.256836\n",
      "Train: step:  19090, time: 0.289, loss: 963.812561\n",
      "Train: step:  19100, time: 0.200, loss: 1273.836670\n",
      "Train: step:  19110, time: 0.256, loss: 965.747437\n",
      "Train: step:  19120, time: 0.202, loss: 1420.719604\n",
      "Train: step:  19130, time: 0.240, loss: 754.570007\n",
      "Train: step:  19140, time: 0.261, loss: 1465.823242\n",
      "Train: step:  19150, time: 0.234, loss: 785.577332\n",
      "Train: step:  19160, time: 0.206, loss: 3713.489014\n",
      "Train: step:  19170, time: 0.246, loss: 2756.482666\n",
      "Train: step:  19180, time: 0.202, loss: 825.440186\n",
      "Train: step:  19190, time: 0.232, loss: 1924.212646\n",
      "Train: step:  19200, time: 0.248, loss: 3133.472656\n",
      "Train: step:  19210, time: 0.204, loss: 1391.457886\n",
      "Train: step:  19220, time: 0.269, loss: 1777.274170\n",
      "Train: step:  19230, time: 0.277, loss: 2718.950928\n",
      "Train: step:  19240, time: 0.241, loss: 2384.965576\n",
      "Train: step:  19250, time: 0.229, loss: 2867.359375\n",
      "Train: step:  19260, time: 0.238, loss: 2090.134277\n",
      "Train: step:  19270, time: 0.263, loss: 3001.072998\n",
      "Train: step:  19280, time: 0.245, loss: 2395.079834\n",
      "Train: step:  19290, time: 0.245, loss: 2543.232666\n",
      "Train: step:  19300, time: 0.238, loss: 247.905457\n",
      "Train: step:  19310, time: 0.229, loss: 2084.486328\n",
      "Train: step:  19320, time: 0.260, loss: 1745.096069\n",
      "Train: step:  19330, time: 0.214, loss: 1590.452148\n",
      "Train: step:  19340, time: 0.230, loss: 2797.549561\n",
      "Train: step:  19350, time: 0.294, loss: 1641.549072\n",
      "Train: step:  19360, time: 0.289, loss: 1045.328979\n",
      "Train: step:  19370, time: 0.236, loss: 1297.554321\n",
      "Train: step:  19380, time: 0.230, loss: 3966.970703\n",
      "Train: step:  19390, time: 0.259, loss: 1743.040283\n",
      "Train: step:  19400, time: 0.252, loss: 3831.712158\n",
      "Train: step:  19410, time: 0.254, loss: 528.680359\n",
      "Train: step:  19420, time: 0.231, loss: 564.329590\n",
      "Train: step:  19430, time: 0.237, loss: 2930.398682\n",
      "Train: step:  19440, time: 0.241, loss: 1243.051880\n",
      "Train: step:  19450, time: 0.262, loss: 3224.413086\n",
      "Train: step:  19460, time: 0.234, loss: 1690.685181\n",
      "Train: step:  19470, time: 0.235, loss: 3710.444580\n",
      "Train: step:  19480, time: 0.269, loss: 235.077896\n",
      "Train: step:  19490, time: 0.246, loss: 1168.100220\n",
      "Train: step:  19500, time: 0.276, loss: 1527.526855\n",
      "Train: step:  19510, time: 0.260, loss: 2205.010986\n",
      "Train: step:  19520, time: 0.234, loss: 1204.799438\n",
      "Train: step:  19530, time: 0.261, loss: 873.163879\n",
      "Train: step:  19540, time: 0.249, loss: 1661.173218\n",
      "Train: step:  19550, time: 0.206, loss: 2350.537598\n",
      "Train: step:  19560, time: 0.230, loss: 1490.404785\n",
      "Train: step:  19570, time: 0.273, loss: 1657.818726\n",
      "Train: step:  19580, time: 0.228, loss: 2755.607178\n",
      "Train: step:  19590, time: 0.234, loss: 514.985901\n",
      "Train: step:  19600, time: 0.234, loss: 719.566223\n",
      "Train: step:  19610, time: 0.236, loss: 657.787415\n",
      "Train: step:  19620, time: 0.232, loss: 1250.163940\n",
      "Train: step:  19630, time: 0.232, loss: 2707.063721\n",
      "Train: step:  19640, time: 0.258, loss: 2866.527344\n",
      "Train: step:  19650, time: 0.252, loss: 1004.659973\n",
      "Train: step:  19660, time: 0.256, loss: 3611.383057\n",
      "Train: step:  19670, time: 0.235, loss: 1659.201904\n",
      "Train: step:  19680, time: 0.257, loss: 2024.757568\n",
      "Train: step:  19690, time: 0.246, loss: 1156.599243\n",
      "Train: step:  19700, time: 0.234, loss: 3307.761719\n",
      "Train: step:  19710, time: 0.242, loss: 2112.797607\n",
      "Train: step:  19720, time: 0.237, loss: 1557.022095\n",
      "Train: step:  19730, time: 0.254, loss: 557.758728\n",
      "Train: step:  19740, time: 0.229, loss: 1928.649292\n",
      "Train: step:  19750, time: 0.233, loss: 3410.659424\n",
      "Train: step:  19760, time: 0.233, loss: 3606.375977\n",
      "Train: step:  19770, time: 0.229, loss: 3234.916260\n",
      "Train: step:  19780, time: 0.223, loss: 568.228271\n",
      "Train: step:  19790, time: 0.235, loss: 593.651794\n",
      "Train: step:  19800, time: 0.227, loss: 1931.679932\n",
      "Train: step:  19810, time: 0.279, loss: 2864.796143\n",
      "Train: step:  19820, time: 0.248, loss: 1256.826172\n",
      "Train: step:  19830, time: 0.260, loss: 1915.517334\n",
      "Train: step:  19840, time: 0.226, loss: 2310.555176\n",
      "Train: step:  19850, time: 0.208, loss: 909.171631\n",
      "Train: step:  19860, time: 0.232, loss: 657.395508\n",
      "Train: step:  19870, time: 0.200, loss: 2493.856934\n",
      "Train: step:  19880, time: 0.247, loss: 1357.865967\n",
      "Train: step:  19890, time: 0.234, loss: 1829.486450\n",
      "Train: step:  19900, time: 0.267, loss: 2351.281250\n",
      "Train: step:  19910, time: 0.247, loss: 1226.001587\n",
      "Train: step:  19920, time: 0.202, loss: 1324.976807\n",
      "Train: step:  19930, time: 0.202, loss: 2675.770020\n",
      "Train: step:  19940, time: 0.257, loss: 1639.282104\n",
      "Train: step:  19950, time: 0.264, loss: 1948.241821\n",
      "Train: step:  19960, time: 0.282, loss: 3157.985596\n",
      "Train: step:  19970, time: 0.261, loss: 1924.090332\n",
      "Train: step:  19980, time: 0.261, loss: 239.850876\n",
      "Train: step:  19990, time: 0.297, loss: 1481.754150\n",
      "Train: step:  20000, time: 0.205, loss: 3540.265381\n",
      "Train: step:  20010, time: 0.199, loss: 956.757141\n",
      "Train: step:  20020, time: 0.238, loss: 834.636597\n",
      "Train: step:  20030, time: 0.241, loss: 2667.495605\n",
      "Train: step:  20040, time: 0.242, loss: 1095.398560\n",
      "Train: step:  20050, time: 0.244, loss: 1629.767334\n",
      "Train: step:  20060, time: 0.272, loss: 830.596497\n",
      "Train: step:  20070, time: 0.197, loss: 2715.878662\n",
      "Train: step:  20080, time: 0.255, loss: 3080.859863\n",
      "Train: step:  20090, time: 0.274, loss: 2519.963379\n",
      "Train: step:  20100, time: 0.237, loss: 1570.728271\n",
      "Train: step:  20110, time: 0.198, loss: 2594.815430\n",
      "Train: step:  20120, time: 0.278, loss: 252.029709\n",
      "Train: step:  20130, time: 0.247, loss: 2641.237549\n",
      "Train: step:  20140, time: 0.261, loss: 2043.952637\n",
      "Train: step:  20150, time: 0.258, loss: 2193.533203\n",
      "Train: step:  20160, time: 0.283, loss: 1940.729858\n",
      "Train: step:  20170, time: 0.233, loss: 3326.624512\n",
      "Train: step:  20180, time: 0.283, loss: 1037.632202\n",
      "Train: step:  20190, time: 0.213, loss: 2019.779785\n",
      "Train: step:  20200, time: 0.235, loss: 1572.713501\n",
      "Train: step:  20210, time: 0.218, loss: 1276.151733\n",
      "Train: step:  20220, time: 0.285, loss: 1192.159058\n",
      "Train: step:  20230, time: 0.239, loss: 2409.092041\n",
      "Train: step:  20240, time: 0.226, loss: 765.088623\n",
      "Train: step:  20250, time: 0.234, loss: 2811.736328\n",
      "Train: step:  20260, time: 0.254, loss: 913.756409\n",
      "Train: step:  20270, time: 0.222, loss: 704.323914\n",
      "Train: step:  20280, time: 0.241, loss: 2376.437256\n",
      "Train: step:  20290, time: 0.220, loss: 2232.432617\n",
      "Train: step:  20300, time: 0.228, loss: 2474.943115\n",
      "Train: step:  20310, time: 0.222, loss: 1577.715576\n",
      "Train: step:  20320, time: 0.220, loss: 1444.845825\n",
      "Train: step:  20330, time: 0.231, loss: 2731.408447\n",
      "Train: step:  20340, time: 0.241, loss: 2224.165527\n",
      "Train: step:  20350, time: 0.215, loss: 1071.075317\n",
      "Train: step:  20360, time: 0.258, loss: 4486.795898\n",
      "Train: step:  20370, time: 0.227, loss: 2664.334961\n",
      "Train: step:  20380, time: 0.252, loss: 1688.727905\n",
      "Train: step:  20390, time: 0.238, loss: 1356.520142\n",
      "Train: step:  20400, time: 0.222, loss: 1105.756104\n",
      "Train: step:  20410, time: 0.230, loss: 691.913269\n",
      "Train: step:  20420, time: 0.226, loss: 2389.226318\n",
      "Train: step:  20430, time: 0.239, loss: 1431.194336\n",
      "Train: step:  20440, time: 0.231, loss: 1593.538940\n",
      "Train: step:  20450, time: 0.234, loss: 2691.329102\n",
      "Train: step:  20460, time: 0.220, loss: 1486.855591\n",
      "Train: step:  20470, time: 0.274, loss: 1722.757812\n",
      "Train: step:  20480, time: 0.218, loss: 1599.748169\n",
      "Train: step:  20490, time: 0.271, loss: 1215.893677\n",
      "Train: step:  20500, time: 0.219, loss: 1578.403076\n",
      "Train: step:  20510, time: 0.226, loss: 791.213928\n",
      "Train: step:  20520, time: 0.224, loss: 766.666687\n",
      "Train: step:  20530, time: 0.240, loss: 1632.467163\n",
      "Train: step:  20540, time: 0.243, loss: 2399.222168\n",
      "Train: step:  20550, time: 0.259, loss: 2838.949707\n",
      "Train: step:  20560, time: 0.262, loss: 1540.112305\n",
      "Train: step:  20570, time: 0.253, loss: 2202.010498\n",
      "Train: step:  20580, time: 0.254, loss: 1338.489136\n",
      "Train: step:  20590, time: 0.229, loss: 1931.147705\n",
      "Train: step:  20600, time: 0.281, loss: 977.622986\n",
      "Train: step:  20610, time: 0.242, loss: 718.903259\n",
      "Train: step:  20620, time: 0.259, loss: 968.235474\n",
      "Train: step:  20630, time: 0.252, loss: 418.269653\n",
      "Train: step:  20640, time: 0.262, loss: 1296.172974\n",
      "Train: step:  20650, time: 0.234, loss: 2136.685791\n",
      "Train: step:  20660, time: 0.257, loss: 1854.752441\n",
      "Train: step:  20670, time: 0.232, loss: 1470.129272\n",
      "Train: step:  20680, time: 0.231, loss: 1946.940918\n",
      "Train: step:  20690, time: 0.264, loss: 494.134613\n",
      "Train: step:  20700, time: 0.221, loss: 2909.964600\n",
      "Train: step:  20710, time: 0.243, loss: 1391.789185\n",
      "Train: step:  20720, time: 0.229, loss: 2593.136963\n",
      "Train: step:  20730, time: 0.230, loss: 2762.474121\n",
      "Train: step:  20740, time: 0.284, loss: 2057.922363\n",
      "Train: step:  20750, time: 0.254, loss: 741.799561\n",
      "Train: step:  20760, time: 0.237, loss: 4207.104980\n",
      "Train: step:  20770, time: 0.229, loss: 1467.256104\n",
      "Train: step:  20780, time: 0.222, loss: 1124.559082\n",
      "Train: step:  20790, time: 0.231, loss: 2665.121094\n",
      "Train: step:  20800, time: 0.220, loss: 2054.083008\n",
      "Train: step:  20810, time: 0.229, loss: 2028.561523\n",
      "Train: step:  20820, time: 0.231, loss: 298.133057\n",
      "Train: step:  20830, time: 0.225, loss: 587.892334\n",
      "Train: step:  20840, time: 0.231, loss: 1806.196289\n",
      "Train: step:  20850, time: 0.226, loss: 1763.862915\n",
      "Train: step:  20860, time: 0.228, loss: 3260.007080\n",
      "Train: step:  20870, time: 0.260, loss: 4173.723633\n",
      "Train: step:  20880, time: 0.241, loss: 3107.461426\n",
      "Train: step:  20890, time: 0.223, loss: 1212.635864\n",
      "Train: step:  20900, time: 0.248, loss: 3129.875977\n",
      "Train: step:  20910, time: 0.232, loss: 2798.919189\n",
      "Train: step:  20920, time: 0.228, loss: 2833.937988\n",
      "Train: step:  20930, time: 0.248, loss: 2442.900146\n",
      "Train: step:  20940, time: 0.226, loss: 2073.067627\n",
      "Train: step:  20950, time: 0.246, loss: 2573.304688\n",
      "Train: step:  20960, time: 0.268, loss: 2590.127686\n",
      "Train: step:  20970, time: 0.232, loss: 1847.283936\n",
      "Train: step:  20980, time: 0.250, loss: 3827.166748\n",
      "Train: step:  20990, time: 0.227, loss: 1282.479004\n",
      "Train: step:  21000, time: 0.271, loss: 2684.368164\n",
      "Train: step:  21010, time: 0.277, loss: 1110.259888\n",
      "Train: step:  21020, time: 0.224, loss: 1763.372070\n",
      "Train: step:  21030, time: 0.223, loss: 1848.727295\n",
      "Train: step:  21040, time: 0.221, loss: 1519.819336\n",
      "Train: step:  21050, time: 0.220, loss: 1751.397705\n",
      "Train: step:  21060, time: 0.228, loss: 1221.413574\n",
      "Train: step:  21070, time: 0.264, loss: 2136.282227\n",
      "Train: step:  21080, time: 0.217, loss: 3488.450439\n",
      "Train: step:  21090, time: 0.233, loss: 1902.833984\n",
      "Train: step:  21100, time: 0.251, loss: 1787.432617\n",
      "Train: step:  21110, time: 0.247, loss: 1951.616821\n",
      "Train: step:  21120, time: 0.227, loss: 291.607697\n",
      "Train: step:  21130, time: 0.236, loss: 3095.117188\n",
      "Train: step:  21140, time: 0.254, loss: 3097.118896\n",
      "Train: step:  21150, time: 0.258, loss: 1065.068481\n",
      "Train: step:  21160, time: 0.229, loss: 2028.586304\n",
      "Train: step:  21170, time: 0.227, loss: 1098.314087\n",
      "Train: step:  21180, time: 0.260, loss: 2086.955811\n",
      "Train: step:  21190, time: 0.258, loss: 2551.248047\n",
      "Train: step:  21200, time: 0.239, loss: 1606.639648\n",
      "Train: step:  21210, time: 0.218, loss: 1419.134888\n",
      "Train: step:  21220, time: 0.237, loss: 2703.374512\n",
      "Train: step:  21230, time: 0.253, loss: 1581.619507\n",
      "Train: step:  21240, time: 0.226, loss: 1862.015015\n",
      "Train: step:  21250, time: 0.243, loss: 2047.957397\n",
      "Train: step:  21260, time: 0.230, loss: 1314.828369\n",
      "Train: step:  21270, time: 0.226, loss: 1589.239624\n",
      "Train: step:  21280, time: 0.223, loss: 1961.118774\n",
      "Train: step:  21290, time: 0.227, loss: 1368.012085\n",
      "Train: step:  21300, time: 0.217, loss: 2073.283691\n",
      "Train: step:  21310, time: 0.229, loss: 1903.203247\n",
      "Train: step:  21320, time: 0.217, loss: 1705.708374\n",
      "Train: step:  21330, time: 0.217, loss: 401.724243\n",
      "Train: step:  21340, time: 0.262, loss: 198.474503\n",
      "Train: step:  21350, time: 0.252, loss: 1501.186768\n",
      "Train: step:  21360, time: 0.279, loss: 985.185303\n",
      "Train: step:  21370, time: 0.223, loss: 1312.043701\n",
      "Train: step:  21380, time: 0.266, loss: 1732.201172\n",
      "Train: step:  21390, time: 0.221, loss: 1778.199463\n",
      "Train: step:  21400, time: 0.232, loss: 3341.813232\n",
      "Train: step:  21410, time: 0.217, loss: 1575.761353\n",
      "Train: step:  21420, time: 0.245, loss: 2882.398438\n",
      "Train: step:  21430, time: 0.222, loss: 3625.211670\n",
      "Train: step:  21440, time: 0.261, loss: 1944.996460\n",
      "Train: step:  21450, time: 0.276, loss: 3850.424805\n",
      "Train: step:  21460, time: 0.224, loss: 1534.787720\n",
      "Train: step:  21470, time: 0.234, loss: 1539.279541\n",
      "Train: step:  21480, time: 0.226, loss: 1337.979858\n",
      "Train: step:  21490, time: 0.228, loss: 393.648895\n",
      "Train: step:  21500, time: 0.228, loss: 1634.612061\n",
      "Train: step:  21510, time: 0.222, loss: 3175.329590\n",
      "Train: step:  21520, time: 0.229, loss: 1052.600342\n",
      "Train: step:  21530, time: 0.225, loss: 1435.746460\n",
      "Train: step:  21540, time: 0.225, loss: 1564.258301\n",
      "Train: step:  21550, time: 0.221, loss: 2737.684082\n",
      "Train: step:  21560, time: 0.224, loss: 604.635437\n",
      "Train: step:  21570, time: 0.252, loss: 1452.588379\n",
      "Train: step:  21580, time: 0.233, loss: 3767.257080\n",
      "Train: step:  21590, time: 0.234, loss: 1154.326538\n",
      "Train: step:  21600, time: 0.227, loss: 4143.505371\n",
      "Train: step:  21610, time: 0.233, loss: 1357.978516\n",
      "Train: step:  21620, time: 0.226, loss: 1649.533325\n",
      "Train: step:  21630, time: 0.240, loss: 1207.151733\n",
      "Train: step:  21640, time: 0.229, loss: 2856.786865\n",
      "Train: step:  21650, time: 0.229, loss: 1038.806763\n",
      "Train: step:  21660, time: 0.233, loss: 2414.583252\n",
      "Train: step:  21670, time: 0.229, loss: 593.901001\n",
      "Train: step:  21680, time: 0.228, loss: 537.055786\n",
      "Train: step:  21690, time: 0.227, loss: 2198.051758\n",
      "Train: step:  21700, time: 0.249, loss: 3875.267578\n",
      "Train: step:  21710, time: 0.222, loss: 496.395691\n",
      "Train: step:  21720, time: 0.228, loss: 3410.828369\n",
      "Train: step:  21730, time: 0.260, loss: 1701.553955\n",
      "Train: step:  21740, time: 0.225, loss: 2455.235596\n",
      "Train: step:  21750, time: 0.223, loss: 1835.894775\n",
      "Train: step:  21760, time: 0.224, loss: 2183.850342\n",
      "Train: step:  21770, time: 0.224, loss: 2107.100830\n",
      "Train: step:  21780, time: 0.207, loss: 2092.841309\n",
      "Train: step:  21790, time: 0.252, loss: 1306.851807\n",
      "Train: step:  21800, time: 0.228, loss: 1746.756958\n",
      "Train: step:  21810, time: 0.261, loss: 2337.082520\n",
      "Train: step:  21820, time: 0.261, loss: 2631.253418\n",
      "Train: step:  21830, time: 0.235, loss: 332.606537\n",
      "Train: step:  21840, time: 0.223, loss: 2667.834717\n",
      "Train: step:  21850, time: 0.219, loss: 3188.574707\n",
      "Train: step:  21860, time: 0.222, loss: 2645.784668\n",
      "Train: step:  21870, time: 0.223, loss: 2574.418945\n",
      "Train: step:  21880, time: 0.260, loss: 1628.121216\n",
      "Train: step:  21890, time: 0.268, loss: 2770.091309\n",
      "Train: step:  21900, time: 0.221, loss: 2220.628174\n",
      "Train: step:  21910, time: 0.229, loss: 1899.040161\n",
      "Train: step:  21920, time: 0.216, loss: 2186.604248\n",
      "Train: step:  21930, time: 0.253, loss: 2182.593750\n",
      "Train: step:  21940, time: 0.250, loss: 2663.798584\n",
      "Train: step:  21950, time: 0.219, loss: 2437.936279\n",
      "Train: step:  21960, time: 0.218, loss: 2032.001221\n",
      "Train: step:  21970, time: 0.223, loss: 1922.663452\n",
      "Train: step:  21980, time: 0.232, loss: 3634.617188\n",
      "Train: step:  21990, time: 0.229, loss: 1228.652100\n",
      "Train: step:  22000, time: 0.230, loss: 2815.292236\n",
      "Train: step:  22010, time: 0.219, loss: 2525.704346\n",
      "Train: step:  22020, time: 0.257, loss: 1408.578979\n",
      "Train: step:  22030, time: 0.226, loss: 2723.103516\n",
      "Train: step:  22040, time: 0.243, loss: 2428.568848\n",
      "Train: step:  22050, time: 0.261, loss: 3018.251465\n",
      "Train: step:  22060, time: 0.231, loss: 1052.894775\n",
      "Train: step:  22070, time: 0.261, loss: 4170.577637\n",
      "Train: step:  22080, time: 0.269, loss: 2385.879883\n",
      "Train: step:  22090, time: 0.259, loss: 2061.476318\n",
      "Train: step:  22100, time: 0.230, loss: 2025.036011\n",
      "Train: step:  22110, time: 0.233, loss: 683.315002\n",
      "Train: step:  22120, time: 0.226, loss: 1201.333984\n",
      "Train: step:  22130, time: 0.233, loss: 1674.427246\n",
      "Train: step:  22140, time: 0.231, loss: 3124.866699\n",
      "Train: step:  22150, time: 0.261, loss: 2524.812012\n",
      "Train: step:  22160, time: 0.257, loss: 1401.648804\n",
      "Train: step:  22170, time: 0.227, loss: 2944.587158\n",
      "Train: step:  22180, time: 0.232, loss: 2008.806396\n",
      "Train: step:  22190, time: 0.237, loss: 2625.564941\n",
      "Train: step:  22200, time: 0.227, loss: 829.967224\n",
      "Train: step:  22210, time: 0.260, loss: 4370.545410\n",
      "Train: step:  22220, time: 0.234, loss: 2537.816406\n",
      "Train: step:  22230, time: 0.232, loss: 3367.824463\n",
      "Train: step:  22240, time: 0.257, loss: 2314.006592\n",
      "Train: step:  22250, time: 0.230, loss: 383.100891\n",
      "Train: step:  22260, time: 0.227, loss: 2069.497803\n",
      "Train: step:  22270, time: 0.243, loss: 1961.938843\n",
      "Train: step:  22280, time: 0.220, loss: 3397.125732\n",
      "Train: step:  22290, time: 0.256, loss: 2338.855713\n",
      "Train: step:  22300, time: 0.228, loss: 1946.127808\n",
      "Train: step:  22310, time: 0.205, loss: 2000.652710\n",
      "Train: step:  22320, time: 0.209, loss: 692.764893\n",
      "Train: step:  22330, time: 0.205, loss: 3016.465576\n",
      "Train: step:  22340, time: 0.221, loss: 1458.250244\n",
      "Train: step:  22350, time: 0.227, loss: 1838.544556\n",
      "Train: step:  22360, time: 0.279, loss: 442.410034\n",
      "Train: step:  22370, time: 0.254, loss: 2166.818359\n",
      "Train: step:  22380, time: 0.289, loss: 778.374084\n",
      "Train: step:  22390, time: 0.249, loss: 638.359009\n",
      "Train: step:  22400, time: 0.238, loss: 1092.963867\n",
      "Train: step:  22410, time: 0.223, loss: 2871.869629\n",
      "Train: step:  22420, time: 0.224, loss: 1053.425537\n",
      "Train: step:  22430, time: 0.259, loss: 2843.392334\n",
      "Train: step:  22440, time: 0.250, loss: 759.047058\n",
      "Train: step:  22450, time: 0.281, loss: 1749.068359\n",
      "Train: step:  22460, time: 0.222, loss: 1707.841797\n",
      "Train: step:  22470, time: 0.228, loss: 1071.200928\n",
      "Train: step:  22480, time: 0.227, loss: 1115.755249\n",
      "Train: step:  22490, time: 0.226, loss: 2457.041260\n",
      "Train: step:  22500, time: 0.268, loss: 677.251282\n",
      "Train: step:  22510, time: 0.221, loss: 736.080688\n",
      "Train: step:  22520, time: 0.227, loss: 1038.322388\n",
      "Train: step:  22530, time: 0.209, loss: 1962.359009\n",
      "Train: step:  22540, time: 0.237, loss: 1006.759155\n",
      "Train: step:  22550, time: 0.267, loss: 446.500122\n",
      "Train: step:  22560, time: 0.226, loss: 397.394867\n",
      "Train: step:  22570, time: 0.231, loss: 2229.241699\n",
      "Train: step:  22580, time: 0.224, loss: 3523.073975\n",
      "Train: step:  22590, time: 0.247, loss: 1895.290771\n",
      "Train: step:  22600, time: 0.252, loss: 1510.857666\n",
      "Train: step:  22610, time: 0.219, loss: 3177.011719\n",
      "Train: step:  22620, time: 0.212, loss: 963.597290\n",
      "Train: step:  22630, time: 0.234, loss: 3065.614746\n",
      "Train: step:  22640, time: 0.224, loss: 3745.423096\n",
      "Train: step:  22650, time: 0.231, loss: 609.660217\n",
      "Train: step:  22660, time: 0.283, loss: 2305.046387\n",
      "Train: step:  22670, time: 0.229, loss: 3210.798828\n",
      "Train: step:  22680, time: 0.280, loss: 2077.929688\n",
      "Train: step:  22690, time: 0.245, loss: 454.526489\n",
      "Train: step:  22700, time: 0.240, loss: 1274.161499\n",
      "Train: step:  22710, time: 0.232, loss: 1326.947876\n",
      "Train: step:  22720, time: 0.236, loss: 2049.857422\n",
      "Train: step:  22730, time: 0.232, loss: 2741.595215\n",
      "Train: step:  22740, time: 0.273, loss: 1773.298706\n",
      "Train: step:  22750, time: 0.238, loss: 3259.357178\n",
      "Train: step:  22760, time: 0.232, loss: 1613.455200\n",
      "Train: step:  22770, time: 0.227, loss: 1490.658691\n",
      "Train: step:  22780, time: 0.262, loss: 485.345703\n",
      "Train: step:  22790, time: 0.250, loss: 1151.645996\n",
      "Train: step:  22800, time: 0.229, loss: 3132.742676\n",
      "Train: step:  22810, time: 0.220, loss: 628.035583\n",
      "Train: step:  22820, time: 0.277, loss: 2001.202271\n",
      "Train: step:  22830, time: 0.218, loss: 1461.671631\n",
      "Train: step:  22840, time: 0.264, loss: 1507.504883\n",
      "Train: step:  22850, time: 0.258, loss: 1105.633057\n",
      "Train: step:  22860, time: 0.227, loss: 1843.195801\n",
      "Train: step:  22870, time: 0.223, loss: 2410.968506\n",
      "Train: step:  22880, time: 0.216, loss: 1700.859985\n",
      "Train: step:  22890, time: 0.217, loss: 411.022675\n",
      "Train: step:  22900, time: 0.218, loss: 937.101746\n",
      "Train: step:  22910, time: 0.220, loss: 2564.878174\n",
      "Train: step:  22920, time: 0.228, loss: 1711.200073\n",
      "Train: step:  22930, time: 0.221, loss: 2650.401123\n",
      "Train: step:  22940, time: 0.217, loss: 1790.659180\n",
      "Train: step:  22950, time: 0.236, loss: 857.685486\n",
      "Train: step:  22960, time: 0.268, loss: 1395.637573\n",
      "Train: step:  22970, time: 0.224, loss: 941.877014\n",
      "Train: step:  22980, time: 0.228, loss: 670.748657\n",
      "Train: step:  22990, time: 0.238, loss: 1398.150146\n",
      "Train: step:  23000, time: 0.238, loss: 1202.432251\n",
      "Train: step:  23010, time: 0.267, loss: 857.799622\n",
      "Train: step:  23020, time: 0.226, loss: 1082.591675\n",
      "Train: step:  23030, time: 0.274, loss: 1398.082397\n",
      "Train: step:  23040, time: 0.229, loss: 1750.150635\n",
      "Train: step:  23050, time: 0.242, loss: 1655.194214\n",
      "Train: step:  23060, time: 0.227, loss: 1134.464844\n",
      "Train: step:  23070, time: 0.280, loss: 1817.968262\n",
      "Train: step:  23080, time: 0.250, loss: 2442.324463\n",
      "Train: step:  23090, time: 0.237, loss: 1064.622070\n",
      "Train: step:  23100, time: 0.224, loss: 1967.055664\n",
      "Train: step:  23110, time: 0.266, loss: 5224.743652\n",
      "Train: step:  23120, time: 0.241, loss: 2518.564453\n",
      "Train: step:  23130, time: 0.226, loss: 905.535645\n",
      "Train: step:  23140, time: 0.230, loss: 1622.571289\n",
      "Train: step:  23150, time: 0.280, loss: 2114.551514\n",
      "Train: step:  23160, time: 0.230, loss: 3718.229980\n",
      "Train: step:  23170, time: 0.231, loss: 2495.358643\n",
      "Train: step:  23180, time: 0.216, loss: 1708.649170\n",
      "Train: step:  23190, time: 0.257, loss: 1033.461792\n",
      "Train: step:  23200, time: 0.233, loss: 1469.686035\n",
      "Train: step:  23210, time: 0.236, loss: 326.113861\n",
      "Train: step:  23220, time: 0.268, loss: 1153.417603\n",
      "Train: step:  23230, time: 0.265, loss: 2204.440186\n",
      "Train: step:  23240, time: 0.225, loss: 2164.112061\n",
      "Train: step:  23250, time: 0.255, loss: 2153.644287\n",
      "Train: step:  23260, time: 0.258, loss: 4035.458740\n",
      "Train: step:  23270, time: 0.210, loss: 2769.083252\n",
      "Train: step:  23280, time: 0.244, loss: 1749.087524\n",
      "Train: step:  23290, time: 0.267, loss: 3971.120117\n",
      "Train: step:  23300, time: 0.234, loss: 1138.992188\n",
      "Train: step:  23310, time: 0.225, loss: 3689.517822\n",
      "Train: step:  23320, time: 0.224, loss: 3728.252197\n",
      "Train: step:  23330, time: 0.262, loss: 1791.784424\n",
      "Train: step:  23340, time: 0.224, loss: 2294.752441\n",
      "Train: step:  23350, time: 0.228, loss: 2192.633057\n",
      "Train: step:  23360, time: 0.232, loss: 2211.153564\n",
      "Train: step:  23370, time: 0.224, loss: 2604.048096\n",
      "Train: step:  23380, time: 0.267, loss: 2239.031494\n",
      "Train: step:  23390, time: 0.253, loss: 2494.875732\n",
      "Train: step:  23400, time: 0.225, loss: 374.421936\n",
      "Train: step:  23410, time: 0.234, loss: 1748.403809\n",
      "Train: step:  23420, time: 0.244, loss: 1655.409058\n",
      "Train: step:  23430, time: 0.230, loss: 811.312744\n",
      "Train: step:  23440, time: 0.251, loss: 753.398254\n",
      "Train: step:  23450, time: 0.227, loss: 3528.701172\n",
      "Train: step:  23460, time: 0.255, loss: 2234.691406\n",
      "Train: step:  23470, time: 0.230, loss: 441.717102\n",
      "Train: step:  23480, time: 0.229, loss: 3217.798340\n",
      "Train: step:  23490, time: 0.260, loss: 2513.832764\n",
      "Train: step:  23500, time: 0.224, loss: 1843.982788\n",
      "Train: step:  23510, time: 0.231, loss: 2599.055176\n",
      "Train: step:  23520, time: 0.264, loss: 1188.140869\n",
      "Train: step:  23530, time: 0.228, loss: 3141.278076\n",
      "Train: step:  23540, time: 0.253, loss: 612.119446\n",
      "Train: step:  23550, time: 0.224, loss: 3138.850342\n",
      "Train: step:  23560, time: 0.219, loss: 2013.974121\n",
      "Train: step:  23570, time: 0.217, loss: 2525.108154\n",
      "Train: step:  23580, time: 0.254, loss: 4828.017578\n",
      "Train: step:  23590, time: 0.259, loss: 1838.941406\n",
      "Train: step:  23600, time: 0.228, loss: 3089.450684\n",
      "Train: step:  23610, time: 0.225, loss: 4149.225098\n",
      "Train: step:  23620, time: 0.218, loss: 3226.072021\n",
      "Train: step:  23630, time: 0.291, loss: 3714.359863\n",
      "Train: step:  23640, time: 0.250, loss: 852.263733\n",
      "Train: step:  23650, time: 0.247, loss: 3884.414307\n",
      "Train: step:  23660, time: 0.238, loss: 1651.328613\n",
      "Train: step:  23670, time: 0.222, loss: 2791.609375\n",
      "Train: step:  23680, time: 0.229, loss: 1191.718872\n",
      "Train: step:  23690, time: 0.277, loss: 3846.163818\n",
      "Train: step:  23700, time: 0.228, loss: 2083.414062\n",
      "Train: step:  23710, time: 0.221, loss: 1104.431396\n",
      "Train: step:  23720, time: 0.230, loss: 2734.986816\n",
      "Train: step:  23730, time: 0.228, loss: 1737.204712\n",
      "Train: step:  23740, time: 0.270, loss: 967.749451\n",
      "Train: step:  23750, time: 0.228, loss: 1044.064575\n",
      "Train: step:  23760, time: 0.259, loss: 2640.786865\n",
      "Train: step:  23770, time: 0.229, loss: 1486.819214\n",
      "Train: step:  23780, time: 0.253, loss: 2903.561523\n",
      "Train: step:  23790, time: 0.225, loss: 3625.683594\n",
      "Train: step:  23800, time: 0.224, loss: 1297.600708\n",
      "Train: step:  23810, time: 0.230, loss: 1358.473755\n",
      "Train: step:  23820, time: 0.228, loss: 3455.386475\n",
      "Train: step:  23830, time: 0.249, loss: 1371.546021\n",
      "Train: step:  23840, time: 0.225, loss: 2582.201660\n",
      "Train: step:  23850, time: 0.277, loss: 1182.941528\n",
      "Train: step:  23860, time: 0.220, loss: 1097.166992\n",
      "Train: step:  23870, time: 0.222, loss: 1336.705200\n",
      "Train: step:  23880, time: 0.226, loss: 392.892242\n",
      "Train: step:  23890, time: 0.221, loss: 874.431824\n",
      "Train: step:  23900, time: 0.280, loss: 1529.484253\n",
      "Train: step:  23910, time: 0.218, loss: 1115.197876\n",
      "Train: step:  23920, time: 0.218, loss: 4120.689453\n",
      "Train: step:  23930, time: 0.216, loss: 675.308838\n",
      "Train: step:  23940, time: 0.224, loss: 3595.334229\n",
      "Train: step:  23950, time: 0.227, loss: 3416.542480\n",
      "Train: step:  23960, time: 0.220, loss: 958.802246\n",
      "Train: step:  23970, time: 0.223, loss: 2107.905518\n",
      "Train: step:  23980, time: 0.227, loss: 1272.515015\n",
      "Train: step:  23990, time: 0.256, loss: 1807.566650\n",
      "Train: step:  24000, time: 0.220, loss: 1730.930298\n",
      "Train: step:  24010, time: 0.217, loss: 1604.506104\n",
      "Train: step:  24020, time: 0.233, loss: 1594.848389\n",
      "Train: step:  24030, time: 0.220, loss: 1982.329102\n",
      "Train: step:  24040, time: 0.241, loss: 2025.974365\n",
      "Train: step:  24050, time: 0.223, loss: 1957.382324\n",
      "Train: step:  24060, time: 0.223, loss: 658.055481\n",
      "Train: step:  24070, time: 0.235, loss: 1252.551514\n",
      "Train: step:  24080, time: 0.249, loss: 1432.671631\n",
      "Train: step:  24090, time: 0.270, loss: 539.455139\n",
      "Train: step:  24100, time: 0.222, loss: 3365.175049\n",
      "Train: step:  24110, time: 0.250, loss: 1719.179321\n",
      "Train: step:  24120, time: 0.249, loss: 3625.691406\n",
      "Train: step:  24130, time: 0.228, loss: 3333.739258\n",
      "Train: step:  24140, time: 0.222, loss: 1251.600708\n",
      "Train: step:  24150, time: 0.253, loss: 2097.839355\n",
      "Train: step:  24160, time: 0.249, loss: 1539.112427\n",
      "Train: step:  24170, time: 0.248, loss: 1901.835449\n",
      "Train: step:  24180, time: 0.232, loss: 956.884277\n",
      "Train: step:  24190, time: 0.229, loss: 2956.533203\n",
      "Train: step:  24200, time: 0.231, loss: 1743.513794\n",
      "Train: step:  24210, time: 0.239, loss: 1517.844482\n",
      "Train: step:  24220, time: 0.256, loss: 2098.045654\n",
      "Train: step:  24230, time: 0.258, loss: 872.371033\n",
      "Train: step:  24240, time: 0.235, loss: 1505.281372\n",
      "Train: step:  24250, time: 0.226, loss: 1280.931030\n",
      "Train: step:  24260, time: 0.257, loss: 3237.403076\n",
      "Train: step:  24270, time: 0.230, loss: 1739.548828\n",
      "Train: step:  24280, time: 0.224, loss: 344.529419\n",
      "Train: step:  24290, time: 0.249, loss: 1605.139160\n",
      "Train: step:  24300, time: 0.274, loss: 1278.244263\n",
      "Train: step:  24310, time: 0.225, loss: 997.879822\n",
      "Train: step:  24320, time: 0.218, loss: 3088.521484\n",
      "Train: step:  24330, time: 0.256, loss: 1490.067139\n",
      "Train: step:  24340, time: 0.231, loss: 1310.963501\n",
      "Train: step:  24350, time: 0.220, loss: 1142.047485\n",
      "Train: step:  24360, time: 0.235, loss: 1526.776245\n",
      "Train: step:  24370, time: 0.249, loss: 1693.728760\n",
      "Train: step:  24380, time: 0.221, loss: 1645.185913\n",
      "Train: step:  24390, time: 0.258, loss: 237.531891\n",
      "Train: step:  24400, time: 0.230, loss: 1729.444946\n",
      "Train: step:  24410, time: 0.231, loss: 2926.139648\n",
      "Train: step:  24420, time: 0.226, loss: 1035.117676\n",
      "Train: step:  24430, time: 0.224, loss: 2377.583252\n",
      "Train: step:  24440, time: 0.253, loss: 2229.660400\n",
      "Train: step:  24450, time: 0.248, loss: 1514.233398\n",
      "Train: step:  24460, time: 0.246, loss: 551.533569\n",
      "Train: step:  24470, time: 0.251, loss: 1390.150879\n",
      "Train: step:  24480, time: 0.230, loss: 1951.310059\n",
      "Train: step:  24490, time: 0.229, loss: 3268.406494\n",
      "Train: step:  24500, time: 0.234, loss: 1918.029663\n",
      "Train: step:  24510, time: 0.239, loss: 1537.192017\n",
      "Train: step:  24520, time: 0.252, loss: 938.909363\n",
      "Train: step:  24530, time: 0.249, loss: 1596.579834\n",
      "Train: step:  24540, time: 0.256, loss: 2896.102783\n",
      "Train: step:  24550, time: 0.224, loss: 1751.784424\n",
      "Train: step:  24560, time: 0.266, loss: 2392.264893\n",
      "Train: step:  24570, time: 0.261, loss: 3332.603027\n",
      "Train: step:  24580, time: 0.234, loss: 2020.362427\n",
      "Train: step:  24590, time: 0.248, loss: 575.488281\n",
      "Train: step:  24600, time: 0.222, loss: 1465.558228\n",
      "Train: step:  24610, time: 0.254, loss: 2787.367432\n",
      "Train: step:  24620, time: 0.227, loss: 714.917664\n",
      "Train: step:  24630, time: 0.253, loss: 2689.776855\n",
      "Train: step:  24640, time: 0.251, loss: 1125.039307\n",
      "Train: step:  24650, time: 0.259, loss: 1478.102051\n",
      "Train: step:  24660, time: 0.258, loss: 290.017365\n",
      "Train: step:  24670, time: 0.262, loss: 1200.741577\n",
      "Train: step:  24680, time: 0.304, loss: 531.527710\n",
      "Train: step:  24690, time: 0.239, loss: 1782.475952\n",
      "Train: step:  24700, time: 0.256, loss: 3358.465820\n",
      "Train: step:  24710, time: 0.254, loss: 1958.202393\n",
      "Train: step:  24720, time: 0.267, loss: 1327.133789\n",
      "Train: step:  24730, time: 0.255, loss: 1144.076050\n",
      "Train: step:  24740, time: 0.243, loss: 2584.943604\n",
      "Train: step:  24750, time: 0.257, loss: 1499.903687\n",
      "Train: step:  24760, time: 0.260, loss: 996.504150\n",
      "Train: step:  24770, time: 0.254, loss: 589.527588\n",
      "Train: step:  24780, time: 0.268, loss: 3337.511719\n",
      "Train: step:  24790, time: 0.279, loss: 2411.861084\n",
      "Train: step:  24800, time: 0.237, loss: 1271.787476\n",
      "Train: step:  24810, time: 0.268, loss: 1089.483643\n",
      "Train: step:  24820, time: 0.232, loss: 2279.379395\n",
      "Train: step:  24830, time: 0.288, loss: 559.402649\n",
      "Train: step:  24840, time: 0.272, loss: 2778.401123\n",
      "Train: step:  24850, time: 0.242, loss: 1250.357422\n",
      "Train: step:  24860, time: 0.286, loss: 3992.202148\n",
      "Train: step:  24870, time: 0.237, loss: 621.897339\n",
      "Train: step:  24880, time: 0.281, loss: 1435.467407\n",
      "Train: step:  24890, time: 0.239, loss: 5207.639160\n",
      "Train: step:  24900, time: 0.240, loss: 2285.244629\n",
      "Train: step:  24910, time: 0.234, loss: 2000.943359\n",
      "Train: step:  24920, time: 0.239, loss: 1915.382690\n",
      "Train: step:  24930, time: 0.244, loss: 1931.841675\n",
      "Train: step:  24940, time: 0.237, loss: 1809.712891\n",
      "Train: step:  24950, time: 0.250, loss: 1863.150146\n",
      "Train: step:  24960, time: 0.237, loss: 2470.252686\n",
      "Train: step:  24970, time: 0.236, loss: 2532.888184\n",
      "Train: step:  24980, time: 0.257, loss: 2230.083740\n",
      "Train: step:  24990, time: 0.239, loss: 1435.195923\n",
      "Train: step:  25000, time: 0.268, loss: 2797.446289\n",
      "Train: step:  25010, time: 0.231, loss: 1834.964966\n",
      "Train: step:  25020, time: 0.249, loss: 2458.472656\n",
      "Train: step:  25030, time: 0.269, loss: 1228.997314\n",
      "Train: step:  25040, time: 0.250, loss: 1546.803833\n",
      "Train: step:  25050, time: 0.239, loss: 1111.529785\n",
      "Train: step:  25060, time: 0.265, loss: 1695.998657\n",
      "Train: step:  25070, time: 0.249, loss: 2254.382324\n",
      "Train: step:  25080, time: 0.241, loss: 2002.791504\n",
      "Train: step:  25090, time: 0.268, loss: 3170.586182\n",
      "Train: step:  25100, time: 0.245, loss: 1105.430786\n",
      "Train: step:  25110, time: 0.246, loss: 4274.970215\n",
      "Train: step:  25120, time: 0.263, loss: 1671.457764\n",
      "Train: step:  25130, time: 0.252, loss: 698.259705\n",
      "Train: step:  25140, time: 0.248, loss: 2168.195068\n",
      "Train: step:  25150, time: 0.245, loss: 1747.409668\n",
      "Train: step:  25160, time: 0.232, loss: 387.126770\n",
      "Train: step:  25170, time: 0.258, loss: 1969.607544\n",
      "Train: step:  25180, time: 0.236, loss: 4761.980469\n",
      "Train: step:  25190, time: 0.251, loss: 2661.400391\n",
      "Train: step:  25200, time: 0.229, loss: 2908.975830\n",
      "Train: step:  25210, time: 0.288, loss: 1267.710327\n",
      "Train: step:  25220, time: 0.253, loss: 1136.454346\n",
      "Train: step:  25230, time: 0.240, loss: 1819.656128\n",
      "Train: step:  25240, time: 0.282, loss: 1167.024658\n",
      "Train: step:  25250, time: 0.231, loss: 678.659180\n",
      "Train: step:  25260, time: 0.229, loss: 1655.445435\n",
      "Train: step:  25270, time: 0.253, loss: 2915.535400\n",
      "Train: step:  25280, time: 0.238, loss: 723.119446\n",
      "Train: step:  25290, time: 0.265, loss: 2607.762451\n",
      "Train: step:  25300, time: 0.224, loss: 3320.929932\n",
      "Train: step:  25310, time: 0.253, loss: 2251.038574\n",
      "Train: step:  25320, time: 0.232, loss: 1452.309692\n",
      "Train: step:  25330, time: 0.265, loss: 900.543518\n",
      "Train: step:  25340, time: 0.252, loss: 1982.380249\n",
      "Train: step:  25350, time: 0.232, loss: 3557.469971\n",
      "Train: step:  25360, time: 0.234, loss: 822.962830\n",
      "Train: step:  25370, time: 0.235, loss: 646.427795\n",
      "Train: step:  25380, time: 0.233, loss: 1336.910278\n",
      "Train: step:  25390, time: 0.257, loss: 2727.526367\n",
      "Train: step:  25400, time: 0.270, loss: 503.447815\n",
      "Train: step:  25410, time: 0.226, loss: 1651.950439\n",
      "Train: step:  25420, time: 0.244, loss: 1090.009277\n",
      "Train: step:  25430, time: 0.232, loss: 2188.181885\n",
      "Train: step:  25440, time: 0.245, loss: 1711.685425\n",
      "Train: step:  25450, time: 0.234, loss: 2757.975830\n",
      "Train: step:  25460, time: 0.252, loss: 802.685974\n",
      "Train: step:  25470, time: 0.239, loss: 3514.486816\n",
      "Train: step:  25480, time: 0.231, loss: 604.265808\n",
      "Train: step:  25490, time: 0.267, loss: 2387.450928\n",
      "Train: step:  25500, time: 0.261, loss: 1993.376343\n",
      "Train: step:  25510, time: 0.236, loss: 897.673279\n",
      "Train: step:  25520, time: 0.217, loss: 2730.047363\n",
      "Train: step:  25530, time: 0.239, loss: 1599.754150\n",
      "Train: step:  25540, time: 0.274, loss: 960.912109\n",
      "Train: step:  25550, time: 0.244, loss: 3357.819336\n",
      "Train: step:  25560, time: 0.236, loss: 1662.989380\n",
      "Train: step:  25570, time: 0.249, loss: 2297.614258\n",
      "Train: step:  25580, time: 0.242, loss: 1750.499634\n",
      "Train: step:  25590, time: 0.231, loss: 2816.647949\n",
      "Train: step:  25600, time: 0.232, loss: 744.248596\n",
      "Train: step:  25610, time: 0.245, loss: 3090.695557\n",
      "Train: step:  25620, time: 0.233, loss: 1598.862793\n",
      "Train: step:  25630, time: 0.260, loss: 1099.540405\n",
      "Train: step:  25640, time: 0.229, loss: 947.886841\n",
      "Train: step:  25650, time: 0.224, loss: 2828.357910\n",
      "Train: step:  25660, time: 0.236, loss: 1327.908813\n",
      "Train: step:  25670, time: 0.224, loss: 1693.277222\n",
      "Train: step:  25680, time: 0.255, loss: 1727.589966\n",
      "Train: step:  25690, time: 0.226, loss: 1751.729126\n",
      "Train: step:  25700, time: 0.254, loss: 1419.841064\n",
      "Train: step:  25710, time: 0.222, loss: 469.744568\n",
      "Train: step:  25720, time: 0.230, loss: 496.736786\n",
      "Train: step:  25730, time: 0.244, loss: 3955.176270\n",
      "Train: step:  25740, time: 0.248, loss: 342.301514\n",
      "Train: step:  25750, time: 0.237, loss: 1162.101685\n",
      "Train: step:  25760, time: 0.237, loss: 1984.869263\n",
      "Train: step:  25770, time: 0.253, loss: 907.674194\n",
      "Train: step:  25780, time: 0.232, loss: 1416.039673\n",
      "Train: step:  25790, time: 0.264, loss: 2823.020508\n",
      "Train: step:  25800, time: 0.230, loss: 941.266724\n",
      "Train: step:  25810, time: 0.243, loss: 1239.403442\n",
      "Train: step:  25820, time: 0.250, loss: 526.976318\n",
      "Train: step:  25830, time: 0.230, loss: 589.560730\n",
      "Train: step:  25840, time: 0.232, loss: 1655.734863\n",
      "Train: step:  25850, time: 0.226, loss: 969.423279\n",
      "Train: step:  25860, time: 0.235, loss: 2033.521362\n",
      "Train: step:  25870, time: 0.284, loss: 1749.013672\n",
      "Train: step:  25880, time: 0.222, loss: 565.117615\n",
      "Train: step:  25890, time: 0.224, loss: 473.246765\n",
      "Train: step:  25900, time: 0.246, loss: 739.336182\n",
      "Train: step:  25910, time: 0.231, loss: 2948.746582\n",
      "Train: step:  25920, time: 0.229, loss: 1952.363770\n",
      "Train: step:  25930, time: 0.233, loss: 2897.871582\n",
      "Train: step:  25940, time: 0.267, loss: 1625.781860\n",
      "Train: step:  25950, time: 0.253, loss: 2609.650879\n",
      "Train: step:  25960, time: 0.232, loss: 1964.629272\n",
      "Train: step:  25970, time: 0.254, loss: 2438.061035\n",
      "Train: step:  25980, time: 0.272, loss: 694.119873\n",
      "Train: step:  25990, time: 0.222, loss: 927.257935\n",
      "Train: step:  26000, time: 0.256, loss: 820.433838\n",
      "Train: step:  26010, time: 0.237, loss: 4071.314697\n",
      "Train: step:  26020, time: 0.245, loss: 1446.368896\n",
      "Train: step:  26030, time: 0.234, loss: 1616.338135\n",
      "Train: step:  26040, time: 0.276, loss: 1764.573975\n",
      "Train: step:  26050, time: 0.232, loss: 1749.331177\n",
      "Train: step:  26060, time: 0.235, loss: 2432.376221\n",
      "Train: step:  26070, time: 0.230, loss: 1743.655640\n",
      "Train: step:  26080, time: 0.225, loss: 1188.949951\n",
      "Train: step:  26090, time: 0.233, loss: 1195.134521\n",
      "Train: step:  26100, time: 0.242, loss: 2636.129883\n",
      "Train: step:  26110, time: 0.258, loss: 1542.706055\n",
      "Train: step:  26120, time: 0.237, loss: 1882.699097\n",
      "Train: step:  26130, time: 0.253, loss: 1787.490234\n",
      "Train: step:  26140, time: 0.246, loss: 3790.651611\n",
      "Train: step:  26150, time: 0.260, loss: 2132.432861\n",
      "Train: step:  26160, time: 0.252, loss: 672.960205\n",
      "Train: step:  26170, time: 0.240, loss: 863.058655\n",
      "Train: step:  26180, time: 0.236, loss: 2145.030518\n",
      "Train: step:  26190, time: 0.244, loss: 1974.399902\n",
      "Train: step:  26200, time: 0.242, loss: 422.047150\n",
      "Train: step:  26210, time: 0.251, loss: 2024.510498\n",
      "Train: step:  26220, time: 0.242, loss: 2065.261719\n",
      "Train: step:  26230, time: 0.256, loss: 3442.746094\n",
      "Train: step:  26240, time: 0.246, loss: 1307.975830\n",
      "Train: step:  26250, time: 0.258, loss: 1463.503296\n",
      "Train: step:  26260, time: 0.240, loss: 2587.757324\n",
      "Train: step:  26270, time: 0.245, loss: 1091.909424\n",
      "Train: step:  26280, time: 0.238, loss: 874.329285\n",
      "Train: step:  26290, time: 0.245, loss: 1804.179688\n",
      "Train: step:  26300, time: 0.243, loss: 2455.388428\n",
      "Train: step:  26310, time: 0.261, loss: 2238.563477\n",
      "Train: step:  26320, time: 0.240, loss: 2144.230957\n",
      "Train: step:  26330, time: 0.230, loss: 2063.718750\n",
      "Train: step:  26340, time: 0.241, loss: 1186.250488\n",
      "Train: step:  26350, time: 0.250, loss: 2006.281982\n",
      "Train: step:  26360, time: 0.234, loss: 1594.852417\n",
      "Train: step:  26370, time: 0.241, loss: 2396.502686\n",
      "Train: step:  26380, time: 0.235, loss: 2149.193115\n",
      "Train: step:  26390, time: 0.234, loss: 659.546936\n",
      "Train: step:  26400, time: 0.270, loss: 2079.407471\n",
      "Train: step:  26410, time: 0.235, loss: 1580.169189\n",
      "Train: step:  26420, time: 0.249, loss: 2272.966797\n",
      "Train: step:  26430, time: 0.232, loss: 1494.648193\n",
      "Train: step:  26440, time: 0.239, loss: 3394.904297\n",
      "Train: step:  26450, time: 0.247, loss: 3694.637695\n",
      "Train: step:  26460, time: 0.231, loss: 1114.922974\n",
      "Train: step:  26470, time: 0.241, loss: 1528.642944\n",
      "Train: step:  26480, time: 0.244, loss: 2636.857666\n",
      "Train: step:  26490, time: 0.264, loss: 1350.506104\n",
      "Train: step:  26500, time: 0.263, loss: 1309.912964\n",
      "Train: step:  26510, time: 0.260, loss: 3410.057373\n",
      "Train: step:  26520, time: 0.231, loss: 2220.281738\n",
      "Train: step:  26530, time: 0.258, loss: 1681.715820\n",
      "Train: step:  26540, time: 0.239, loss: 1923.998291\n",
      "Train: step:  26550, time: 0.273, loss: 1467.187134\n",
      "Train: step:  26560, time: 0.230, loss: 3105.939697\n",
      "Train: step:  26570, time: 0.232, loss: 1438.922729\n",
      "Train: step:  26580, time: 0.227, loss: 2268.418945\n",
      "Train: step:  26590, time: 0.226, loss: 427.536835\n",
      "Train: step:  26600, time: 0.228, loss: 827.452698\n",
      "Train: step:  26610, time: 0.237, loss: 304.912628\n",
      "Train: step:  26620, time: 0.251, loss: 3707.657959\n",
      "Train: step:  26630, time: 0.230, loss: 1652.874512\n",
      "Train: step:  26640, time: 0.251, loss: 2891.847900\n",
      "Train: step:  26650, time: 0.224, loss: 1952.063843\n",
      "Train: step:  26660, time: 0.241, loss: 672.899536\n",
      "Train: step:  26670, time: 0.227, loss: 1362.383911\n",
      "Train: step:  26680, time: 0.222, loss: 595.015259\n",
      "Train: step:  26690, time: 0.265, loss: 280.521118\n",
      "Train: step:  26700, time: 0.242, loss: 976.609497\n",
      "Train: step:  26710, time: 0.231, loss: 1067.873901\n",
      "Train: step:  26720, time: 0.237, loss: 643.188538\n",
      "Train: step:  26730, time: 0.236, loss: 2355.380127\n",
      "Train: step:  26740, time: 0.231, loss: 1743.291138\n",
      "Train: step:  26750, time: 0.231, loss: 4342.605469\n",
      "Train: step:  26760, time: 0.241, loss: 758.851562\n",
      "Train: step:  26770, time: 0.238, loss: 871.578857\n",
      "Train: step:  26780, time: 0.230, loss: 1594.598145\n",
      "Train: step:  26790, time: 0.228, loss: 1843.284790\n",
      "Train: step:  26800, time: 0.231, loss: 717.770630\n",
      "Train: step:  26810, time: 0.225, loss: 1808.634277\n",
      "Train: step:  26820, time: 0.232, loss: 640.510437\n",
      "Train: step:  26830, time: 0.238, loss: 1783.565918\n",
      "Train: step:  26840, time: 0.238, loss: 2079.521729\n",
      "Train: step:  26850, time: 0.222, loss: 1081.840088\n",
      "Train: step:  26860, time: 0.234, loss: 3011.805664\n",
      "Train: step:  26870, time: 0.250, loss: 1487.125977\n",
      "Train: step:  26880, time: 0.220, loss: 2321.402832\n",
      "Train: step:  26890, time: 0.251, loss: 475.260742\n",
      "Train: step:  26900, time: 0.221, loss: 1742.383423\n",
      "Train: step:  26910, time: 0.231, loss: 1274.514404\n",
      "Train: step:  26920, time: 0.272, loss: 359.205841\n",
      "Train: step:  26930, time: 0.250, loss: 1601.109131\n",
      "Train: step:  26940, time: 0.249, loss: 1620.724121\n",
      "Train: step:  26950, time: 0.238, loss: 1723.913452\n",
      "Train: step:  26960, time: 0.236, loss: 3995.503174\n",
      "Train: step:  26970, time: 0.237, loss: 1493.434570\n",
      "Train: step:  26980, time: 0.236, loss: 1084.839478\n",
      "Train: step:  26990, time: 0.229, loss: 3066.593018\n",
      "Train: step:  27000, time: 0.262, loss: 349.060852\n",
      "Train: step:  27010, time: 0.260, loss: 1136.941162\n",
      "Train: step:  27020, time: 0.250, loss: 3004.755859\n",
      "Train: step:  27030, time: 0.262, loss: 2875.313477\n",
      "Train: step:  27040, time: 0.276, loss: 2154.601562\n",
      "Train: step:  27050, time: 0.229, loss: 777.417175\n",
      "Train: step:  27060, time: 0.226, loss: 1441.224609\n",
      "Train: step:  27070, time: 0.223, loss: 2588.325439\n",
      "Train: step:  27080, time: 0.231, loss: 909.267883\n",
      "Train: step:  27090, time: 0.261, loss: 1403.965576\n",
      "Train: step:  27100, time: 0.227, loss: 314.944336\n",
      "Train: step:  27110, time: 0.263, loss: 2790.776123\n",
      "Train: step:  27120, time: 0.230, loss: 2902.452881\n",
      "Train: step:  27130, time: 0.224, loss: 1208.467041\n",
      "Train: step:  27140, time: 0.230, loss: 1222.383667\n",
      "Train: step:  27150, time: 0.234, loss: 1389.012573\n",
      "Train: step:  27160, time: 0.258, loss: 3143.201172\n",
      "Train: step:  27170, time: 0.266, loss: 3066.909180\n",
      "Train: step:  27180, time: 0.220, loss: 1257.518677\n",
      "Train: step:  27190, time: 0.272, loss: 1191.884033\n",
      "Train: step:  27200, time: 0.233, loss: 2422.402344\n",
      "Train: step:  27210, time: 0.238, loss: 579.523254\n",
      "Train: step:  27220, time: 0.252, loss: 1883.172852\n",
      "Train: step:  27230, time: 0.264, loss: 2028.579590\n",
      "Train: step:  27240, time: 0.234, loss: 2723.111816\n",
      "Train: step:  27250, time: 0.229, loss: 384.063293\n",
      "Train: step:  27260, time: 0.233, loss: 3585.746826\n",
      "Train: step:  27270, time: 0.230, loss: 3540.393311\n",
      "Train: step:  27280, time: 0.254, loss: 1215.372559\n",
      "Train: step:  27290, time: 0.234, loss: 2012.538330\n",
      "Train: step:  27300, time: 0.240, loss: 966.810974\n",
      "Train: step:  27310, time: 0.296, loss: 2625.424316\n",
      "Train: step:  27320, time: 0.244, loss: 2344.653809\n",
      "Train: step:  27330, time: 0.240, loss: 2322.306885\n",
      "Train: step:  27340, time: 0.260, loss: 1935.178467\n",
      "Train: step:  27350, time: 0.253, loss: 2190.067871\n",
      "Train: step:  27360, time: 0.232, loss: 3612.336182\n",
      "Train: step:  27370, time: 0.235, loss: 2516.898193\n",
      "Train: step:  27380, time: 0.233, loss: 2627.972168\n",
      "Train: step:  27390, time: 0.251, loss: 2119.359131\n",
      "Train: step:  27400, time: 0.230, loss: 2068.806885\n",
      "Train: step:  27410, time: 0.236, loss: 2544.653076\n",
      "Train: step:  27420, time: 0.231, loss: 1804.728149\n",
      "Train: step:  27430, time: 0.255, loss: 3133.450439\n",
      "Train: step:  27440, time: 0.232, loss: 1613.161621\n",
      "Train: step:  27450, time: 0.253, loss: 2071.526855\n",
      "Train: step:  27460, time: 0.252, loss: 2635.472656\n",
      "Train: step:  27470, time: 0.232, loss: 2283.626709\n",
      "Train: step:  27480, time: 0.247, loss: 3505.709229\n",
      "Train: step:  27490, time: 0.230, loss: 2116.927002\n",
      "Train: step:  27500, time: 0.220, loss: 2539.303711\n",
      "Train: step:  27510, time: 0.232, loss: 2155.972656\n",
      "Train: step:  27520, time: 0.233, loss: 371.786530\n",
      "Train: step:  27530, time: 0.229, loss: 1142.627686\n",
      "Train: step:  27540, time: 0.232, loss: 1378.833862\n",
      "Train: step:  27550, time: 0.228, loss: 1788.463501\n",
      "Train: step:  27560, time: 0.234, loss: 556.461365\n",
      "Train: step:  27570, time: 0.226, loss: 1691.412720\n",
      "Train: step:  27580, time: 0.266, loss: 2020.813843\n",
      "Train: step:  27590, time: 0.229, loss: 2219.228271\n",
      "Train: step:  27600, time: 0.225, loss: 2578.647949\n",
      "Train: step:  27610, time: 0.234, loss: 2063.633057\n",
      "Train: step:  27620, time: 0.235, loss: 686.864014\n",
      "Train: step:  27630, time: 0.238, loss: 2291.967041\n",
      "Train: step:  27640, time: 0.233, loss: 1181.643188\n",
      "Train: step:  27650, time: 0.229, loss: 2488.071289\n",
      "Train: step:  27660, time: 0.278, loss: 1479.473633\n",
      "Train: step:  27670, time: 0.261, loss: 3749.737793\n",
      "Train: step:  27680, time: 0.230, loss: 525.624695\n",
      "Train: step:  27690, time: 0.223, loss: 2379.078857\n",
      "Train: step:  27700, time: 0.230, loss: 2751.207031\n",
      "Train: step:  27710, time: 0.230, loss: 1937.225708\n",
      "Train: step:  27720, time: 0.237, loss: 2565.396973\n",
      "Train: step:  27730, time: 0.231, loss: 2142.334961\n",
      "Train: step:  27740, time: 0.261, loss: 2679.083008\n",
      "Train: step:  27750, time: 0.254, loss: 2056.026855\n",
      "Train: step:  27760, time: 0.239, loss: 2566.471924\n",
      "Train: step:  27770, time: 0.240, loss: 1673.499390\n",
      "Train: step:  27780, time: 0.240, loss: 2408.179443\n",
      "Train: step:  27790, time: 0.232, loss: 1323.041626\n",
      "Train: step:  27800, time: 0.236, loss: 2163.866211\n",
      "Train: step:  27810, time: 0.236, loss: 2100.695557\n",
      "Train: step:  27820, time: 0.293, loss: 3433.071533\n",
      "Train: step:  27830, time: 0.232, loss: 1408.166504\n",
      "Train: step:  27840, time: 0.232, loss: 2260.127930\n",
      "Train: step:  27850, time: 0.279, loss: 3404.415283\n",
      "Train: step:  27860, time: 0.228, loss: 948.349609\n",
      "Train: step:  27870, time: 0.231, loss: 2560.785156\n",
      "Train: step:  27880, time: 0.227, loss: 3284.873535\n",
      "Train: step:  27890, time: 0.228, loss: 3212.450928\n",
      "Train: step:  27900, time: 0.232, loss: 422.826996\n",
      "Train: step:  27910, time: 0.237, loss: 2831.845459\n",
      "Train: step:  27920, time: 0.232, loss: 1093.326782\n",
      "Train: step:  27930, time: 0.262, loss: 1191.930054\n",
      "Train: step:  27940, time: 0.230, loss: 1849.799438\n",
      "Train: step:  27950, time: 0.235, loss: 2547.391602\n",
      "Train: step:  27960, time: 0.235, loss: 1711.740601\n",
      "Train: step:  27970, time: 0.251, loss: 1815.604248\n",
      "Train: step:  27980, time: 0.245, loss: 887.865906\n",
      "Train: step:  27990, time: 0.231, loss: 1515.926514\n",
      "Train: step:  28000, time: 0.226, loss: 2995.908447\n",
      "Train: step:  28010, time: 0.264, loss: 2003.558838\n",
      "Train: step:  28020, time: 0.229, loss: 1794.754883\n",
      "Train: step:  28030, time: 0.224, loss: 2296.184082\n",
      "Train: step:  28040, time: 0.249, loss: 2819.102295\n",
      "Train: step:  28050, time: 0.230, loss: 1166.047363\n",
      "Train: step:  28060, time: 0.226, loss: 814.476929\n",
      "Train: step:  28070, time: 0.254, loss: 2915.383301\n",
      "Train: step:  28080, time: 0.236, loss: 1021.361267\n",
      "Train: step:  28090, time: 0.229, loss: 2112.854980\n",
      "Train: step:  28100, time: 0.234, loss: 2306.490723\n",
      "Train: step:  28110, time: 0.224, loss: 1699.418701\n",
      "Train: step:  28120, time: 0.228, loss: 1304.597900\n",
      "Train: step:  28130, time: 0.230, loss: 2515.849854\n",
      "Train: step:  28140, time: 0.234, loss: 2229.076660\n",
      "Train: step:  28150, time: 0.269, loss: 988.993225\n",
      "Train: step:  28160, time: 0.230, loss: 2588.115967\n",
      "Train: step:  28170, time: 0.254, loss: 1838.583130\n",
      "Train: step:  28180, time: 0.234, loss: 1269.956543\n",
      "Train: step:  28190, time: 0.231, loss: 883.521667\n",
      "Train: step:  28200, time: 0.226, loss: 299.500519\n",
      "Train: step:  28210, time: 0.250, loss: 2153.757324\n",
      "Train: step:  28220, time: 0.231, loss: 2559.806641\n",
      "Train: step:  28230, time: 0.249, loss: 1763.055542\n",
      "Train: step:  28240, time: 0.225, loss: 469.859161\n",
      "Train: step:  28250, time: 0.239, loss: 360.033752\n",
      "Train: step:  28260, time: 0.232, loss: 2107.497070\n",
      "Train: step:  28270, time: 0.277, loss: 2309.831055\n",
      "Train: step:  28280, time: 0.230, loss: 753.313965\n",
      "Train: step:  28290, time: 0.242, loss: 2229.624756\n",
      "Train: step:  28300, time: 0.253, loss: 3724.168457\n",
      "Train: step:  28310, time: 0.272, loss: 2264.624756\n",
      "Train: step:  28320, time: 0.266, loss: 2638.670898\n",
      "Train: step:  28330, time: 0.235, loss: 2079.211914\n",
      "Train: step:  28340, time: 0.263, loss: 1249.125366\n",
      "Train: step:  28350, time: 0.240, loss: 266.796783\n",
      "Train: step:  28360, time: 0.236, loss: 779.227844\n",
      "Train: step:  28370, time: 0.236, loss: 844.101501\n",
      "Train: step:  28380, time: 0.260, loss: 1107.140259\n",
      "Train: step:  28390, time: 0.229, loss: 2555.722900\n",
      "Train: step:  28400, time: 0.221, loss: 3215.929688\n",
      "Train: step:  28410, time: 0.242, loss: 4218.888184\n",
      "Train: step:  28420, time: 0.226, loss: 466.332245\n",
      "Train: step:  28430, time: 0.227, loss: 1721.193359\n",
      "Train: step:  28440, time: 0.254, loss: 799.910461\n",
      "Train: step:  28450, time: 0.228, loss: 2461.303711\n",
      "Train: step:  28460, time: 0.280, loss: 2371.423340\n",
      "Train: step:  28470, time: 0.228, loss: 2326.686035\n",
      "Train: step:  28480, time: 0.241, loss: 1900.147949\n",
      "Train: step:  28490, time: 0.224, loss: 542.895264\n",
      "Train: step:  28500, time: 0.240, loss: 1925.933838\n",
      "Train: step:  28510, time: 0.237, loss: 3280.921875\n",
      "Train: step:  28520, time: 0.228, loss: 3062.916016\n",
      "Train: step:  28530, time: 0.234, loss: 235.360825\n",
      "Train: step:  28540, time: 0.237, loss: 764.005554\n",
      "Train: step:  28550, time: 0.235, loss: 3544.316406\n",
      "Train: step:  28560, time: 0.240, loss: 1290.746216\n",
      "Train: step:  28570, time: 0.229, loss: 2550.115967\n",
      "Train: step:  28580, time: 0.242, loss: 1665.938965\n",
      "Train: step:  28590, time: 0.268, loss: 1216.894531\n",
      "Train: step:  28600, time: 0.249, loss: 1234.208008\n",
      "Train: step:  28610, time: 0.252, loss: 831.207825\n",
      "Train: step:  28620, time: 0.233, loss: 1218.738403\n",
      "Train: step:  28630, time: 0.252, loss: 1934.897339\n",
      "Train: step:  28640, time: 0.243, loss: 1614.035645\n",
      "Train: step:  28650, time: 0.232, loss: 744.670349\n",
      "Train: step:  28660, time: 0.227, loss: 581.366516\n",
      "Train: step:  28670, time: 0.231, loss: 1102.688721\n",
      "Train: step:  28680, time: 0.230, loss: 1590.426025\n",
      "Train: step:  28690, time: 0.258, loss: 1132.814819\n",
      "Train: step:  28700, time: 0.250, loss: 784.837646\n",
      "Train: step:  28710, time: 0.227, loss: 2227.065674\n",
      "Train: step:  28720, time: 0.234, loss: 1972.373657\n",
      "Train: step:  28730, time: 0.227, loss: 347.897003\n",
      "Train: step:  28740, time: 0.252, loss: 656.551270\n",
      "Train: step:  28750, time: 0.232, loss: 1733.155762\n",
      "Train: step:  28760, time: 0.256, loss: 1366.422974\n",
      "Train: step:  28770, time: 0.267, loss: 2342.299561\n",
      "Train: step:  28780, time: 0.254, loss: 1900.482422\n",
      "Train: step:  28790, time: 0.237, loss: 2246.644043\n",
      "Train: step:  28800, time: 0.233, loss: 893.648926\n",
      "Train: step:  28810, time: 0.252, loss: 1111.683350\n",
      "Train: step:  28820, time: 0.230, loss: 1168.262451\n",
      "Train: step:  28830, time: 0.230, loss: 1809.826416\n",
      "Train: step:  28840, time: 0.246, loss: 1043.306763\n",
      "Train: step:  28850, time: 0.241, loss: 1732.605835\n",
      "Train: step:  28860, time: 0.228, loss: 2466.506836\n",
      "Train: step:  28870, time: 0.232, loss: 2213.241455\n",
      "Train: step:  28880, time: 0.238, loss: 3104.695557\n",
      "Train: step:  28890, time: 0.239, loss: 2724.556641\n",
      "Train: step:  28900, time: 0.221, loss: 1091.680054\n",
      "Train: step:  28910, time: 0.230, loss: 2536.948975\n",
      "Train: step:  28920, time: 0.229, loss: 2895.788574\n",
      "Train: step:  28930, time: 0.252, loss: 1998.572021\n",
      "Train: step:  28940, time: 0.230, loss: 2728.598633\n",
      "Train: step:  28950, time: 0.236, loss: 1355.103638\n",
      "Train: step:  28960, time: 0.232, loss: 3027.449707\n",
      "Train: step:  28970, time: 0.242, loss: 987.290466\n",
      "Train: step:  28980, time: 0.245, loss: 2469.468018\n",
      "Train: step:  28990, time: 0.269, loss: 1421.402100\n",
      "Train: step:  29000, time: 0.251, loss: 1881.083008\n",
      "Train: step:  29010, time: 0.225, loss: 2436.095947\n",
      "Train: step:  29020, time: 0.261, loss: 738.309570\n",
      "Train: step:  29030, time: 0.279, loss: 2392.949463\n",
      "Train: step:  29040, time: 0.231, loss: 2047.954224\n",
      "Train: step:  29050, time: 0.230, loss: 626.726685\n",
      "Train: step:  29060, time: 0.236, loss: 1554.400024\n",
      "Train: step:  29070, time: 0.231, loss: 1977.643433\n",
      "Train: step:  29080, time: 0.227, loss: 841.599060\n",
      "Train: step:  29090, time: 0.242, loss: 4259.674805\n",
      "Train: step:  29100, time: 0.260, loss: 2458.088379\n",
      "Train: step:  29110, time: 0.224, loss: 2743.042969\n",
      "Train: step:  29120, time: 0.233, loss: 1226.414673\n",
      "Train: step:  29130, time: 0.256, loss: 981.292358\n",
      "Train: step:  29140, time: 0.232, loss: 979.907776\n",
      "Train: step:  29150, time: 0.230, loss: 1879.463745\n",
      "Train: step:  29160, time: 0.253, loss: 228.789886\n",
      "Train: step:  29170, time: 0.245, loss: 1457.331543\n",
      "Train: step:  29180, time: 0.260, loss: 466.024750\n",
      "Train: step:  29190, time: 0.257, loss: 3333.243408\n",
      "Train: step:  29200, time: 0.236, loss: 1483.471802\n",
      "Train: step:  29210, time: 0.227, loss: 957.871033\n",
      "Train: step:  29220, time: 0.231, loss: 687.098694\n",
      "Train: step:  29230, time: 0.252, loss: 993.528748\n",
      "Train: step:  29240, time: 0.227, loss: 1490.323242\n",
      "Train: step:  29250, time: 0.230, loss: 3290.839600\n",
      "Train: step:  29260, time: 0.252, loss: 801.391418\n",
      "Train: step:  29270, time: 0.240, loss: 1595.385010\n",
      "Train: step:  29280, time: 0.255, loss: 1566.522949\n",
      "Train: step:  29290, time: 0.231, loss: 870.970947\n",
      "Train: step:  29300, time: 0.226, loss: 1830.322021\n",
      "Train: step:  29310, time: 0.236, loss: 1682.976929\n",
      "Train: step:  29320, time: 0.241, loss: 1722.718384\n",
      "Train: step:  29330, time: 0.234, loss: 524.075134\n",
      "Train: step:  29340, time: 0.237, loss: 1836.131714\n",
      "Train: step:  29350, time: 0.253, loss: 2439.967285\n",
      "Train: step:  29360, time: 0.258, loss: 3590.980469\n",
      "Train: step:  29370, time: 0.240, loss: 1437.358398\n",
      "Train: step:  29380, time: 0.237, loss: 1709.874390\n",
      "Train: step:  29390, time: 0.252, loss: 3074.395508\n",
      "Train: step:  29400, time: 0.237, loss: 2606.707275\n",
      "Train: step:  29410, time: 0.250, loss: 915.692200\n",
      "Train: step:  29420, time: 0.224, loss: 2285.140869\n",
      "Train: step:  29430, time: 0.224, loss: 1845.562256\n",
      "Train: step:  29440, time: 0.224, loss: 2709.648682\n",
      "Train: step:  29450, time: 0.223, loss: 3072.306641\n",
      "Train: step:  29460, time: 0.227, loss: 3076.364502\n",
      "Train: step:  29470, time: 0.227, loss: 693.362122\n",
      "Train: step:  29480, time: 0.227, loss: 797.500854\n",
      "Train: step:  29490, time: 0.234, loss: 3909.740723\n",
      "Train: step:  29500, time: 0.223, loss: 2750.945312\n",
      "Train: step:  29510, time: 0.232, loss: 860.068726\n",
      "Train: step:  29520, time: 0.228, loss: 765.339294\n",
      "Train: step:  29530, time: 0.275, loss: 348.047455\n",
      "Train: step:  29540, time: 0.225, loss: 2943.215820\n",
      "Train: step:  29550, time: 0.217, loss: 3407.909912\n",
      "Train: step:  29560, time: 0.251, loss: 1772.188965\n",
      "Train: step:  29570, time: 0.263, loss: 1022.988647\n",
      "Train: step:  29580, time: 0.270, loss: 2092.552734\n",
      "Train: step:  29590, time: 0.235, loss: 1519.204590\n",
      "Train: step:  29600, time: 0.233, loss: 3606.287354\n",
      "Train: step:  29610, time: 0.251, loss: 1583.335327\n",
      "Train: step:  29620, time: 0.261, loss: 2099.976562\n",
      "Train: step:  29630, time: 0.273, loss: 2137.353516\n",
      "Train: step:  29640, time: 0.229, loss: 673.588867\n",
      "Train: step:  29650, time: 0.238, loss: 2668.222900\n",
      "Train: step:  29660, time: 0.254, loss: 1215.264648\n",
      "Train: step:  29670, time: 0.226, loss: 2626.267578\n",
      "Train: step:  29680, time: 0.229, loss: 3233.870850\n",
      "Train: step:  29690, time: 0.250, loss: 3124.107666\n",
      "Train: step:  29700, time: 0.259, loss: 386.177490\n",
      "Train: step:  29710, time: 0.258, loss: 2287.702393\n",
      "Train: step:  29720, time: 0.228, loss: 2492.798584\n",
      "Train: step:  29730, time: 0.232, loss: 848.696350\n",
      "Train: step:  29740, time: 0.233, loss: 2184.694336\n",
      "Train: step:  29750, time: 0.230, loss: 1333.098022\n",
      "Train: step:  29760, time: 0.232, loss: 1151.384033\n",
      "Train: step:  29770, time: 0.224, loss: 2494.279297\n",
      "Train: step:  29780, time: 0.235, loss: 1659.445435\n",
      "Train: step:  29790, time: 0.269, loss: 1067.744995\n",
      "Train: step:  29800, time: 0.228, loss: 4001.696045\n",
      "Train: step:  29810, time: 0.235, loss: 1539.156860\n",
      "Train: step:  29820, time: 0.235, loss: 433.805023\n",
      "Train: step:  29830, time: 0.236, loss: 2637.503906\n",
      "Train: step:  29840, time: 0.227, loss: 1378.331787\n",
      "Train: step:  29850, time: 0.277, loss: 1242.888428\n",
      "Train: step:  29860, time: 0.233, loss: 2540.399414\n",
      "Train: step:  29870, time: 0.232, loss: 1954.296753\n",
      "Train: step:  29880, time: 0.232, loss: 2008.104248\n",
      "Train: step:  29890, time: 0.231, loss: 1096.987549\n",
      "Train: step:  29900, time: 0.254, loss: 2348.256104\n",
      "Train: step:  29910, time: 0.244, loss: 1454.677490\n",
      "Train: step:  29920, time: 0.263, loss: 629.037964\n",
      "Train: step:  29930, time: 0.258, loss: 1760.901855\n",
      "Train: step:  29940, time: 0.232, loss: 2399.203857\n",
      "Train: step:  29950, time: 0.254, loss: 1958.248535\n",
      "Train: step:  29960, time: 0.228, loss: 1082.956787\n",
      "Train: step:  29970, time: 0.226, loss: 1690.952393\n",
      "Train: step:  29980, time: 0.234, loss: 2838.313965\n",
      "Train: step:  29990, time: 0.231, loss: 388.805420\n",
      "Train: step:  30000, time: 0.231, loss: 2964.844727\n",
      "Train: step:  30010, time: 0.231, loss: 1295.617554\n",
      "Train: step:  30020, time: 0.231, loss: 3337.026123\n",
      "Train: step:  30030, time: 0.233, loss: 3026.957520\n",
      "Train: step:  30040, time: 0.223, loss: 1331.249023\n",
      "Train: step:  30050, time: 0.226, loss: 749.283813\n",
      "Train: step:  30060, time: 0.228, loss: 3128.953369\n",
      "Train: step:  30070, time: 0.244, loss: 1608.602783\n",
      "Train: step:  30080, time: 0.229, loss: 1421.340210\n",
      "Train: step:  30090, time: 0.236, loss: 2416.818359\n",
      "Train: step:  30100, time: 0.235, loss: 2185.369873\n",
      "Train: step:  30110, time: 0.262, loss: 421.041809\n",
      "Train: step:  30120, time: 0.221, loss: 629.331665\n",
      "Train: step:  30130, time: 0.251, loss: 1709.752930\n",
      "Train: step:  30140, time: 0.258, loss: 1996.414307\n",
      "Train: step:  30150, time: 0.221, loss: 4574.505859\n",
      "Train: step:  30160, time: 0.240, loss: 1751.813354\n",
      "Train: step:  30170, time: 0.232, loss: 985.423523\n",
      "Train: step:  30180, time: 0.255, loss: 1738.182739\n",
      "Train: step:  30190, time: 0.234, loss: 663.883362\n",
      "Train: step:  30200, time: 0.298, loss: 1290.724976\n",
      "Train: step:  30210, time: 0.251, loss: 2340.598633\n",
      "Train: step:  30220, time: 0.250, loss: 2440.516602\n",
      "Train: step:  30230, time: 0.258, loss: 667.085632\n",
      "Train: step:  30240, time: 0.268, loss: 2943.946045\n",
      "Train: step:  30250, time: 0.257, loss: 1470.025513\n",
      "Train: step:  30260, time: 0.234, loss: 1052.394043\n",
      "Train: step:  30270, time: 0.225, loss: 1428.923340\n",
      "Train: step:  30280, time: 0.261, loss: 1802.618164\n",
      "Train: step:  30290, time: 0.257, loss: 675.391174\n",
      "Train: step:  30300, time: 0.267, loss: 1610.400879\n",
      "Train: step:  30310, time: 0.235, loss: 391.188324\n",
      "Train: step:  30320, time: 0.215, loss: 1450.676025\n",
      "Train: step:  30330, time: 0.236, loss: 1323.535156\n",
      "Train: step:  30340, time: 0.256, loss: 1487.431763\n",
      "Train: step:  30350, time: 0.253, loss: 3123.483398\n",
      "Train: step:  30360, time: 0.230, loss: 3913.049316\n",
      "Train: step:  30370, time: 0.242, loss: 1841.764404\n",
      "Train: step:  30380, time: 0.247, loss: 863.120605\n",
      "Train: step:  30390, time: 0.240, loss: 1932.091309\n",
      "Train: step:  30400, time: 0.225, loss: 2089.231445\n",
      "Train: step:  30410, time: 0.243, loss: 2157.752686\n",
      "Train: step:  30420, time: 0.254, loss: 3461.492188\n",
      "Train: step:  30430, time: 0.270, loss: 1956.626587\n",
      "Train: step:  30440, time: 0.235, loss: 2902.161133\n",
      "Train: step:  30450, time: 0.228, loss: 393.906433\n",
      "Train: step:  30460, time: 0.240, loss: 3545.929199\n",
      "Train: step:  30470, time: 0.237, loss: 2465.981201\n",
      "Train: step:  30480, time: 0.235, loss: 2033.556885\n",
      "Train: step:  30490, time: 0.226, loss: 902.101196\n",
      "Train: step:  30500, time: 0.237, loss: 1762.282593\n",
      "Train: step:  30510, time: 0.251, loss: 2986.165527\n",
      "Train: step:  30520, time: 0.277, loss: 1561.361572\n",
      "Train: step:  30530, time: 0.223, loss: 2888.201416\n",
      "Train: step:  30540, time: 0.234, loss: 1278.751343\n",
      "Train: step:  30550, time: 0.236, loss: 2506.978271\n",
      "Train: step:  30560, time: 0.246, loss: 232.399155\n",
      "Train: step:  30570, time: 0.255, loss: 505.641602\n",
      "Train: step:  30580, time: 0.236, loss: 1026.495483\n",
      "Train: step:  30590, time: 0.246, loss: 1286.132568\n",
      "Train: step:  30600, time: 0.266, loss: 792.867249\n",
      "Train: step:  30610, time: 0.233, loss: 1625.815918\n",
      "Train: step:  30620, time: 0.228, loss: 1812.147827\n",
      "Train: step:  30630, time: 0.229, loss: 1285.341431\n",
      "Train: step:  30640, time: 0.241, loss: 2196.051758\n",
      "Train: step:  30650, time: 0.253, loss: 1350.499390\n",
      "Train: step:  30660, time: 0.229, loss: 2938.655273\n",
      "Train: step:  30670, time: 0.253, loss: 3555.178223\n",
      "Train: step:  30680, time: 0.234, loss: 1607.514648\n",
      "Train: step:  30690, time: 0.233, loss: 377.112335\n",
      "Train: step:  30700, time: 0.263, loss: 2250.428955\n",
      "Train: step:  30710, time: 0.229, loss: 1809.818604\n",
      "Train: step:  30720, time: 0.257, loss: 1906.008667\n",
      "Train: step:  30730, time: 0.246, loss: 2798.679199\n",
      "Train: step:  30740, time: 0.273, loss: 2000.822632\n",
      "Train: step:  30750, time: 0.256, loss: 3190.521240\n",
      "Train: step:  30760, time: 0.275, loss: 2132.154297\n",
      "Train: step:  30770, time: 0.230, loss: 2630.698730\n",
      "Train: step:  30780, time: 0.259, loss: 3505.003906\n",
      "Train: step:  30790, time: 0.252, loss: 512.465149\n",
      "Train: step:  30800, time: 0.238, loss: 1917.980103\n",
      "Train: step:  30810, time: 0.251, loss: 852.986267\n",
      "Train: step:  30820, time: 0.267, loss: 3378.226807\n",
      "Train: step:  30830, time: 0.237, loss: 2280.557373\n",
      "Train: step:  30840, time: 0.233, loss: 2019.245850\n",
      "Train: step:  30850, time: 0.239, loss: 1487.762207\n",
      "Train: step:  30860, time: 0.248, loss: 1926.193115\n",
      "Train: step:  30870, time: 0.237, loss: 3290.668945\n",
      "Train: step:  30880, time: 0.226, loss: 995.328796\n",
      "Train: step:  30890, time: 0.239, loss: 1752.801392\n",
      "Train: step:  30900, time: 0.243, loss: 1826.580688\n",
      "Train: step:  30910, time: 0.235, loss: 1802.812012\n",
      "Train: step:  30920, time: 0.226, loss: 2089.987061\n",
      "Train: step:  30930, time: 0.242, loss: 422.657990\n",
      "Train: step:  30940, time: 0.250, loss: 1165.002441\n",
      "Train: step:  30950, time: 0.231, loss: 2842.053955\n",
      "Train: step:  30960, time: 0.229, loss: 3323.914551\n",
      "Train: step:  30970, time: 0.230, loss: 365.980438\n",
      "Train: step:  30980, time: 0.280, loss: 2468.193848\n",
      "Train: step:  30990, time: 0.244, loss: 3000.581299\n",
      "Train: step:  31000, time: 0.222, loss: 895.838928\n",
      "Train: step:  31010, time: 0.236, loss: 3824.941162\n",
      "Train: step:  31020, time: 0.259, loss: 1750.087524\n",
      "Train: step:  31030, time: 0.246, loss: 781.209412\n",
      "Train: step:  31040, time: 0.277, loss: 957.790527\n",
      "Train: step:  31050, time: 0.244, loss: 604.509583\n",
      "Train: step:  31060, time: 0.239, loss: 1672.424561\n",
      "Train: step:  31070, time: 0.232, loss: 1149.865234\n",
      "Train: step:  31080, time: 0.226, loss: 2615.376953\n",
      "Train: step:  31090, time: 0.219, loss: 3525.792725\n",
      "Train: step:  31100, time: 0.226, loss: 3062.313477\n",
      "Train: step:  31110, time: 0.225, loss: 1767.113159\n",
      "Train: step:  31120, time: 0.224, loss: 2817.289062\n",
      "Train: step:  31130, time: 0.235, loss: 977.660095\n",
      "Train: step:  31140, time: 0.253, loss: 893.572510\n",
      "Train: step:  31150, time: 0.232, loss: 2112.039062\n",
      "Train: step:  31160, time: 0.236, loss: 2515.173340\n",
      "Train: step:  31170, time: 0.240, loss: 2845.540039\n",
      "Train: step:  31180, time: 0.237, loss: 287.067719\n",
      "Train: step:  31190, time: 0.231, loss: 2414.524170\n",
      "Train: step:  31200, time: 0.239, loss: 1531.262573\n",
      "Train: step:  31210, time: 0.258, loss: 2651.753906\n",
      "Train: step:  31220, time: 0.261, loss: 3238.124512\n",
      "Train: step:  31230, time: 0.225, loss: 672.173645\n",
      "Train: step:  31240, time: 0.226, loss: 1250.816284\n",
      "Train: step:  31250, time: 0.255, loss: 868.931152\n",
      "Train: step:  31260, time: 0.221, loss: 2259.779053\n",
      "Train: step:  31270, time: 0.260, loss: 1838.196167\n",
      "Train: step:  31280, time: 0.236, loss: 3138.221924\n",
      "Train: step:  31290, time: 0.239, loss: 3036.265381\n",
      "Train: step:  31300, time: 0.260, loss: 4171.825684\n",
      "Train: step:  31310, time: 0.229, loss: 3650.931641\n",
      "Train: step:  31320, time: 0.236, loss: 1200.914917\n",
      "Train: step:  31330, time: 0.238, loss: 2267.395996\n",
      "Train: step:  31340, time: 0.236, loss: 270.744995\n",
      "Train: step:  31350, time: 0.234, loss: 3376.585205\n",
      "Train: step:  31360, time: 0.237, loss: 1123.328369\n",
      "Train: step:  31370, time: 0.230, loss: 2208.512939\n",
      "Train: step:  31380, time: 0.286, loss: 3104.419189\n",
      "Train: step:  31390, time: 0.231, loss: 950.942749\n",
      "Train: step:  31400, time: 0.237, loss: 1020.452942\n",
      "Train: step:  31410, time: 0.236, loss: 2389.547363\n",
      "Train: step:  31420, time: 0.229, loss: 2184.981934\n",
      "Train: step:  31430, time: 0.229, loss: 1153.733521\n",
      "Train: step:  31440, time: 0.235, loss: 1094.304077\n",
      "Train: step:  31450, time: 0.236, loss: 2112.963867\n",
      "Train: step:  31460, time: 0.231, loss: 2153.442383\n",
      "Train: step:  31470, time: 0.262, loss: 2009.222168\n",
      "Train: step:  31480, time: 0.224, loss: 1562.037109\n",
      "Train: step:  31490, time: 0.269, loss: 2227.459717\n",
      "Train: step:  31500, time: 0.220, loss: 1839.373047\n",
      "Train: step:  31510, time: 0.251, loss: 2286.591309\n",
      "Train: step:  31520, time: 0.237, loss: 800.741760\n",
      "Train: step:  31530, time: 0.231, loss: 2564.150879\n",
      "Train: step:  31540, time: 0.250, loss: 1523.247192\n",
      "Train: step:  31550, time: 0.262, loss: 1455.543213\n",
      "Train: step:  31560, time: 0.220, loss: 2170.246826\n",
      "Train: step:  31570, time: 0.269, loss: 2114.328125\n",
      "Train: step:  31580, time: 0.228, loss: 315.323059\n",
      "Train: step:  31590, time: 0.235, loss: 2548.626953\n",
      "Train: step:  31600, time: 0.269, loss: 2397.508545\n",
      "Train: step:  31610, time: 0.233, loss: 2401.331299\n",
      "Train: step:  31620, time: 0.274, loss: 3525.286865\n",
      "Train: step:  31630, time: 0.277, loss: 367.351746\n",
      "Train: step:  31640, time: 0.227, loss: 3069.873535\n",
      "Train: step:  31650, time: 0.228, loss: 3307.253418\n",
      "Train: step:  31660, time: 0.281, loss: 795.055725\n",
      "Train: step:  31670, time: 0.251, loss: 830.355164\n",
      "Train: step:  31680, time: 0.238, loss: 1451.156494\n",
      "Train: step:  31690, time: 0.228, loss: 3055.176514\n",
      "Train: step:  31700, time: 0.289, loss: 2550.938965\n",
      "Train: step:  31710, time: 0.225, loss: 2122.390381\n",
      "Train: step:  31720, time: 0.241, loss: 1074.660522\n",
      "Train: step:  31730, time: 0.232, loss: 940.788635\n",
      "Train: step:  31740, time: 0.258, loss: 1983.583984\n",
      "Train: step:  31750, time: 0.231, loss: 2369.663086\n",
      "Train: step:  31760, time: 0.235, loss: 1176.360962\n",
      "Train: step:  31770, time: 0.270, loss: 2159.708740\n",
      "Train: step:  31780, time: 0.239, loss: 594.575256\n",
      "Train: step:  31790, time: 0.229, loss: 2375.572510\n",
      "Train: step:  31800, time: 0.267, loss: 1935.119507\n",
      "Train: step:  31810, time: 0.242, loss: 1499.266968\n",
      "Train: step:  31820, time: 0.239, loss: 1736.458862\n",
      "Train: step:  31830, time: 0.245, loss: 1627.872559\n",
      "Train: step:  31840, time: 0.240, loss: 428.762939\n",
      "Train: step:  31850, time: 0.261, loss: 1443.328857\n",
      "Train: step:  31860, time: 0.242, loss: 1163.290283\n",
      "Train: step:  31870, time: 0.237, loss: 1223.913330\n",
      "Train: step:  31880, time: 0.239, loss: 1951.688354\n",
      "Train: step:  31890, time: 0.237, loss: 3495.664795\n",
      "Train: step:  31900, time: 0.235, loss: 2236.496582\n",
      "Train: step:  31910, time: 0.263, loss: 491.230133\n",
      "Train: step:  31920, time: 0.240, loss: 1799.363525\n",
      "Train: step:  31930, time: 0.243, loss: 2416.180908\n",
      "Train: step:  31940, time: 0.230, loss: 646.996399\n",
      "Train: step:  31950, time: 0.241, loss: 461.002411\n",
      "Train: step:  31960, time: 0.233, loss: 303.218170\n",
      "Train: step:  31970, time: 0.238, loss: 3307.411865\n",
      "Train: step:  31980, time: 0.231, loss: 2015.522339\n",
      "Train: step:  31990, time: 0.256, loss: 975.216736\n",
      "Train: step:  32000, time: 0.263, loss: 2568.315430\n",
      "Train: step:  32010, time: 0.254, loss: 1064.278076\n",
      "Train: step:  32020, time: 0.274, loss: 773.853516\n",
      "Train: step:  32030, time: 0.230, loss: 1345.886230\n",
      "Train: step:  32040, time: 0.260, loss: 1518.814819\n",
      "Train: step:  32050, time: 0.233, loss: 2611.979492\n",
      "Train: step:  32060, time: 0.237, loss: 1729.355103\n",
      "Train: step:  32070, time: 0.237, loss: 1384.491943\n",
      "Train: step:  32080, time: 0.251, loss: 2092.754639\n",
      "Train: step:  32090, time: 0.255, loss: 539.505127\n",
      "Train: step:  32100, time: 0.259, loss: 881.792908\n",
      "Train: step:  32110, time: 0.229, loss: 2121.677979\n",
      "Train: step:  32120, time: 0.232, loss: 1549.904907\n",
      "Train: step:  32130, time: 0.269, loss: 1946.730469\n",
      "Train: step:  32140, time: 0.235, loss: 925.670776\n",
      "Train: step:  32150, time: 0.227, loss: 3132.352539\n",
      "Train: step:  32160, time: 0.239, loss: 1288.997803\n",
      "Train: step:  32170, time: 0.225, loss: 631.480713\n",
      "Train: step:  32180, time: 0.240, loss: 2534.760254\n",
      "Train: step:  32190, time: 0.252, loss: 2364.140381\n",
      "Train: step:  32200, time: 0.235, loss: 1702.280884\n",
      "Train: step:  32210, time: 0.234, loss: 2135.952637\n",
      "Train: step:  32220, time: 0.227, loss: 3345.449707\n",
      "Train: step:  32230, time: 0.231, loss: 4368.345215\n",
      "Train: step:  32240, time: 0.233, loss: 1587.384888\n",
      "Train: step:  32250, time: 0.222, loss: 371.568146\n",
      "Train: step:  32260, time: 0.224, loss: 2186.801758\n",
      "Train: step:  32270, time: 0.230, loss: 2241.316162\n",
      "Train: step:  32280, time: 0.234, loss: 2560.893555\n",
      "Train: step:  32290, time: 0.263, loss: 1787.095581\n",
      "Train: step:  32300, time: 0.237, loss: 1946.716797\n",
      "Train: step:  32310, time: 0.233, loss: 713.164734\n",
      "Train: step:  32320, time: 0.252, loss: 1101.360352\n",
      "Train: step:  32330, time: 0.239, loss: 3380.542725\n",
      "Train: step:  32340, time: 0.238, loss: 1698.608398\n",
      "Train: step:  32350, time: 0.231, loss: 1746.285645\n",
      "Train: step:  32360, time: 0.233, loss: 1305.612061\n",
      "Train: step:  32370, time: 0.238, loss: 2085.235352\n",
      "Train: step:  32380, time: 0.258, loss: 857.090881\n",
      "Train: step:  32390, time: 0.236, loss: 704.811829\n",
      "Train: step:  32400, time: 0.239, loss: 3382.161377\n",
      "Train: step:  32410, time: 0.264, loss: 466.014923\n",
      "Train: step:  32420, time: 0.262, loss: 3836.786377\n",
      "Train: step:  32430, time: 0.231, loss: 2079.843750\n",
      "Train: step:  32440, time: 0.255, loss: 4187.907715\n",
      "Train: step:  32450, time: 0.254, loss: 1932.099854\n",
      "Train: step:  32460, time: 0.253, loss: 1593.074097\n",
      "Train: step:  32470, time: 0.263, loss: 618.495300\n",
      "Train: step:  32480, time: 0.236, loss: 1196.220703\n",
      "Train: step:  32490, time: 0.259, loss: 2698.032227\n",
      "Train: step:  32500, time: 0.227, loss: 1185.630981\n",
      "Train: step:  32510, time: 0.236, loss: 2316.623779\n",
      "Train: step:  32520, time: 0.225, loss: 3418.201660\n",
      "Train: step:  32530, time: 0.264, loss: 2844.498047\n",
      "Train: step:  32540, time: 0.272, loss: 1586.767944\n",
      "Train: step:  32550, time: 0.235, loss: 1914.189331\n",
      "Train: step:  32560, time: 0.234, loss: 3350.984131\n",
      "Train: step:  32570, time: 0.232, loss: 1255.060425\n",
      "Train: step:  32580, time: 0.265, loss: 2351.075439\n",
      "Train: step:  32590, time: 0.225, loss: 2719.692383\n",
      "Train: step:  32600, time: 0.217, loss: 2611.344482\n",
      "Train: step:  32610, time: 0.244, loss: 2761.679199\n",
      "Train: step:  32620, time: 0.223, loss: 795.196594\n",
      "Train: step:  32630, time: 0.224, loss: 1120.498047\n",
      "Train: step:  32640, time: 0.226, loss: 3827.137451\n",
      "Train: step:  32650, time: 0.221, loss: 1662.332764\n",
      "Train: step:  32660, time: 0.249, loss: 1846.697632\n",
      "Train: step:  32670, time: 0.218, loss: 1205.072266\n",
      "Train: step:  32680, time: 0.246, loss: 2572.027832\n",
      "Train: step:  32690, time: 0.222, loss: 1823.659546\n",
      "Train: step:  32700, time: 0.246, loss: 2506.611084\n",
      "Train: step:  32710, time: 0.237, loss: 2187.924072\n",
      "Train: step:  32720, time: 0.226, loss: 1510.873413\n",
      "Train: step:  32730, time: 0.226, loss: 2005.208374\n",
      "Train: step:  32740, time: 0.315, loss: 1767.497437\n",
      "Train: step:  32750, time: 0.227, loss: 1846.631348\n",
      "Train: step:  32760, time: 0.231, loss: 2395.155762\n",
      "Train: step:  32770, time: 0.226, loss: 1791.189575\n",
      "Train: step:  32780, time: 0.228, loss: 3834.494873\n",
      "Train: step:  32790, time: 0.229, loss: 2072.595947\n",
      "Train: step:  32800, time: 0.224, loss: 763.267944\n",
      "Train: step:  32810, time: 0.255, loss: 3537.199219\n",
      "Train: step:  32820, time: 0.228, loss: 783.359253\n",
      "Train: step:  32830, time: 0.222, loss: 1646.176270\n",
      "Train: step:  32840, time: 0.218, loss: 2914.381348\n",
      "Train: step:  32850, time: 0.217, loss: 3581.492188\n",
      "Train: step:  32860, time: 0.224, loss: 2215.003906\n",
      "Train: step:  32870, time: 0.223, loss: 4097.680176\n",
      "Train: step:  32880, time: 0.246, loss: 2341.814697\n",
      "Train: step:  32890, time: 0.247, loss: 1857.570068\n",
      "Train: step:  32900, time: 0.230, loss: 1329.961548\n",
      "Train: step:  32910, time: 0.237, loss: 618.428345\n",
      "Train: step:  32920, time: 0.227, loss: 1460.886230\n",
      "Train: step:  32930, time: 0.230, loss: 1250.269287\n",
      "Train: step:  32940, time: 0.224, loss: 1228.985107\n",
      "Train: step:  32950, time: 0.226, loss: 584.185364\n",
      "Train: step:  32960, time: 0.227, loss: 2916.838379\n",
      "Train: step:  32970, time: 0.248, loss: 2633.760742\n",
      "Train: step:  32980, time: 0.245, loss: 933.721985\n",
      "Train: step:  32990, time: 0.227, loss: 1837.890015\n",
      "Train: step:  33000, time: 0.230, loss: 1185.826904\n",
      "Train: step:  33010, time: 0.222, loss: 255.182175\n",
      "Train: step:  33020, time: 0.223, loss: 531.651123\n",
      "Train: step:  33030, time: 0.216, loss: 1690.497803\n",
      "Train: step:  33040, time: 0.226, loss: 1428.842773\n",
      "Train: step:  33050, time: 0.227, loss: 1475.407593\n",
      "Train: step:  33060, time: 0.226, loss: 329.916656\n",
      "Train: step:  33070, time: 0.251, loss: 1439.745850\n",
      "Train: step:  33080, time: 0.249, loss: 3208.262207\n",
      "Train: step:  33090, time: 0.288, loss: 2793.364258\n",
      "Train: step:  33100, time: 0.221, loss: 2449.810547\n",
      "Train: step:  33110, time: 0.256, loss: 3239.250000\n",
      "Train: step:  33120, time: 0.276, loss: 1718.049561\n",
      "Train: step:  33130, time: 0.226, loss: 1880.878418\n",
      "Train: step:  33140, time: 0.226, loss: 1029.476929\n",
      "Train: step:  33150, time: 0.230, loss: 1630.056152\n",
      "Train: step:  33160, time: 0.225, loss: 2224.297119\n",
      "Train: step:  33170, time: 0.234, loss: 1131.404419\n",
      "Train: step:  33180, time: 0.234, loss: 2078.074463\n",
      "Train: step:  33190, time: 0.240, loss: 2610.725342\n",
      "Train: step:  33200, time: 0.225, loss: 1175.473022\n",
      "Train: step:  33210, time: 0.230, loss: 3011.387207\n",
      "Train: step:  33220, time: 0.220, loss: 750.958252\n",
      "Train: step:  33230, time: 0.232, loss: 2021.227173\n",
      "Train: step:  33240, time: 0.235, loss: 2698.087646\n",
      "Train: step:  33250, time: 0.271, loss: 1734.872559\n",
      "Train: step:  33260, time: 0.226, loss: 591.145935\n",
      "Train: step:  33270, time: 0.232, loss: 2990.603027\n",
      "Train: step:  33280, time: 0.224, loss: 973.436890\n",
      "Train: step:  33290, time: 0.216, loss: 1327.335449\n",
      "Train: step:  33300, time: 0.232, loss: 2263.118652\n",
      "Train: step:  33310, time: 0.224, loss: 1260.185059\n",
      "Train: step:  33320, time: 0.230, loss: 2332.776367\n",
      "Train: step:  33330, time: 0.222, loss: 2984.897461\n",
      "Train: step:  33340, time: 0.234, loss: 771.269897\n",
      "Train: step:  33350, time: 0.257, loss: 2942.745361\n",
      "Train: step:  33360, time: 0.224, loss: 1837.673340\n",
      "Train: step:  33370, time: 0.222, loss: 1546.082764\n",
      "Train: step:  33380, time: 0.237, loss: 2478.791992\n",
      "Train: step:  33390, time: 0.230, loss: 3620.754883\n",
      "Train: step:  33400, time: 0.239, loss: 1084.857544\n",
      "Train: step:  33410, time: 0.250, loss: 2077.109375\n",
      "Train: step:  33420, time: 0.231, loss: 842.768555\n",
      "Train: step:  33430, time: 0.231, loss: 1718.469238\n",
      "Train: step:  33440, time: 0.255, loss: 567.323853\n",
      "Train: step:  33450, time: 0.236, loss: 1244.290039\n",
      "Train: step:  33460, time: 0.252, loss: 2399.657471\n",
      "Train: step:  33470, time: 0.264, loss: 2666.186523\n",
      "Train: step:  33480, time: 0.270, loss: 2129.655273\n",
      "Train: step:  33490, time: 0.227, loss: 1475.607544\n",
      "Train: step:  33500, time: 0.251, loss: 2392.682861\n",
      "Train: step:  33510, time: 0.249, loss: 630.174744\n",
      "Train: step:  33520, time: 0.232, loss: 1058.750000\n",
      "Train: step:  33530, time: 0.237, loss: 2274.087402\n",
      "Train: step:  33540, time: 0.229, loss: 3117.219727\n",
      "Train: step:  33550, time: 0.231, loss: 2663.116455\n",
      "Train: step:  33560, time: 0.252, loss: 3114.945312\n",
      "Train: step:  33570, time: 0.235, loss: 2204.595459\n",
      "Train: step:  33580, time: 0.254, loss: 705.025391\n",
      "Train: step:  33590, time: 0.237, loss: 1765.890991\n",
      "Train: step:  33600, time: 0.232, loss: 778.105530\n",
      "Train: step:  33610, time: 0.235, loss: 1693.644409\n",
      "Train: step:  33620, time: 0.242, loss: 1366.558228\n",
      "Train: step:  33630, time: 0.231, loss: 3108.064453\n",
      "Train: step:  33640, time: 0.228, loss: 2927.172119\n",
      "Train: step:  33650, time: 0.233, loss: 1357.857666\n",
      "Train: step:  33660, time: 0.228, loss: 1701.917236\n",
      "Train: step:  33670, time: 0.253, loss: 1234.599731\n",
      "Train: step:  33680, time: 0.270, loss: 1205.500610\n",
      "Train: step:  33690, time: 0.231, loss: 714.314941\n",
      "Train: step:  33700, time: 0.234, loss: 1335.874512\n",
      "Train: step:  33710, time: 0.223, loss: 1254.383423\n",
      "Train: step:  33720, time: 0.226, loss: 3050.397461\n",
      "Train: step:  33730, time: 0.254, loss: 707.051575\n",
      "Train: step:  33740, time: 0.222, loss: 1762.250977\n",
      "Train: step:  33750, time: 0.222, loss: 2681.188965\n",
      "Train: step:  33760, time: 0.222, loss: 1520.555176\n",
      "Train: step:  33770, time: 0.276, loss: 2458.217285\n",
      "Train: step:  33780, time: 0.231, loss: 2120.507080\n",
      "Train: step:  33790, time: 0.223, loss: 2061.645996\n",
      "Train: step:  33800, time: 0.265, loss: 1387.264160\n",
      "Train: step:  33810, time: 0.260, loss: 2325.683594\n",
      "Train: step:  33820, time: 0.273, loss: 1884.925781\n",
      "Train: step:  33830, time: 0.223, loss: 1858.818604\n",
      "Train: step:  33840, time: 0.266, loss: 2413.112549\n",
      "Train: step:  33850, time: 0.242, loss: 3137.525635\n",
      "Train: step:  33860, time: 0.239, loss: 3231.679688\n",
      "Train: step:  33870, time: 0.251, loss: 1450.044312\n",
      "Train: step:  33880, time: 0.253, loss: 1009.165466\n",
      "Train: step:  33890, time: 0.250, loss: 2373.843018\n",
      "Train: step:  33900, time: 0.226, loss: 4036.102051\n",
      "Train: step:  33910, time: 0.228, loss: 467.882568\n",
      "Train: step:  33920, time: 0.233, loss: 1940.470337\n",
      "Train: step:  33930, time: 0.222, loss: 1975.591553\n",
      "Train: step:  33940, time: 0.236, loss: 2457.139160\n",
      "Train: step:  33950, time: 0.232, loss: 790.341309\n",
      "Train: step:  33960, time: 0.259, loss: 2266.706543\n",
      "Train: step:  33970, time: 0.230, loss: 2044.556030\n",
      "Train: step:  33980, time: 0.245, loss: 1795.828003\n",
      "Train: step:  33990, time: 0.228, loss: 957.827576\n",
      "Train: step:  34000, time: 0.276, loss: 1559.263184\n",
      "Train: step:  34010, time: 0.227, loss: 638.513123\n",
      "Train: step:  34020, time: 0.227, loss: 3035.770508\n",
      "Train: step:  34030, time: 0.270, loss: 3164.830811\n",
      "Train: step:  34040, time: 0.272, loss: 644.886902\n",
      "Train: step:  34050, time: 0.241, loss: 1794.369629\n",
      "Train: step:  34060, time: 0.227, loss: 1133.241211\n",
      "Train: step:  34070, time: 0.252, loss: 1929.206543\n",
      "Train: step:  34080, time: 0.226, loss: 2539.293945\n",
      "Train: step:  34090, time: 0.253, loss: 1352.049072\n",
      "Train: step:  34100, time: 0.236, loss: 2653.427734\n",
      "Train: step:  34110, time: 0.237, loss: 934.240479\n",
      "Train: step:  34120, time: 0.233, loss: 1967.057251\n",
      "Train: step:  34130, time: 0.237, loss: 2022.622559\n",
      "Train: step:  34140, time: 0.234, loss: 1313.481201\n",
      "Train: step:  34150, time: 0.253, loss: 1621.558838\n",
      "Train: step:  34160, time: 0.230, loss: 1300.776489\n",
      "Train: step:  34170, time: 0.261, loss: 2777.046143\n",
      "Train: step:  34180, time: 0.233, loss: 1097.801636\n",
      "Train: step:  34190, time: 0.232, loss: 1689.667480\n",
      "Train: step:  34200, time: 0.231, loss: 1786.553955\n",
      "Train: step:  34210, time: 0.237, loss: 1365.090088\n",
      "Train: step:  34220, time: 0.228, loss: 3093.529541\n",
      "Train: step:  34230, time: 0.263, loss: 1122.766724\n",
      "Train: step:  34240, time: 0.225, loss: 2227.161621\n",
      "Train: step:  34250, time: 0.234, loss: 1892.945190\n",
      "Train: step:  34260, time: 0.231, loss: 1753.492920\n",
      "Train: step:  34270, time: 0.259, loss: 1745.505249\n",
      "Train: step:  34280, time: 0.235, loss: 1097.197388\n",
      "Train: step:  34290, time: 0.254, loss: 1893.449463\n",
      "Train: step:  34300, time: 0.228, loss: 1856.419922\n",
      "Train: step:  34310, time: 0.265, loss: 1708.103394\n",
      "Train: step:  34320, time: 0.252, loss: 1839.863647\n",
      "Train: step:  34330, time: 0.240, loss: 2132.391113\n",
      "Train: step:  34340, time: 0.242, loss: 2119.162109\n",
      "Train: step:  34350, time: 0.221, loss: 1565.333252\n",
      "Train: step:  34360, time: 0.233, loss: 2457.572754\n",
      "Train: step:  34370, time: 0.230, loss: 1178.678345\n",
      "Train: step:  34380, time: 0.227, loss: 1346.270508\n",
      "Train: step:  34390, time: 0.245, loss: 2326.579346\n",
      "Train: step:  34400, time: 0.230, loss: 1446.067749\n",
      "Train: step:  34410, time: 0.250, loss: 791.181274\n",
      "Train: step:  34420, time: 0.232, loss: 3098.735840\n",
      "Train: step:  34430, time: 0.226, loss: 3022.464844\n",
      "Train: step:  34440, time: 0.274, loss: 1931.417480\n",
      "Train: step:  34450, time: 0.252, loss: 1570.750000\n",
      "Train: step:  34460, time: 0.236, loss: 1131.164307\n",
      "Train: step:  34470, time: 0.232, loss: 2886.170166\n",
      "Train: step:  34480, time: 0.254, loss: 1733.640503\n",
      "Train: step:  34490, time: 0.267, loss: 2878.218018\n",
      "Train: step:  34500, time: 0.263, loss: 700.689331\n",
      "Train: step:  34510, time: 0.269, loss: 1446.162354\n",
      "Train: step:  34520, time: 0.259, loss: 822.022095\n",
      "Train: step:  34530, time: 0.239, loss: 2936.725098\n",
      "Train: step:  34540, time: 0.252, loss: 2366.943604\n",
      "Train: step:  34550, time: 0.240, loss: 2853.178711\n",
      "Train: step:  34560, time: 0.227, loss: 2776.891357\n",
      "Train: step:  34570, time: 0.231, loss: 2143.567383\n",
      "Train: step:  34580, time: 0.263, loss: 1773.303589\n",
      "Train: step:  34590, time: 0.232, loss: 1214.378784\n",
      "Train: step:  34600, time: 0.275, loss: 1952.841797\n",
      "Train: step:  34610, time: 0.226, loss: 3947.475342\n",
      "Train: step:  34620, time: 0.227, loss: 589.066040\n",
      "Train: step:  34630, time: 0.235, loss: 376.282745\n",
      "Train: step:  34640, time: 0.251, loss: 3023.434814\n",
      "Train: step:  34650, time: 0.221, loss: 2638.205078\n",
      "Train: step:  34660, time: 0.230, loss: 2348.256104\n",
      "Train: step:  34670, time: 0.271, loss: 2786.456543\n",
      "Train: step:  34680, time: 0.292, loss: 2587.541016\n",
      "Train: step:  34690, time: 0.226, loss: 2627.588623\n",
      "Train: step:  34700, time: 0.253, loss: 1842.635010\n",
      "Train: step:  34710, time: 0.227, loss: 2882.245117\n",
      "Train: step:  34720, time: 0.230, loss: 3937.851807\n",
      "Train: step:  34730, time: 0.225, loss: 2086.656006\n",
      "Train: step:  34740, time: 0.273, loss: 3289.945557\n",
      "Train: step:  34750, time: 0.224, loss: 1206.069824\n",
      "Train: step:  34760, time: 0.258, loss: 2810.505615\n",
      "Train: step:  34770, time: 0.250, loss: 1529.354370\n",
      "Train: step:  34780, time: 0.251, loss: 198.731583\n",
      "Train: step:  34790, time: 0.243, loss: 1311.083252\n",
      "Train: step:  34800, time: 0.224, loss: 2187.101318\n",
      "Train: step:  34810, time: 0.222, loss: 308.608154\n",
      "Train: step:  34820, time: 0.224, loss: 2496.225586\n",
      "Train: step:  34830, time: 0.247, loss: 1159.764526\n",
      "Train: step:  34840, time: 0.249, loss: 2621.176758\n",
      "Train: step:  34850, time: 0.218, loss: 1779.043457\n",
      "Train: step:  34860, time: 0.270, loss: 1323.789795\n",
      "Train: step:  34870, time: 0.264, loss: 2424.552246\n",
      "Train: step:  34880, time: 0.257, loss: 1004.473572\n",
      "Train: step:  34890, time: 0.272, loss: 409.455017\n",
      "Train: step:  34900, time: 0.264, loss: 2768.502441\n",
      "Train: step:  34910, time: 0.233, loss: 1971.605103\n",
      "Train: step:  34920, time: 0.239, loss: 2546.816162\n",
      "Train: step:  34930, time: 0.233, loss: 1823.430664\n",
      "Train: step:  34940, time: 0.238, loss: 783.319580\n",
      "Train: step:  34950, time: 0.233, loss: 1946.286987\n",
      "Train: step:  34960, time: 0.248, loss: 1425.979370\n",
      "Train: step:  34970, time: 0.235, loss: 1480.671509\n",
      "Train: step:  34980, time: 0.240, loss: 4913.703125\n",
      "Train: step:  34990, time: 0.242, loss: 1945.374268\n",
      "Train: step:  35000, time: 0.237, loss: 3311.913086\n",
      "Train: step:  35010, time: 0.236, loss: 2853.594482\n",
      "Train: step:  35020, time: 0.240, loss: 2745.594238\n",
      "Train: step:  35030, time: 0.236, loss: 1174.219727\n",
      "Train: step:  35040, time: 0.277, loss: 1029.189819\n",
      "Train: step:  35050, time: 0.229, loss: 2717.336914\n",
      "Train: step:  35060, time: 0.232, loss: 3049.528809\n",
      "Train: step:  35070, time: 0.241, loss: 2822.062988\n",
      "Train: step:  35080, time: 0.340, loss: 294.208344\n",
      "Train: step:  35090, time: 0.239, loss: 547.582397\n",
      "Train: step:  35100, time: 0.243, loss: 3191.830078\n",
      "Train: step:  35110, time: 0.255, loss: 1014.066345\n",
      "Train: step:  35120, time: 0.252, loss: 859.484863\n",
      "Train: step:  35130, time: 0.236, loss: 685.003357\n",
      "Train: step:  35140, time: 0.244, loss: 1543.724609\n",
      "Train: step:  35150, time: 0.268, loss: 2072.625732\n",
      "Train: step:  35160, time: 0.230, loss: 1488.611938\n",
      "Train: step:  35170, time: 0.248, loss: 2803.848389\n",
      "Train: step:  35180, time: 0.254, loss: 1863.998047\n",
      "Train: step:  35190, time: 0.235, loss: 918.375854\n",
      "Train: step:  35200, time: 0.231, loss: 2767.638672\n",
      "Train: step:  35210, time: 0.274, loss: 2414.686523\n",
      "Train: step:  35220, time: 0.234, loss: 1202.536743\n",
      "Train: step:  35230, time: 0.266, loss: 3587.637939\n",
      "Train: step:  35240, time: 0.278, loss: 381.876190\n",
      "Train: step:  35250, time: 0.247, loss: 2408.325684\n",
      "Train: step:  35260, time: 0.235, loss: 2242.394043\n",
      "Train: step:  35270, time: 0.236, loss: 2276.140137\n",
      "Train: step:  35280, time: 0.252, loss: 2788.626465\n",
      "Train: step:  35290, time: 0.261, loss: 1612.945190\n",
      "Train: step:  35300, time: 0.241, loss: 744.310364\n",
      "Train: step:  35310, time: 0.232, loss: 1322.804565\n",
      "Train: step:  35320, time: 0.249, loss: 2977.768555\n",
      "Train: step:  35330, time: 0.240, loss: 2112.553711\n",
      "Train: step:  35340, time: 0.284, loss: 3369.076172\n",
      "Train: step:  35350, time: 0.254, loss: 1255.049438\n",
      "Train: step:  35360, time: 0.252, loss: 2255.625488\n",
      "Train: step:  35370, time: 0.272, loss: 1793.490112\n",
      "Train: step:  35380, time: 0.276, loss: 1771.968262\n",
      "Train: step:  35390, time: 0.279, loss: 620.655029\n",
      "Train: step:  35400, time: 0.242, loss: 1592.634277\n",
      "Train: step:  35410, time: 0.238, loss: 2372.260010\n",
      "Train: step:  35420, time: 0.247, loss: 2157.981445\n",
      "Train: step:  35430, time: 0.249, loss: 1549.758911\n",
      "Train: step:  35440, time: 0.275, loss: 1222.155762\n",
      "Train: step:  35450, time: 0.260, loss: 728.265442\n",
      "Train: step:  35460, time: 0.246, loss: 2755.666260\n",
      "Train: step:  35470, time: 0.266, loss: 2461.205078\n",
      "Train: step:  35480, time: 0.243, loss: 1077.674438\n",
      "Train: step:  35490, time: 0.259, loss: 1875.291870\n",
      "Train: step:  35500, time: 0.216, loss: 2289.993408\n",
      "Train: step:  35510, time: 0.243, loss: 3466.677490\n",
      "Train: step:  35520, time: 0.309, loss: 804.691772\n",
      "Train: step:  35530, time: 0.247, loss: 900.589355\n",
      "Train: step:  35540, time: 0.248, loss: 1519.808228\n",
      "Train: step:  35550, time: 0.246, loss: 819.661377\n",
      "Train: step:  35560, time: 0.243, loss: 2689.073242\n",
      "Train: step:  35570, time: 0.244, loss: 1597.490723\n",
      "Train: step:  35580, time: 0.244, loss: 2116.114502\n",
      "Train: step:  35590, time: 0.254, loss: 2659.105225\n",
      "Train: step:  35600, time: 0.256, loss: 2366.865967\n",
      "Train: step:  35610, time: 0.256, loss: 2255.672119\n",
      "Train: step:  35620, time: 0.253, loss: 3171.219482\n",
      "Train: step:  35630, time: 0.238, loss: 1340.116455\n",
      "Train: step:  35640, time: 0.248, loss: 2434.264893\n",
      "Train: step:  35650, time: 0.274, loss: 2154.017334\n",
      "Train: step:  35660, time: 0.284, loss: 687.937439\n",
      "Train: step:  35670, time: 0.248, loss: 5141.887207\n",
      "Train: step:  35680, time: 0.240, loss: 897.336548\n",
      "Train: step:  35690, time: 0.236, loss: 2482.212891\n",
      "Train: step:  35700, time: 0.247, loss: 1688.735840\n",
      "Train: step:  35710, time: 0.283, loss: 260.872894\n",
      "Train: step:  35720, time: 0.250, loss: 680.355225\n",
      "Train: step:  35730, time: 0.257, loss: 2782.879883\n",
      "Train: step:  35740, time: 0.251, loss: 2102.484863\n",
      "Train: step:  35750, time: 0.242, loss: 692.648315\n",
      "Train: step:  35760, time: 0.246, loss: 2444.500488\n",
      "Train: step:  35770, time: 0.276, loss: 1857.182861\n",
      "Train: step:  35780, time: 0.238, loss: 1914.067993\n",
      "Train: step:  35790, time: 0.223, loss: 550.819397\n",
      "Train: step:  35800, time: 0.239, loss: 741.441772\n",
      "Train: step:  35810, time: 0.265, loss: 1521.063232\n",
      "Train: step:  35820, time: 0.251, loss: 616.698181\n",
      "Train: step:  35830, time: 0.258, loss: 1603.559326\n",
      "Train: step:  35840, time: 0.231, loss: 4193.081543\n",
      "Train: step:  35850, time: 0.229, loss: 549.453552\n",
      "Train: step:  35860, time: 0.235, loss: 662.627136\n",
      "Train: step:  35870, time: 0.234, loss: 2258.661133\n",
      "Train: step:  35880, time: 0.247, loss: 2974.885742\n",
      "Train: step:  35890, time: 0.255, loss: 1390.935303\n",
      "Train: step:  35900, time: 0.219, loss: 1682.384644\n",
      "Train: step:  35910, time: 0.232, loss: 1954.276123\n",
      "Train: step:  35920, time: 0.222, loss: 1256.209473\n",
      "Train: step:  35930, time: 0.227, loss: 2639.432373\n",
      "Train: step:  35940, time: 0.222, loss: 2391.167725\n",
      "Train: step:  35950, time: 0.216, loss: 1951.655518\n",
      "Train: step:  35960, time: 0.221, loss: 332.512604\n",
      "Train: step:  35970, time: 0.216, loss: 3425.553711\n",
      "Train: step:  35980, time: 0.237, loss: 2158.447266\n",
      "Train: step:  35990, time: 0.265, loss: 3051.842529\n",
      "Train: step:  36000, time: 0.218, loss: 1936.819214\n",
      "Train: step:  36010, time: 0.228, loss: 2959.624756\n",
      "Train: step:  36020, time: 0.223, loss: 2772.514160\n",
      "Train: step:  36030, time: 0.227, loss: 415.683258\n",
      "Train: step:  36040, time: 0.263, loss: 2794.406738\n",
      "Train: step:  36050, time: 0.231, loss: 2065.258057\n",
      "Train: step:  36060, time: 0.226, loss: 4160.515137\n",
      "Train: step:  36070, time: 0.230, loss: 1161.965454\n",
      "Train: step:  36080, time: 0.271, loss: 3732.679199\n",
      "Train: step:  36090, time: 0.242, loss: 3944.150635\n",
      "Train: step:  36100, time: 0.234, loss: 2437.012207\n",
      "Train: step:  36110, time: 0.221, loss: 2039.025024\n",
      "Train: step:  36120, time: 0.234, loss: 2068.539307\n",
      "Train: step:  36130, time: 0.237, loss: 2436.904541\n",
      "Train: step:  36140, time: 0.271, loss: 2441.344482\n",
      "Train: step:  36150, time: 0.279, loss: 1415.859009\n",
      "Train: step:  36160, time: 0.220, loss: 1595.165283\n",
      "Train: step:  36170, time: 0.241, loss: 2370.681885\n",
      "Train: step:  36180, time: 0.229, loss: 651.240479\n",
      "Train: step:  36190, time: 0.237, loss: 2255.349121\n",
      "Train: step:  36200, time: 0.224, loss: 2059.695068\n",
      "Train: step:  36210, time: 0.265, loss: 820.127869\n",
      "Train: step:  36220, time: 0.237, loss: 584.171204\n",
      "Train: step:  36230, time: 0.231, loss: 1653.304199\n",
      "Train: step:  36240, time: 0.238, loss: 2259.078125\n",
      "Train: step:  36250, time: 0.247, loss: 3690.509033\n",
      "Train: step:  36260, time: 0.238, loss: 2369.390381\n",
      "Train: step:  36270, time: 0.237, loss: 614.073303\n",
      "Train: step:  36280, time: 0.243, loss: 2047.068481\n",
      "Train: step:  36290, time: 0.227, loss: 1371.186523\n",
      "Train: step:  36300, time: 0.231, loss: 2693.781738\n",
      "Train: step:  36310, time: 0.239, loss: 864.132874\n",
      "Train: step:  36320, time: 0.232, loss: 2616.315918\n",
      "Train: step:  36330, time: 0.235, loss: 4498.175781\n",
      "Train: step:  36340, time: 0.220, loss: 1855.207275\n",
      "Train: step:  36350, time: 0.262, loss: 191.778366\n",
      "Train: step:  36360, time: 0.240, loss: 3152.110840\n",
      "Train: step:  36370, time: 0.249, loss: 2354.883057\n",
      "Train: step:  36380, time: 0.237, loss: 2482.713135\n",
      "Train: step:  36390, time: 0.241, loss: 1172.383911\n",
      "Train: step:  36400, time: 0.283, loss: 1190.673706\n",
      "Train: step:  36410, time: 0.236, loss: 3134.565186\n",
      "Train: step:  36420, time: 0.232, loss: 1987.746460\n",
      "Train: step:  36430, time: 0.242, loss: 1660.776367\n",
      "Train: step:  36440, time: 0.263, loss: 3979.529785\n",
      "Train: step:  36450, time: 0.236, loss: 1412.233887\n",
      "Train: step:  36460, time: 0.243, loss: 2036.493896\n",
      "Train: step:  36470, time: 0.246, loss: 383.049957\n",
      "Train: step:  36480, time: 0.267, loss: 3015.252197\n",
      "Train: step:  36490, time: 0.243, loss: 570.949341\n",
      "Train: step:  36500, time: 0.257, loss: 2743.264404\n",
      "Train: step:  36510, time: 0.232, loss: 878.931702\n",
      "Train: step:  36520, time: 0.231, loss: 2905.784424\n",
      "Train: step:  36530, time: 0.243, loss: 1175.039795\n",
      "Train: step:  36540, time: 0.231, loss: 2037.888672\n",
      "Train: step:  36550, time: 0.238, loss: 3171.322510\n",
      "Train: step:  36560, time: 0.236, loss: 919.610901\n",
      "Train: step:  36570, time: 0.271, loss: 1728.550049\n",
      "Train: step:  36580, time: 0.245, loss: 2124.869141\n",
      "Train: step:  36590, time: 0.253, loss: 3310.776367\n",
      "Train: step:  36600, time: 0.221, loss: 1687.657715\n",
      "Train: step:  36610, time: 0.251, loss: 3021.845459\n",
      "Train: step:  36620, time: 0.249, loss: 2562.176270\n",
      "Train: step:  36630, time: 0.227, loss: 2107.365723\n",
      "Train: step:  36640, time: 0.228, loss: 2073.062744\n",
      "Train: step:  36650, time: 0.227, loss: 1393.550171\n",
      "Train: step:  36660, time: 0.237, loss: 630.326111\n",
      "Train: step:  36670, time: 0.220, loss: 300.051147\n",
      "Train: step:  36680, time: 0.274, loss: 3627.460938\n",
      "Train: step:  36690, time: 0.216, loss: 345.716675\n",
      "Train: step:  36700, time: 0.255, loss: 3074.354004\n",
      "Train: step:  36710, time: 0.234, loss: 1735.267944\n",
      "Train: step:  36720, time: 0.239, loss: 1910.786743\n",
      "Train: step:  36730, time: 0.240, loss: 4999.963867\n",
      "Train: step:  36740, time: 0.274, loss: 647.329773\n",
      "Train: step:  36750, time: 0.258, loss: 1871.352783\n",
      "Train: step:  36760, time: 0.239, loss: 2455.621582\n",
      "Train: step:  36770, time: 0.237, loss: 260.962738\n",
      "Train: step:  36780, time: 0.233, loss: 2022.339478\n",
      "Train: step:  36790, time: 0.233, loss: 2934.112305\n",
      "Train: step:  36800, time: 0.254, loss: 2456.761719\n",
      "Train: step:  36810, time: 0.236, loss: 1666.861694\n",
      "Train: step:  36820, time: 0.235, loss: 2065.869141\n",
      "Train: step:  36830, time: 0.227, loss: 569.944946\n",
      "Train: step:  36840, time: 0.228, loss: 1131.921509\n",
      "Train: step:  36850, time: 0.223, loss: 1777.674194\n",
      "Train: step:  36860, time: 0.241, loss: 1260.342041\n",
      "Train: step:  36870, time: 0.241, loss: 886.421326\n",
      "Train: step:  36880, time: 0.235, loss: 1242.913452\n",
      "Train: step:  36890, time: 0.281, loss: 1402.776123\n",
      "Train: step:  36900, time: 0.238, loss: 518.997253\n",
      "Train: step:  36910, time: 0.234, loss: 2190.284912\n",
      "Train: step:  36920, time: 0.264, loss: 4055.563721\n",
      "Train: step:  36930, time: 0.233, loss: 1233.752075\n",
      "Train: step:  36940, time: 0.238, loss: 1048.732910\n",
      "Train: step:  36950, time: 0.228, loss: 3555.506104\n",
      "Train: step:  36960, time: 0.236, loss: 2016.917236\n",
      "Train: step:  36970, time: 0.237, loss: 2137.236816\n",
      "Train: step:  36980, time: 0.243, loss: 2632.197021\n",
      "Train: step:  36990, time: 0.261, loss: 1576.437744\n",
      "Train: step:  37000, time: 0.231, loss: 1731.785034\n",
      "Train: step:  37010, time: 0.237, loss: 2679.052490\n",
      "Train: step:  37020, time: 0.269, loss: 2328.641602\n",
      "Train: step:  37030, time: 0.235, loss: 1328.454590\n",
      "Train: step:  37040, time: 0.253, loss: 1282.879150\n",
      "Train: step:  37050, time: 0.269, loss: 709.635864\n",
      "Train: step:  37060, time: 0.236, loss: 2593.386230\n",
      "Train: step:  37070, time: 0.250, loss: 671.205688\n",
      "Train: step:  37080, time: 0.238, loss: 1703.177368\n",
      "Train: step:  37090, time: 0.230, loss: 1265.375000\n",
      "Train: step:  37100, time: 0.264, loss: 1209.319946\n",
      "Train: step:  37110, time: 0.227, loss: 2349.129395\n",
      "Train: step:  37120, time: 0.234, loss: 2487.946045\n",
      "Train: step:  37130, time: 0.223, loss: 3334.072510\n",
      "Train: step:  37140, time: 0.224, loss: 2698.419678\n",
      "Train: step:  37150, time: 0.234, loss: 1255.157349\n",
      "Train: step:  37160, time: 0.233, loss: 2040.808105\n",
      "Train: step:  37170, time: 0.255, loss: 1660.954956\n",
      "Train: step:  37180, time: 0.238, loss: 726.464600\n",
      "Train: step:  37190, time: 0.247, loss: 2448.677979\n",
      "Train: step:  37200, time: 0.229, loss: 568.680115\n",
      "Train: step:  37210, time: 0.227, loss: 1158.380493\n",
      "Train: step:  37220, time: 0.228, loss: 2126.791748\n",
      "Train: step:  37230, time: 0.235, loss: 1462.520874\n",
      "Train: step:  37240, time: 0.265, loss: 1746.447510\n",
      "Train: step:  37250, time: 0.239, loss: 826.062134\n",
      "Train: step:  37260, time: 0.234, loss: 1582.734741\n",
      "Train: step:  37270, time: 0.237, loss: 1742.584839\n",
      "Train: step:  37280, time: 0.240, loss: 1993.594727\n",
      "Train: step:  37290, time: 0.244, loss: 778.671204\n",
      "Train: step:  37300, time: 0.258, loss: 2859.795654\n",
      "Train: step:  37310, time: 0.262, loss: 2605.731445\n",
      "Train: step:  37320, time: 0.266, loss: 2906.701172\n",
      "Train: step:  37330, time: 0.237, loss: 1788.665527\n",
      "Train: step:  37340, time: 0.242, loss: 1122.897217\n",
      "Train: step:  37350, time: 0.257, loss: 2691.057861\n",
      "Train: step:  37360, time: 0.231, loss: 1810.146362\n",
      "Train: step:  37370, time: 0.270, loss: 2483.179199\n",
      "Train: step:  37380, time: 0.250, loss: 1341.442871\n",
      "Train: step:  37390, time: 0.287, loss: 1767.669312\n",
      "Train: step:  37400, time: 0.237, loss: 655.430237\n",
      "Train: step:  37410, time: 0.230, loss: 513.213806\n",
      "Train: step:  37420, time: 0.238, loss: 2293.572754\n",
      "Train: step:  37430, time: 0.242, loss: 845.915833\n",
      "Train: step:  37440, time: 0.238, loss: 2949.273193\n",
      "Train: step:  37450, time: 0.303, loss: 2759.686768\n",
      "Train: step:  37460, time: 0.238, loss: 1601.597290\n",
      "Train: step:  37470, time: 0.220, loss: 1786.636108\n",
      "Train: step:  37480, time: 0.224, loss: 1044.386475\n",
      "Train: step:  37490, time: 0.225, loss: 1137.067749\n",
      "Train: step:  37500, time: 0.226, loss: 376.134552\n",
      "Train: step:  37510, time: 0.217, loss: 1486.941650\n",
      "Train: step:  37520, time: 0.261, loss: 1922.143433\n",
      "Train: step:  37530, time: 0.231, loss: 1420.583130\n",
      "Train: step:  37540, time: 0.240, loss: 3053.060547\n",
      "Train: step:  37550, time: 0.241, loss: 2449.877197\n",
      "Train: step:  37560, time: 0.234, loss: 3536.398193\n",
      "Train: step:  37570, time: 0.233, loss: 2286.408936\n",
      "Train: step:  37580, time: 0.225, loss: 3001.121582\n",
      "Train: step:  37590, time: 0.229, loss: 1720.555786\n",
      "Train: step:  37600, time: 0.233, loss: 936.741150\n",
      "Train: step:  37610, time: 0.231, loss: 2278.291748\n",
      "Train: step:  37620, time: 0.233, loss: 1732.030151\n",
      "Train: step:  37630, time: 0.230, loss: 2478.792236\n",
      "Train: step:  37640, time: 0.231, loss: 2054.677246\n",
      "Train: step:  37650, time: 0.368, loss: 1365.584351\n",
      "Train: step:  37660, time: 0.244, loss: 1032.489868\n",
      "Train: step:  37670, time: 0.235, loss: 2234.301758\n",
      "Train: step:  37680, time: 0.268, loss: 2824.874756\n",
      "Train: step:  37690, time: 0.242, loss: 2104.665039\n",
      "Train: step:  37700, time: 0.260, loss: 987.618713\n",
      "Train: step:  37710, time: 0.258, loss: 1607.352539\n",
      "Train: step:  37720, time: 0.232, loss: 2908.649902\n",
      "Train: step:  37730, time: 0.256, loss: 3310.763428\n",
      "Train: step:  37740, time: 0.255, loss: 3316.272217\n",
      "Train: step:  37750, time: 0.268, loss: 1038.861816\n",
      "Train: step:  37760, time: 0.245, loss: 676.045227\n",
      "Train: step:  37770, time: 0.266, loss: 1705.510010\n",
      "Train: step:  37780, time: 0.324, loss: 1687.505615\n",
      "Train: step:  37790, time: 0.245, loss: 1732.225098\n",
      "Train: step:  37800, time: 0.237, loss: 1692.718018\n",
      "Train: step:  37810, time: 0.225, loss: 1149.654663\n",
      "Train: step:  37820, time: 0.264, loss: 2996.354492\n",
      "Train: step:  37830, time: 0.256, loss: 3298.660400\n",
      "Train: step:  37840, time: 0.242, loss: 1419.531006\n",
      "Train: step:  37850, time: 0.243, loss: 1328.786011\n",
      "Train: step:  37860, time: 0.234, loss: 1224.938477\n",
      "Train: step:  37870, time: 0.251, loss: 1506.227417\n",
      "Train: step:  37880, time: 0.245, loss: 1590.329834\n",
      "Train: step:  37890, time: 0.244, loss: 1137.865479\n",
      "Train: step:  37900, time: 0.254, loss: 2226.910645\n",
      "Train: step:  37910, time: 0.243, loss: 3823.034912\n",
      "Train: step:  37920, time: 0.251, loss: 2471.908447\n",
      "Train: step:  37930, time: 0.236, loss: 1161.820801\n",
      "Train: step:  37940, time: 0.240, loss: 1019.906433\n",
      "Train: step:  37950, time: 0.248, loss: 1797.241699\n",
      "Train: step:  37960, time: 0.245, loss: 1640.136841\n",
      "Train: step:  37970, time: 0.239, loss: 1363.734375\n",
      "Train: step:  37980, time: 0.246, loss: 3555.181396\n",
      "Train: step:  37990, time: 0.283, loss: 1897.569946\n",
      "Train: step:  38000, time: 0.257, loss: 1965.320923\n",
      "Train: step:  38010, time: 0.252, loss: 528.551453\n",
      "Train: step:  38020, time: 0.295, loss: 1604.608887\n",
      "Train: step:  38030, time: 0.244, loss: 1626.238281\n",
      "Train: step:  38040, time: 0.242, loss: 1272.760376\n",
      "Train: step:  38050, time: 0.240, loss: 1049.796509\n",
      "Train: step:  38060, time: 0.268, loss: 2170.096924\n",
      "Train: step:  38070, time: 0.243, loss: 1223.569214\n",
      "Train: step:  38080, time: 0.247, loss: 1544.728760\n",
      "Train: step:  38090, time: 0.242, loss: 2214.328369\n",
      "Train: step:  38100, time: 0.270, loss: 2339.114746\n",
      "Train: step:  38110, time: 0.245, loss: 1971.297852\n",
      "Train: step:  38120, time: 0.242, loss: 1043.282104\n",
      "Train: step:  38130, time: 0.241, loss: 2060.507080\n",
      "Train: step:  38140, time: 0.275, loss: 2715.449951\n",
      "Train: step:  38150, time: 0.270, loss: 2152.186279\n",
      "Train: step:  38160, time: 0.252, loss: 2227.365723\n",
      "Train: step:  38170, time: 0.244, loss: 1361.990845\n",
      "Train: step:  38180, time: 0.282, loss: 3121.577637\n",
      "Train: step:  38190, time: 0.227, loss: 2411.796875\n",
      "Train: step:  38200, time: 0.265, loss: 2012.341675\n",
      "Train: step:  38210, time: 0.239, loss: 2892.170410\n",
      "Train: step:  38220, time: 0.267, loss: 555.717224\n",
      "Train: step:  38230, time: 0.302, loss: 1088.822021\n",
      "Train: step:  38240, time: 0.246, loss: 1409.221313\n",
      "Train: step:  38250, time: 0.239, loss: 1326.730957\n",
      "Train: step:  38260, time: 0.244, loss: 928.268188\n",
      "Train: step:  38270, time: 0.245, loss: 1076.827759\n",
      "Train: step:  38280, time: 0.278, loss: 2145.491211\n",
      "Train: step:  38290, time: 0.240, loss: 2789.638916\n",
      "Train: step:  38300, time: 0.242, loss: 2328.893311\n",
      "Train: step:  38310, time: 0.241, loss: 2857.866943\n",
      "Train: step:  38320, time: 0.244, loss: 260.420441\n",
      "Train: step:  38330, time: 0.274, loss: 2466.846191\n",
      "Train: step:  38340, time: 0.242, loss: 886.358704\n",
      "Train: step:  38350, time: 0.253, loss: 571.926514\n",
      "Train: step:  38360, time: 0.269, loss: 3385.465576\n",
      "Train: step:  38370, time: 0.248, loss: 1877.857300\n",
      "Train: step:  38380, time: 0.241, loss: 2639.950684\n",
      "Train: step:  38390, time: 0.236, loss: 1663.692017\n",
      "Train: step:  38400, time: 0.247, loss: 3238.544678\n",
      "Train: step:  38410, time: 0.262, loss: 2270.434082\n",
      "Train: step:  38420, time: 0.238, loss: 1155.700073\n",
      "Train: step:  38430, time: 0.244, loss: 1990.818481\n",
      "Train: step:  38440, time: 0.253, loss: 3475.969727\n",
      "Train: step:  38450, time: 0.245, loss: 714.495544\n",
      "Train: step:  38460, time: 0.271, loss: 3084.506104\n",
      "Train: step:  38470, time: 0.249, loss: 1645.624634\n",
      "Train: step:  38480, time: 0.246, loss: 3586.130615\n",
      "Train: step:  38490, time: 0.288, loss: 2384.132324\n",
      "Train: step:  38500, time: 0.249, loss: 2790.469482\n",
      "Train: step:  38510, time: 0.239, loss: 1622.120850\n",
      "Train: step:  38520, time: 0.276, loss: 1505.428345\n",
      "Train: step:  38530, time: 0.238, loss: 482.535004\n",
      "Train: step:  38540, time: 0.247, loss: 1107.748291\n",
      "Train: step:  38550, time: 0.267, loss: 541.653320\n",
      "Train: step:  38560, time: 0.236, loss: 2694.041748\n",
      "Train: step:  38570, time: 0.245, loss: 1732.260986\n",
      "Train: step:  38580, time: 0.263, loss: 1106.091309\n",
      "Train: step:  38590, time: 0.240, loss: 1893.329590\n",
      "Train: step:  38600, time: 0.238, loss: 4288.894043\n",
      "Train: step:  38610, time: 0.238, loss: 3843.003662\n",
      "Train: step:  38620, time: 0.280, loss: 2576.454102\n",
      "Train: step:  38630, time: 0.236, loss: 2067.294189\n",
      "Train: step:  38640, time: 0.246, loss: 2248.023438\n",
      "Train: step:  38650, time: 0.249, loss: 927.966248\n",
      "Train: step:  38660, time: 0.258, loss: 4221.007812\n",
      "Train: step:  38670, time: 0.249, loss: 559.753967\n",
      "Train: step:  38680, time: 0.238, loss: 2912.241943\n",
      "Train: step:  38690, time: 0.248, loss: 2619.614258\n",
      "Train: step:  38700, time: 0.269, loss: 3170.469971\n",
      "Train: step:  38710, time: 0.264, loss: 1316.529175\n",
      "Train: step:  38720, time: 0.246, loss: 3701.062500\n",
      "Train: step:  38730, time: 0.244, loss: 337.118652\n",
      "Train: step:  38740, time: 0.252, loss: 2703.497070\n",
      "Train: step:  38750, time: 0.236, loss: 1644.037354\n",
      "Train: step:  38760, time: 0.269, loss: 2317.032959\n",
      "Train: step:  38770, time: 0.264, loss: 2626.901855\n",
      "Train: step:  38780, time: 0.310, loss: 733.098999\n",
      "Train: step:  38790, time: 0.238, loss: 2230.487305\n",
      "Train: step:  38800, time: 0.257, loss: 1883.155396\n",
      "Train: step:  38810, time: 0.246, loss: 1717.213257\n",
      "Train: step:  38820, time: 0.274, loss: 1128.986572\n",
      "Train: step:  38830, time: 0.295, loss: 1840.977661\n",
      "Train: step:  38840, time: 0.270, loss: 1336.869751\n",
      "Train: step:  38850, time: 0.243, loss: 3358.538818\n",
      "Train: step:  38860, time: 0.261, loss: 1423.824219\n",
      "Train: step:  38870, time: 0.263, loss: 904.533752\n",
      "Train: step:  38880, time: 0.248, loss: 3075.517578\n",
      "Train: step:  38890, time: 0.246, loss: 2081.316895\n",
      "Train: step:  38900, time: 0.266, loss: 554.589600\n",
      "Train: step:  38910, time: 0.228, loss: 3733.000000\n",
      "Train: step:  38920, time: 0.209, loss: 2720.440674\n",
      "Train: step:  38930, time: 0.204, loss: 1597.738403\n",
      "Train: step:  38940, time: 0.200, loss: 1785.452637\n",
      "Train: step:  38950, time: 0.225, loss: 3403.573242\n",
      "Train: step:  38960, time: 0.211, loss: 1822.301270\n",
      "Train: step:  38970, time: 0.225, loss: 3872.955322\n",
      "Train: step:  38980, time: 0.208, loss: 2800.452393\n",
      "Train: step:  38990, time: 0.198, loss: 1029.081665\n",
      "Train: step:  39000, time: 0.233, loss: 3311.529053\n",
      "Train: step:  39010, time: 0.238, loss: 3371.196289\n",
      "Train: step:  39020, time: 0.224, loss: 899.303223\n",
      "Train: step:  39030, time: 0.208, loss: 3346.029541\n",
      "Train: step:  39040, time: 0.223, loss: 3564.223389\n",
      "Train: step:  39050, time: 0.224, loss: 1885.401245\n",
      "Train: step:  39060, time: 0.245, loss: 1466.908569\n",
      "Train: step:  39070, time: 0.204, loss: 1112.549805\n",
      "Train: step:  39080, time: 0.206, loss: 2562.145508\n",
      "Train: step:  39090, time: 0.209, loss: 1509.567139\n",
      "Train: step:  39100, time: 0.236, loss: 2017.501465\n",
      "Train: step:  39110, time: 0.229, loss: 818.876038\n",
      "Train: step:  39120, time: 0.207, loss: 728.491150\n",
      "Train: step:  39130, time: 0.249, loss: 2721.792969\n",
      "Train: step:  39140, time: 0.209, loss: 3235.300781\n",
      "Train: step:  39150, time: 0.249, loss: 3680.365479\n",
      "Train: step:  39160, time: 0.225, loss: 1047.084473\n",
      "Train: step:  39170, time: 0.206, loss: 977.421570\n",
      "Train: step:  39180, time: 0.206, loss: 486.408752\n",
      "Train: step:  39190, time: 0.224, loss: 1091.664307\n",
      "Train: step:  39200, time: 0.200, loss: 1211.118896\n",
      "Train: step:  39210, time: 0.215, loss: 690.894470\n",
      "Train: step:  39220, time: 0.209, loss: 1453.086670\n",
      "Train: step:  39230, time: 0.203, loss: 2693.086914\n",
      "Train: step:  39240, time: 0.223, loss: 393.632538\n",
      "Train: step:  39250, time: 0.203, loss: 666.802856\n",
      "Train: step:  39260, time: 0.200, loss: 3014.328857\n",
      "Train: step:  39270, time: 0.211, loss: 1760.091553\n",
      "Train: step:  39280, time: 0.205, loss: 1413.038086\n",
      "Train: step:  39290, time: 0.207, loss: 1010.405579\n",
      "Train: step:  39300, time: 0.208, loss: 1557.003296\n",
      "Train: step:  39310, time: 0.222, loss: 2169.944824\n",
      "Train: step:  39320, time: 0.205, loss: 2382.438232\n",
      "Train: step:  39330, time: 0.203, loss: 1912.192993\n",
      "Train: step:  39340, time: 0.210, loss: 634.622192\n",
      "Train: step:  39350, time: 0.217, loss: 3202.674805\n",
      "Train: step:  39360, time: 0.200, loss: 1744.890503\n",
      "Train: step:  39370, time: 0.210, loss: 1869.908691\n",
      "Train: step:  39380, time: 0.231, loss: 2423.902344\n",
      "Train: step:  39390, time: 0.208, loss: 2195.597900\n",
      "Train: step:  39400, time: 0.206, loss: 1147.102417\n",
      "Train: step:  39410, time: 0.230, loss: 2189.738770\n",
      "Train: step:  39420, time: 0.211, loss: 950.779907\n",
      "Train: step:  39430, time: 0.204, loss: 2485.575684\n",
      "Train: step:  39440, time: 0.224, loss: 1689.185181\n",
      "Train: step:  39450, time: 0.215, loss: 1238.091187\n",
      "Train: step:  39460, time: 0.225, loss: 2032.516602\n",
      "Train: step:  39470, time: 0.224, loss: 2270.417969\n",
      "Train: step:  39480, time: 0.207, loss: 3401.012207\n",
      "Train: step:  39490, time: 0.230, loss: 1923.319092\n",
      "Train: step:  39500, time: 0.231, loss: 593.191833\n",
      "Train: step:  39510, time: 0.205, loss: 407.501617\n",
      "Train: step:  39520, time: 0.231, loss: 1236.196411\n",
      "Train: step:  39530, time: 0.212, loss: 2891.336182\n",
      "Train: step:  39540, time: 0.235, loss: 1311.926147\n",
      "Train: step:  39550, time: 0.213, loss: 889.291199\n",
      "Train: step:  39560, time: 0.225, loss: 3210.945801\n",
      "Train: step:  39570, time: 0.212, loss: 2722.672119\n",
      "Train: step:  39580, time: 0.218, loss: 2811.715820\n",
      "Train: step:  39590, time: 0.209, loss: 1208.462646\n",
      "Train: step:  39600, time: 0.255, loss: 1823.918579\n",
      "Train: step:  39610, time: 0.212, loss: 2858.418457\n",
      "Train: step:  39620, time: 0.224, loss: 1598.180908\n",
      "Train: step:  39630, time: 0.227, loss: 1369.641235\n",
      "Train: step:  39640, time: 0.212, loss: 1601.897339\n",
      "Train: step:  39650, time: 0.251, loss: 1167.746582\n",
      "Train: step:  39660, time: 0.251, loss: 754.645020\n",
      "Train: step:  39670, time: 0.225, loss: 2167.184082\n",
      "Train: step:  39680, time: 0.216, loss: 1946.317627\n",
      "Train: step:  39690, time: 0.220, loss: 1752.714844\n",
      "Train: step:  39700, time: 0.228, loss: 745.603516\n",
      "Train: step:  39710, time: 0.218, loss: 2476.300781\n",
      "Train: step:  39720, time: 0.216, loss: 994.365234\n",
      "Train: step:  39730, time: 0.250, loss: 3722.690674\n",
      "Train: step:  39740, time: 0.227, loss: 2699.735107\n",
      "Train: step:  39750, time: 0.209, loss: 1571.197388\n",
      "Train: step:  39760, time: 0.231, loss: 592.304443\n",
      "Train: step:  39770, time: 0.214, loss: 2371.786133\n",
      "Train: step:  39780, time: 0.241, loss: 1542.625732\n",
      "Train: step:  39790, time: 0.232, loss: 1826.948608\n",
      "Train: step:  39800, time: 0.229, loss: 1256.725098\n",
      "Train: step:  39810, time: 0.222, loss: 1825.570068\n",
      "Train: step:  39820, time: 0.219, loss: 230.278809\n",
      "Train: step:  39830, time: 0.219, loss: 2290.117188\n",
      "Train: step:  39840, time: 0.225, loss: 2661.522949\n",
      "Train: step:  39850, time: 0.222, loss: 2273.977295\n",
      "Train: step:  39860, time: 0.222, loss: 2420.818604\n",
      "Train: step:  39870, time: 0.209, loss: 1813.953979\n",
      "Train: step:  39880, time: 0.226, loss: 956.936523\n",
      "Train: step:  39890, time: 0.285, loss: 212.142792\n",
      "Train: step:  39900, time: 0.219, loss: 979.372375\n",
      "Train: step:  39910, time: 0.208, loss: 1077.366089\n",
      "Train: step:  39920, time: 0.210, loss: 972.775269\n",
      "Train: step:  39930, time: 0.215, loss: 2790.713135\n",
      "Train: step:  39940, time: 0.213, loss: 895.312195\n",
      "Train: step:  39950, time: 0.211, loss: 1881.922974\n",
      "Train: step:  39960, time: 0.235, loss: 2499.734131\n",
      "Train: step:  39970, time: 0.231, loss: 869.035034\n",
      "Train: step:  39980, time: 0.199, loss: 499.357300\n",
      "Train: step:  39990, time: 0.220, loss: 1843.295898\n",
      "Train: step:  40000, time: 0.209, loss: 1172.977539\n",
      "Train: step:  40010, time: 0.212, loss: 2717.893555\n",
      "Train: step:  40020, time: 0.229, loss: 1808.344849\n",
      "Train: step:  40030, time: 0.229, loss: 1950.507812\n",
      "Train: step:  40040, time: 0.259, loss: 1944.894897\n",
      "Train: step:  40050, time: 0.214, loss: 2771.500732\n",
      "Train: step:  40060, time: 0.239, loss: 1378.172485\n",
      "Train: step:  40070, time: 0.214, loss: 3012.612305\n",
      "Train: step:  40080, time: 0.244, loss: 1638.679443\n",
      "Train: step:  40090, time: 0.212, loss: 2743.643311\n",
      "Train: step:  40100, time: 0.206, loss: 1474.803711\n",
      "Train: step:  40110, time: 0.206, loss: 2034.622070\n",
      "Train: step:  40120, time: 0.218, loss: 3218.569336\n",
      "Train: step:  40130, time: 0.237, loss: 2287.689941\n",
      "Train: step:  40140, time: 0.243, loss: 2153.979736\n",
      "Train: step:  40150, time: 0.220, loss: 550.173889\n",
      "Train: step:  40160, time: 0.274, loss: 1598.388428\n",
      "Train: step:  40170, time: 0.244, loss: 2678.509033\n",
      "Train: step:  40180, time: 0.222, loss: 1747.765747\n",
      "Train: step:  40190, time: 0.247, loss: 3340.150879\n",
      "Train: step:  40200, time: 0.254, loss: 1735.390137\n",
      "Train: step:  40210, time: 0.236, loss: 1577.308105\n",
      "Train: step:  40220, time: 0.256, loss: 2225.850342\n",
      "Train: step:  40230, time: 0.267, loss: 1718.776611\n",
      "Train: step:  40240, time: 0.257, loss: 982.425171\n",
      "Train: step:  40250, time: 0.283, loss: 301.155579\n",
      "Train: step:  40260, time: 0.235, loss: 2565.625732\n",
      "Train: step:  40270, time: 0.242, loss: 2443.467041\n",
      "Train: step:  40280, time: 0.242, loss: 2140.726562\n",
      "Train: step:  40290, time: 0.397, loss: 1691.470703\n",
      "Train: step:  40300, time: 0.249, loss: 1067.335083\n",
      "Train: step:  40310, time: 0.270, loss: 1632.057861\n",
      "Train: step:  40320, time: 0.247, loss: 1828.432861\n",
      "Train: step:  40330, time: 0.240, loss: 1864.591187\n",
      "Train: step:  40340, time: 0.244, loss: 3255.872314\n",
      "Train: step:  40350, time: 0.248, loss: 1561.766968\n",
      "Train: step:  40360, time: 0.249, loss: 2169.420898\n",
      "Train: step:  40370, time: 0.268, loss: 3100.071045\n",
      "Train: step:  40380, time: 0.258, loss: 423.946686\n",
      "Train: step:  40390, time: 0.262, loss: 2031.781372\n",
      "Train: step:  40400, time: 0.258, loss: 1529.890137\n",
      "Train: step:  40410, time: 0.243, loss: 1023.218628\n",
      "Train: step:  40420, time: 0.322, loss: 2082.857422\n",
      "Train: step:  40430, time: 0.240, loss: 244.532425\n",
      "Train: step:  40440, time: 0.270, loss: 2173.244629\n",
      "Train: step:  40450, time: 0.265, loss: 3578.433594\n",
      "Train: step:  40460, time: 0.286, loss: 2323.205078\n",
      "Train: step:  40470, time: 0.242, loss: 2730.512451\n",
      "Train: step:  40480, time: 0.239, loss: 2841.164307\n",
      "Train: step:  40490, time: 0.260, loss: 4346.386230\n",
      "Train: step:  40500, time: 0.269, loss: 1051.849731\n",
      "Train: step:  40510, time: 0.242, loss: 1680.084229\n",
      "Train: step:  40520, time: 0.298, loss: 799.363464\n",
      "Train: step:  40530, time: 0.262, loss: 1132.252563\n",
      "Train: step:  40540, time: 0.304, loss: 2190.835693\n",
      "Train: step:  40550, time: 0.238, loss: 1054.664062\n",
      "Train: step:  40560, time: 0.236, loss: 1024.278442\n",
      "Train: step:  40570, time: 0.241, loss: 2938.780518\n",
      "Train: step:  40580, time: 0.259, loss: 1206.408569\n",
      "Train: step:  40590, time: 0.229, loss: 1067.274536\n",
      "Train: step:  40600, time: 0.240, loss: 1458.497192\n",
      "Train: step:  40610, time: 0.228, loss: 757.654602\n",
      "Train: step:  40620, time: 0.229, loss: 1800.814209\n",
      "Train: step:  40630, time: 0.237, loss: 794.138062\n",
      "Train: step:  40640, time: 0.257, loss: 3112.436279\n",
      "Train: step:  40650, time: 0.236, loss: 435.273560\n",
      "Train: step:  40660, time: 0.237, loss: 1552.029053\n",
      "Train: step:  40670, time: 0.288, loss: 1350.429199\n",
      "Train: step:  40680, time: 0.256, loss: 3565.111084\n",
      "Train: step:  40690, time: 0.255, loss: 4121.981445\n",
      "Train: step:  40700, time: 0.251, loss: 1174.244995\n",
      "Train: step:  40710, time: 0.264, loss: 2775.453369\n",
      "Train: step:  40720, time: 0.235, loss: 639.909485\n",
      "Train: step:  40730, time: 0.267, loss: 2040.001587\n",
      "Train: step:  40740, time: 0.243, loss: 3005.236816\n",
      "Train: step:  40750, time: 0.241, loss: 1132.093750\n",
      "Train: step:  40760, time: 0.244, loss: 2682.991455\n",
      "Train: step:  40770, time: 0.267, loss: 1773.744019\n",
      "Train: step:  40780, time: 0.281, loss: 2027.719727\n",
      "Train: step:  40790, time: 0.245, loss: 1179.415161\n",
      "Train: step:  40800, time: 0.245, loss: 2439.319824\n",
      "Train: step:  40810, time: 0.256, loss: 2204.274170\n",
      "Train: step:  40820, time: 0.250, loss: 1641.091309\n",
      "Train: step:  40830, time: 0.242, loss: 5707.531250\n",
      "Train: step:  40840, time: 0.255, loss: 1205.195435\n",
      "Train: step:  40850, time: 0.244, loss: 1814.853760\n",
      "Train: step:  40860, time: 0.244, loss: 1051.182129\n",
      "Train: step:  40870, time: 0.275, loss: 2182.629883\n",
      "Train: step:  40880, time: 0.264, loss: 290.134857\n",
      "Train: step:  40890, time: 0.265, loss: 2159.799316\n",
      "Train: step:  40900, time: 0.271, loss: 776.742371\n",
      "Train: step:  40910, time: 0.245, loss: 603.160767\n",
      "Train: step:  40920, time: 0.251, loss: 1879.284546\n",
      "Train: step:  40930, time: 0.240, loss: 1922.595703\n",
      "Train: step:  40940, time: 0.247, loss: 1096.222900\n",
      "Train: step:  40950, time: 0.249, loss: 335.221039\n",
      "Train: step:  40960, time: 0.243, loss: 1189.968140\n",
      "Train: step:  40970, time: 0.265, loss: 3691.964600\n",
      "Train: step:  40980, time: 0.248, loss: 1542.537231\n",
      "Train: step:  40990, time: 0.238, loss: 2802.184082\n",
      "Train: step:  41000, time: 0.237, loss: 2683.841309\n",
      "Train: step:  41010, time: 0.247, loss: 2190.895264\n",
      "Train: step:  41020, time: 0.236, loss: 549.345886\n",
      "Train: step:  41030, time: 0.257, loss: 2510.652100\n",
      "Train: step:  41040, time: 0.236, loss: 3363.400391\n",
      "Train: step:  41050, time: 0.267, loss: 1252.269897\n",
      "Train: step:  41060, time: 0.242, loss: 1425.162476\n",
      "Train: step:  41070, time: 0.244, loss: 1764.897949\n",
      "Train: step:  41080, time: 0.313, loss: 1288.202515\n",
      "Train: step:  41090, time: 0.238, loss: 1866.966309\n",
      "Train: step:  41100, time: 0.234, loss: 2409.334717\n",
      "Train: step:  41110, time: 0.262, loss: 1320.908569\n",
      "Train: step:  41120, time: 0.233, loss: 2261.934814\n",
      "Train: step:  41130, time: 0.243, loss: 1315.084717\n",
      "Train: step:  41140, time: 0.236, loss: 852.153320\n",
      "Train: step:  41150, time: 0.235, loss: 5271.205566\n",
      "Train: step:  41160, time: 0.246, loss: 3709.629395\n",
      "Train: step:  41170, time: 0.241, loss: 639.260010\n",
      "Train: step:  41180, time: 0.248, loss: 1801.610840\n",
      "Train: step:  41190, time: 0.243, loss: 491.918976\n",
      "Train: step:  41200, time: 0.244, loss: 1593.298340\n",
      "Train: step:  41210, time: 0.240, loss: 2398.548096\n",
      "Train: step:  41220, time: 0.241, loss: 2399.757324\n",
      "Train: step:  41230, time: 0.265, loss: 3062.751709\n",
      "Train: step:  41240, time: 0.239, loss: 917.020630\n",
      "Train: step:  41250, time: 0.235, loss: 1312.407837\n",
      "Train: step:  41260, time: 0.257, loss: 2566.887207\n",
      "Train: step:  41270, time: 0.271, loss: 1399.812012\n",
      "Train: step:  41280, time: 0.243, loss: 985.796631\n",
      "Train: step:  41290, time: 0.239, loss: 1291.280396\n",
      "Train: step:  41300, time: 0.263, loss: 1944.341431\n",
      "Train: step:  41310, time: 0.250, loss: 3031.386230\n",
      "Train: step:  41320, time: 0.268, loss: 1091.102295\n",
      "Train: step:  41330, time: 0.267, loss: 3067.238037\n",
      "Train: step:  41340, time: 0.244, loss: 3146.676025\n",
      "Train: step:  41350, time: 0.266, loss: 597.763977\n",
      "Train: step:  41360, time: 0.240, loss: 1769.273682\n",
      "Train: step:  41370, time: 0.273, loss: 878.989380\n",
      "Train: step:  41380, time: 0.253, loss: 2365.060791\n",
      "Train: step:  41390, time: 0.246, loss: 2779.383789\n",
      "Train: step:  41400, time: 0.245, loss: 2612.959961\n",
      "Train: step:  41410, time: 0.270, loss: 2851.584961\n",
      "Train: step:  41420, time: 0.242, loss: 3054.963135\n",
      "Train: step:  41430, time: 0.268, loss: 1153.936523\n",
      "Train: step:  41440, time: 0.244, loss: 2659.018799\n",
      "Train: step:  41450, time: 0.254, loss: 1185.993652\n",
      "Train: step:  41460, time: 0.253, loss: 2730.967285\n",
      "Train: step:  41470, time: 0.242, loss: 2299.702881\n",
      "Train: step:  41480, time: 0.269, loss: 1881.068481\n",
      "Train: step:  41490, time: 0.244, loss: 1280.698608\n",
      "Train: step:  41500, time: 0.277, loss: 980.622803\n",
      "Train: step:  41510, time: 0.241, loss: 3930.488037\n",
      "Train: step:  41520, time: 0.273, loss: 2426.072266\n",
      "Train: step:  41530, time: 0.237, loss: 456.784576\n",
      "Train: step:  41540, time: 0.243, loss: 1944.863647\n",
      "Train: step:  41550, time: 0.281, loss: 2217.486572\n",
      "Train: step:  41560, time: 0.257, loss: 1183.564209\n",
      "Train: step:  41570, time: 0.241, loss: 1903.305420\n",
      "Train: step:  41580, time: 0.240, loss: 2915.684814\n",
      "Train: step:  41590, time: 0.238, loss: 3299.771240\n",
      "Train: step:  41600, time: 0.252, loss: 858.849243\n",
      "Train: step:  41610, time: 0.246, loss: 2569.371094\n",
      "Train: step:  41620, time: 0.241, loss: 153.122833\n",
      "Train: step:  41630, time: 0.244, loss: 1754.626953\n",
      "Train: step:  41640, time: 0.242, loss: 2096.981201\n",
      "Train: step:  41650, time: 0.249, loss: 2319.940674\n",
      "Train: step:  41660, time: 0.256, loss: 3193.900879\n",
      "Train: step:  41670, time: 0.263, loss: 2501.281494\n",
      "Train: step:  41680, time: 0.238, loss: 1382.438965\n",
      "Train: step:  41690, time: 0.277, loss: 592.159241\n",
      "Train: step:  41700, time: 0.261, loss: 589.682800\n",
      "Train: step:  41710, time: 0.235, loss: 1874.697754\n",
      "Train: step:  41720, time: 0.262, loss: 1144.324951\n",
      "Train: step:  41730, time: 0.260, loss: 3275.058594\n",
      "Train: step:  41740, time: 0.268, loss: 1518.266724\n",
      "Train: step:  41750, time: 0.238, loss: 1355.520264\n",
      "Train: step:  41760, time: 0.248, loss: 2822.813232\n",
      "Train: step:  41770, time: 0.243, loss: 1121.344727\n",
      "Train: step:  41780, time: 0.244, loss: 244.854553\n",
      "Train: step:  41790, time: 0.235, loss: 870.438538\n",
      "Train: step:  41800, time: 0.245, loss: 2670.511963\n",
      "Train: step:  41810, time: 0.236, loss: 2588.893066\n",
      "Train: step:  41820, time: 0.243, loss: 3682.375977\n",
      "Train: step:  41830, time: 0.243, loss: 1176.842407\n",
      "Train: step:  41840, time: 0.231, loss: 1628.188843\n",
      "Train: step:  41850, time: 0.246, loss: 1855.797607\n",
      "Train: step:  41860, time: 0.266, loss: 941.397095\n",
      "Train: step:  41870, time: 0.246, loss: 1930.470459\n",
      "Train: step:  41880, time: 0.241, loss: 235.162598\n",
      "Train: step:  41890, time: 0.271, loss: 629.751892\n",
      "Train: step:  41900, time: 0.242, loss: 2889.733154\n",
      "Train: step:  41910, time: 0.235, loss: 3161.568604\n",
      "Train: step:  41920, time: 0.233, loss: 2229.942139\n",
      "Train: step:  41930, time: 0.230, loss: 2025.369629\n",
      "Train: step:  41940, time: 0.218, loss: 1426.412842\n",
      "Train: step:  41950, time: 0.227, loss: 2989.714355\n",
      "Train: step:  41960, time: 0.227, loss: 1113.366699\n",
      "Train: step:  41970, time: 0.234, loss: 2250.537598\n",
      "Train: step:  41980, time: 0.230, loss: 2057.020264\n",
      "Train: step:  41990, time: 0.233, loss: 674.679932\n",
      "Train: step:  42000, time: 0.240, loss: 1442.993652\n",
      "Train: step:  42010, time: 0.246, loss: 4638.594727\n",
      "Train: step:  42020, time: 0.230, loss: 448.344574\n",
      "Train: step:  42030, time: 0.237, loss: 2611.478027\n",
      "Train: step:  42040, time: 0.225, loss: 2596.006592\n",
      "Train: step:  42050, time: 0.219, loss: 2980.705078\n",
      "Train: step:  42060, time: 0.223, loss: 1266.637451\n",
      "Train: step:  42070, time: 0.221, loss: 2199.178223\n",
      "Train: step:  42080, time: 0.220, loss: 1211.427612\n",
      "Train: step:  42090, time: 0.224, loss: 1842.147095\n",
      "Train: step:  42100, time: 0.221, loss: 392.744781\n",
      "Train: step:  42110, time: 0.227, loss: 808.074219\n",
      "Train: step:  42120, time: 0.228, loss: 1503.900879\n",
      "Train: step:  42130, time: 0.230, loss: 2711.358643\n",
      "Train: step:  42140, time: 0.214, loss: 1169.944214\n",
      "Train: step:  42150, time: 0.231, loss: 1925.185425\n",
      "Train: step:  42160, time: 0.225, loss: 1691.793091\n",
      "Train: step:  42170, time: 0.236, loss: 368.280457\n",
      "Train: step:  42180, time: 0.263, loss: 1847.572754\n",
      "Train: step:  42190, time: 0.261, loss: 1351.315308\n",
      "Train: step:  42200, time: 0.238, loss: 1864.846313\n",
      "Train: step:  42210, time: 0.228, loss: 1219.349609\n",
      "Train: step:  42220, time: 0.275, loss: 2615.141846\n",
      "Train: step:  42230, time: 0.233, loss: 2185.634521\n",
      "Train: step:  42240, time: 0.238, loss: 1192.400879\n",
      "Train: step:  42250, time: 0.262, loss: 2791.454834\n",
      "Train: step:  42260, time: 0.224, loss: 3154.707764\n",
      "Train: step:  42270, time: 0.227, loss: 2916.704346\n",
      "Train: step:  42280, time: 0.229, loss: 2474.616943\n",
      "Train: step:  42290, time: 0.276, loss: 262.810669\n",
      "Train: step:  42300, time: 0.226, loss: 3016.843018\n",
      "Train: step:  42310, time: 0.231, loss: 1681.879761\n",
      "Train: step:  42320, time: 0.237, loss: 3373.096191\n",
      "Train: step:  42330, time: 0.221, loss: 2334.786865\n",
      "Train: step:  42340, time: 0.232, loss: 2033.997925\n",
      "Train: step:  42350, time: 0.235, loss: 5867.506836\n",
      "Train: step:  42360, time: 0.235, loss: 2200.377197\n",
      "Train: step:  42370, time: 0.251, loss: 3373.243896\n",
      "Train: step:  42380, time: 0.243, loss: 2998.444580\n",
      "Train: step:  42390, time: 0.238, loss: 580.053589\n",
      "Train: step:  42400, time: 0.242, loss: 1971.276978\n",
      "Train: step:  42410, time: 0.236, loss: 1130.818848\n",
      "Train: step:  42420, time: 0.241, loss: 1283.651367\n",
      "Train: step:  42430, time: 0.239, loss: 2507.744141\n",
      "Train: step:  42440, time: 0.240, loss: 3427.664551\n",
      "Train: step:  42450, time: 0.270, loss: 1593.346436\n",
      "Train: step:  42460, time: 0.248, loss: 1558.936035\n",
      "Train: step:  42470, time: 0.244, loss: 1375.404175\n",
      "Train: step:  42480, time: 0.241, loss: 1955.034912\n",
      "Train: step:  42490, time: 0.226, loss: 2626.896729\n",
      "Train: step:  42500, time: 0.225, loss: 1807.878418\n",
      "Train: step:  42510, time: 0.231, loss: 2844.070801\n",
      "Train: step:  42520, time: 0.239, loss: 1822.322144\n",
      "Train: step:  42530, time: 0.227, loss: 1678.765747\n",
      "Train: step:  42540, time: 0.222, loss: 3242.243896\n",
      "Train: step:  42550, time: 0.219, loss: 840.979980\n",
      "Train: step:  42560, time: 0.235, loss: 642.896851\n",
      "Train: step:  42570, time: 0.248, loss: 3731.948242\n",
      "Train: step:  42580, time: 0.245, loss: 1698.199707\n",
      "Train: step:  42590, time: 0.263, loss: 1852.664673\n",
      "Train: step:  42600, time: 0.231, loss: 2994.156982\n",
      "Train: step:  42610, time: 0.224, loss: 2385.774414\n",
      "Train: step:  42620, time: 0.256, loss: 1737.747803\n",
      "Train: step:  42630, time: 0.246, loss: 2199.040527\n",
      "Train: step:  42640, time: 0.255, loss: 2134.162598\n",
      "Train: step:  42650, time: 0.253, loss: 4346.980469\n",
      "Train: step:  42660, time: 0.251, loss: 1215.808594\n",
      "Train: step:  42670, time: 0.248, loss: 644.245361\n",
      "Train: step:  42680, time: 0.263, loss: 1337.316406\n",
      "Train: step:  42690, time: 0.237, loss: 1635.138672\n",
      "Train: step:  42700, time: 0.249, loss: 3020.058594\n",
      "Train: step:  42710, time: 0.260, loss: 2278.599609\n",
      "Train: step:  42720, time: 0.234, loss: 1778.646362\n",
      "Train: step:  42730, time: 0.229, loss: 1449.079346\n",
      "Train: step:  42740, time: 0.234, loss: 1249.934570\n",
      "Train: step:  42750, time: 0.236, loss: 1342.937378\n",
      "Train: step:  42760, time: 0.235, loss: 2114.029297\n",
      "Train: step:  42770, time: 0.224, loss: 855.664001\n",
      "Train: step:  42780, time: 0.237, loss: 3829.428955\n",
      "Train: step:  42790, time: 0.238, loss: 3815.282471\n",
      "Train: step:  42800, time: 0.234, loss: 1309.377075\n",
      "Train: step:  42810, time: 0.262, loss: 2673.079590\n",
      "Train: step:  42820, time: 0.269, loss: 1168.736450\n",
      "Train: step:  42830, time: 0.230, loss: 1223.930542\n",
      "Train: step:  42840, time: 0.239, loss: 1521.209229\n",
      "Train: step:  42850, time: 0.262, loss: 3228.695557\n",
      "Train: step:  42860, time: 0.235, loss: 408.591766\n",
      "Train: step:  42870, time: 0.235, loss: 708.654419\n",
      "Train: step:  42880, time: 0.224, loss: 2942.476807\n",
      "Train: step:  42890, time: 0.278, loss: 753.297363\n",
      "Train: step:  42900, time: 0.253, loss: 2080.198730\n",
      "Train: step:  42910, time: 0.239, loss: 521.084778\n",
      "Train: step:  42920, time: 0.235, loss: 1573.813477\n",
      "Train: step:  42930, time: 0.268, loss: 2668.779541\n",
      "Train: step:  42940, time: 0.255, loss: 799.286438\n",
      "Train: step:  42950, time: 0.256, loss: 2343.631104\n",
      "Train: step:  42960, time: 0.233, loss: 2022.654907\n",
      "Train: step:  42970, time: 0.240, loss: 2962.726807\n",
      "Train: step:  42980, time: 0.258, loss: 1843.296021\n",
      "Train: step:  42990, time: 0.235, loss: 3235.727539\n",
      "Train: step:  43000, time: 0.236, loss: 1114.279419\n",
      "Train: step:  43010, time: 0.239, loss: 3989.963135\n",
      "Train: step:  43020, time: 0.239, loss: 1928.331665\n",
      "Train: step:  43030, time: 0.240, loss: 1307.737915\n",
      "Train: step:  43040, time: 0.234, loss: 2594.819336\n",
      "Train: step:  43050, time: 0.238, loss: 3460.862549\n",
      "Train: step:  43060, time: 0.282, loss: 3210.518066\n",
      "Train: step:  43070, time: 0.245, loss: 1777.139648\n",
      "Train: step:  43080, time: 0.234, loss: 1649.420044\n",
      "Train: step:  43090, time: 0.282, loss: 2057.466064\n",
      "Train: step:  43100, time: 0.237, loss: 3553.906494\n",
      "Train: step:  43110, time: 0.258, loss: 1634.009033\n",
      "Train: step:  43120, time: 0.260, loss: 2154.527100\n",
      "Train: step:  43130, time: 0.263, loss: 1050.136719\n",
      "Train: step:  43140, time: 0.232, loss: 1443.278198\n",
      "Train: step:  43150, time: 0.239, loss: 1386.687744\n",
      "Train: step:  43160, time: 0.234, loss: 2260.480225\n",
      "Train: step:  43170, time: 0.237, loss: 898.119202\n",
      "Train: step:  43180, time: 0.256, loss: 1652.226929\n",
      "Train: step:  43190, time: 0.265, loss: 2972.183838\n",
      "Train: step:  43200, time: 0.231, loss: 2519.883301\n",
      "Train: step:  43210, time: 0.240, loss: 4292.366699\n",
      "Train: step:  43220, time: 0.271, loss: 3550.351807\n",
      "Train: step:  43230, time: 0.237, loss: 3134.253174\n",
      "Train: step:  43240, time: 0.237, loss: 517.620605\n",
      "Train: step:  43250, time: 0.255, loss: 1059.494385\n",
      "Train: step:  43260, time: 0.238, loss: 2475.221924\n",
      "Train: step:  43270, time: 0.259, loss: 1453.601196\n",
      "Train: step:  43280, time: 0.239, loss: 2400.094238\n",
      "Train: step:  43290, time: 0.269, loss: 2536.107910\n",
      "Train: step:  43300, time: 0.267, loss: 2070.589844\n",
      "Train: step:  43310, time: 0.264, loss: 419.836609\n",
      "Train: step:  43320, time: 0.234, loss: 1707.848145\n",
      "Train: step:  43330, time: 0.239, loss: 3710.657959\n",
      "Train: step:  43340, time: 0.256, loss: 2156.581787\n",
      "Train: step:  43350, time: 0.244, loss: 522.203186\n",
      "Train: step:  43360, time: 0.244, loss: 2412.771973\n",
      "Train: step:  43370, time: 0.236, loss: 251.215775\n",
      "Train: step:  43380, time: 0.251, loss: 1544.319824\n",
      "Train: step:  43390, time: 0.228, loss: 498.210602\n",
      "Train: step:  43400, time: 0.234, loss: 314.573944\n",
      "Train: step:  43410, time: 0.272, loss: 2527.122803\n",
      "Train: step:  43420, time: 0.242, loss: 1444.835571\n",
      "Train: step:  43430, time: 0.265, loss: 2068.809570\n",
      "Train: step:  43440, time: 0.270, loss: 1697.640381\n",
      "Train: step:  43450, time: 0.240, loss: 2436.336914\n",
      "Train: step:  43460, time: 0.237, loss: 3297.302490\n",
      "Train: step:  43470, time: 0.240, loss: 1448.346924\n",
      "Train: step:  43480, time: 0.239, loss: 1434.191772\n",
      "Train: step:  43490, time: 0.237, loss: 2163.150635\n",
      "Train: step:  43500, time: 0.240, loss: 1119.435425\n",
      "Train: step:  43510, time: 0.240, loss: 3512.017578\n",
      "Train: step:  43520, time: 0.268, loss: 2499.975342\n",
      "Train: step:  43530, time: 0.240, loss: 602.105652\n",
      "Train: step:  43540, time: 0.241, loss: 2737.839111\n",
      "Train: step:  43550, time: 0.249, loss: 1272.589478\n",
      "Train: step:  43560, time: 0.258, loss: 1672.553589\n",
      "Train: step:  43570, time: 0.264, loss: 2308.412598\n",
      "Train: step:  43580, time: 0.277, loss: 2065.200684\n",
      "Train: step:  43590, time: 0.236, loss: 1733.892334\n",
      "Train: step:  43600, time: 0.267, loss: 1320.193970\n",
      "Train: step:  43610, time: 0.266, loss: 1224.067871\n",
      "Train: step:  43620, time: 0.237, loss: 787.083618\n",
      "Train: step:  43630, time: 0.261, loss: 1540.523926\n",
      "Train: step:  43640, time: 0.261, loss: 1206.422119\n",
      "Train: step:  43650, time: 0.238, loss: 893.574463\n",
      "Train: step:  43660, time: 0.240, loss: 2735.976807\n",
      "Train: step:  43670, time: 0.243, loss: 978.509155\n",
      "Train: step:  43680, time: 0.233, loss: 2424.868408\n",
      "Train: step:  43690, time: 0.234, loss: 2527.156494\n",
      "Train: step:  43700, time: 0.237, loss: 2671.314697\n",
      "Train: step:  43710, time: 0.246, loss: 2245.599609\n",
      "Train: step:  43720, time: 0.269, loss: 1836.432617\n",
      "Train: step:  43730, time: 0.244, loss: 2398.466064\n",
      "Train: step:  43740, time: 0.243, loss: 722.619019\n",
      "Train: step:  43750, time: 0.234, loss: 315.779694\n",
      "Train: step:  43760, time: 0.230, loss: 3816.758545\n",
      "Train: step:  43770, time: 0.229, loss: 728.420837\n",
      "Train: step:  43780, time: 0.248, loss: 1783.075684\n",
      "Train: step:  43790, time: 0.225, loss: 2619.824951\n",
      "Train: step:  43800, time: 0.225, loss: 1431.784546\n",
      "Train: step:  43810, time: 0.223, loss: 1685.836670\n",
      "Train: step:  43820, time: 0.225, loss: 716.165527\n",
      "Train: step:  43830, time: 0.216, loss: 2723.574219\n",
      "Train: step:  43840, time: 0.232, loss: 2245.722656\n",
      "Train: step:  43850, time: 0.261, loss: 3771.175049\n",
      "Train: step:  43860, time: 0.227, loss: 1138.310913\n",
      "Train: step:  43870, time: 0.239, loss: 791.368225\n",
      "Train: step:  43880, time: 0.231, loss: 959.395630\n",
      "Train: step:  43890, time: 0.246, loss: 2142.751953\n",
      "Train: step:  43900, time: 0.233, loss: 904.538269\n",
      "Train: step:  43910, time: 0.228, loss: 3734.204346\n",
      "Train: step:  43920, time: 0.223, loss: 498.131561\n",
      "Train: step:  43930, time: 0.233, loss: 1783.093994\n",
      "Train: step:  43940, time: 0.233, loss: 2352.457275\n",
      "Train: step:  43950, time: 0.241, loss: 1127.949707\n",
      "Train: step:  43960, time: 0.253, loss: 1101.924316\n",
      "Train: step:  43970, time: 0.261, loss: 3126.288574\n",
      "Train: step:  43980, time: 0.257, loss: 1155.510132\n",
      "Train: step:  43990, time: 0.235, loss: 1757.708130\n",
      "Train: step:  44000, time: 0.235, loss: 744.714539\n",
      "Train: step:  44010, time: 0.233, loss: 3148.889160\n",
      "Train: step:  44020, time: 0.229, loss: 2466.519531\n",
      "Train: step:  44030, time: 0.238, loss: 3094.886719\n",
      "Train: step:  44040, time: 0.251, loss: 2500.405273\n",
      "Train: step:  44050, time: 0.248, loss: 3097.133789\n",
      "Train: step:  44060, time: 0.235, loss: 316.166473\n",
      "Train: step:  44070, time: 0.260, loss: 2072.831299\n",
      "Train: step:  44080, time: 0.247, loss: 2532.622803\n",
      "Train: step:  44090, time: 0.251, loss: 1413.574219\n",
      "Train: step:  44100, time: 0.255, loss: 2912.134521\n",
      "Train: step:  44110, time: 0.257, loss: 2328.858643\n",
      "Train: step:  44120, time: 0.233, loss: 212.533157\n",
      "Train: step:  44130, time: 0.221, loss: 1053.353027\n",
      "Train: step:  44140, time: 0.226, loss: 1384.614380\n",
      "Train: step:  44150, time: 0.261, loss: 3181.885742\n",
      "Train: step:  44160, time: 0.233, loss: 574.326782\n",
      "Train: step:  44170, time: 0.221, loss: 1672.665527\n",
      "Train: step:  44180, time: 0.227, loss: 1869.646606\n",
      "Train: step:  44190, time: 0.230, loss: 1276.994751\n",
      "Train: step:  44200, time: 0.228, loss: 1309.244019\n",
      "Train: step:  44210, time: 0.232, loss: 1906.818115\n",
      "Train: step:  44220, time: 0.259, loss: 1725.503418\n",
      "Train: step:  44230, time: 0.246, loss: 1404.860962\n",
      "Train: step:  44240, time: 0.234, loss: 1132.775024\n",
      "Train: step:  44250, time: 0.233, loss: 1634.105347\n",
      "Train: step:  44260, time: 0.267, loss: 2534.635986\n",
      "Train: step:  44270, time: 0.233, loss: 2588.079834\n",
      "Train: step:  44280, time: 0.233, loss: 865.640991\n",
      "Train: step:  44290, time: 0.258, loss: 767.175232\n",
      "Train: step:  44300, time: 0.243, loss: 479.059387\n",
      "Train: step:  44310, time: 0.239, loss: 2986.748779\n",
      "Train: step:  44320, time: 0.241, loss: 2360.679932\n",
      "Train: step:  44330, time: 0.264, loss: 535.627930\n",
      "Train: step:  44340, time: 0.277, loss: 650.905579\n",
      "Train: step:  44350, time: 0.220, loss: 1875.707031\n",
      "Train: step:  44360, time: 0.247, loss: 2951.267822\n",
      "Train: step:  44370, time: 0.249, loss: 358.024872\n",
      "Train: step:  44380, time: 0.229, loss: 2041.623291\n",
      "Train: step:  44390, time: 0.237, loss: 1631.348267\n",
      "Train: step:  44400, time: 0.242, loss: 2775.639404\n",
      "Train: step:  44410, time: 0.223, loss: 520.552734\n",
      "Train: step:  44420, time: 0.239, loss: 3249.467529\n",
      "Train: step:  44430, time: 0.226, loss: 916.613342\n",
      "Train: step:  44440, time: 0.238, loss: 2409.318359\n",
      "Train: step:  44450, time: 0.266, loss: 959.323792\n",
      "Train: step:  44460, time: 0.243, loss: 1791.296509\n",
      "Train: step:  44470, time: 0.264, loss: 2212.741455\n",
      "Train: step:  44480, time: 0.240, loss: 2366.747803\n",
      "Train: step:  44490, time: 0.234, loss: 3534.185791\n",
      "Train: step:  44500, time: 0.234, loss: 786.826294\n",
      "Train: step:  44510, time: 0.238, loss: 2054.343262\n",
      "Train: step:  44520, time: 0.279, loss: 1265.223389\n",
      "Train: step:  44530, time: 0.236, loss: 1423.492920\n",
      "Train: step:  44540, time: 0.244, loss: 1562.558838\n",
      "Train: step:  44550, time: 0.238, loss: 1488.498291\n",
      "Train: step:  44560, time: 0.255, loss: 1471.605591\n",
      "Train: step:  44570, time: 0.235, loss: 423.756683\n",
      "Train: step:  44580, time: 0.244, loss: 1682.025757\n",
      "Train: step:  44590, time: 0.238, loss: 1921.708496\n",
      "Train: step:  44600, time: 0.248, loss: 1936.684082\n",
      "Train: step:  44610, time: 0.244, loss: 3091.712646\n",
      "Train: step:  44620, time: 0.233, loss: 505.334473\n",
      "Train: step:  44630, time: 0.240, loss: 1386.285034\n",
      "Train: step:  44640, time: 0.238, loss: 966.104492\n",
      "Train: step:  44650, time: 0.275, loss: 2161.080078\n",
      "Train: step:  44660, time: 0.254, loss: 625.750549\n",
      "Train: step:  44670, time: 0.266, loss: 1812.276733\n",
      "Train: step:  44680, time: 0.242, loss: 1244.246582\n",
      "Train: step:  44690, time: 0.241, loss: 2676.589844\n",
      "Train: step:  44700, time: 0.235, loss: 1827.315918\n",
      "Train: step:  44710, time: 0.261, loss: 817.067688\n",
      "Train: step:  44720, time: 0.243, loss: 2974.742676\n",
      "Train: step:  44730, time: 0.241, loss: 2942.994141\n",
      "Train: step:  44740, time: 0.240, loss: 838.486633\n",
      "Train: step:  44750, time: 0.238, loss: 2238.265625\n",
      "Train: step:  44760, time: 0.283, loss: 4240.081543\n",
      "Train: step:  44770, time: 0.309, loss: 3220.863281\n",
      "Train: step:  44780, time: 0.245, loss: 927.873291\n",
      "Train: step:  44790, time: 0.290, loss: 2892.408936\n",
      "Train: step:  44800, time: 0.239, loss: 791.767273\n",
      "Train: step:  44810, time: 0.262, loss: 1779.410156\n",
      "Train: step:  44820, time: 0.238, loss: 736.236816\n",
      "Train: step:  44830, time: 0.249, loss: 3206.158936\n",
      "Train: step:  44840, time: 0.278, loss: 2762.576416\n",
      "Train: step:  44850, time: 0.236, loss: 1922.976562\n",
      "Train: step:  44860, time: 0.242, loss: 1022.987305\n",
      "Train: step:  44870, time: 0.235, loss: 2749.286865\n",
      "Train: step:  44880, time: 0.252, loss: 2312.257568\n",
      "Train: step:  44890, time: 0.238, loss: 3058.968994\n",
      "Train: step:  44900, time: 0.266, loss: 1120.972046\n",
      "Train: step:  44910, time: 0.263, loss: 769.351501\n",
      "Train: step:  44920, time: 0.236, loss: 867.384521\n",
      "Train: step:  44930, time: 0.257, loss: 462.221008\n",
      "Train: step:  44940, time: 0.271, loss: 527.469360\n",
      "Train: step:  44950, time: 0.247, loss: 3452.424561\n",
      "Train: step:  44960, time: 0.240, loss: 1392.178223\n",
      "Train: step:  44970, time: 0.232, loss: 3280.482422\n",
      "Train: step:  44980, time: 0.250, loss: 3626.562256\n",
      "Train: step:  44990, time: 0.245, loss: 2493.078125\n",
      "Train: step:  45000, time: 0.259, loss: 2857.378662\n",
      "Train: step:  45010, time: 0.240, loss: 2590.241943\n",
      "Train: step:  45020, time: 0.239, loss: 2314.623535\n",
      "Train: step:  45030, time: 0.244, loss: 2790.440918\n",
      "Train: step:  45040, time: 0.239, loss: 1558.870483\n",
      "Train: step:  45050, time: 0.236, loss: 1079.932373\n",
      "Train: step:  45060, time: 0.266, loss: 604.444763\n",
      "Train: step:  45070, time: 0.241, loss: 2037.270508\n",
      "Train: step:  45080, time: 0.243, loss: 3494.923340\n",
      "Train: step:  45090, time: 0.245, loss: 1231.511963\n",
      "Train: step:  45100, time: 0.275, loss: 2268.614258\n",
      "Train: step:  45110, time: 0.236, loss: 2954.353271\n",
      "Train: step:  45120, time: 0.277, loss: 2052.075439\n",
      "Train: step:  45130, time: 0.227, loss: 358.797089\n",
      "Train: step:  45140, time: 0.239, loss: 1306.776733\n",
      "Train: step:  45150, time: 0.234, loss: 3559.087402\n",
      "Train: step:  45160, time: 0.235, loss: 1669.179077\n",
      "Train: step:  45170, time: 0.241, loss: 300.586670\n",
      "Train: step:  45180, time: 0.242, loss: 2488.547607\n",
      "Train: step:  45190, time: 0.269, loss: 674.199402\n",
      "Train: step:  45200, time: 0.240, loss: 2349.343506\n",
      "Train: step:  45210, time: 0.235, loss: 1980.643311\n",
      "Train: step:  45220, time: 0.301, loss: 1836.208740\n",
      "Train: step:  45230, time: 0.239, loss: 825.116638\n",
      "Train: step:  45240, time: 0.240, loss: 2929.875977\n",
      "Train: step:  45250, time: 0.245, loss: 1541.936035\n",
      "Train: step:  45260, time: 0.242, loss: 5078.353027\n",
      "Train: step:  45270, time: 0.264, loss: 1852.989868\n",
      "Train: step:  45280, time: 0.242, loss: 3068.460693\n",
      "Train: step:  45290, time: 0.236, loss: 1543.189331\n",
      "Train: step:  45300, time: 0.232, loss: 795.197327\n",
      "Train: step:  45310, time: 0.248, loss: 1108.507568\n",
      "Train: step:  45320, time: 0.258, loss: 1641.963745\n",
      "Train: step:  45330, time: 0.231, loss: 2037.176514\n",
      "Train: step:  45340, time: 0.247, loss: 1645.210571\n",
      "Train: step:  45350, time: 0.268, loss: 1955.880127\n",
      "Train: step:  45360, time: 0.239, loss: 2367.279297\n",
      "Train: step:  45370, time: 0.287, loss: 1754.289429\n",
      "Train: step:  45380, time: 0.238, loss: 1841.855225\n",
      "Train: step:  45390, time: 0.232, loss: 2787.225830\n",
      "Train: step:  45400, time: 0.235, loss: 693.501709\n",
      "Train: step:  45410, time: 0.239, loss: 1841.392700\n",
      "Train: step:  45420, time: 0.236, loss: 2132.306152\n",
      "Train: step:  45430, time: 0.238, loss: 1607.435669\n",
      "Train: step:  45440, time: 0.243, loss: 1714.609619\n",
      "Train: step:  45450, time: 0.261, loss: 437.592438\n",
      "Train: step:  45460, time: 0.289, loss: 2079.500488\n",
      "Train: step:  45470, time: 0.242, loss: 1144.540405\n",
      "Train: step:  45480, time: 0.242, loss: 2728.135498\n",
      "Train: step:  45490, time: 0.232, loss: 2877.547607\n",
      "Train: step:  45500, time: 0.302, loss: 2655.279297\n",
      "Train: step:  45510, time: 0.225, loss: 2829.513672\n",
      "Train: step:  45520, time: 0.233, loss: 1835.348633\n",
      "Train: step:  45530, time: 0.223, loss: 2068.593750\n",
      "Train: step:  45540, time: 0.243, loss: 1838.023682\n",
      "Train: step:  45550, time: 0.234, loss: 1880.420776\n",
      "Train: step:  45560, time: 0.231, loss: 1683.795898\n",
      "Train: step:  45570, time: 0.272, loss: 1882.345947\n",
      "Train: step:  45580, time: 0.257, loss: 740.950195\n",
      "Train: step:  45590, time: 0.231, loss: 2389.550537\n",
      "Train: step:  45600, time: 0.273, loss: 3136.229736\n",
      "Train: step:  45610, time: 0.231, loss: 691.858459\n",
      "Train: step:  45620, time: 0.236, loss: 622.396790\n",
      "Train: step:  45630, time: 0.225, loss: 339.438477\n",
      "Train: step:  45640, time: 0.229, loss: 666.525696\n",
      "Train: step:  45650, time: 0.245, loss: 1319.620361\n",
      "Train: step:  45660, time: 0.234, loss: 1752.527344\n",
      "Train: step:  45670, time: 0.236, loss: 1657.598755\n",
      "Train: step:  45680, time: 0.249, loss: 1888.382080\n",
      "Train: step:  45690, time: 0.243, loss: 2019.578857\n",
      "Train: step:  45700, time: 0.229, loss: 3674.366455\n",
      "Train: step:  45710, time: 0.226, loss: 2179.239502\n",
      "Train: step:  45720, time: 0.234, loss: 2613.536865\n",
      "Train: step:  45730, time: 0.220, loss: 2621.261719\n",
      "Train: step:  45740, time: 0.235, loss: 860.312439\n",
      "Train: step:  45750, time: 0.228, loss: 2688.499268\n",
      "Train: step:  45760, time: 0.224, loss: 2815.691895\n",
      "Train: step:  45770, time: 0.225, loss: 1055.192505\n",
      "Train: step:  45780, time: 0.228, loss: 2693.916260\n",
      "Train: step:  45790, time: 0.229, loss: 2542.673096\n",
      "Train: step:  45800, time: 0.234, loss: 811.996582\n",
      "Train: step:  45810, time: 0.248, loss: 695.275757\n",
      "Train: step:  45820, time: 0.219, loss: 3087.070312\n",
      "Train: step:  45830, time: 0.254, loss: 3264.795898\n",
      "Train: step:  45840, time: 0.230, loss: 1521.927490\n",
      "Train: step:  45850, time: 0.228, loss: 1424.846802\n",
      "Train: step:  45860, time: 0.221, loss: 4664.875000\n",
      "Train: step:  45870, time: 0.248, loss: 1046.624878\n",
      "Train: step:  45880, time: 0.236, loss: 1903.269409\n",
      "Train: step:  45890, time: 0.256, loss: 4613.914062\n",
      "Train: step:  45900, time: 0.248, loss: 1896.377930\n",
      "Train: step:  45910, time: 0.246, loss: 2837.008057\n",
      "Train: step:  45920, time: 0.254, loss: 797.911133\n",
      "Train: step:  45930, time: 0.272, loss: 3103.460449\n",
      "Train: step:  45940, time: 0.232, loss: 2954.884033\n",
      "Train: step:  45950, time: 0.238, loss: 3407.931641\n",
      "Train: step:  45960, time: 0.240, loss: 2325.940186\n",
      "Train: step:  45970, time: 0.239, loss: 2459.511719\n",
      "Train: step:  45980, time: 0.241, loss: 2695.953613\n",
      "Train: step:  45990, time: 0.238, loss: 1172.647339\n",
      "Train: step:  46000, time: 0.241, loss: 2800.606934\n",
      "Train: step:  46010, time: 0.231, loss: 1132.250854\n",
      "Train: step:  46020, time: 0.237, loss: 832.795227\n",
      "Train: step:  46030, time: 0.236, loss: 2079.450928\n",
      "Train: step:  46040, time: 0.263, loss: 864.270203\n",
      "Train: step:  46050, time: 0.241, loss: 2711.057373\n",
      "Train: step:  46060, time: 0.265, loss: 2459.074951\n",
      "Train: step:  46070, time: 0.239, loss: 1751.040771\n",
      "Train: step:  46080, time: 0.238, loss: 1960.845093\n",
      "Train: step:  46090, time: 0.238, loss: 686.968933\n",
      "Train: step:  46100, time: 0.242, loss: 2735.836426\n",
      "Train: step:  46110, time: 0.240, loss: 2701.241211\n",
      "Train: step:  46120, time: 0.243, loss: 2711.173584\n",
      "Train: step:  46130, time: 0.247, loss: 2083.698486\n",
      "Train: step:  46140, time: 0.240, loss: 2468.290527\n",
      "Train: step:  46150, time: 0.246, loss: 3951.164062\n",
      "Train: step:  46160, time: 0.247, loss: 1590.047363\n",
      "Train: step:  46170, time: 0.233, loss: 1496.264404\n",
      "Train: step:  46180, time: 0.241, loss: 471.376495\n",
      "Train: step:  46190, time: 0.233, loss: 2392.293945\n",
      "Train: step:  46200, time: 0.252, loss: 2746.131836\n",
      "Train: step:  46210, time: 0.235, loss: 1940.597534\n",
      "Train: step:  46220, time: 0.235, loss: 2048.645264\n",
      "Train: step:  46230, time: 0.242, loss: 1241.703003\n",
      "Train: step:  46240, time: 0.237, loss: 2212.923828\n",
      "Train: step:  46250, time: 0.248, loss: 462.432281\n",
      "Train: step:  46260, time: 0.231, loss: 2071.232178\n",
      "Train: step:  46270, time: 0.238, loss: 1133.673584\n",
      "Train: step:  46280, time: 0.235, loss: 2619.510498\n",
      "Train: step:  46290, time: 0.262, loss: 1919.915771\n",
      "Train: step:  46300, time: 0.243, loss: 1459.022705\n",
      "Train: step:  46310, time: 0.237, loss: 1214.576172\n",
      "Train: step:  46320, time: 0.257, loss: 2398.231689\n",
      "Train: step:  46330, time: 0.260, loss: 564.595642\n",
      "Train: step:  46340, time: 0.252, loss: 1550.734863\n",
      "Train: step:  46350, time: 0.226, loss: 3363.376953\n",
      "Train: step:  46360, time: 0.228, loss: 2313.731445\n",
      "Train: step:  46370, time: 0.221, loss: 1295.564209\n",
      "Train: step:  46380, time: 0.256, loss: 1088.777710\n",
      "Train: step:  46390, time: 0.241, loss: 1634.691284\n",
      "Train: step:  46400, time: 0.234, loss: 2005.359131\n",
      "Train: step:  46410, time: 0.237, loss: 2342.685547\n",
      "Train: step:  46420, time: 0.258, loss: 789.622009\n",
      "Train: step:  46430, time: 0.237, loss: 238.215866\n",
      "Train: step:  46440, time: 0.233, loss: 2507.501465\n",
      "Train: step:  46450, time: 0.234, loss: 2136.812256\n",
      "Train: step:  46460, time: 0.234, loss: 319.691284\n",
      "Train: step:  46470, time: 0.224, loss: 2805.166992\n",
      "Train: step:  46480, time: 0.227, loss: 1637.147827\n",
      "Train: step:  46490, time: 0.239, loss: 1548.387085\n",
      "Train: step:  46500, time: 0.260, loss: 2084.288086\n",
      "Train: step:  46510, time: 0.232, loss: 1901.838379\n",
      "Train: step:  46520, time: 0.248, loss: 3908.180664\n",
      "Train: step:  46530, time: 0.231, loss: 1756.052002\n",
      "Train: step:  46540, time: 0.248, loss: 1700.806763\n",
      "Train: step:  46550, time: 0.237, loss: 3257.110352\n",
      "Train: step:  46560, time: 0.237, loss: 1716.162476\n",
      "Train: step:  46570, time: 0.267, loss: 1327.680054\n",
      "Train: step:  46580, time: 0.235, loss: 1308.538818\n",
      "Train: step:  46590, time: 0.237, loss: 2576.122559\n",
      "Train: step:  46600, time: 0.248, loss: 1669.161621\n",
      "Train: step:  46610, time: 0.240, loss: 604.379822\n",
      "Train: step:  46620, time: 0.243, loss: 850.399353\n",
      "Train: step:  46630, time: 0.242, loss: 310.741486\n",
      "Train: step:  46640, time: 0.248, loss: 1708.244629\n",
      "Train: step:  46650, time: 0.242, loss: 1384.993652\n",
      "Train: step:  46660, time: 0.254, loss: 334.331909\n",
      "Train: step:  46670, time: 0.248, loss: 3668.379150\n",
      "Train: step:  46680, time: 0.264, loss: 1681.193359\n",
      "Train: step:  46690, time: 0.249, loss: 3392.675049\n",
      "Train: step:  46700, time: 0.247, loss: 2880.309570\n",
      "Train: step:  46710, time: 0.237, loss: 582.733337\n",
      "Train: step:  46720, time: 0.242, loss: 2502.710693\n",
      "Train: step:  46730, time: 0.245, loss: 1098.311401\n",
      "Train: step:  46740, time: 0.235, loss: 3323.093018\n",
      "Train: step:  46750, time: 0.250, loss: 789.490784\n",
      "Train: step:  46760, time: 0.268, loss: 2122.830322\n",
      "Train: step:  46770, time: 0.283, loss: 1405.558350\n",
      "Train: step:  46780, time: 0.232, loss: 1947.763428\n",
      "Train: step:  46790, time: 0.239, loss: 2684.437988\n",
      "Train: step:  46800, time: 0.269, loss: 3020.670898\n",
      "Train: step:  46810, time: 0.266, loss: 1273.659668\n",
      "Train: step:  46820, time: 0.263, loss: 2580.005127\n",
      "Train: step:  46830, time: 0.239, loss: 2656.774170\n",
      "Train: step:  46840, time: 0.285, loss: 1592.037476\n",
      "Train: step:  46850, time: 0.237, loss: 437.816589\n",
      "Train: step:  46860, time: 0.247, loss: 282.256653\n",
      "Train: step:  46870, time: 0.252, loss: 1554.880493\n",
      "Train: step:  46880, time: 0.244, loss: 2599.564697\n",
      "Train: step:  46890, time: 0.236, loss: 2085.609131\n",
      "Train: step:  46900, time: 0.254, loss: 1766.305176\n",
      "Train: step:  46910, time: 0.259, loss: 1398.963989\n",
      "Train: step:  46920, time: 0.234, loss: 823.232483\n",
      "Train: step:  46930, time: 0.234, loss: 1732.802002\n",
      "Train: step:  46940, time: 0.233, loss: 1286.983276\n",
      "Train: step:  46950, time: 0.237, loss: 1759.969482\n",
      "Train: step:  46960, time: 0.239, loss: 656.350098\n",
      "Train: step:  46970, time: 0.254, loss: 1059.707520\n",
      "Train: step:  46980, time: 0.231, loss: 2359.090576\n",
      "Train: step:  46990, time: 0.228, loss: 3355.852783\n",
      "Train: step:  47000, time: 0.232, loss: 1619.120117\n",
      "Train: step:  47010, time: 0.226, loss: 1558.338623\n",
      "Train: step:  47020, time: 0.225, loss: 352.662506\n",
      "Train: step:  47030, time: 0.242, loss: 1591.468750\n",
      "Train: step:  47040, time: 0.230, loss: 1226.838135\n",
      "Train: step:  47050, time: 0.222, loss: 2913.938721\n",
      "Train: step:  47060, time: 0.265, loss: 1630.638428\n",
      "Train: step:  47070, time: 0.229, loss: 2609.120605\n",
      "Train: step:  47080, time: 0.247, loss: 3089.637939\n",
      "Train: step:  47090, time: 0.260, loss: 305.239624\n",
      "Train: step:  47100, time: 0.241, loss: 1294.415894\n",
      "Train: step:  47110, time: 0.242, loss: 2997.164551\n",
      "Train: step:  47120, time: 0.249, loss: 859.429626\n",
      "Train: step:  47130, time: 0.255, loss: 953.811829\n",
      "Train: step:  47140, time: 0.237, loss: 1990.911987\n",
      "Train: step:  47150, time: 0.259, loss: 2324.383057\n",
      "Train: step:  47160, time: 0.266, loss: 2681.974121\n",
      "Train: step:  47170, time: 0.274, loss: 2767.989746\n",
      "Train: step:  47180, time: 0.242, loss: 1175.796753\n",
      "Train: step:  47190, time: 0.237, loss: 1922.385010\n",
      "Train: step:  47200, time: 0.231, loss: 2201.157227\n",
      "Train: step:  47210, time: 0.234, loss: 1498.272461\n",
      "Train: step:  47220, time: 0.232, loss: 1059.729736\n",
      "Train: step:  47230, time: 0.265, loss: 3675.364502\n",
      "Train: step:  47240, time: 0.235, loss: 2016.551392\n",
      "Train: step:  47250, time: 0.243, loss: 1593.421143\n",
      "Train: step:  47260, time: 0.240, loss: 2562.542480\n",
      "Train: step:  47270, time: 0.242, loss: 2078.869385\n",
      "Train: step:  47280, time: 0.264, loss: 2326.124512\n",
      "Train: step:  47290, time: 0.262, loss: 1920.213013\n",
      "Train: step:  47300, time: 0.237, loss: 5194.505371\n",
      "Train: step:  47310, time: 0.260, loss: 339.074005\n",
      "Train: step:  47320, time: 0.268, loss: 2719.395020\n",
      "Train: step:  47330, time: 0.236, loss: 4624.130371\n",
      "Train: step:  47340, time: 0.266, loss: 2565.675537\n",
      "Train: step:  47350, time: 0.245, loss: 4573.645020\n",
      "Train: step:  47360, time: 0.222, loss: 2287.842529\n",
      "Train: step:  47370, time: 0.249, loss: 2280.706787\n",
      "Train: step:  47380, time: 0.273, loss: 1739.262695\n",
      "Train: step:  47390, time: 0.235, loss: 2998.704834\n",
      "Train: step:  47400, time: 0.246, loss: 506.089996\n",
      "Train: step:  47410, time: 0.242, loss: 1787.253418\n",
      "Train: step:  47420, time: 0.243, loss: 2400.868652\n",
      "Train: step:  47430, time: 0.241, loss: 2660.877686\n",
      "Train: step:  47440, time: 0.260, loss: 2312.292480\n",
      "Train: step:  47450, time: 0.236, loss: 2817.428467\n",
      "Train: step:  47460, time: 0.241, loss: 630.016724\n",
      "Train: step:  47470, time: 0.248, loss: 1552.647461\n",
      "Train: step:  47480, time: 0.235, loss: 934.042236\n",
      "Train: step:  47490, time: 0.238, loss: 522.721497\n",
      "Train: step:  47500, time: 0.268, loss: 1607.045654\n",
      "Train: step:  47510, time: 0.262, loss: 2732.047852\n",
      "Train: step:  47520, time: 0.238, loss: 1369.829224\n",
      "Train: step:  47530, time: 0.235, loss: 1369.524292\n",
      "Train: step:  47540, time: 0.234, loss: 2302.403076\n",
      "Train: step:  47550, time: 0.235, loss: 3122.129639\n",
      "Train: step:  47560, time: 0.239, loss: 1178.560669\n",
      "Train: step:  47570, time: 0.262, loss: 2180.867432\n",
      "Train: step:  47580, time: 0.233, loss: 1422.378174\n",
      "Train: step:  47590, time: 0.231, loss: 203.510223\n",
      "Train: step:  47600, time: 0.223, loss: 1970.851318\n",
      "Train: step:  47610, time: 0.232, loss: 1161.701294\n",
      "Train: step:  47620, time: 0.255, loss: 583.343384\n",
      "Train: step:  47630, time: 0.229, loss: 3760.463867\n",
      "Train: step:  47640, time: 0.225, loss: 1762.173828\n",
      "Train: step:  47650, time: 0.260, loss: 871.675049\n",
      "Train: step:  47660, time: 0.222, loss: 694.445740\n",
      "Train: step:  47670, time: 0.230, loss: 485.071289\n",
      "Train: step:  47680, time: 0.227, loss: 1213.612671\n",
      "Train: step:  47690, time: 0.216, loss: 2045.556030\n",
      "Train: step:  47700, time: 0.262, loss: 1013.795166\n",
      "Train: step:  47710, time: 0.242, loss: 2892.197021\n",
      "Train: step:  47720, time: 0.265, loss: 1319.741089\n",
      "Train: step:  47730, time: 0.233, loss: 3152.180664\n",
      "Train: step:  47740, time: 0.224, loss: 2301.298340\n",
      "Train: step:  47750, time: 0.235, loss: 644.830811\n",
      "Train: step:  47760, time: 0.245, loss: 1882.022339\n",
      "Train: step:  47770, time: 0.242, loss: 1730.569946\n",
      "Train: step:  47780, time: 0.252, loss: 4311.932617\n",
      "Train: step:  47790, time: 0.239, loss: 2740.069092\n",
      "Train: step:  47800, time: 0.241, loss: 863.821655\n",
      "Train: step:  47810, time: 0.235, loss: 619.129150\n",
      "Train: step:  47820, time: 0.239, loss: 483.790283\n",
      "Train: step:  47830, time: 0.263, loss: 2898.885742\n",
      "Train: step:  47840, time: 0.231, loss: 1254.127075\n",
      "Train: step:  47850, time: 0.270, loss: 3622.423096\n",
      "Train: step:  47860, time: 0.233, loss: 205.805023\n",
      "Train: step:  47870, time: 0.240, loss: 537.158142\n",
      "Train: step:  47880, time: 0.233, loss: 1808.188843\n",
      "Train: step:  47890, time: 0.237, loss: 1277.059082\n",
      "Train: step:  47900, time: 0.246, loss: 1226.833862\n",
      "Train: step:  47910, time: 0.268, loss: 1817.714478\n",
      "Train: step:  47920, time: 0.264, loss: 1640.804443\n",
      "Train: step:  47930, time: 0.250, loss: 2174.944336\n",
      "Train: step:  47940, time: 0.245, loss: 559.937073\n",
      "Train: step:  47950, time: 0.246, loss: 1937.839600\n",
      "Train: step:  47960, time: 0.251, loss: 2455.328125\n",
      "Train: step:  47970, time: 0.255, loss: 2707.724121\n",
      "Train: step:  47980, time: 0.253, loss: 1106.901245\n",
      "Train: step:  47990, time: 0.258, loss: 1874.619263\n",
      "Train: step:  48000, time: 0.239, loss: 1095.496582\n",
      "Train: step:  48010, time: 0.264, loss: 3098.141113\n",
      "Train: step:  48020, time: 0.254, loss: 1768.062744\n",
      "Train: step:  48030, time: 0.232, loss: 1331.993652\n",
      "Train: step:  48040, time: 0.238, loss: 2479.746338\n",
      "Train: step:  48050, time: 0.234, loss: 2026.695679\n",
      "Train: step:  48060, time: 0.233, loss: 2251.329834\n",
      "Train: step:  48070, time: 0.239, loss: 1962.474487\n",
      "Train: step:  48080, time: 0.240, loss: 230.116852\n",
      "Train: step:  48090, time: 0.269, loss: 3379.734619\n",
      "Train: step:  48100, time: 0.239, loss: 1078.861938\n",
      "Train: step:  48110, time: 0.274, loss: 2295.269287\n",
      "Train: step:  48120, time: 0.238, loss: 1022.419861\n",
      "Train: step:  48130, time: 0.263, loss: 1343.531860\n",
      "Train: step:  48140, time: 0.269, loss: 3506.463135\n",
      "Train: step:  48150, time: 0.242, loss: 2784.124756\n",
      "Train: step:  48160, time: 0.239, loss: 1424.973145\n",
      "Train: step:  48170, time: 0.272, loss: 2762.642822\n",
      "Train: step:  48180, time: 0.263, loss: 1926.815063\n",
      "Train: step:  48190, time: 0.245, loss: 848.490662\n",
      "Train: step:  48200, time: 0.243, loss: 3391.315186\n",
      "Train: step:  48210, time: 0.236, loss: 853.328796\n",
      "Train: step:  48220, time: 0.219, loss: 1490.422852\n",
      "Train: step:  48230, time: 0.265, loss: 785.807495\n",
      "Train: step:  48240, time: 0.238, loss: 1309.673584\n",
      "Train: step:  48250, time: 0.243, loss: 571.367737\n",
      "Train: step:  48260, time: 0.263, loss: 498.240021\n",
      "Train: step:  48270, time: 0.245, loss: 1722.633667\n",
      "Train: step:  48280, time: 0.243, loss: 3129.199219\n",
      "Train: step:  48290, time: 0.242, loss: 3024.905273\n",
      "Train: step:  48300, time: 0.236, loss: 1084.889526\n",
      "Train: step:  48310, time: 0.222, loss: 602.093933\n",
      "Train: step:  48320, time: 0.227, loss: 2653.020996\n",
      "Train: step:  48330, time: 0.223, loss: 2304.285889\n",
      "Train: step:  48340, time: 0.238, loss: 1278.993042\n",
      "Train: step:  48350, time: 0.234, loss: 2348.024414\n",
      "Train: step:  48360, time: 0.226, loss: 2504.524658\n",
      "Train: step:  48370, time: 0.243, loss: 1749.785034\n",
      "Train: step:  48380, time: 0.243, loss: 4716.491699\n",
      "Train: step:  48390, time: 0.231, loss: 1448.622803\n",
      "Train: step:  48400, time: 0.268, loss: 1692.164307\n",
      "Train: step:  48410, time: 0.257, loss: 1970.359375\n",
      "Train: step:  48420, time: 0.240, loss: 3576.226074\n",
      "Train: step:  48430, time: 0.234, loss: 3140.240723\n",
      "Train: step:  48440, time: 0.233, loss: 2180.802979\n",
      "Train: step:  48450, time: 0.236, loss: 732.450684\n",
      "Train: step:  48460, time: 0.281, loss: 698.104675\n",
      "Train: step:  48470, time: 0.235, loss: 996.801331\n",
      "Train: step:  48480, time: 0.237, loss: 1384.519653\n",
      "Train: step:  48490, time: 0.234, loss: 1991.521118\n",
      "Train: step:  48500, time: 0.252, loss: 2821.116455\n",
      "Train: step:  48510, time: 0.234, loss: 1097.431030\n",
      "Train: step:  48520, time: 0.251, loss: 1611.260498\n",
      "Train: step:  48530, time: 0.239, loss: 1400.379028\n",
      "Train: step:  48540, time: 0.244, loss: 1700.729614\n",
      "Train: step:  48550, time: 0.235, loss: 1096.667725\n",
      "Train: step:  48560, time: 0.231, loss: 2478.285400\n",
      "Train: step:  48570, time: 0.239, loss: 3299.240723\n",
      "Train: step:  48580, time: 0.240, loss: 2046.791992\n",
      "Train: step:  48590, time: 0.243, loss: 1139.774048\n",
      "Train: step:  48600, time: 0.245, loss: 4041.621582\n",
      "Train: step:  48610, time: 0.253, loss: 2061.489990\n",
      "Train: step:  48620, time: 0.234, loss: 3251.054688\n",
      "Train: step:  48630, time: 0.223, loss: 2608.134766\n",
      "Train: step:  48640, time: 0.224, loss: 502.586304\n",
      "Train: step:  48650, time: 0.262, loss: 1281.788940\n",
      "Train: step:  48660, time: 0.234, loss: 2271.364990\n",
      "Train: step:  48670, time: 0.223, loss: 1320.526489\n",
      "Train: step:  48680, time: 0.234, loss: 1105.383179\n",
      "Train: step:  48690, time: 0.244, loss: 2516.881348\n",
      "Train: step:  48700, time: 0.285, loss: 2184.559082\n",
      "Train: step:  48710, time: 0.225, loss: 2113.719482\n",
      "Train: step:  48720, time: 0.226, loss: 4061.410645\n",
      "Train: step:  48730, time: 0.250, loss: 679.962158\n",
      "Train: step:  48740, time: 0.235, loss: 2113.646729\n",
      "Train: step:  48750, time: 0.229, loss: 2387.376465\n",
      "Train: step:  48760, time: 0.253, loss: 2466.065430\n",
      "Train: step:  48770, time: 0.260, loss: 925.125610\n",
      "Train: step:  48780, time: 0.236, loss: 595.008728\n",
      "Train: step:  48790, time: 0.257, loss: 1019.826416\n",
      "Train: step:  48800, time: 0.237, loss: 1145.613281\n",
      "Train: step:  48810, time: 0.269, loss: 1689.801514\n",
      "Train: step:  48820, time: 0.267, loss: 844.902283\n",
      "Train: step:  48830, time: 0.251, loss: 1883.557373\n",
      "Train: step:  48840, time: 0.236, loss: 2982.822754\n",
      "Train: step:  48850, time: 0.239, loss: 1565.752808\n",
      "Train: step:  48860, time: 0.240, loss: 1234.669189\n",
      "Train: step:  48870, time: 0.248, loss: 909.873657\n",
      "Train: step:  48880, time: 0.244, loss: 1504.755005\n",
      "Train: step:  48890, time: 0.240, loss: 1615.470825\n",
      "Train: step:  48900, time: 0.243, loss: 731.919434\n",
      "Train: step:  48910, time: 0.232, loss: 522.297668\n",
      "Train: step:  48920, time: 0.249, loss: 1555.161499\n",
      "Train: step:  48930, time: 0.227, loss: 2400.365479\n",
      "Train: step:  48940, time: 0.225, loss: 3042.967285\n",
      "Train: step:  48950, time: 0.234, loss: 1589.256958\n",
      "Train: step:  48960, time: 0.235, loss: 2295.558594\n",
      "Train: step:  48970, time: 0.222, loss: 3965.896729\n",
      "Train: step:  48980, time: 0.255, loss: 1571.304321\n",
      "Train: step:  48990, time: 0.231, loss: 4422.747070\n",
      "Train: step:  49000, time: 0.231, loss: 2926.185547\n",
      "Train: step:  49010, time: 0.229, loss: 1209.121704\n",
      "Train: step:  49020, time: 0.222, loss: 3819.201172\n",
      "Train: step:  49030, time: 0.227, loss: 1926.826294\n",
      "Train: step:  49040, time: 0.224, loss: 607.726868\n",
      "Train: step:  49050, time: 0.234, loss: 1400.509399\n",
      "Train: step:  49060, time: 0.247, loss: 1280.957397\n",
      "Train: step:  49070, time: 0.233, loss: 2298.479980\n",
      "Train: step:  49080, time: 0.241, loss: 420.719635\n",
      "Train: step:  49090, time: 0.253, loss: 1731.205444\n",
      "Train: step:  49100, time: 0.240, loss: 3032.415771\n",
      "Train: step:  49110, time: 0.246, loss: 1076.035889\n",
      "Train: step:  49120, time: 0.236, loss: 2746.960693\n",
      "Train: step:  49130, time: 0.255, loss: 2747.931885\n",
      "Train: step:  49140, time: 0.238, loss: 2059.364502\n",
      "Train: step:  49150, time: 0.270, loss: 755.181030\n",
      "Train: step:  49160, time: 0.231, loss: 2554.375000\n",
      "Train: step:  49170, time: 0.232, loss: 2755.397461\n",
      "Train: step:  49180, time: 0.233, loss: 2369.362305\n",
      "Train: step:  49190, time: 0.270, loss: 3191.333740\n",
      "Train: step:  49200, time: 0.234, loss: 537.844666\n",
      "Train: step:  49210, time: 0.238, loss: 1364.413330\n",
      "Train: step:  49220, time: 0.279, loss: 3426.165283\n",
      "Train: step:  49230, time: 0.241, loss: 1667.711426\n",
      "Train: step:  49240, time: 0.251, loss: 2448.696533\n",
      "Train: step:  49250, time: 0.240, loss: 2318.009277\n",
      "Train: step:  49260, time: 0.247, loss: 1051.344238\n",
      "Train: step:  49270, time: 0.251, loss: 3558.793213\n",
      "Train: step:  49280, time: 0.244, loss: 2079.841553\n",
      "Train: step:  49290, time: 0.245, loss: 706.266357\n",
      "Train: step:  49300, time: 0.269, loss: 2469.746338\n",
      "Train: step:  49310, time: 0.243, loss: 1051.345215\n",
      "Train: step:  49320, time: 0.241, loss: 2542.603271\n",
      "Train: step:  49330, time: 0.248, loss: 3213.008301\n",
      "Train: step:  49340, time: 0.247, loss: 1635.363159\n",
      "Train: step:  49350, time: 0.260, loss: 3229.760742\n",
      "Train: step:  49360, time: 0.238, loss: 1476.460083\n",
      "Train: step:  49370, time: 0.235, loss: 2371.542969\n",
      "Train: step:  49380, time: 0.242, loss: 1901.783936\n",
      "Train: step:  49390, time: 0.231, loss: 3604.123535\n",
      "Train: step:  49400, time: 0.236, loss: 2124.299805\n",
      "Train: step:  49410, time: 0.227, loss: 394.632721\n",
      "Train: step:  49420, time: 0.234, loss: 439.437073\n",
      "Train: step:  49430, time: 0.253, loss: 4121.708496\n",
      "Train: step:  49440, time: 0.257, loss: 1604.380127\n",
      "Train: step:  49450, time: 0.237, loss: 2366.362793\n",
      "Train: step:  49460, time: 0.254, loss: 550.685974\n",
      "Train: step:  49470, time: 0.229, loss: 1492.535522\n",
      "Train: step:  49480, time: 0.224, loss: 685.517639\n",
      "Train: step:  49490, time: 0.225, loss: 1870.745850\n",
      "Train: step:  49500, time: 0.234, loss: 2572.031494\n",
      "Train: step:  49510, time: 0.242, loss: 2387.984863\n",
      "Train: step:  49520, time: 0.252, loss: 2278.003174\n",
      "Train: step:  49530, time: 0.231, loss: 1024.380249\n",
      "Train: step:  49540, time: 0.228, loss: 1374.082764\n",
      "Train: step:  49550, time: 0.232, loss: 2166.708008\n",
      "Train: step:  49560, time: 0.224, loss: 2077.206787\n",
      "Train: step:  49570, time: 0.230, loss: 3220.718750\n",
      "Train: step:  49580, time: 0.236, loss: 273.190826\n",
      "Train: step:  49590, time: 0.232, loss: 1348.532349\n",
      "Train: step:  49600, time: 0.227, loss: 1585.016479\n",
      "Train: step:  49610, time: 0.223, loss: 445.582794\n",
      "Train: step:  49620, time: 0.252, loss: 262.938721\n",
      "Train: step:  49630, time: 0.233, loss: 3255.421143\n",
      "Train: step:  49640, time: 0.233, loss: 2666.267090\n",
      "Train: step:  49650, time: 0.227, loss: 1719.166870\n",
      "Train: step:  49660, time: 0.228, loss: 555.026672\n",
      "Train: step:  49670, time: 0.250, loss: 2008.514893\n",
      "Train: step:  49680, time: 0.262, loss: 2219.185059\n",
      "Train: step:  49690, time: 0.223, loss: 1042.476929\n",
      "Train: step:  49700, time: 0.255, loss: 1930.881104\n",
      "Train: step:  49710, time: 0.251, loss: 958.382080\n",
      "Train: step:  49720, time: 0.241, loss: 2686.713623\n",
      "Train: step:  49730, time: 0.222, loss: 2778.098633\n",
      "Train: step:  49740, time: 0.233, loss: 1968.946533\n",
      "Train: step:  49750, time: 0.250, loss: 800.293396\n",
      "Train: step:  49760, time: 0.228, loss: 2610.847412\n",
      "Train: step:  49770, time: 0.259, loss: 598.612854\n",
      "Train: step:  49780, time: 0.233, loss: 2106.931641\n",
      "Train: step:  49790, time: 0.230, loss: 1088.926270\n",
      "Train: step:  49800, time: 0.229, loss: 3849.815430\n",
      "Train: step:  49810, time: 0.304, loss: 1577.037231\n",
      "Train: step:  49820, time: 0.276, loss: 1843.376831\n",
      "Train: step:  49830, time: 0.245, loss: 2069.862305\n",
      "Train: step:  49840, time: 0.258, loss: 2928.052979\n",
      "Train: step:  49850, time: 0.237, loss: 2144.724121\n",
      "Train: step:  49860, time: 0.254, loss: 816.774536\n",
      "Train: step:  49870, time: 0.235, loss: 3526.767334\n",
      "Train: step:  49880, time: 0.232, loss: 503.503967\n",
      "Train: step:  49890, time: 0.233, loss: 1623.750488\n",
      "Train: step:  49900, time: 0.222, loss: 452.508453\n",
      "Train: step:  49910, time: 0.227, loss: 1395.442627\n",
      "Train: step:  49920, time: 0.265, loss: 4370.401367\n",
      "Train: step:  49930, time: 0.263, loss: 1868.800171\n",
      "Train: step:  49940, time: 0.236, loss: 449.756622\n",
      "Train: step:  49950, time: 0.237, loss: 1423.022339\n",
      "Train: step:  49960, time: 0.235, loss: 154.353241\n",
      "Train: step:  49970, time: 0.237, loss: 2375.342285\n",
      "Train: step:  49980, time: 0.247, loss: 762.656006\n",
      "Train: step:  49990, time: 0.250, loss: 3625.996826\n",
      "Train: step:  50000, time: 0.231, loss: 2855.288086\n",
      "Train: step:  50010, time: 0.229, loss: 455.697906\n",
      "Train: step:  50020, time: 0.225, loss: 1497.971436\n",
      "Train: step:  50030, time: 0.241, loss: 1991.315186\n",
      "Train: step:  50040, time: 0.228, loss: 1049.256592\n",
      "Train: step:  50050, time: 0.243, loss: 1303.777100\n",
      "Train: step:  50060, time: 0.229, loss: 627.679932\n",
      "Train: step:  50070, time: 0.271, loss: 342.981201\n",
      "Train: step:  50080, time: 0.251, loss: 402.491821\n",
      "Train: step:  50090, time: 0.243, loss: 2796.459717\n",
      "Train: step:  50100, time: 0.233, loss: 1963.962891\n",
      "Train: step:  50110, time: 0.235, loss: 1264.243042\n",
      "Train: step:  50120, time: 0.234, loss: 1365.129883\n",
      "Train: step:  50130, time: 0.231, loss: 2519.420898\n",
      "Train: step:  50140, time: 0.232, loss: 1524.581055\n",
      "Train: step:  50150, time: 0.239, loss: 2216.453613\n",
      "Train: step:  50160, time: 0.267, loss: 2281.289551\n",
      "Train: step:  50170, time: 0.226, loss: 639.557922\n",
      "Train: step:  50180, time: 0.241, loss: 1128.392334\n",
      "Train: step:  50190, time: 0.257, loss: 1017.904419\n",
      "Train: step:  50200, time: 0.237, loss: 1254.337646\n",
      "Train: step:  50210, time: 0.235, loss: 480.191864\n",
      "Train: step:  50220, time: 0.242, loss: 1283.034180\n",
      "Train: step:  50230, time: 0.227, loss: 1802.290894\n",
      "Train: step:  50240, time: 0.236, loss: 3113.908936\n",
      "Train: step:  50250, time: 0.243, loss: 3434.847900\n",
      "Train: step:  50260, time: 0.246, loss: 1641.187744\n",
      "Train: step:  50270, time: 0.247, loss: 3539.877686\n",
      "Train: step:  50280, time: 0.238, loss: 872.916931\n",
      "Train: step:  50290, time: 0.265, loss: 2920.976074\n",
      "Train: step:  50300, time: 0.259, loss: 1700.715820\n",
      "Train: step:  50310, time: 0.270, loss: 643.137634\n",
      "Train: step:  50320, time: 0.271, loss: 1546.173584\n",
      "Train: step:  50330, time: 0.234, loss: 2972.086426\n",
      "Train: step:  50340, time: 0.232, loss: 1675.721680\n",
      "Train: step:  50350, time: 0.251, loss: 1992.308960\n",
      "Train: step:  50360, time: 0.242, loss: 1083.993652\n",
      "Train: step:  50370, time: 0.268, loss: 2950.867432\n",
      "Train: step:  50380, time: 0.239, loss: 945.109009\n",
      "Train: step:  50390, time: 0.247, loss: 3004.656006\n",
      "Train: step:  50400, time: 0.218, loss: 2099.558105\n",
      "Train: step:  50410, time: 0.222, loss: 2825.400391\n",
      "Train: step:  50420, time: 0.225, loss: 3860.004639\n",
      "Train: step:  50430, time: 0.232, loss: 963.165161\n",
      "Train: step:  50440, time: 0.250, loss: 1028.565552\n",
      "Train: step:  50450, time: 0.226, loss: 2918.018799\n",
      "Train: step:  50460, time: 0.227, loss: 1100.420288\n",
      "Train: step:  50470, time: 0.227, loss: 1878.192139\n",
      "Train: step:  50480, time: 0.259, loss: 870.809998\n",
      "Train: step:  50490, time: 0.225, loss: 1033.484741\n",
      "Train: step:  50500, time: 0.227, loss: 427.294006\n",
      "Train: step:  50510, time: 0.279, loss: 3476.347900\n",
      "Train: step:  50520, time: 0.236, loss: 766.407837\n",
      "Train: step:  50530, time: 0.265, loss: 2249.558594\n",
      "Train: step:  50540, time: 0.233, loss: 2431.247803\n",
      "Train: step:  50550, time: 0.260, loss: 2551.819092\n",
      "Train: step:  50560, time: 0.236, loss: 2270.522705\n",
      "Train: step:  50570, time: 0.224, loss: 3590.051514\n",
      "Train: step:  50580, time: 0.234, loss: 3303.966553\n",
      "Train: step:  50590, time: 0.236, loss: 1602.848999\n",
      "Train: step:  50600, time: 0.228, loss: 3620.815430\n",
      "Train: step:  50610, time: 0.248, loss: 2705.995850\n",
      "Train: step:  50620, time: 0.277, loss: 2267.250732\n",
      "Train: step:  50630, time: 0.275, loss: 1680.530884\n",
      "Train: step:  50640, time: 0.227, loss: 2000.297119\n",
      "Train: step:  50650, time: 0.225, loss: 1021.457092\n",
      "Train: step:  50660, time: 0.226, loss: 1222.558228\n",
      "Train: step:  50670, time: 0.268, loss: 3405.606934\n",
      "Train: step:  50680, time: 0.230, loss: 762.476562\n",
      "Train: step:  50690, time: 0.258, loss: 1674.291992\n",
      "Train: step:  50700, time: 0.228, loss: 2328.345215\n",
      "Train: step:  50710, time: 0.265, loss: 634.763916\n",
      "Train: step:  50720, time: 0.227, loss: 2278.108154\n",
      "Train: step:  50730, time: 0.233, loss: 1579.843506\n",
      "Train: step:  50740, time: 0.228, loss: 1035.707275\n",
      "Train: step:  50750, time: 0.235, loss: 1276.547485\n",
      "Train: step:  50760, time: 0.229, loss: 2554.597412\n",
      "Train: step:  50770, time: 0.261, loss: 1558.457520\n",
      "Train: step:  50780, time: 0.225, loss: 852.088989\n",
      "Train: step:  50790, time: 0.231, loss: 653.243103\n",
      "Train: step:  50800, time: 0.232, loss: 1397.841675\n",
      "Train: step:  50810, time: 0.230, loss: 889.722595\n",
      "Train: step:  50820, time: 0.234, loss: 1065.180298\n",
      "Train: step:  50830, time: 0.233, loss: 1584.437256\n",
      "Train: step:  50840, time: 0.256, loss: 3561.216797\n",
      "Train: step:  50850, time: 0.237, loss: 2656.345703\n",
      "Train: step:  50860, time: 0.227, loss: 2351.442627\n",
      "Train: step:  50870, time: 0.231, loss: 3161.339844\n",
      "Train: step:  50880, time: 0.260, loss: 1682.907715\n",
      "Train: step:  50890, time: 0.225, loss: 808.201172\n",
      "Train: step:  50900, time: 0.236, loss: 1146.731079\n",
      "Train: step:  50910, time: 0.262, loss: 3719.708252\n",
      "Train: step:  50920, time: 0.232, loss: 266.646484\n",
      "Train: step:  50930, time: 0.249, loss: 1921.050903\n",
      "Train: step:  50940, time: 0.258, loss: 2236.987305\n",
      "Train: step:  50950, time: 0.227, loss: 2779.527100\n",
      "Train: step:  50960, time: 0.228, loss: 280.655853\n",
      "Train: step:  50970, time: 0.220, loss: 1807.005859\n",
      "Train: step:  50980, time: 0.229, loss: 2487.554443\n",
      "Train: step:  50990, time: 0.221, loss: 402.681183\n",
      "Train: step:  51000, time: 0.235, loss: 2219.880371\n",
      "Train: step:  51010, time: 0.228, loss: 2461.495605\n",
      "Train: step:  51020, time: 0.240, loss: 2504.318848\n",
      "Train: step:  51030, time: 0.233, loss: 3281.450928\n",
      "Train: step:  51040, time: 0.249, loss: 4372.652832\n",
      "Train: step:  51050, time: 0.267, loss: 1273.876099\n",
      "Train: step:  51060, time: 0.226, loss: 786.264160\n",
      "Train: step:  51070, time: 0.266, loss: 425.609436\n",
      "Train: step:  51080, time: 0.235, loss: 578.303894\n",
      "Train: step:  51090, time: 0.225, loss: 662.824280\n",
      "Train: step:  51100, time: 0.221, loss: 908.457642\n",
      "Train: step:  51110, time: 0.256, loss: 2297.497314\n",
      "Train: step:  51120, time: 0.239, loss: 2205.188232\n",
      "Train: step:  51130, time: 0.234, loss: 1689.672119\n",
      "Train: step:  51140, time: 0.224, loss: 276.292023\n",
      "Train: step:  51150, time: 0.224, loss: 953.752319\n",
      "Train: step:  51160, time: 0.236, loss: 2693.724365\n",
      "Train: step:  51170, time: 0.250, loss: 3280.357422\n",
      "Train: step:  51180, time: 0.227, loss: 359.022003\n",
      "Train: step:  51190, time: 0.229, loss: 2901.194580\n",
      "Train: step:  51200, time: 0.224, loss: 1197.027832\n",
      "Train: step:  51210, time: 0.240, loss: 2319.521973\n",
      "Train: step:  51220, time: 0.225, loss: 1219.556763\n",
      "Train: step:  51230, time: 0.223, loss: 1615.057983\n",
      "Train: step:  51240, time: 0.255, loss: 852.289062\n",
      "Train: step:  51250, time: 0.231, loss: 2526.010742\n",
      "Train: step:  51260, time: 0.233, loss: 2708.249756\n",
      "Train: step:  51270, time: 0.230, loss: 1034.130005\n",
      "Train: step:  51280, time: 0.239, loss: 1257.814209\n",
      "Train: step:  51290, time: 0.224, loss: 1423.906738\n",
      "Train: step:  51300, time: 0.224, loss: 3114.294189\n",
      "Train: step:  51310, time: 0.221, loss: 2110.345459\n",
      "Train: step:  51320, time: 0.255, loss: 1080.642456\n",
      "Train: step:  51330, time: 0.224, loss: 1413.209229\n",
      "Train: step:  51340, time: 0.226, loss: 336.901367\n",
      "Train: step:  51350, time: 0.227, loss: 1213.266357\n",
      "Train: step:  51360, time: 0.222, loss: 3348.264160\n",
      "Train: step:  51370, time: 0.266, loss: 1607.759644\n",
      "Train: step:  51380, time: 0.258, loss: 1944.899658\n",
      "Train: step:  51390, time: 0.228, loss: 3820.552246\n",
      "Train: step:  51400, time: 0.232, loss: 3078.314697\n",
      "Train: step:  51410, time: 0.233, loss: 2975.277832\n",
      "Train: step:  51420, time: 0.243, loss: 1578.934326\n",
      "Train: step:  51430, time: 0.229, loss: 1971.700806\n",
      "Train: step:  51440, time: 0.234, loss: 2849.266602\n",
      "Train: step:  51450, time: 0.231, loss: 2827.279297\n",
      "Train: step:  51460, time: 0.240, loss: 2251.043945\n",
      "Train: step:  51470, time: 0.264, loss: 496.743866\n",
      "Train: step:  51480, time: 0.256, loss: 1459.067139\n",
      "Train: step:  51490, time: 0.230, loss: 630.639526\n",
      "Train: step:  51500, time: 0.235, loss: 3731.260498\n",
      "Train: step:  51510, time: 0.231, loss: 2331.957031\n",
      "Train: step:  51520, time: 0.270, loss: 2808.047607\n",
      "Train: step:  51530, time: 0.254, loss: 1068.371948\n",
      "Train: step:  51540, time: 0.234, loss: 704.006042\n",
      "Train: step:  51550, time: 0.239, loss: 2684.486084\n",
      "Train: step:  51560, time: 0.236, loss: 1059.669922\n",
      "Train: step:  51570, time: 0.266, loss: 3477.477783\n",
      "Train: step:  51580, time: 0.236, loss: 529.492004\n",
      "Train: step:  51590, time: 0.232, loss: 3235.454102\n",
      "Train: step:  51600, time: 0.263, loss: 783.402527\n",
      "Train: step:  51610, time: 0.239, loss: 1252.942261\n",
      "Train: step:  51620, time: 0.246, loss: 572.657532\n",
      "Train: step:  51630, time: 0.244, loss: 2801.410889\n",
      "Train: step:  51640, time: 0.277, loss: 1733.256470\n",
      "Train: step:  51650, time: 0.244, loss: 1487.020508\n",
      "Train: step:  51660, time: 0.246, loss: 734.201721\n",
      "Train: step:  51670, time: 0.236, loss: 1250.373291\n",
      "Train: step:  51680, time: 0.238, loss: 980.402222\n",
      "Train: step:  51690, time: 0.260, loss: 2250.791260\n",
      "Train: step:  51700, time: 0.259, loss: 1102.752075\n",
      "Train: step:  51710, time: 0.247, loss: 1564.550659\n",
      "Train: step:  51720, time: 0.247, loss: 1575.004639\n",
      "Train: step:  51730, time: 0.280, loss: 714.284790\n",
      "Train: step:  51740, time: 0.247, loss: 1478.770020\n",
      "Train: step:  51750, time: 0.255, loss: 1405.808105\n",
      "Train: step:  51760, time: 0.244, loss: 2982.668945\n",
      "Train: step:  51770, time: 0.243, loss: 1259.878296\n",
      "Train: step:  51780, time: 0.247, loss: 2153.691895\n",
      "Train: step:  51790, time: 0.243, loss: 912.560730\n",
      "Train: step:  51800, time: 0.238, loss: 928.688110\n",
      "Train: step:  51810, time: 0.242, loss: 3777.767822\n",
      "Train: step:  51820, time: 0.235, loss: 847.840515\n",
      "Train: step:  51830, time: 0.242, loss: 3256.072510\n",
      "Train: step:  51840, time: 0.240, loss: 4581.808594\n",
      "Train: step:  51850, time: 0.244, loss: 879.583130\n",
      "Train: step:  51860, time: 0.235, loss: 309.371155\n",
      "Train: step:  51870, time: 0.238, loss: 2138.784180\n",
      "Train: step:  51880, time: 0.244, loss: 1137.871460\n",
      "Train: step:  51890, time: 0.240, loss: 3014.671631\n",
      "Train: step:  51900, time: 0.236, loss: 533.819519\n",
      "Train: step:  51910, time: 0.266, loss: 394.099609\n",
      "Train: step:  51920, time: 0.263, loss: 1082.958984\n",
      "Train: step:  51930, time: 0.249, loss: 2137.703613\n",
      "Train: step:  51940, time: 0.242, loss: 1302.501831\n",
      "Train: step:  51950, time: 0.246, loss: 1793.873901\n",
      "Train: step:  51960, time: 0.232, loss: 2990.577881\n",
      "Train: step:  51970, time: 0.257, loss: 1676.373413\n",
      "Train: step:  51980, time: 0.276, loss: 1463.110474\n",
      "Train: step:  51990, time: 0.245, loss: 1239.314575\n",
      "Train: step:  52000, time: 0.245, loss: 2962.176514\n",
      "Train: step:  52010, time: 0.273, loss: 1697.523438\n",
      "Train: step:  52020, time: 0.242, loss: 1962.744141\n",
      "Train: step:  52030, time: 0.285, loss: 2141.118896\n",
      "Train: step:  52040, time: 0.242, loss: 1191.178101\n",
      "Train: step:  52050, time: 0.265, loss: 1943.685181\n",
      "Train: step:  52060, time: 0.262, loss: 818.341309\n",
      "Train: step:  52070, time: 0.248, loss: 3162.255127\n",
      "Train: step:  52080, time: 0.241, loss: 1639.156494\n",
      "Train: step:  52090, time: 0.237, loss: 1257.056885\n",
      "Train: step:  52100, time: 0.233, loss: 2166.135010\n",
      "Train: step:  52110, time: 0.237, loss: 148.184174\n",
      "Train: step:  52120, time: 0.241, loss: 1502.049683\n",
      "Train: step:  52130, time: 0.269, loss: 2632.814941\n",
      "Train: step:  52140, time: 0.267, loss: 1256.100952\n",
      "Train: step:  52150, time: 0.244, loss: 1670.103027\n",
      "Train: step:  52160, time: 0.251, loss: 553.312073\n",
      "Train: step:  52170, time: 0.245, loss: 2623.969971\n",
      "Train: step:  52180, time: 0.240, loss: 2751.989746\n",
      "Train: step:  52190, time: 0.252, loss: 919.093994\n",
      "Train: step:  52200, time: 0.244, loss: 1606.865723\n",
      "Train: step:  52210, time: 0.240, loss: 1594.258667\n",
      "Train: step:  52220, time: 0.248, loss: 2663.977295\n",
      "Train: step:  52230, time: 0.250, loss: 1957.755737\n",
      "Train: step:  52240, time: 0.236, loss: 990.035583\n",
      "Train: step:  52250, time: 0.247, loss: 1703.033936\n",
      "Train: step:  52260, time: 0.241, loss: 3605.141113\n",
      "Train: step:  52270, time: 0.255, loss: 2059.540039\n",
      "Train: step:  52280, time: 0.261, loss: 1586.234253\n",
      "Train: step:  52290, time: 0.248, loss: 2604.668457\n",
      "Train: step:  52300, time: 0.261, loss: 2195.110596\n",
      "Train: step:  52310, time: 0.242, loss: 4098.308594\n",
      "Train: step:  52320, time: 0.235, loss: 2851.717773\n",
      "Train: step:  52330, time: 0.236, loss: 1296.384766\n",
      "Train: step:  52340, time: 0.238, loss: 1884.152832\n",
      "Train: step:  52350, time: 0.275, loss: 2538.443848\n",
      "Train: step:  52360, time: 0.286, loss: 1490.385376\n",
      "Train: step:  52370, time: 0.257, loss: 904.910217\n",
      "Train: step:  52380, time: 0.244, loss: 1366.771240\n",
      "Train: step:  52390, time: 0.247, loss: 2479.141602\n",
      "Train: step:  52400, time: 0.246, loss: 2077.617920\n",
      "Train: step:  52410, time: 0.239, loss: 3572.325684\n",
      "Train: step:  52420, time: 0.271, loss: 2533.265137\n",
      "Train: step:  52430, time: 0.249, loss: 1998.054810\n",
      "Train: step:  52440, time: 0.232, loss: 1215.707153\n",
      "Train: step:  52450, time: 0.276, loss: 1944.440186\n",
      "Train: step:  52460, time: 0.246, loss: 1027.998901\n",
      "Train: step:  52470, time: 0.250, loss: 2433.367432\n",
      "Train: step:  52480, time: 0.285, loss: 3370.665039\n",
      "Train: step:  52490, time: 0.240, loss: 3254.724365\n",
      "Train: step:  52500, time: 0.240, loss: 2886.619141\n",
      "Train: step:  52510, time: 0.244, loss: 1625.107788\n",
      "Train: step:  52520, time: 0.239, loss: 1481.386719\n",
      "Train: step:  52530, time: 0.265, loss: 1729.261108\n",
      "Train: step:  52540, time: 0.237, loss: 966.948608\n",
      "Train: step:  52550, time: 0.235, loss: 2026.575928\n",
      "Train: step:  52560, time: 0.249, loss: 2270.393311\n",
      "Train: step:  52570, time: 0.241, loss: 3457.089600\n",
      "Train: step:  52580, time: 0.249, loss: 246.922485\n",
      "Train: step:  52590, time: 0.236, loss: 1197.683594\n",
      "Train: step:  52600, time: 0.255, loss: 424.937805\n",
      "Train: step:  52610, time: 0.234, loss: 2017.568604\n",
      "Train: step:  52620, time: 0.239, loss: 731.045532\n",
      "Train: step:  52630, time: 0.235, loss: 1389.302124\n",
      "Train: step:  52640, time: 0.235, loss: 632.860596\n",
      "Train: step:  52650, time: 0.262, loss: 1776.125122\n",
      "Train: step:  52660, time: 0.239, loss: 2354.160156\n",
      "Train: step:  52670, time: 0.246, loss: 1452.077026\n",
      "Train: step:  52680, time: 0.261, loss: 3035.772217\n",
      "Train: step:  52690, time: 0.273, loss: 2358.676025\n",
      "Train: step:  52700, time: 0.242, loss: 4187.539062\n",
      "Train: step:  52710, time: 0.246, loss: 3735.744873\n",
      "Train: step:  52720, time: 0.247, loss: 1915.875000\n",
      "Train: step:  52730, time: 0.250, loss: 2649.121338\n",
      "Train: step:  52740, time: 0.243, loss: 3148.373291\n",
      "Train: step:  52750, time: 0.241, loss: 1967.655762\n",
      "Train: step:  52760, time: 0.273, loss: 1989.303467\n",
      "Train: step:  52770, time: 0.240, loss: 577.881226\n",
      "Train: step:  52780, time: 0.263, loss: 3258.705566\n",
      "Train: step:  52790, time: 0.250, loss: 2079.487061\n",
      "Train: step:  52800, time: 0.243, loss: 1616.670410\n",
      "Train: step:  52810, time: 0.248, loss: 1352.818726\n",
      "Train: step:  52820, time: 0.278, loss: 2262.235840\n",
      "Train: step:  52830, time: 0.242, loss: 2864.661865\n",
      "Train: step:  52840, time: 0.237, loss: 3910.398438\n",
      "Train: step:  52850, time: 0.256, loss: 2009.111328\n",
      "Train: step:  52860, time: 0.268, loss: 1005.123352\n",
      "Train: step:  52870, time: 0.262, loss: 1898.895142\n",
      "Train: step:  52880, time: 0.262, loss: 3371.342041\n",
      "Train: step:  52890, time: 0.248, loss: 1829.801758\n",
      "Train: step:  52900, time: 0.272, loss: 2004.013306\n",
      "Train: step:  52910, time: 0.274, loss: 1160.769653\n",
      "Train: step:  52920, time: 0.240, loss: 2268.927490\n",
      "Train: step:  52930, time: 0.237, loss: 2405.670898\n",
      "Train: step:  52940, time: 0.245, loss: 1712.278931\n",
      "Train: step:  52950, time: 0.243, loss: 2577.602783\n",
      "Train: step:  52960, time: 0.245, loss: 2178.373779\n",
      "Train: step:  52970, time: 0.253, loss: 2166.614746\n",
      "Train: step:  52980, time: 0.245, loss: 692.675781\n",
      "Train: step:  52990, time: 0.280, loss: 2305.713867\n",
      "Train: step:  53000, time: 0.254, loss: 805.908875\n",
      "Train: step:  53010, time: 0.250, loss: 1071.587036\n",
      "Train: step:  53020, time: 0.244, loss: 1363.042847\n",
      "Train: step:  53030, time: 0.277, loss: 526.349365\n",
      "Train: step:  53040, time: 0.296, loss: 2173.208008\n",
      "Train: step:  53050, time: 0.240, loss: 3347.487061\n",
      "Train: step:  53060, time: 0.272, loss: 2241.067871\n",
      "Train: step:  53070, time: 0.247, loss: 2789.022705\n",
      "Train: step:  53080, time: 0.237, loss: 1838.226685\n",
      "Train: step:  53090, time: 0.235, loss: 1337.456543\n",
      "Train: step:  53100, time: 0.245, loss: 408.111908\n",
      "Train: step:  53110, time: 0.250, loss: 2159.126953\n",
      "Train: step:  53120, time: 0.244, loss: 1104.349243\n",
      "Train: step:  53130, time: 0.252, loss: 1525.699951\n",
      "Train: step:  53140, time: 0.238, loss: 353.242615\n",
      "Train: step:  53150, time: 0.245, loss: 3404.573486\n",
      "Train: step:  53160, time: 0.266, loss: 1162.950317\n",
      "Train: step:  53170, time: 0.227, loss: 1672.296997\n",
      "Train: step:  53180, time: 0.239, loss: 1864.280029\n",
      "Train: step:  53190, time: 0.231, loss: 1854.916260\n",
      "Train: step:  53200, time: 0.243, loss: 2818.569580\n",
      "Train: step:  53210, time: 0.255, loss: 2056.600342\n",
      "Train: step:  53220, time: 0.243, loss: 2829.783691\n",
      "Train: step:  53230, time: 0.227, loss: 409.998138\n",
      "Train: step:  53240, time: 0.229, loss: 2467.030273\n",
      "Train: step:  53250, time: 0.230, loss: 2260.058350\n",
      "Train: step:  53260, time: 0.231, loss: 1555.742065\n",
      "Train: step:  53270, time: 0.246, loss: 1741.089722\n",
      "Train: step:  53280, time: 0.227, loss: 432.259735\n",
      "Train: step:  53290, time: 0.230, loss: 2061.214355\n",
      "Train: step:  53300, time: 0.227, loss: 1415.343262\n",
      "Train: step:  53310, time: 0.220, loss: 3746.482178\n",
      "Train: step:  53320, time: 0.219, loss: 2177.775635\n",
      "Train: step:  53330, time: 0.232, loss: 1665.070557\n",
      "Train: step:  53340, time: 0.257, loss: 676.656372\n",
      "Train: step:  53350, time: 0.234, loss: 1478.697266\n",
      "Train: step:  53360, time: 0.231, loss: 1913.450195\n",
      "Train: step:  53370, time: 0.259, loss: 957.035217\n",
      "Train: step:  53380, time: 0.231, loss: 1914.961060\n",
      "Train: step:  53390, time: 0.239, loss: 2419.411621\n",
      "Train: step:  53400, time: 0.285, loss: 2620.919922\n",
      "Train: step:  53410, time: 0.246, loss: 1979.398926\n",
      "Train: step:  53420, time: 0.238, loss: 867.326172\n",
      "Train: step:  53430, time: 0.238, loss: 2484.223145\n",
      "Train: step:  53440, time: 0.248, loss: 1644.820190\n",
      "Train: step:  53450, time: 0.254, loss: 1200.830200\n",
      "Train: step:  53460, time: 0.243, loss: 1864.982178\n",
      "Train: step:  53470, time: 0.250, loss: 1170.908447\n",
      "Train: step:  53480, time: 0.266, loss: 1940.640137\n",
      "Train: step:  53490, time: 0.237, loss: 1953.653320\n",
      "Train: step:  53500, time: 0.246, loss: 895.393372\n",
      "Train: step:  53510, time: 0.262, loss: 586.989807\n",
      "Train: step:  53520, time: 0.239, loss: 1927.841675\n",
      "Train: step:  53530, time: 0.228, loss: 2336.782227\n",
      "Train: step:  53540, time: 0.259, loss: 1515.199219\n",
      "Train: step:  53550, time: 0.244, loss: 1577.480957\n",
      "Train: step:  53560, time: 0.244, loss: 1805.278442\n",
      "Train: step:  53570, time: 0.259, loss: 1625.760864\n",
      "Train: step:  53580, time: 0.267, loss: 2499.088135\n",
      "Train: step:  53590, time: 0.274, loss: 2178.103760\n",
      "Train: step:  53600, time: 0.234, loss: 1496.062988\n",
      "Train: step:  53610, time: 0.232, loss: 412.344177\n",
      "Train: step:  53620, time: 0.256, loss: 365.449341\n",
      "Train: step:  53630, time: 0.242, loss: 954.870361\n",
      "Train: step:  53640, time: 0.235, loss: 274.811859\n",
      "Train: step:  53650, time: 0.238, loss: 2956.309570\n",
      "Train: step:  53660, time: 0.237, loss: 4586.527344\n",
      "Train: step:  53670, time: 0.263, loss: 2764.121338\n",
      "Train: step:  53680, time: 0.230, loss: 2167.946289\n",
      "Train: step:  53690, time: 0.226, loss: 3449.003174\n",
      "Train: step:  53700, time: 0.236, loss: 914.687134\n",
      "Train: step:  53710, time: 0.258, loss: 3547.101074\n",
      "Train: step:  53720, time: 0.238, loss: 1278.244385\n",
      "Train: step:  53730, time: 0.248, loss: 1073.406982\n",
      "Train: step:  53740, time: 0.231, loss: 838.751648\n",
      "Train: step:  53750, time: 0.246, loss: 1845.459106\n",
      "Train: step:  53760, time: 0.242, loss: 1201.075073\n",
      "Train: step:  53770, time: 0.259, loss: 569.302734\n",
      "Train: step:  53780, time: 0.243, loss: 2624.149902\n",
      "Train: step:  53790, time: 0.267, loss: 2587.496582\n",
      "Train: step:  53800, time: 0.242, loss: 479.415253\n",
      "Train: step:  53810, time: 0.243, loss: 742.919067\n",
      "Train: step:  53820, time: 0.253, loss: 754.950562\n",
      "Train: step:  53830, time: 0.243, loss: 1803.566406\n",
      "Train: step:  53840, time: 0.260, loss: 1114.406860\n",
      "Train: step:  53850, time: 0.247, loss: 2395.106689\n",
      "Train: step:  53860, time: 0.242, loss: 1992.533813\n",
      "Train: step:  53870, time: 0.264, loss: 171.962402\n",
      "Train: step:  53880, time: 0.250, loss: 554.143616\n",
      "Train: step:  53890, time: 0.283, loss: 584.339417\n",
      "Train: step:  53900, time: 0.278, loss: 2051.548096\n",
      "Train: step:  53910, time: 0.241, loss: 2220.063721\n",
      "Train: step:  53920, time: 0.275, loss: 4373.007812\n",
      "Train: step:  53930, time: 0.238, loss: 2019.583252\n",
      "Train: step:  53940, time: 0.262, loss: 612.090454\n",
      "Train: step:  53950, time: 0.241, loss: 1426.438110\n",
      "Train: step:  53960, time: 0.266, loss: 1344.180298\n",
      "Train: step:  53970, time: 0.253, loss: 2982.726074\n",
      "Train: step:  53980, time: 0.249, loss: 1743.277954\n",
      "Train: step:  53990, time: 0.256, loss: 2362.553711\n",
      "Train: step:  54000, time: 0.238, loss: 1298.314331\n",
      "Train: step:  54010, time: 0.247, loss: 1524.369629\n",
      "Train: step:  54020, time: 0.240, loss: 1459.336060\n",
      "Train: step:  54030, time: 0.243, loss: 989.147034\n",
      "Train: step:  54040, time: 0.242, loss: 2280.154541\n",
      "Train: step:  54050, time: 0.238, loss: 1267.669312\n",
      "Train: step:  54060, time: 0.242, loss: 1816.551392\n",
      "Train: step:  54070, time: 0.242, loss: 3137.849854\n",
      "Train: step:  54080, time: 0.248, loss: 2349.342041\n",
      "Train: step:  54090, time: 0.242, loss: 691.588074\n",
      "Train: step:  54100, time: 0.245, loss: 1374.312134\n",
      "Train: step:  54110, time: 0.245, loss: 1269.849854\n",
      "Train: step:  54120, time: 0.230, loss: 2222.037109\n",
      "Train: step:  54130, time: 0.228, loss: 1280.926025\n",
      "Train: step:  54140, time: 0.228, loss: 1418.111450\n",
      "Train: step:  54150, time: 0.241, loss: 1818.401123\n",
      "Train: step:  54160, time: 0.255, loss: 1722.158813\n",
      "Train: step:  54170, time: 0.235, loss: 2944.968506\n",
      "Train: step:  54180, time: 0.229, loss: 647.722473\n",
      "Train: step:  54190, time: 0.228, loss: 2188.728027\n",
      "Train: step:  54200, time: 0.262, loss: 1593.354858\n",
      "Train: step:  54210, time: 0.230, loss: 1728.604980\n",
      "Train: step:  54220, time: 0.238, loss: 3335.108643\n",
      "Train: step:  54230, time: 0.269, loss: 1427.317871\n",
      "Train: step:  54240, time: 0.222, loss: 353.737793\n",
      "Train: step:  54250, time: 0.259, loss: 1259.339478\n",
      "Train: step:  54260, time: 0.238, loss: 986.545349\n",
      "Train: step:  54270, time: 0.243, loss: 764.356628\n",
      "Train: step:  54280, time: 0.237, loss: 1368.622925\n",
      "Train: step:  54290, time: 0.251, loss: 2829.904541\n",
      "Train: step:  54300, time: 0.248, loss: 1610.635986\n",
      "Train: step:  54310, time: 0.236, loss: 606.784058\n",
      "Train: step:  54320, time: 0.259, loss: 2584.725586\n",
      "Train: step:  54330, time: 0.230, loss: 300.142395\n",
      "Train: step:  54340, time: 0.265, loss: 2212.258301\n",
      "Train: step:  54350, time: 0.266, loss: 3458.840820\n",
      "Train: step:  54360, time: 0.239, loss: 1700.925293\n",
      "Train: step:  54370, time: 0.241, loss: 1987.404053\n",
      "Train: step:  54380, time: 0.229, loss: 2120.430664\n",
      "Train: step:  54390, time: 0.228, loss: 2177.910400\n",
      "Train: step:  54400, time: 0.228, loss: 2518.979980\n",
      "Train: step:  54410, time: 0.235, loss: 319.892548\n",
      "Train: step:  54420, time: 0.231, loss: 2056.695312\n",
      "Train: step:  54430, time: 0.240, loss: 1711.057739\n",
      "Train: step:  54440, time: 0.230, loss: 2410.203857\n",
      "Train: step:  54450, time: 0.240, loss: 2991.768799\n",
      "Train: step:  54460, time: 0.224, loss: 400.064423\n",
      "Train: step:  54470, time: 0.242, loss: 813.998413\n",
      "Train: step:  54480, time: 0.234, loss: 2420.592285\n",
      "Train: step:  54490, time: 0.247, loss: 1017.906189\n",
      "Train: step:  54500, time: 0.260, loss: 751.828064\n",
      "Train: step:  54510, time: 0.259, loss: 1034.043457\n",
      "Train: step:  54520, time: 0.244, loss: 1646.586792\n",
      "Train: step:  54530, time: 0.248, loss: 2934.838867\n",
      "Train: step:  54540, time: 0.253, loss: 1637.528076\n",
      "Train: step:  54550, time: 0.234, loss: 1843.027222\n",
      "Train: step:  54560, time: 0.227, loss: 597.602844\n",
      "Train: step:  54570, time: 0.229, loss: 2104.793457\n",
      "Train: step:  54580, time: 0.239, loss: 616.347534\n",
      "Train: step:  54590, time: 0.262, loss: 2145.817871\n",
      "Train: step:  54600, time: 0.250, loss: 1425.056274\n",
      "Train: step:  54610, time: 0.229, loss: 1362.188721\n",
      "Train: step:  54620, time: 0.231, loss: 1999.907593\n",
      "Train: step:  54630, time: 0.226, loss: 1978.921143\n",
      "Train: step:  54640, time: 0.232, loss: 2369.434814\n",
      "Train: step:  54650, time: 0.224, loss: 968.277283\n",
      "Train: step:  54660, time: 0.217, loss: 904.960388\n",
      "Train: step:  54670, time: 0.226, loss: 2076.406738\n",
      "Train: step:  54680, time: 0.227, loss: 1876.592896\n",
      "Train: step:  54690, time: 0.225, loss: 1925.745117\n",
      "Train: step:  54700, time: 0.232, loss: 3714.801514\n",
      "Train: step:  54710, time: 0.251, loss: 700.309509\n",
      "Train: step:  54720, time: 0.277, loss: 3104.314453\n",
      "Train: step:  54730, time: 0.220, loss: 2114.752686\n",
      "Train: step:  54740, time: 0.227, loss: 698.461548\n",
      "Train: step:  54750, time: 0.233, loss: 396.328949\n",
      "Train: step:  54760, time: 0.245, loss: 2220.486816\n",
      "Train: step:  54770, time: 0.231, loss: 1284.919067\n",
      "Train: step:  54780, time: 0.224, loss: 229.929108\n",
      "Train: step:  54790, time: 0.246, loss: 3683.733643\n",
      "Train: step:  54800, time: 0.237, loss: 1851.416748\n",
      "Train: step:  54810, time: 0.229, loss: 1993.488403\n",
      "Train: step:  54820, time: 0.263, loss: 2050.482910\n",
      "Train: step:  54830, time: 0.245, loss: 2237.759766\n",
      "Train: step:  54840, time: 0.237, loss: 1984.900269\n",
      "Train: step:  54850, time: 0.233, loss: 733.037537\n",
      "Train: step:  54860, time: 0.242, loss: 1519.246216\n",
      "Train: step:  54870, time: 0.236, loss: 2367.611084\n",
      "Train: step:  54880, time: 0.240, loss: 1032.244019\n",
      "Train: step:  54890, time: 0.232, loss: 880.602783\n",
      "Train: step:  54900, time: 0.246, loss: 1628.661499\n",
      "Train: step:  54910, time: 0.235, loss: 2079.574951\n",
      "Train: step:  54920, time: 0.236, loss: 3996.597900\n",
      "Train: step:  54930, time: 0.266, loss: 1250.084229\n",
      "Train: step:  54940, time: 0.227, loss: 641.043640\n",
      "Train: step:  54950, time: 0.238, loss: 1711.157471\n",
      "Train: step:  54960, time: 0.245, loss: 1945.817017\n",
      "Train: step:  54970, time: 0.228, loss: 3916.643555\n",
      "Train: step:  54980, time: 0.222, loss: 1262.786499\n",
      "Train: step:  54990, time: 0.230, loss: 3540.368164\n",
      "Train: step:  55000, time: 0.315, loss: 749.778625\n",
      "Train: step:  55010, time: 0.229, loss: 3648.969727\n",
      "Train: step:  55020, time: 0.253, loss: 1484.077148\n",
      "Train: step:  55030, time: 0.217, loss: 2436.659912\n",
      "Train: step:  55040, time: 0.227, loss: 2328.359863\n",
      "Train: step:  55050, time: 0.246, loss: 2648.057861\n",
      "Train: step:  55060, time: 0.230, loss: 1397.070190\n",
      "Train: step:  55070, time: 0.253, loss: 1482.315063\n",
      "Train: step:  55080, time: 0.251, loss: 1375.725464\n",
      "Train: step:  55090, time: 0.260, loss: 1996.555176\n",
      "Train: step:  55100, time: 0.228, loss: 465.978149\n",
      "Train: step:  55110, time: 0.257, loss: 3070.714111\n",
      "Train: step:  55120, time: 0.239, loss: 1270.453369\n",
      "Train: step:  55130, time: 0.248, loss: 1524.825684\n",
      "Train: step:  55140, time: 0.241, loss: 1753.986084\n",
      "Train: step:  55150, time: 0.237, loss: 1964.162109\n",
      "Train: step:  55160, time: 0.262, loss: 1281.931763\n",
      "Train: step:  55170, time: 0.243, loss: 2079.026367\n",
      "Train: step:  55180, time: 0.241, loss: 1219.245850\n",
      "Train: step:  55190, time: 0.240, loss: 2825.789062\n",
      "Train: step:  55200, time: 0.244, loss: 1875.194458\n",
      "Train: step:  55210, time: 0.231, loss: 2634.775879\n",
      "Train: step:  55220, time: 0.238, loss: 2756.928955\n",
      "Train: step:  55230, time: 0.252, loss: 533.728271\n",
      "Train: step:  55240, time: 0.262, loss: 1484.603271\n",
      "Train: step:  55250, time: 0.264, loss: 1941.431519\n",
      "Train: step:  55260, time: 0.243, loss: 1226.759033\n",
      "Train: step:  55270, time: 0.247, loss: 1828.933838\n",
      "Train: step:  55280, time: 0.256, loss: 364.526703\n",
      "Train: step:  55290, time: 0.266, loss: 1980.430786\n",
      "Train: step:  55300, time: 0.248, loss: 2134.557129\n",
      "Train: step:  55310, time: 0.239, loss: 2110.030273\n",
      "Train: step:  55320, time: 0.234, loss: 1494.028076\n",
      "Train: step:  55330, time: 0.227, loss: 2774.494629\n",
      "Train: step:  55340, time: 0.234, loss: 1890.374390\n",
      "Train: step:  55350, time: 0.268, loss: 1526.519043\n",
      "Train: step:  55360, time: 0.258, loss: 1258.369263\n",
      "Train: step:  55370, time: 0.235, loss: 1520.604614\n",
      "Train: step:  55380, time: 0.232, loss: 3040.329834\n",
      "Train: step:  55390, time: 0.257, loss: 1276.707031\n",
      "Train: step:  55400, time: 0.236, loss: 2672.429688\n",
      "Train: step:  55410, time: 0.269, loss: 620.497437\n",
      "Train: step:  55420, time: 0.231, loss: 1433.978149\n",
      "Train: step:  55430, time: 0.219, loss: 1152.181152\n",
      "Train: step:  55440, time: 0.223, loss: 2451.119385\n",
      "Train: step:  55450, time: 0.238, loss: 2165.088379\n",
      "Train: step:  55460, time: 0.265, loss: 2233.219971\n",
      "Train: step:  55470, time: 0.262, loss: 1211.567871\n",
      "Train: step:  55480, time: 0.234, loss: 1484.530640\n",
      "Train: step:  55490, time: 0.224, loss: 1177.660767\n",
      "Train: step:  55500, time: 0.253, loss: 1864.389526\n",
      "Train: step:  55510, time: 0.220, loss: 1292.891113\n",
      "Train: step:  55520, time: 0.243, loss: 2491.618164\n",
      "Train: step:  55530, time: 0.225, loss: 1511.742065\n",
      "Train: step:  55540, time: 0.235, loss: 2487.247314\n",
      "Train: step:  55550, time: 0.252, loss: 637.699463\n",
      "Train: step:  55560, time: 0.227, loss: 2055.360352\n",
      "Train: step:  55570, time: 0.234, loss: 709.478271\n",
      "Train: step:  55580, time: 0.220, loss: 1698.895508\n",
      "Train: step:  55590, time: 0.229, loss: 960.140320\n",
      "Train: step:  55600, time: 0.236, loss: 2310.876953\n",
      "Train: step:  55610, time: 0.238, loss: 2875.533447\n",
      "Train: step:  55620, time: 0.223, loss: 3058.739502\n",
      "Train: step:  55630, time: 0.231, loss: 1387.481567\n",
      "Train: step:  55640, time: 0.250, loss: 1422.031006\n",
      "Train: step:  55650, time: 0.252, loss: 2780.205322\n",
      "Train: step:  55660, time: 0.230, loss: 857.092224\n",
      "Train: step:  55670, time: 0.259, loss: 257.966797\n",
      "Train: step:  55680, time: 0.228, loss: 3923.070312\n",
      "Train: step:  55690, time: 0.233, loss: 1636.343750\n",
      "Train: step:  55700, time: 0.257, loss: 785.672852\n",
      "Train: step:  55710, time: 0.254, loss: 1606.880981\n",
      "Train: step:  55720, time: 0.228, loss: 1473.459961\n",
      "Train: step:  55730, time: 0.240, loss: 479.259521\n",
      "Train: step:  55740, time: 0.229, loss: 2279.183838\n",
      "Train: step:  55750, time: 0.254, loss: 3781.619141\n",
      "Train: step:  55760, time: 0.235, loss: 2388.724854\n",
      "Train: step:  55770, time: 0.229, loss: 2276.166016\n",
      "Train: step:  55780, time: 0.234, loss: 1780.679077\n",
      "Train: step:  55790, time: 0.264, loss: 247.528244\n",
      "Train: step:  55800, time: 0.229, loss: 2167.003174\n",
      "Train: step:  55810, time: 0.244, loss: 383.038025\n",
      "Train: step:  55820, time: 0.241, loss: 2162.222656\n",
      "Train: step:  55830, time: 0.240, loss: 2345.313232\n",
      "Train: step:  55840, time: 0.232, loss: 2480.939697\n",
      "Train: step:  55850, time: 0.238, loss: 301.470184\n",
      "Train: step:  55860, time: 0.240, loss: 1380.808350\n",
      "Train: step:  55870, time: 0.237, loss: 1350.494141\n",
      "Train: step:  55880, time: 0.242, loss: 2792.476807\n",
      "Train: step:  55890, time: 0.270, loss: 3622.826416\n",
      "Train: step:  55900, time: 0.240, loss: 1182.517822\n",
      "Train: step:  55910, time: 0.264, loss: 2572.245361\n",
      "Train: step:  55920, time: 0.230, loss: 1289.654297\n",
      "Train: step:  55930, time: 0.227, loss: 237.040985\n",
      "Train: step:  55940, time: 0.227, loss: 1260.059814\n",
      "Train: step:  55950, time: 0.249, loss: 2316.680176\n",
      "Train: step:  55960, time: 0.249, loss: 1838.405518\n",
      "Train: step:  55970, time: 0.233, loss: 1793.853638\n",
      "Train: step:  55980, time: 0.230, loss: 838.514160\n",
      "Train: step:  55990, time: 0.232, loss: 867.661865\n",
      "Train: step:  56000, time: 0.226, loss: 694.415527\n",
      "Train: step:  56010, time: 0.234, loss: 358.029968\n",
      "Train: step:  56020, time: 0.249, loss: 1882.411255\n",
      "Train: step:  56030, time: 0.244, loss: 2289.123047\n",
      "Train: step:  56040, time: 0.234, loss: 2288.219727\n",
      "Train: step:  56050, time: 0.239, loss: 1722.585571\n",
      "Train: step:  56060, time: 0.255, loss: 2615.972656\n",
      "Train: step:  56070, time: 0.243, loss: 1884.291382\n",
      "Train: step:  56080, time: 0.225, loss: 2482.485840\n",
      "Train: step:  56090, time: 0.230, loss: 1825.345093\n",
      "Train: step:  56100, time: 0.250, loss: 2604.546387\n",
      "Train: step:  56110, time: 0.234, loss: 1530.034912\n",
      "Train: step:  56120, time: 0.228, loss: 595.677429\n",
      "Train: step:  56130, time: 0.233, loss: 1871.827148\n",
      "Train: step:  56140, time: 0.235, loss: 1556.615479\n",
      "Train: step:  56150, time: 0.228, loss: 3574.358154\n",
      "Train: step:  56160, time: 0.225, loss: 2824.095215\n",
      "Train: step:  56170, time: 0.232, loss: 2680.508789\n",
      "Train: step:  56180, time: 0.231, loss: 2619.573486\n",
      "Train: step:  56190, time: 0.258, loss: 1589.949585\n",
      "Train: step:  56200, time: 0.229, loss: 1655.773071\n",
      "Train: step:  56210, time: 0.231, loss: 1763.126831\n",
      "Train: step:  56220, time: 0.230, loss: 587.846008\n",
      "Train: step:  56230, time: 0.262, loss: 2491.495361\n",
      "Train: step:  56240, time: 0.234, loss: 624.869141\n",
      "Train: step:  56250, time: 0.227, loss: 2704.281738\n",
      "Train: step:  56260, time: 0.238, loss: 511.787170\n",
      "Train: step:  56270, time: 0.261, loss: 1412.433350\n",
      "Train: step:  56280, time: 0.237, loss: 1669.399292\n",
      "Train: step:  56290, time: 0.240, loss: 3341.896484\n",
      "Train: step:  56300, time: 0.236, loss: 549.521484\n",
      "Train: step:  56310, time: 0.241, loss: 1313.416382\n",
      "Train: step:  56320, time: 0.239, loss: 1718.657593\n",
      "Train: step:  56330, time: 0.236, loss: 796.738892\n",
      "Train: step:  56340, time: 0.237, loss: 783.789246\n",
      "Train: step:  56350, time: 0.232, loss: 2265.030029\n",
      "Train: step:  56360, time: 0.257, loss: 2394.726807\n",
      "Train: step:  56370, time: 0.227, loss: 2589.963623\n",
      "Train: step:  56380, time: 0.240, loss: 2264.440430\n",
      "Train: step:  56390, time: 0.251, loss: 1654.585449\n",
      "Train: step:  56400, time: 0.234, loss: 2666.038818\n",
      "Train: step:  56410, time: 0.230, loss: 2965.879639\n",
      "Train: step:  56420, time: 0.247, loss: 2200.448486\n",
      "Train: step:  56430, time: 0.236, loss: 1406.488647\n",
      "Train: step:  56440, time: 0.258, loss: 735.903381\n",
      "Train: step:  56450, time: 0.232, loss: 185.151398\n",
      "Train: step:  56460, time: 0.231, loss: 1675.380737\n",
      "Train: step:  56470, time: 0.289, loss: 2444.666260\n",
      "Train: step:  56480, time: 0.247, loss: 594.844849\n",
      "Train: step:  56490, time: 0.254, loss: 2230.510254\n",
      "Train: step:  56500, time: 0.230, loss: 1143.644653\n",
      "Train: step:  56510, time: 0.255, loss: 3523.241455\n",
      "Train: step:  56520, time: 0.231, loss: 4036.939941\n",
      "Train: step:  56530, time: 0.228, loss: 1385.092041\n",
      "Train: step:  56540, time: 0.251, loss: 221.683319\n",
      "Train: step:  56550, time: 0.237, loss: 2119.513672\n",
      "Train: step:  56560, time: 0.232, loss: 2613.190186\n",
      "Train: step:  56570, time: 0.233, loss: 1627.659058\n",
      "Train: step:  56580, time: 0.232, loss: 2485.354980\n",
      "Train: step:  56590, time: 0.237, loss: 629.794373\n",
      "Train: step:  56600, time: 0.235, loss: 877.128113\n",
      "Train: step:  56610, time: 0.232, loss: 1413.447510\n",
      "Train: step:  56620, time: 0.227, loss: 511.480713\n",
      "Train: step:  56630, time: 0.221, loss: 2973.739258\n",
      "Train: step:  56640, time: 0.252, loss: 1471.237427\n",
      "Train: step:  56650, time: 0.256, loss: 1759.757446\n",
      "Train: step:  56660, time: 0.249, loss: 2748.241699\n",
      "Train: step:  56670, time: 0.255, loss: 1631.827271\n",
      "Train: step:  56680, time: 0.227, loss: 1740.100952\n",
      "Train: step:  56690, time: 0.228, loss: 1289.784424\n",
      "Train: step:  56700, time: 0.264, loss: 3751.614014\n",
      "Train: step:  56710, time: 0.230, loss: 1019.860596\n",
      "Train: step:  56720, time: 0.226, loss: 2819.415283\n",
      "Train: step:  56730, time: 0.248, loss: 2167.649658\n",
      "Train: step:  56740, time: 0.227, loss: 1003.249390\n",
      "Train: step:  56750, time: 0.238, loss: 3580.185547\n",
      "Train: step:  56760, time: 0.254, loss: 2502.952393\n",
      "Train: step:  56770, time: 0.234, loss: 1158.790649\n",
      "Train: step:  56780, time: 0.239, loss: 1687.248779\n",
      "Train: step:  56790, time: 0.259, loss: 1692.815796\n",
      "Train: step:  56800, time: 0.254, loss: 534.983887\n",
      "Train: step:  56810, time: 0.255, loss: 2701.550537\n",
      "Train: step:  56820, time: 0.238, loss: 882.209900\n",
      "Train: step:  56830, time: 0.241, loss: 972.133911\n",
      "Train: step:  56840, time: 0.243, loss: 2537.508301\n",
      "Train: step:  56850, time: 0.237, loss: 2624.265137\n",
      "Train: step:  56860, time: 0.227, loss: 983.435608\n",
      "Train: step:  56870, time: 0.257, loss: 3451.087158\n",
      "Train: step:  56880, time: 0.273, loss: 3058.835449\n",
      "Train: step:  56890, time: 0.226, loss: 1645.152954\n",
      "Train: step:  56900, time: 0.236, loss: 692.051147\n",
      "Train: step:  56910, time: 0.254, loss: 680.097961\n",
      "Train: step:  56920, time: 0.236, loss: 1059.089478\n",
      "Train: step:  56930, time: 0.264, loss: 1776.670898\n",
      "Train: step:  56940, time: 0.234, loss: 1195.743774\n",
      "Train: step:  56950, time: 0.231, loss: 1914.418945\n",
      "Train: step:  56960, time: 0.223, loss: 2954.561279\n",
      "Train: step:  56970, time: 0.227, loss: 2253.673828\n",
      "Train: step:  56980, time: 0.298, loss: 731.165710\n",
      "Train: step:  56990, time: 0.254, loss: 444.518738\n",
      "Train: step:  57000, time: 0.279, loss: 835.229553\n",
      "Train: step:  57010, time: 0.249, loss: 2760.730225\n",
      "Train: step:  57020, time: 0.230, loss: 1055.330688\n",
      "Train: step:  57030, time: 0.253, loss: 1144.291138\n",
      "Train: step:  57040, time: 0.271, loss: 2204.178955\n",
      "Train: step:  57050, time: 0.226, loss: 1278.811279\n",
      "Train: step:  57060, time: 0.255, loss: 3754.192383\n",
      "Train: step:  57070, time: 0.227, loss: 334.574158\n",
      "Train: step:  57080, time: 0.228, loss: 1558.615112\n",
      "Train: step:  57090, time: 0.231, loss: 1605.005737\n",
      "Train: step:  57100, time: 0.269, loss: 2198.186768\n",
      "Train: step:  57110, time: 0.227, loss: 1380.857178\n",
      "Train: step:  57120, time: 0.229, loss: 3564.434814\n",
      "Train: step:  57130, time: 0.230, loss: 3223.096436\n",
      "Train: step:  57140, time: 0.260, loss: 2876.686035\n",
      "Train: step:  57150, time: 0.270, loss: 2386.099365\n",
      "Train: step:  57160, time: 0.225, loss: 2649.095215\n",
      "Train: step:  57170, time: 0.230, loss: 2532.848145\n",
      "Train: step:  57180, time: 0.232, loss: 2100.404785\n",
      "Train: step:  57190, time: 0.253, loss: 1976.758667\n",
      "Train: step:  57200, time: 0.253, loss: 2041.543945\n",
      "Train: step:  57210, time: 0.245, loss: 1327.946899\n",
      "Train: step:  57220, time: 0.239, loss: 478.755829\n",
      "Train: step:  57230, time: 0.232, loss: 1696.580078\n",
      "Train: step:  57240, time: 0.251, loss: 744.042969\n",
      "Train: step:  57250, time: 0.220, loss: 359.095520\n",
      "Train: step:  57260, time: 0.235, loss: 2377.097900\n",
      "Train: step:  57270, time: 0.239, loss: 1307.641357\n",
      "Train: step:  57280, time: 0.254, loss: 1714.189819\n",
      "Train: step:  57290, time: 0.225, loss: 728.283691\n",
      "Train: step:  57300, time: 0.232, loss: 2005.221191\n",
      "Train: step:  57310, time: 0.260, loss: 1576.237793\n",
      "Train: step:  57320, time: 0.262, loss: 2333.083008\n",
      "Train: step:  57330, time: 0.232, loss: 1685.636841\n",
      "Train: step:  57340, time: 0.279, loss: 1434.159424\n",
      "Train: step:  57350, time: 0.238, loss: 2262.545898\n",
      "Train: step:  57360, time: 0.236, loss: 2520.172119\n",
      "Train: step:  57370, time: 0.236, loss: 2867.177734\n",
      "Train: step:  57380, time: 0.233, loss: 609.303772\n",
      "Train: step:  57390, time: 0.235, loss: 2212.229004\n",
      "Train: step:  57400, time: 0.272, loss: 1243.089600\n",
      "Train: step:  57410, time: 0.237, loss: 784.806458\n",
      "Train: step:  57420, time: 0.226, loss: 2083.506592\n",
      "Train: step:  57430, time: 0.226, loss: 2409.246094\n",
      "Train: step:  57440, time: 0.233, loss: 468.484863\n",
      "Train: step:  57450, time: 0.231, loss: 1657.322388\n",
      "Train: step:  57460, time: 0.222, loss: 1723.402100\n",
      "Train: step:  57470, time: 0.263, loss: 1007.326721\n",
      "Train: step:  57480, time: 0.226, loss: 1720.001099\n",
      "Train: step:  57490, time: 0.224, loss: 2378.770752\n",
      "Train: step:  57500, time: 0.264, loss: 3976.423340\n",
      "Train: step:  57510, time: 0.249, loss: 1504.789429\n",
      "Train: step:  57520, time: 0.227, loss: 3063.729004\n",
      "Train: step:  57530, time: 0.226, loss: 876.396301\n",
      "Train: step:  57540, time: 0.233, loss: 1584.036255\n",
      "Train: step:  57550, time: 0.263, loss: 2509.721680\n",
      "Train: step:  57560, time: 0.229, loss: 1668.586670\n",
      "Train: step:  57570, time: 0.225, loss: 3803.577148\n",
      "Train: step:  57580, time: 0.223, loss: 1022.336060\n",
      "Train: step:  57590, time: 0.254, loss: 2623.553223\n",
      "Train: step:  57600, time: 0.234, loss: 525.573914\n",
      "Train: step:  57610, time: 0.232, loss: 2161.182861\n",
      "Train: step:  57620, time: 0.240, loss: 3278.507568\n",
      "Train: step:  57630, time: 0.252, loss: 1949.375732\n",
      "Train: step:  57640, time: 0.258, loss: 3365.666748\n",
      "Train: step:  57650, time: 0.231, loss: 1965.256714\n",
      "Train: step:  57660, time: 0.244, loss: 527.659668\n",
      "Train: step:  57670, time: 0.263, loss: 849.531494\n",
      "Train: step:  57680, time: 0.249, loss: 2168.307861\n",
      "Train: step:  57690, time: 0.231, loss: 457.639801\n",
      "Train: step:  57700, time: 0.223, loss: 2625.568604\n",
      "Train: step:  57710, time: 0.229, loss: 2901.486084\n",
      "Train: step:  57720, time: 0.249, loss: 2521.077148\n",
      "Train: step:  57730, time: 0.254, loss: 346.575500\n",
      "Train: step:  57740, time: 0.221, loss: 2834.450684\n",
      "Train: step:  57750, time: 0.249, loss: 2394.569580\n",
      "Train: step:  57760, time: 0.265, loss: 3496.989502\n",
      "Train: step:  57770, time: 0.229, loss: 787.962585\n",
      "Train: step:  57780, time: 0.244, loss: 1216.534424\n",
      "Train: step:  57790, time: 0.229, loss: 1785.687500\n",
      "Train: step:  57800, time: 0.222, loss: 4343.652832\n",
      "Train: step:  57810, time: 0.238, loss: 593.768127\n",
      "Train: step:  57820, time: 0.241, loss: 2533.271240\n",
      "Train: step:  57830, time: 0.231, loss: 858.271362\n",
      "Train: step:  57840, time: 0.227, loss: 818.256226\n",
      "Train: step:  57850, time: 0.227, loss: 1815.098267\n",
      "Train: step:  57860, time: 0.233, loss: 3302.904785\n",
      "Train: step:  57870, time: 0.234, loss: 2893.086182\n",
      "Train: step:  57880, time: 0.236, loss: 1626.665649\n",
      "Train: step:  57890, time: 0.231, loss: 1832.793823\n",
      "Train: step:  57900, time: 0.266, loss: 2169.983154\n",
      "Train: step:  57910, time: 0.257, loss: 221.526794\n",
      "Train: step:  57920, time: 0.221, loss: 3475.500732\n",
      "Train: step:  57930, time: 0.238, loss: 1369.157715\n",
      "Train: step:  57940, time: 0.228, loss: 3701.330566\n",
      "Train: step:  57950, time: 0.228, loss: 2062.771973\n",
      "Train: step:  57960, time: 0.255, loss: 1584.069336\n",
      "Train: step:  57970, time: 0.237, loss: 1696.296875\n",
      "Train: step:  57980, time: 0.256, loss: 799.892456\n",
      "Train: step:  57990, time: 0.226, loss: 361.954407\n",
      "Train: step:  58000, time: 0.241, loss: 2532.215088\n",
      "Train: step:  58010, time: 0.230, loss: 1449.171021\n",
      "Train: step:  58020, time: 0.226, loss: 992.752380\n",
      "Train: step:  58030, time: 0.224, loss: 2031.296875\n",
      "Train: step:  58040, time: 0.233, loss: 1105.652710\n",
      "Train: step:  58050, time: 0.235, loss: 1156.295532\n",
      "Train: step:  58060, time: 0.230, loss: 899.297791\n",
      "Train: step:  58070, time: 0.267, loss: 3405.645264\n",
      "Train: step:  58080, time: 0.226, loss: 3071.735840\n",
      "Train: step:  58090, time: 0.255, loss: 4635.697754\n",
      "Train: step:  58100, time: 0.227, loss: 2099.410889\n",
      "Train: step:  58110, time: 0.266, loss: 308.442902\n",
      "Train: step:  58120, time: 0.222, loss: 808.740723\n",
      "Train: step:  58130, time: 0.239, loss: 1386.066650\n",
      "Train: step:  58140, time: 0.218, loss: 1951.176758\n",
      "Train: step:  58150, time: 0.231, loss: 296.267792\n",
      "Train: step:  58160, time: 0.278, loss: 2280.484375\n",
      "Train: step:  58170, time: 0.226, loss: 1694.102417\n",
      "Train: step:  58180, time: 0.223, loss: 2010.203613\n",
      "Train: step:  58190, time: 0.230, loss: 1614.661133\n",
      "Train: step:  58200, time: 0.236, loss: 361.061310\n",
      "Train: step:  58210, time: 0.224, loss: 822.716370\n",
      "Train: step:  58220, time: 0.228, loss: 2377.742432\n",
      "Train: step:  58230, time: 0.240, loss: 921.158142\n",
      "Train: step:  58240, time: 0.242, loss: 1736.334473\n",
      "Train: step:  58250, time: 0.237, loss: 1903.657837\n",
      "Train: step:  58260, time: 0.237, loss: 913.654053\n",
      "Train: step:  58270, time: 0.268, loss: 1760.408569\n",
      "Train: step:  58280, time: 0.242, loss: 1469.447632\n",
      "Train: step:  58290, time: 0.239, loss: 2216.129639\n",
      "Train: step:  58300, time: 0.237, loss: 1656.613525\n",
      "Train: step:  58310, time: 0.256, loss: 3352.026123\n",
      "Train: step:  58320, time: 0.247, loss: 2209.875244\n",
      "Train: step:  58330, time: 0.255, loss: 414.777496\n",
      "Train: step:  58340, time: 0.231, loss: 2402.918701\n",
      "Train: step:  58350, time: 0.231, loss: 1755.634399\n",
      "Train: step:  58360, time: 0.225, loss: 2961.959229\n",
      "Train: step:  58370, time: 0.234, loss: 1009.639832\n",
      "Train: step:  58380, time: 0.229, loss: 3100.198486\n",
      "Train: step:  58390, time: 0.224, loss: 2504.140137\n",
      "Train: step:  58400, time: 0.234, loss: 3074.632568\n",
      "Train: step:  58410, time: 0.244, loss: 1501.254517\n",
      "Train: step:  58420, time: 0.257, loss: 2884.718262\n",
      "Train: step:  58430, time: 0.229, loss: 1711.709839\n",
      "Train: step:  58440, time: 0.234, loss: 3089.117432\n",
      "Train: step:  58450, time: 0.250, loss: 2626.902832\n",
      "Train: step:  58460, time: 0.223, loss: 461.163086\n",
      "Train: step:  58470, time: 0.262, loss: 434.394135\n",
      "Train: step:  58480, time: 0.231, loss: 1113.259644\n",
      "Train: step:  58490, time: 0.227, loss: 957.561340\n",
      "Train: step:  58500, time: 0.222, loss: 1520.162964\n",
      "Train: step:  58510, time: 0.222, loss: 1610.392700\n",
      "Train: step:  58520, time: 0.240, loss: 992.088745\n",
      "Train: step:  58530, time: 0.227, loss: 2511.658203\n",
      "Train: step:  58540, time: 0.221, loss: 1548.407715\n",
      "Train: step:  58550, time: 0.257, loss: 2437.720215\n",
      "Train: step:  58560, time: 0.234, loss: 3526.233154\n",
      "Train: step:  58570, time: 0.223, loss: 1673.781128\n",
      "Train: step:  58580, time: 0.225, loss: 2335.916260\n",
      "Train: step:  58590, time: 0.240, loss: 751.668823\n",
      "Train: step:  58600, time: 0.225, loss: 2255.976318\n",
      "Train: step:  58610, time: 0.251, loss: 653.886353\n",
      "Train: step:  58620, time: 0.220, loss: 1112.329590\n",
      "Train: step:  58630, time: 0.243, loss: 644.361633\n",
      "Train: step:  58640, time: 0.250, loss: 1668.693359\n",
      "Train: step:  58650, time: 0.220, loss: 4371.468750\n",
      "Train: step:  58660, time: 0.228, loss: 2079.715576\n",
      "Train: step:  58670, time: 0.234, loss: 2692.299072\n",
      "Train: step:  58680, time: 0.223, loss: 2213.029297\n",
      "Train: step:  58690, time: 0.237, loss: 820.361389\n",
      "Train: step:  58700, time: 0.257, loss: 2515.996826\n",
      "Train: step:  58710, time: 0.230, loss: 599.267517\n",
      "Train: step:  58720, time: 0.220, loss: 2277.987061\n",
      "Train: step:  58730, time: 0.223, loss: 742.498169\n",
      "Train: step:  58740, time: 0.234, loss: 1641.790527\n",
      "Train: step:  58750, time: 0.232, loss: 2073.814453\n",
      "Train: step:  58760, time: 0.233, loss: 3242.254639\n",
      "Train: step:  58770, time: 0.223, loss: 1297.077759\n",
      "Train: step:  58780, time: 0.234, loss: 3291.225830\n",
      "Train: step:  58790, time: 0.240, loss: 1916.889526\n",
      "Train: step:  58800, time: 0.250, loss: 2765.268066\n",
      "Train: step:  58810, time: 0.239, loss: 1996.760864\n",
      "Train: step:  58820, time: 0.222, loss: 1408.862915\n",
      "Train: step:  58830, time: 0.226, loss: 2918.729980\n",
      "Train: step:  58840, time: 0.225, loss: 2626.395508\n",
      "Train: step:  58850, time: 0.235, loss: 515.164856\n",
      "Train: step:  58860, time: 0.222, loss: 1250.762207\n",
      "Train: step:  58870, time: 0.216, loss: 1486.516602\n",
      "Train: step:  58880, time: 0.269, loss: 736.362122\n",
      "Train: step:  58890, time: 0.264, loss: 1901.129028\n",
      "Train: step:  58900, time: 0.217, loss: 3972.378906\n",
      "Train: step:  58910, time: 0.218, loss: 1648.568726\n",
      "Train: step:  58920, time: 0.218, loss: 2268.401611\n",
      "Train: step:  58930, time: 0.227, loss: 3189.451416\n",
      "Train: step:  58940, time: 0.263, loss: 1465.252197\n",
      "Train: step:  58950, time: 0.232, loss: 1310.033203\n",
      "Train: step:  58960, time: 0.226, loss: 1704.175415\n",
      "Train: step:  58970, time: 0.253, loss: 1504.114502\n",
      "Train: step:  58980, time: 0.223, loss: 1227.432251\n",
      "Train: step:  58990, time: 0.230, loss: 1635.198242\n",
      "Train: step:  59000, time: 0.252, loss: 1329.478638\n",
      "Train: step:  59010, time: 0.226, loss: 1240.593262\n",
      "Train: step:  59020, time: 0.228, loss: 330.996277\n",
      "Train: step:  59030, time: 0.252, loss: 1120.604858\n",
      "Train: step:  59040, time: 0.229, loss: 1181.063965\n",
      "Train: step:  59050, time: 0.236, loss: 1611.992432\n",
      "Train: step:  59060, time: 0.258, loss: 296.763306\n",
      "Train: step:  59070, time: 0.230, loss: 3852.179688\n",
      "Train: step:  59080, time: 0.254, loss: 2144.419922\n",
      "Train: step:  59090, time: 0.252, loss: 2320.329590\n",
      "Train: step:  59100, time: 0.229, loss: 4074.982422\n",
      "Train: step:  59110, time: 0.224, loss: 3359.845703\n",
      "Train: step:  59120, time: 0.219, loss: 2874.199219\n",
      "Train: step:  59130, time: 0.224, loss: 1902.180664\n",
      "Train: step:  59140, time: 0.285, loss: 2167.981445\n",
      "Train: step:  59150, time: 0.240, loss: 1384.057007\n",
      "Train: step:  59160, time: 0.251, loss: 1555.810425\n",
      "Train: step:  59170, time: 0.231, loss: 356.718536\n",
      "Train: step:  59180, time: 0.233, loss: 1812.846191\n",
      "Train: step:  59190, time: 0.251, loss: 937.026306\n",
      "Train: step:  59200, time: 0.268, loss: 649.750366\n",
      "Train: step:  59210, time: 0.263, loss: 1682.678467\n",
      "Train: step:  59220, time: 0.238, loss: 1899.700806\n",
      "Train: step:  59230, time: 0.238, loss: 285.990387\n",
      "Train: step:  59240, time: 0.247, loss: 1505.600220\n",
      "Train: step:  59250, time: 0.238, loss: 1647.567871\n",
      "Train: step:  59260, time: 0.238, loss: 1558.990723\n",
      "Train: step:  59270, time: 0.248, loss: 2238.761963\n",
      "Train: step:  59280, time: 0.229, loss: 688.547668\n",
      "Train: step:  59290, time: 0.250, loss: 1266.486328\n",
      "Train: step:  59300, time: 0.256, loss: 2484.777344\n",
      "Train: step:  59310, time: 0.226, loss: 193.966492\n",
      "Train: step:  59320, time: 0.232, loss: 1416.741577\n",
      "Train: step:  59330, time: 0.236, loss: 3248.973877\n",
      "Train: step:  59340, time: 0.229, loss: 1361.294067\n",
      "Train: step:  59350, time: 0.225, loss: 2981.761719\n",
      "Train: step:  59360, time: 0.233, loss: 1319.793335\n",
      "Train: step:  59370, time: 0.224, loss: 2537.414062\n",
      "Train: step:  59380, time: 0.251, loss: 1430.923706\n",
      "Train: step:  59390, time: 0.221, loss: 851.124634\n",
      "Train: step:  59400, time: 0.232, loss: 613.884094\n",
      "Train: step:  59410, time: 0.230, loss: 3622.474609\n",
      "Train: step:  59420, time: 0.237, loss: 3617.912109\n",
      "Train: step:  59430, time: 0.228, loss: 1038.212158\n",
      "Train: step:  59440, time: 0.240, loss: 1731.123657\n",
      "Train: step:  59450, time: 0.230, loss: 323.666412\n",
      "Train: step:  59460, time: 0.227, loss: 3662.851074\n",
      "Train: step:  59470, time: 0.236, loss: 2962.104980\n",
      "Train: step:  59480, time: 0.225, loss: 2959.223877\n",
      "Train: step:  59490, time: 0.226, loss: 1587.474121\n",
      "Train: step:  59500, time: 0.227, loss: 457.405518\n",
      "Train: step:  59510, time: 0.221, loss: 1734.677612\n",
      "Train: step:  59520, time: 0.228, loss: 869.310181\n",
      "Train: step:  59530, time: 0.231, loss: 2211.007324\n",
      "Train: step:  59540, time: 0.254, loss: 643.015991\n",
      "Train: step:  59550, time: 0.248, loss: 790.384827\n",
      "Train: step:  59560, time: 0.233, loss: 469.456238\n",
      "Train: step:  59570, time: 0.242, loss: 1090.751831\n",
      "Train: step:  59580, time: 0.224, loss: 934.602356\n",
      "Train: step:  59590, time: 0.235, loss: 864.787659\n",
      "Train: step:  59600, time: 0.220, loss: 2413.830811\n",
      "Train: step:  59610, time: 0.228, loss: 2293.370850\n",
      "Train: step:  59620, time: 0.262, loss: 1935.899170\n",
      "Train: step:  59630, time: 0.259, loss: 446.488373\n",
      "Train: step:  59640, time: 0.257, loss: 1138.034424\n",
      "Train: step:  59650, time: 0.219, loss: 864.292664\n",
      "Train: step:  59660, time: 0.235, loss: 682.384399\n",
      "Train: step:  59670, time: 0.267, loss: 1525.435059\n",
      "Train: step:  59680, time: 0.238, loss: 2194.028320\n",
      "Train: step:  59690, time: 0.248, loss: 1349.378662\n",
      "Train: step:  59700, time: 0.235, loss: 3062.354004\n",
      "Train: step:  59710, time: 0.233, loss: 334.025238\n",
      "Train: step:  59720, time: 0.236, loss: 2804.395020\n",
      "Train: step:  59730, time: 0.265, loss: 1532.339355\n",
      "Train: step:  59740, time: 0.253, loss: 2437.762939\n",
      "Train: step:  59750, time: 0.247, loss: 2553.041992\n",
      "Train: step:  59760, time: 0.241, loss: 2066.619629\n",
      "Train: step:  59770, time: 0.239, loss: 1193.416870\n",
      "Train: step:  59780, time: 0.242, loss: 1696.756104\n",
      "Train: step:  59790, time: 0.237, loss: 1589.002930\n",
      "Train: step:  59800, time: 0.253, loss: 620.911438\n",
      "Train: step:  59810, time: 0.232, loss: 2293.510498\n",
      "Train: step:  59820, time: 0.257, loss: 470.517029\n",
      "Train: step:  59830, time: 0.229, loss: 2832.697998\n",
      "Train: step:  59840, time: 0.231, loss: 2756.607422\n",
      "Train: step:  59850, time: 0.236, loss: 2567.081055\n",
      "Train: step:  59860, time: 0.265, loss: 2802.586426\n",
      "Train: step:  59870, time: 0.229, loss: 2361.877930\n",
      "Train: step:  59880, time: 0.259, loss: 1329.832275\n",
      "Train: step:  59890, time: 0.256, loss: 2273.007568\n",
      "Train: step:  59900, time: 0.258, loss: 2701.311523\n",
      "Train: step:  59910, time: 0.238, loss: 794.386963\n",
      "Train: step:  59920, time: 0.240, loss: 2103.108398\n",
      "Train: step:  59930, time: 0.262, loss: 1078.268677\n",
      "Train: step:  59940, time: 0.264, loss: 2362.036377\n",
      "Train: step:  59950, time: 0.235, loss: 1199.998047\n",
      "Train: step:  59960, time: 0.238, loss: 1221.349609\n",
      "Train: step:  59970, time: 0.241, loss: 1097.976929\n",
      "Train: step:  59980, time: 0.235, loss: 1653.088623\n",
      "Train: step:  59990, time: 0.271, loss: 2612.893799\n",
      "Train: step:  60000, time: 0.251, loss: 1969.651855\n",
      "Train: step:  60010, time: 0.247, loss: 499.869965\n",
      "Train: step:  60020, time: 0.240, loss: 1514.374512\n",
      "Train: step:  60030, time: 0.242, loss: 2050.191895\n",
      "Train: step:  60040, time: 0.240, loss: 1310.824341\n",
      "Train: step:  60050, time: 0.239, loss: 1710.910767\n",
      "Train: step:  60060, time: 0.235, loss: 973.676514\n",
      "Train: step:  60070, time: 0.249, loss: 3220.376709\n",
      "Train: step:  60080, time: 0.245, loss: 3217.026611\n",
      "Train: step:  60090, time: 0.238, loss: 3240.300049\n",
      "Train: step:  60100, time: 0.255, loss: 3806.802002\n",
      "Train: step:  60110, time: 0.246, loss: 3392.672852\n",
      "Train: step:  60120, time: 0.271, loss: 2161.049805\n",
      "Train: step:  60130, time: 0.245, loss: 2439.810791\n",
      "Train: step:  60140, time: 0.239, loss: 1179.692627\n",
      "Train: step:  60150, time: 0.270, loss: 2272.915771\n",
      "Train: step:  60160, time: 0.277, loss: 2319.330566\n",
      "Train: step:  60170, time: 0.253, loss: 712.642700\n",
      "Train: step:  60180, time: 0.242, loss: 552.998230\n",
      "Train: step:  60190, time: 0.265, loss: 1585.751221\n",
      "Train: step:  60200, time: 0.282, loss: 708.142029\n",
      "Train: step:  60210, time: 0.255, loss: 2024.699097\n",
      "Train: step:  60220, time: 0.243, loss: 1118.666016\n",
      "Train: step:  60230, time: 0.242, loss: 1691.671021\n",
      "Train: step:  60240, time: 0.253, loss: 1951.840698\n",
      "Train: step:  60250, time: 0.245, loss: 1508.177979\n",
      "Train: step:  60260, time: 0.251, loss: 675.193420\n",
      "Train: step:  60270, time: 0.241, loss: 2505.144287\n",
      "Train: step:  60280, time: 0.243, loss: 4083.890381\n",
      "Train: step:  60290, time: 0.248, loss: 2781.458740\n",
      "Train: step:  60300, time: 0.238, loss: 2133.379639\n",
      "Train: step:  60310, time: 0.241, loss: 2168.888184\n",
      "Train: step:  60320, time: 0.240, loss: 2389.531738\n",
      "Train: step:  60330, time: 0.242, loss: 2088.719238\n",
      "Train: step:  60340, time: 0.246, loss: 2168.593018\n",
      "Train: step:  60350, time: 0.235, loss: 2069.331055\n",
      "Train: step:  60360, time: 0.239, loss: 1077.377197\n",
      "Train: step:  60370, time: 0.237, loss: 1176.359619\n",
      "Train: step:  60380, time: 0.263, loss: 1685.167847\n",
      "Train: step:  60390, time: 0.223, loss: 1990.578735\n",
      "Train: step:  60400, time: 0.229, loss: 933.430420\n",
      "Train: step:  60410, time: 0.221, loss: 1341.714966\n",
      "Train: step:  60420, time: 0.240, loss: 3405.249512\n",
      "Train: step:  60430, time: 0.229, loss: 591.218994\n",
      "Train: step:  60440, time: 0.231, loss: 2559.090332\n",
      "Train: step:  60450, time: 0.221, loss: 1909.860474\n",
      "Train: step:  60460, time: 0.237, loss: 1624.955933\n",
      "Train: step:  60470, time: 0.237, loss: 1386.782715\n",
      "Train: step:  60480, time: 0.225, loss: 467.471283\n",
      "Train: step:  60490, time: 0.230, loss: 2505.665771\n",
      "Train: step:  60500, time: 0.257, loss: 2438.381348\n",
      "Train: step:  60510, time: 0.226, loss: 1482.136353\n",
      "Train: step:  60520, time: 0.273, loss: 2111.173096\n",
      "Train: step:  60530, time: 0.230, loss: 464.994995\n",
      "Train: step:  60540, time: 0.238, loss: 3030.846924\n",
      "Train: step:  60550, time: 0.236, loss: 1033.690430\n",
      "Train: step:  60560, time: 0.241, loss: 2786.830322\n",
      "Train: step:  60570, time: 0.232, loss: 1213.911133\n",
      "Train: step:  60580, time: 0.242, loss: 2477.158203\n",
      "Train: step:  60590, time: 0.232, loss: 386.325104\n",
      "Train: step:  60600, time: 0.237, loss: 715.601257\n",
      "Train: step:  60610, time: 0.232, loss: 1822.394775\n",
      "Train: step:  60620, time: 0.238, loss: 1120.041748\n",
      "Train: step:  60630, time: 0.267, loss: 2850.817383\n",
      "Train: step:  60640, time: 0.255, loss: 677.407410\n",
      "Train: step:  60650, time: 0.239, loss: 661.048950\n",
      "Train: step:  60660, time: 0.231, loss: 735.635986\n",
      "Train: step:  60670, time: 0.244, loss: 2248.707275\n",
      "Train: step:  60680, time: 0.264, loss: 2387.019775\n",
      "Train: step:  60690, time: 0.244, loss: 400.852753\n",
      "Train: step:  60700, time: 0.251, loss: 2348.023926\n",
      "Train: step:  60710, time: 0.284, loss: 2897.349609\n",
      "Train: step:  60720, time: 0.251, loss: 294.603638\n",
      "Train: step:  60730, time: 0.241, loss: 2814.758789\n",
      "Train: step:  60740, time: 0.262, loss: 1224.512207\n",
      "Train: step:  60750, time: 0.249, loss: 2156.359863\n",
      "Train: step:  60760, time: 0.278, loss: 1835.935425\n",
      "Train: step:  60770, time: 0.257, loss: 1643.631592\n",
      "Train: step:  60780, time: 0.234, loss: 1427.594849\n",
      "Train: step:  60790, time: 0.251, loss: 2924.459717\n",
      "Train: step:  60800, time: 0.236, loss: 1051.857178\n",
      "Train: step:  60810, time: 0.232, loss: 2077.193359\n",
      "Train: step:  60820, time: 0.264, loss: 1217.207520\n",
      "Train: step:  60830, time: 0.263, loss: 2211.913086\n",
      "Train: step:  60840, time: 0.293, loss: 1003.812988\n",
      "Train: step:  60850, time: 0.239, loss: 2788.684326\n",
      "Train: step:  60860, time: 0.242, loss: 2485.390625\n",
      "Train: step:  60870, time: 0.250, loss: 2984.914795\n",
      "Train: step:  60880, time: 0.250, loss: 1520.806885\n",
      "Train: step:  60890, time: 0.245, loss: 2672.629150\n",
      "Train: step:  60900, time: 0.246, loss: 1643.898560\n",
      "Train: step:  60910, time: 0.242, loss: 792.255005\n",
      "Train: step:  60920, time: 0.235, loss: 2084.021484\n",
      "Train: step:  60930, time: 0.241, loss: 1266.030273\n",
      "Train: step:  60940, time: 0.239, loss: 1074.276245\n",
      "Train: step:  60950, time: 0.233, loss: 1900.913940\n",
      "Train: step:  60960, time: 0.250, loss: 2101.399170\n",
      "Train: step:  60970, time: 0.263, loss: 1445.509277\n",
      "Train: step:  60980, time: 0.237, loss: 1150.756470\n",
      "Train: step:  60990, time: 0.226, loss: 1545.383667\n",
      "Train: step:  61000, time: 0.227, loss: 1408.163574\n",
      "Train: step:  61010, time: 0.237, loss: 2466.457764\n",
      "Train: step:  61020, time: 0.229, loss: 2486.027100\n",
      "Train: step:  61030, time: 0.217, loss: 977.644958\n",
      "Train: step:  61040, time: 0.242, loss: 1588.742432\n",
      "Train: step:  61050, time: 0.228, loss: 1321.921631\n",
      "Train: step:  61060, time: 0.262, loss: 714.960449\n",
      "Train: step:  61070, time: 0.225, loss: 816.080017\n",
      "Train: step:  61080, time: 0.233, loss: 475.382355\n",
      "Train: step:  61090, time: 0.220, loss: 2370.993164\n",
      "Train: step:  61100, time: 0.281, loss: 1102.335327\n",
      "Train: step:  61110, time: 0.223, loss: 961.774475\n",
      "Train: step:  61120, time: 0.232, loss: 4850.506348\n",
      "Train: step:  61130, time: 0.230, loss: 1304.123413\n",
      "Train: step:  61140, time: 0.236, loss: 3528.439941\n",
      "Train: step:  61150, time: 0.225, loss: 2037.800049\n",
      "Train: step:  61160, time: 0.226, loss: 1053.156372\n",
      "Train: step:  61170, time: 0.237, loss: 2669.762451\n",
      "Train: step:  61180, time: 0.246, loss: 2221.426758\n",
      "Train: step:  61190, time: 0.224, loss: 2139.512939\n",
      "Train: step:  61200, time: 0.231, loss: 1663.419067\n",
      "Train: step:  61210, time: 0.220, loss: 1969.554321\n",
      "Train: step:  61220, time: 0.256, loss: 2096.415527\n",
      "Train: step:  61230, time: 0.224, loss: 183.472794\n",
      "Train: step:  61240, time: 0.253, loss: 952.708862\n",
      "Train: step:  61250, time: 0.230, loss: 3307.175293\n",
      "Train: step:  61260, time: 0.225, loss: 2277.400879\n",
      "Train: step:  61270, time: 0.226, loss: 4497.416504\n",
      "Train: step:  61280, time: 0.219, loss: 946.864380\n",
      "Train: step:  61290, time: 0.229, loss: 2594.827637\n",
      "Train: step:  61300, time: 0.228, loss: 1864.971436\n",
      "Train: step:  61310, time: 0.229, loss: 2714.177979\n",
      "Train: step:  61320, time: 0.267, loss: 2054.273682\n",
      "Train: step:  61330, time: 0.226, loss: 3313.666016\n",
      "Train: step:  61340, time: 0.231, loss: 2051.464844\n",
      "Train: step:  61350, time: 0.259, loss: 2591.034180\n",
      "Train: step:  61360, time: 0.237, loss: 1810.375488\n",
      "Train: step:  61370, time: 0.228, loss: 1930.504395\n",
      "Train: step:  61380, time: 0.224, loss: 646.125488\n",
      "Train: step:  61390, time: 0.247, loss: 465.049011\n",
      "Train: step:  61400, time: 0.259, loss: 2737.986572\n",
      "Train: step:  61410, time: 0.234, loss: 354.843506\n",
      "Train: step:  61420, time: 0.220, loss: 1930.122681\n",
      "Train: step:  61430, time: 0.212, loss: 706.032349\n",
      "Train: step:  61440, time: 0.223, loss: 3324.254883\n",
      "Train: step:  61450, time: 0.244, loss: 2016.404419\n",
      "Train: step:  61460, time: 0.240, loss: 3728.728271\n",
      "Train: step:  61470, time: 0.227, loss: 1590.742310\n",
      "Train: step:  61480, time: 0.222, loss: 2087.139404\n",
      "Train: step:  61490, time: 0.229, loss: 869.991089\n",
      "Train: step:  61500, time: 0.243, loss: 901.767151\n",
      "Train: step:  61510, time: 0.220, loss: 974.232849\n",
      "Train: step:  61520, time: 0.255, loss: 381.935059\n",
      "Train: step:  61530, time: 0.222, loss: 1042.052979\n",
      "Train: step:  61540, time: 0.224, loss: 889.474121\n",
      "Train: step:  61550, time: 0.214, loss: 3399.294189\n",
      "Train: step:  61560, time: 0.214, loss: 435.417114\n",
      "Train: step:  61570, time: 0.234, loss: 2247.592773\n",
      "Train: step:  61580, time: 0.229, loss: 3381.523682\n",
      "Train: step:  61590, time: 0.253, loss: 888.705383\n",
      "Train: step:  61600, time: 0.217, loss: 2702.913086\n",
      "Train: step:  61610, time: 0.255, loss: 1133.779175\n",
      "Train: step:  61620, time: 0.221, loss: 2420.306885\n",
      "Train: step:  61630, time: 0.233, loss: 1754.800049\n",
      "Train: step:  61640, time: 0.221, loss: 1263.501831\n",
      "Train: step:  61650, time: 0.216, loss: 330.090698\n",
      "Train: step:  61660, time: 0.258, loss: 683.654785\n",
      "Train: step:  61670, time: 0.230, loss: 1294.783569\n",
      "Train: step:  61680, time: 0.227, loss: 2382.036621\n",
      "Train: step:  61690, time: 0.226, loss: 1425.586792\n",
      "Train: step:  61700, time: 0.217, loss: 1147.279297\n",
      "Train: step:  61710, time: 0.226, loss: 1478.104126\n",
      "Train: step:  61720, time: 0.254, loss: 632.753601\n",
      "Train: step:  61730, time: 0.218, loss: 1904.499023\n",
      "Train: step:  61740, time: 0.248, loss: 2544.342285\n",
      "Train: step:  61750, time: 0.218, loss: 2009.989990\n",
      "Train: step:  61760, time: 0.224, loss: 4287.717773\n",
      "Train: step:  61770, time: 0.220, loss: 302.046722\n",
      "Train: step:  61780, time: 0.246, loss: 1575.429321\n",
      "Train: step:  61790, time: 0.222, loss: 2068.160889\n",
      "Train: step:  61800, time: 0.250, loss: 1066.564087\n",
      "Train: step:  61810, time: 0.216, loss: 455.939148\n",
      "Train: step:  61820, time: 0.229, loss: 2038.521729\n",
      "Train: step:  61830, time: 0.227, loss: 1152.136475\n",
      "Train: step:  61840, time: 0.233, loss: 1993.202637\n",
      "Train: step:  61850, time: 0.219, loss: 1344.185425\n",
      "Train: step:  61860, time: 0.229, loss: 1497.466919\n",
      "Train: step:  61870, time: 0.222, loss: 814.659729\n",
      "Train: step:  61880, time: 0.249, loss: 2818.386475\n",
      "Train: step:  61890, time: 0.227, loss: 1202.249146\n",
      "Train: step:  61900, time: 0.221, loss: 1919.504272\n",
      "Train: step:  61910, time: 0.225, loss: 3169.039551\n",
      "Train: step:  61920, time: 0.224, loss: 3105.180176\n",
      "Train: step:  61930, time: 0.259, loss: 1002.414001\n",
      "Train: step:  61940, time: 0.218, loss: 2182.062988\n",
      "Train: step:  61950, time: 0.222, loss: 2263.344727\n",
      "Train: step:  61960, time: 0.221, loss: 2466.418701\n",
      "Train: step:  61970, time: 0.220, loss: 1961.883789\n",
      "Train: step:  61980, time: 0.227, loss: 286.130310\n",
      "Train: step:  61990, time: 0.230, loss: 616.144653\n",
      "Train: step:  62000, time: 0.229, loss: 1700.923340\n",
      "Train: step:  62010, time: 0.263, loss: 1308.419189\n",
      "Train: step:  62020, time: 0.231, loss: 3745.335205\n",
      "Train: step:  62030, time: 0.267, loss: 3644.052734\n",
      "Train: step:  62040, time: 0.225, loss: 3057.543945\n",
      "Train: step:  62050, time: 0.217, loss: 2287.426270\n",
      "Train: step:  62060, time: 0.227, loss: 1862.492798\n",
      "Train: step:  62070, time: 0.215, loss: 1844.752197\n",
      "Train: step:  62080, time: 0.232, loss: 2811.908691\n",
      "Train: step:  62090, time: 0.252, loss: 837.859375\n",
      "Train: step:  62100, time: 0.230, loss: 2227.350098\n",
      "Train: step:  62110, time: 0.240, loss: 794.902100\n",
      "Train: step:  62120, time: 0.262, loss: 2836.065918\n",
      "Train: step:  62130, time: 0.220, loss: 1321.560913\n",
      "Train: step:  62140, time: 0.217, loss: 514.055176\n",
      "Train: step:  62150, time: 0.219, loss: 2657.891357\n",
      "Train: step:  62160, time: 0.229, loss: 3400.782471\n",
      "Train: step:  62170, time: 0.227, loss: 1421.621826\n",
      "Train: step:  62180, time: 0.216, loss: 2198.846924\n",
      "Train: step:  62190, time: 0.253, loss: 1303.423462\n",
      "Train: step:  62200, time: 0.218, loss: 3142.260254\n",
      "Train: step:  62210, time: 0.219, loss: 1843.603271\n",
      "Train: step:  62220, time: 0.274, loss: 2151.070312\n",
      "Train: step:  62230, time: 0.220, loss: 2543.424316\n",
      "Train: step:  62240, time: 0.220, loss: 1544.587280\n",
      "Train: step:  62250, time: 0.222, loss: 669.607239\n",
      "Train: step:  62260, time: 0.228, loss: 3730.399414\n",
      "Train: step:  62270, time: 0.220, loss: 955.159912\n",
      "Train: step:  62280, time: 0.222, loss: 2167.135010\n",
      "Train: step:  62290, time: 0.215, loss: 1493.398682\n",
      "Train: step:  62300, time: 0.219, loss: 1909.844116\n",
      "Train: step:  62310, time: 0.237, loss: 2920.827637\n",
      "Train: step:  62320, time: 0.225, loss: 5011.009766\n",
      "Train: step:  62330, time: 0.226, loss: 842.250854\n",
      "Train: step:  62340, time: 0.227, loss: 2273.410645\n",
      "Train: step:  62350, time: 0.221, loss: 1330.770996\n",
      "Train: step:  62360, time: 0.257, loss: 1448.928101\n",
      "Train: step:  62370, time: 0.225, loss: 1188.241943\n",
      "Train: step:  62380, time: 0.251, loss: 1022.169861\n",
      "Train: step:  62390, time: 0.217, loss: 2554.227295\n",
      "Train: step:  62400, time: 0.225, loss: 1038.865723\n",
      "Train: step:  62410, time: 0.258, loss: 1554.820801\n",
      "Train: step:  62420, time: 0.267, loss: 2728.162598\n",
      "Train: step:  62430, time: 0.257, loss: 539.099609\n",
      "Train: step:  62440, time: 0.246, loss: 2901.156494\n",
      "Train: step:  62450, time: 0.226, loss: 3388.452637\n",
      "Train: step:  62460, time: 0.263, loss: 2139.485107\n",
      "Train: step:  62470, time: 0.259, loss: 2394.238281\n",
      "Train: step:  62480, time: 0.226, loss: 1624.260986\n",
      "Train: step:  62490, time: 0.220, loss: 1500.127808\n",
      "Train: step:  62500, time: 0.236, loss: 356.377594\n",
      "Train: step:  62510, time: 0.221, loss: 2602.770020\n",
      "Train: step:  62520, time: 0.245, loss: 1647.047363\n",
      "Train: step:  62530, time: 0.249, loss: 775.705444\n",
      "Train: step:  62540, time: 0.264, loss: 2411.044678\n",
      "Train: step:  62550, time: 0.231, loss: 990.246399\n",
      "Train: step:  62560, time: 0.264, loss: 901.205566\n",
      "Train: step:  62570, time: 0.225, loss: 2856.038086\n",
      "Train: step:  62580, time: 0.245, loss: 2491.098877\n",
      "Train: step:  62590, time: 0.263, loss: 2060.312988\n",
      "Train: step:  62600, time: 0.219, loss: 2001.363037\n",
      "Train: step:  62610, time: 0.228, loss: 1684.472656\n",
      "Train: step:  62620, time: 0.229, loss: 279.508209\n",
      "Train: step:  62630, time: 0.259, loss: 1639.460938\n",
      "Train: step:  62640, time: 0.228, loss: 2945.275635\n",
      "Train: step:  62650, time: 0.216, loss: 1539.538574\n",
      "Train: step:  62660, time: 0.245, loss: 1331.344116\n",
      "Train: step:  62670, time: 0.221, loss: 2399.037354\n",
      "Train: step:  62680, time: 0.223, loss: 1455.739624\n",
      "Train: step:  62690, time: 0.221, loss: 2832.745605\n",
      "Train: step:  62700, time: 0.227, loss: 369.100250\n",
      "Train: step:  62710, time: 0.224, loss: 2034.130737\n",
      "Train: step:  62720, time: 0.247, loss: 2180.557617\n",
      "Train: step:  62730, time: 0.228, loss: 617.685425\n",
      "Train: step:  62740, time: 0.262, loss: 1947.551514\n",
      "Train: step:  62750, time: 0.241, loss: 1270.292847\n",
      "Train: step:  62760, time: 0.224, loss: 512.182434\n",
      "Train: step:  62770, time: 0.241, loss: 4163.939453\n",
      "Train: step:  62780, time: 0.223, loss: 1799.457397\n",
      "Train: step:  62790, time: 0.229, loss: 2218.979492\n",
      "Train: step:  62800, time: 0.222, loss: 2460.099854\n",
      "Train: step:  62810, time: 0.238, loss: 3761.839600\n",
      "Train: step:  62820, time: 0.262, loss: 826.806885\n",
      "Train: step:  62830, time: 0.223, loss: 1296.864502\n",
      "Train: step:  62840, time: 0.228, loss: 375.191315\n",
      "Train: step:  62850, time: 0.257, loss: 188.666428\n",
      "Train: step:  62860, time: 0.229, loss: 3350.905518\n",
      "Train: step:  62870, time: 0.254, loss: 1197.804688\n",
      "Train: step:  62880, time: 0.230, loss: 1581.579346\n",
      "Train: step:  62890, time: 0.226, loss: 1499.350220\n",
      "Train: step:  62900, time: 0.262, loss: 670.437500\n",
      "Train: step:  62910, time: 0.219, loss: 2416.186035\n",
      "Train: step:  62920, time: 0.222, loss: 1093.665283\n",
      "Train: step:  62930, time: 0.220, loss: 2150.485840\n",
      "Train: step:  62940, time: 0.219, loss: 2061.689453\n",
      "Train: step:  62950, time: 0.267, loss: 2536.388672\n",
      "Train: step:  62960, time: 0.223, loss: 2097.359131\n",
      "Train: step:  62970, time: 0.228, loss: 2406.323730\n",
      "Train: step:  62980, time: 0.225, loss: 2767.839111\n",
      "Train: step:  62990, time: 0.229, loss: 2466.696533\n",
      "Train: step:  63000, time: 0.247, loss: 1347.644653\n",
      "Train: step:  63010, time: 0.232, loss: 3088.264404\n",
      "Train: step:  63020, time: 0.251, loss: 1373.795654\n",
      "Train: step:  63030, time: 0.231, loss: 1453.698120\n",
      "Train: step:  63040, time: 0.229, loss: 1428.721924\n",
      "Train: step:  63050, time: 0.224, loss: 1782.909546\n",
      "Train: step:  63060, time: 0.226, loss: 1968.814575\n",
      "Train: step:  63070, time: 0.238, loss: 1640.025879\n",
      "Train: step:  63080, time: 0.222, loss: 1843.028198\n",
      "Train: step:  63090, time: 0.229, loss: 686.104797\n",
      "Train: step:  63100, time: 0.237, loss: 735.782043\n",
      "Train: step:  63110, time: 0.238, loss: 1221.212280\n",
      "Train: step:  63120, time: 0.235, loss: 2576.086914\n",
      "Train: step:  63130, time: 0.240, loss: 523.829651\n",
      "Train: step:  63140, time: 0.262, loss: 1998.332397\n",
      "Train: step:  63150, time: 0.226, loss: 1443.119751\n",
      "Train: step:  63160, time: 0.233, loss: 963.451538\n",
      "Train: step:  63170, time: 0.226, loss: 1148.799072\n",
      "Train: step:  63180, time: 0.248, loss: 1328.671021\n",
      "Train: step:  63190, time: 0.232, loss: 2837.509766\n",
      "Train: step:  63200, time: 0.228, loss: 2800.705811\n",
      "Train: step:  63210, time: 0.252, loss: 3274.286377\n",
      "Train: step:  63220, time: 0.232, loss: 2009.208496\n",
      "Train: step:  63230, time: 0.250, loss: 1962.721436\n",
      "Train: step:  63240, time: 0.224, loss: 277.162872\n",
      "Train: step:  63250, time: 0.214, loss: 1158.181396\n",
      "Train: step:  63260, time: 0.229, loss: 2577.065674\n",
      "Train: step:  63270, time: 0.230, loss: 2216.188965\n",
      "Train: step:  63280, time: 0.221, loss: 1009.150513\n",
      "Train: step:  63290, time: 0.251, loss: 1102.525146\n",
      "Train: step:  63300, time: 0.229, loss: 1286.014404\n",
      "Train: step:  63310, time: 0.233, loss: 503.522705\n",
      "Train: step:  63320, time: 0.226, loss: 1500.488281\n",
      "Train: step:  63330, time: 0.282, loss: 1943.758789\n",
      "Train: step:  63340, time: 0.225, loss: 613.789429\n",
      "Train: step:  63350, time: 0.254, loss: 1046.334839\n",
      "Train: step:  63360, time: 0.232, loss: 1165.523682\n",
      "Train: step:  63370, time: 0.243, loss: 1413.926392\n",
      "Train: step:  63380, time: 0.231, loss: 2078.794189\n",
      "Train: step:  63390, time: 0.233, loss: 2604.639893\n",
      "Train: step:  63400, time: 0.229, loss: 549.707764\n",
      "Train: step:  63410, time: 0.213, loss: 2035.035522\n",
      "Train: step:  63420, time: 0.218, loss: 1763.084717\n",
      "Train: step:  63430, time: 0.224, loss: 2103.101562\n",
      "Train: step:  63440, time: 0.236, loss: 698.172852\n",
      "Train: step:  63450, time: 0.234, loss: 2205.643311\n",
      "Train: step:  63460, time: 0.260, loss: 1317.517334\n",
      "Train: step:  63470, time: 0.228, loss: 1039.820801\n",
      "Train: step:  63480, time: 0.228, loss: 3139.657227\n",
      "Train: step:  63490, time: 0.253, loss: 704.610352\n",
      "Train: step:  63500, time: 0.228, loss: 1241.823120\n",
      "Train: step:  63510, time: 0.234, loss: 1503.082886\n",
      "Train: step:  63520, time: 0.223, loss: 544.220398\n",
      "Train: step:  63530, time: 0.236, loss: 2909.089600\n",
      "Train: step:  63540, time: 0.248, loss: 1639.607422\n",
      "Train: step:  63550, time: 0.232, loss: 1808.814575\n",
      "Train: step:  63560, time: 0.237, loss: 2401.429932\n",
      "Train: step:  63570, time: 0.231, loss: 870.878662\n",
      "Train: step:  63580, time: 0.243, loss: 1587.265503\n",
      "Train: step:  63590, time: 0.255, loss: 1255.615356\n",
      "Train: step:  63600, time: 0.257, loss: 839.417847\n",
      "Train: step:  63610, time: 0.226, loss: 1577.522949\n",
      "Train: step:  63620, time: 0.234, loss: 3876.241455\n",
      "Train: step:  63630, time: 0.237, loss: 1696.093506\n",
      "Train: step:  63640, time: 0.241, loss: 1138.221191\n",
      "Train: step:  63650, time: 0.239, loss: 834.340393\n",
      "Train: step:  63660, time: 0.260, loss: 356.006134\n",
      "Train: step:  63670, time: 0.237, loss: 1777.959106\n",
      "Train: step:  63680, time: 0.276, loss: 2345.429199\n",
      "Train: step:  63690, time: 0.249, loss: 1242.640869\n",
      "Train: step:  63700, time: 0.242, loss: 2136.400879\n",
      "Train: step:  63710, time: 0.261, loss: 1219.606079\n",
      "Train: step:  63720, time: 0.237, loss: 1475.293945\n",
      "Train: step:  63730, time: 0.241, loss: 2781.424805\n",
      "Train: step:  63740, time: 0.259, loss: 3274.920166\n",
      "Train: step:  63750, time: 0.276, loss: 1759.025024\n",
      "Train: step:  63760, time: 0.238, loss: 3118.455078\n",
      "Train: step:  63770, time: 0.230, loss: 3637.543213\n",
      "Train: step:  63780, time: 0.235, loss: 901.695801\n",
      "Train: step:  63790, time: 0.241, loss: 1968.094971\n",
      "Train: step:  63800, time: 0.248, loss: 3259.218994\n",
      "Train: step:  63810, time: 0.232, loss: 998.452393\n",
      "Train: step:  63820, time: 0.265, loss: 1991.894165\n",
      "Train: step:  63830, time: 0.241, loss: 2938.219971\n",
      "Train: step:  63840, time: 0.280, loss: 1637.571289\n",
      "Train: step:  63850, time: 0.241, loss: 2714.035400\n",
      "Train: step:  63860, time: 0.248, loss: 2489.656494\n",
      "Train: step:  63870, time: 0.235, loss: 669.495544\n",
      "Train: step:  63880, time: 0.245, loss: 1664.301392\n",
      "Train: step:  63890, time: 0.240, loss: 1115.924561\n",
      "Train: step:  63900, time: 0.264, loss: 1685.476196\n",
      "Train: step:  63910, time: 0.240, loss: 1337.161011\n",
      "Train: step:  63920, time: 0.242, loss: 1170.865601\n",
      "Train: step:  63930, time: 0.247, loss: 2320.580811\n",
      "Train: step:  63940, time: 0.251, loss: 2171.305908\n",
      "Train: step:  63950, time: 0.264, loss: 2448.101318\n",
      "Train: step:  63960, time: 0.239, loss: 1135.624512\n",
      "Train: step:  63970, time: 0.270, loss: 2609.665771\n",
      "Train: step:  63980, time: 0.236, loss: 492.726624\n",
      "Train: step:  63990, time: 0.285, loss: 525.184448\n",
      "Train: step:  64000, time: 0.258, loss: 1598.289673\n",
      "Train: step:  64010, time: 0.270, loss: 2275.168213\n",
      "Train: step:  64020, time: 0.245, loss: 1849.028198\n",
      "Train: step:  64030, time: 0.248, loss: 2396.510010\n",
      "Train: step:  64040, time: 0.238, loss: 2926.821533\n",
      "Train: step:  64050, time: 0.233, loss: 2263.075195\n",
      "Train: step:  64060, time: 0.239, loss: 1904.422485\n",
      "Train: step:  64070, time: 0.242, loss: 1094.940918\n",
      "Train: step:  64080, time: 0.250, loss: 2450.625488\n",
      "Train: step:  64090, time: 0.242, loss: 1285.634399\n",
      "Train: step:  64100, time: 0.241, loss: 2551.942871\n",
      "Train: step:  64110, time: 0.253, loss: 2347.445068\n",
      "Train: step:  64120, time: 0.249, loss: 446.702637\n",
      "Train: step:  64130, time: 0.253, loss: 929.618896\n",
      "Train: step:  64140, time: 0.246, loss: 1398.862183\n",
      "Train: step:  64150, time: 0.237, loss: 1695.304077\n",
      "Train: step:  64160, time: 0.261, loss: 2644.266602\n",
      "Train: step:  64170, time: 0.243, loss: 3112.435791\n",
      "Train: step:  64180, time: 0.246, loss: 1471.601318\n",
      "Train: step:  64190, time: 0.239, loss: 1775.199829\n",
      "Train: step:  64200, time: 0.280, loss: 2692.135010\n",
      "Train: step:  64210, time: 0.260, loss: 2274.160400\n",
      "Train: step:  64220, time: 0.242, loss: 204.597855\n",
      "Train: step:  64230, time: 0.235, loss: 1548.054443\n",
      "Train: step:  64240, time: 0.246, loss: 1346.658813\n",
      "Train: step:  64250, time: 0.241, loss: 1901.570068\n",
      "Train: step:  64260, time: 0.245, loss: 1332.743530\n",
      "Train: step:  64270, time: 0.240, loss: 2197.892334\n",
      "Train: step:  64280, time: 0.257, loss: 1035.772705\n",
      "Train: step:  64290, time: 0.244, loss: 2392.410889\n",
      "Train: step:  64300, time: 0.245, loss: 1513.974487\n",
      "Train: step:  64310, time: 0.240, loss: 904.410522\n",
      "Train: step:  64320, time: 0.238, loss: 2783.043701\n",
      "Train: step:  64330, time: 0.239, loss: 2278.811768\n",
      "Train: step:  64340, time: 0.255, loss: 1336.017578\n",
      "Train: step:  64350, time: 0.245, loss: 1231.897949\n",
      "Train: step:  64360, time: 0.277, loss: 1651.513062\n",
      "Train: step:  64370, time: 0.277, loss: 1171.068848\n",
      "Train: step:  64380, time: 0.270, loss: 2230.831787\n",
      "Train: step:  64390, time: 0.239, loss: 766.608459\n",
      "Train: step:  64400, time: 0.238, loss: 3297.684814\n",
      "Train: step:  64410, time: 0.228, loss: 2175.277588\n",
      "Train: step:  64420, time: 0.239, loss: 1197.348145\n",
      "Train: step:  64430, time: 0.244, loss: 1289.038452\n",
      "Train: step:  64440, time: 0.267, loss: 3795.454834\n",
      "Train: step:  64450, time: 0.274, loss: 1603.638184\n",
      "Train: step:  64460, time: 0.242, loss: 2283.117676\n",
      "Train: step:  64470, time: 0.279, loss: 2489.580811\n",
      "Train: step:  64480, time: 0.246, loss: 2622.062744\n",
      "Train: step:  64490, time: 0.265, loss: 1026.234375\n",
      "Train: step:  64500, time: 0.239, loss: 2149.628174\n",
      "Train: step:  64510, time: 0.237, loss: 496.548279\n",
      "Train: step:  64520, time: 0.242, loss: 1655.856079\n",
      "Train: step:  64530, time: 0.250, loss: 2629.533203\n",
      "Train: step:  64540, time: 0.247, loss: 1941.598877\n",
      "Train: step:  64550, time: 0.276, loss: 702.759583\n",
      "Train: step:  64560, time: 0.247, loss: 2490.614258\n",
      "Train: step:  64570, time: 0.244, loss: 1993.281494\n",
      "Train: step:  64580, time: 0.240, loss: 1858.263306\n",
      "Train: step:  64590, time: 0.247, loss: 1978.678833\n",
      "Train: step:  64600, time: 0.244, loss: 1567.544312\n",
      "Train: step:  64610, time: 0.243, loss: 4114.350586\n",
      "Train: step:  64620, time: 0.246, loss: 3671.248535\n",
      "Train: step:  64630, time: 0.234, loss: 595.336853\n",
      "Train: step:  64640, time: 0.237, loss: 1590.655762\n",
      "Train: step:  64650, time: 0.245, loss: 572.343079\n",
      "Train: step:  64660, time: 0.244, loss: 2738.379639\n",
      "Train: step:  64670, time: 0.242, loss: 1807.150513\n",
      "Train: step:  64680, time: 0.239, loss: 3121.793457\n",
      "Train: step:  64690, time: 0.270, loss: 1882.123047\n",
      "Train: step:  64700, time: 0.251, loss: 2018.651855\n",
      "Train: step:  64710, time: 0.247, loss: 1029.638428\n",
      "Train: step:  64720, time: 0.247, loss: 2471.926270\n",
      "Train: step:  64730, time: 0.235, loss: 1257.827637\n",
      "Train: step:  64740, time: 0.241, loss: 252.457748\n",
      "Train: step:  64750, time: 0.266, loss: 2207.089844\n",
      "Train: step:  64760, time: 0.280, loss: 1851.312012\n",
      "Train: step:  64770, time: 0.266, loss: 508.819031\n",
      "Train: step:  64780, time: 0.242, loss: 2555.387451\n",
      "Train: step:  64790, time: 0.232, loss: 1441.620239\n",
      "Train: step:  64800, time: 0.270, loss: 1789.358276\n",
      "Train: step:  64810, time: 0.238, loss: 3361.306152\n",
      "Train: step:  64820, time: 0.275, loss: 991.335876\n",
      "Train: step:  64830, time: 0.246, loss: 1264.992432\n",
      "Train: step:  64840, time: 0.236, loss: 2838.207275\n",
      "Train: step:  64850, time: 0.238, loss: 2816.968506\n",
      "Train: step:  64860, time: 0.246, loss: 378.140228\n",
      "Train: step:  64870, time: 0.266, loss: 851.994019\n",
      "Train: step:  64880, time: 0.283, loss: 1715.685547\n",
      "Train: step:  64890, time: 0.295, loss: 1229.817749\n",
      "Train: step:  64900, time: 0.239, loss: 1201.119141\n",
      "Train: step:  64910, time: 0.265, loss: 2401.068359\n",
      "Train: step:  64920, time: 0.282, loss: 3084.327148\n",
      "Train: step:  64930, time: 0.279, loss: 799.530884\n",
      "Train: step:  64940, time: 0.254, loss: 3431.913574\n",
      "Train: step:  64950, time: 0.244, loss: 3144.185547\n",
      "Train: step:  64960, time: 0.274, loss: 967.981445\n",
      "Train: step:  64970, time: 0.259, loss: 1338.374023\n",
      "Train: step:  64980, time: 0.262, loss: 1144.940552\n",
      "Train: step:  64990, time: 0.236, loss: 2615.187256\n",
      "Train: step:  65000, time: 0.298, loss: 2098.374268\n",
      "Train: step:  65010, time: 0.241, loss: 2090.130127\n",
      "Train: step:  65020, time: 0.239, loss: 1331.851685\n",
      "Train: step:  65030, time: 0.276, loss: 2002.057007\n",
      "Train: step:  65040, time: 0.245, loss: 1056.475342\n",
      "Train: step:  65050, time: 0.264, loss: 3756.876221\n",
      "Train: step:  65060, time: 0.247, loss: 696.061646\n",
      "Train: step:  65070, time: 0.247, loss: 1002.072571\n",
      "Train: step:  65080, time: 0.258, loss: 1688.835083\n",
      "Train: step:  65090, time: 0.237, loss: 1578.611572\n",
      "Train: step:  65100, time: 0.273, loss: 1253.188354\n",
      "Train: step:  65110, time: 0.238, loss: 3401.626221\n",
      "Train: step:  65120, time: 0.243, loss: 1973.139526\n",
      "Train: step:  65130, time: 0.248, loss: 5878.000488\n",
      "Train: step:  65140, time: 0.273, loss: 1988.187012\n",
      "Train: step:  65150, time: 0.243, loss: 621.786011\n",
      "Train: step:  65160, time: 0.242, loss: 1185.176514\n",
      "Train: step:  65170, time: 0.244, loss: 2180.979980\n",
      "Train: step:  65180, time: 0.243, loss: 2366.240234\n",
      "Train: step:  65190, time: 0.237, loss: 2030.787842\n",
      "Train: step:  65200, time: 0.240, loss: 965.079590\n",
      "Train: step:  65210, time: 0.249, loss: 2064.181152\n",
      "Train: step:  65220, time: 0.243, loss: 2210.121826\n",
      "Train: step:  65230, time: 0.244, loss: 981.489075\n",
      "Train: step:  65240, time: 0.271, loss: 2623.589844\n",
      "Train: step:  65250, time: 0.268, loss: 804.462463\n",
      "Train: step:  65260, time: 0.241, loss: 685.199646\n",
      "Train: step:  65270, time: 0.242, loss: 1370.130737\n",
      "Train: step:  65280, time: 0.243, loss: 3556.341553\n",
      "Train: step:  65290, time: 0.241, loss: 1834.759888\n",
      "Train: step:  65300, time: 0.246, loss: 955.542297\n",
      "Train: step:  65310, time: 0.236, loss: 1683.879883\n",
      "Train: step:  65320, time: 0.245, loss: 466.304626\n",
      "Train: step:  65330, time: 0.235, loss: 1604.124634\n",
      "Train: step:  65340, time: 0.244, loss: 411.473541\n",
      "Train: step:  65350, time: 0.271, loss: 1695.244873\n",
      "Train: step:  65360, time: 0.250, loss: 1266.692749\n",
      "Train: step:  65370, time: 0.239, loss: 2496.025146\n",
      "Train: step:  65380, time: 0.265, loss: 1211.902954\n",
      "Train: step:  65390, time: 0.241, loss: 2941.470947\n",
      "Train: step:  65400, time: 0.244, loss: 3295.450439\n",
      "Train: step:  65410, time: 0.266, loss: 1078.335693\n",
      "Train: step:  65420, time: 0.268, loss: 309.617737\n",
      "Train: step:  65430, time: 0.235, loss: 1330.957397\n",
      "Train: step:  65440, time: 0.251, loss: 1338.213013\n",
      "Train: step:  65450, time: 0.268, loss: 2840.744385\n",
      "Train: step:  65460, time: 0.247, loss: 1736.653198\n",
      "Train: step:  65470, time: 0.241, loss: 1775.479126\n",
      "Train: step:  65480, time: 0.237, loss: 259.309143\n",
      "Train: step:  65490, time: 0.242, loss: 541.003967\n",
      "Train: step:  65500, time: 0.235, loss: 878.272034\n",
      "Train: step:  65510, time: 0.236, loss: 2465.193115\n",
      "Train: step:  65520, time: 0.234, loss: 1019.534851\n",
      "Train: step:  65530, time: 0.242, loss: 1368.781494\n",
      "Train: step:  65540, time: 0.247, loss: 2212.811768\n",
      "Train: step:  65550, time: 0.270, loss: 1661.881104\n",
      "Train: step:  65560, time: 0.244, loss: 2246.625000\n",
      "Train: step:  65570, time: 0.243, loss: 916.095825\n",
      "Train: step:  65580, time: 0.266, loss: 1960.110840\n",
      "Train: step:  65590, time: 0.237, loss: 1385.426880\n",
      "Train: step:  65600, time: 0.240, loss: 1950.309570\n",
      "Train: step:  65610, time: 0.243, loss: 2968.734863\n",
      "Train: step:  65620, time: 0.274, loss: 1346.249756\n",
      "Train: step:  65630, time: 0.249, loss: 3299.795410\n",
      "Train: step:  65640, time: 0.237, loss: 2105.203125\n",
      "Train: step:  65650, time: 0.234, loss: 2783.218262\n",
      "Train: step:  65660, time: 0.239, loss: 1264.845703\n",
      "Train: step:  65670, time: 0.237, loss: 2316.643311\n",
      "Train: step:  65680, time: 0.274, loss: 873.639404\n",
      "Train: step:  65690, time: 0.247, loss: 429.584869\n",
      "Train: step:  65700, time: 0.254, loss: 2094.444092\n",
      "Train: step:  65710, time: 0.244, loss: 1182.583008\n",
      "Train: step:  65720, time: 0.245, loss: 3224.606689\n",
      "Train: step:  65730, time: 0.238, loss: 1275.746948\n",
      "Train: step:  65740, time: 0.265, loss: 2005.399170\n",
      "Train: step:  65750, time: 0.234, loss: 2944.934570\n",
      "Train: step:  65760, time: 0.240, loss: 331.592224\n",
      "Train: step:  65770, time: 0.327, loss: 1663.477905\n",
      "Train: step:  65780, time: 0.249, loss: 2735.730713\n",
      "Train: step:  65790, time: 0.231, loss: 3373.513428\n",
      "Train: step:  65800, time: 0.246, loss: 2280.446289\n",
      "Train: step:  65810, time: 0.231, loss: 1417.387085\n",
      "Train: step:  65820, time: 0.244, loss: 2829.146729\n",
      "Train: step:  65830, time: 0.242, loss: 3013.855225\n",
      "Train: step:  65840, time: 0.240, loss: 683.438782\n",
      "Train: step:  65850, time: 0.262, loss: 4027.313965\n",
      "Train: step:  65860, time: 0.261, loss: 1377.440430\n",
      "Train: step:  65870, time: 0.239, loss: 562.723633\n",
      "Train: step:  65880, time: 0.231, loss: 4086.315918\n",
      "Train: step:  65890, time: 0.247, loss: 1075.862915\n",
      "Train: step:  65900, time: 0.272, loss: 2045.760986\n",
      "Train: step:  65910, time: 0.237, loss: 2037.176514\n",
      "Train: step:  65920, time: 0.238, loss: 1532.120361\n",
      "Train: step:  65930, time: 0.269, loss: 1350.152466\n",
      "Train: step:  65940, time: 0.232, loss: 932.915771\n",
      "Train: step:  65950, time: 0.240, loss: 1123.664917\n",
      "Train: step:  65960, time: 0.235, loss: 2332.788818\n",
      "Train: step:  65970, time: 0.233, loss: 1494.717163\n",
      "Train: step:  65980, time: 0.290, loss: 1655.910889\n",
      "Train: step:  65990, time: 0.242, loss: 2356.220459\n",
      "Train: step:  66000, time: 0.264, loss: 1636.848267\n",
      "Train: step:  66010, time: 0.277, loss: 2485.389160\n",
      "Train: step:  66020, time: 0.238, loss: 2198.522705\n",
      "Train: step:  66030, time: 0.261, loss: 2956.583008\n",
      "Train: step:  66040, time: 0.240, loss: 1823.466553\n",
      "Train: step:  66050, time: 0.260, loss: 2324.205322\n",
      "Train: step:  66060, time: 0.265, loss: 1229.924927\n",
      "Train: step:  66070, time: 0.241, loss: 2245.963379\n",
      "Train: step:  66080, time: 0.240, loss: 1741.601318\n",
      "Train: step:  66090, time: 0.242, loss: 2969.142334\n",
      "Train: step:  66100, time: 0.267, loss: 1402.513550\n",
      "Train: step:  66110, time: 0.260, loss: 1490.027222\n",
      "Train: step:  66120, time: 0.247, loss: 2969.177246\n",
      "Train: step:  66130, time: 0.241, loss: 2175.045898\n",
      "Train: step:  66140, time: 0.243, loss: 3230.348877\n",
      "Train: step:  66150, time: 0.261, loss: 2843.323486\n",
      "Train: step:  66160, time: 0.244, loss: 2574.960449\n",
      "Train: step:  66170, time: 0.266, loss: 2277.003662\n",
      "Train: step:  66180, time: 0.273, loss: 977.060913\n",
      "Train: step:  66190, time: 0.251, loss: 1234.917480\n",
      "Train: step:  66200, time: 0.250, loss: 318.740173\n",
      "Train: step:  66210, time: 0.267, loss: 1074.978394\n",
      "Train: step:  66220, time: 0.270, loss: 2219.924072\n",
      "Train: step:  66230, time: 0.250, loss: 1741.883423\n",
      "Train: step:  66240, time: 0.240, loss: 1245.132935\n",
      "Train: step:  66250, time: 0.268, loss: 886.398804\n",
      "Train: step:  66260, time: 0.233, loss: 408.497375\n",
      "Train: step:  66270, time: 0.277, loss: 1271.900146\n",
      "Train: step:  66280, time: 0.237, loss: 1324.921265\n",
      "Train: step:  66290, time: 0.237, loss: 1444.708496\n",
      "Train: step:  66300, time: 0.239, loss: 2567.095703\n",
      "Train: step:  66310, time: 0.251, loss: 2649.277588\n",
      "Train: step:  66320, time: 0.259, loss: 2112.346680\n",
      "Train: step:  66330, time: 0.238, loss: 1240.086304\n",
      "Train: step:  66340, time: 0.279, loss: 1626.756836\n",
      "Train: step:  66350, time: 0.240, loss: 992.385620\n",
      "Train: step:  66360, time: 0.264, loss: 974.469666\n",
      "Train: step:  66370, time: 0.228, loss: 2136.912109\n",
      "Train: step:  66380, time: 0.236, loss: 2651.922607\n",
      "Train: step:  66390, time: 0.239, loss: 686.428528\n",
      "Train: step:  66400, time: 0.244, loss: 2114.079102\n",
      "Train: step:  66410, time: 0.238, loss: 868.675659\n",
      "Train: step:  66420, time: 0.233, loss: 893.119080\n",
      "Train: step:  66430, time: 0.237, loss: 927.220642\n",
      "Train: step:  66440, time: 0.234, loss: 3026.247803\n",
      "Train: step:  66450, time: 0.241, loss: 2225.280029\n",
      "Train: step:  66460, time: 0.236, loss: 989.359619\n",
      "Train: step:  66470, time: 0.243, loss: 3404.192383\n",
      "Train: step:  66480, time: 0.239, loss: 2921.925781\n",
      "Train: step:  66490, time: 0.240, loss: 1681.293823\n",
      "Train: step:  66500, time: 0.258, loss: 3752.716797\n",
      "Train: step:  66510, time: 0.236, loss: 2063.054932\n",
      "Train: step:  66520, time: 0.240, loss: 1195.478394\n",
      "Train: step:  66530, time: 0.260, loss: 4296.317871\n",
      "Train: step:  66540, time: 0.248, loss: 1251.303345\n",
      "Train: step:  66550, time: 0.278, loss: 1034.626221\n",
      "Train: step:  66560, time: 0.238, loss: 1120.995483\n",
      "Train: step:  66570, time: 0.238, loss: 2575.350342\n",
      "Train: step:  66580, time: 0.244, loss: 3131.621094\n",
      "Train: step:  66590, time: 0.266, loss: 1413.226807\n",
      "Train: step:  66600, time: 0.270, loss: 1570.208496\n",
      "Train: step:  66610, time: 0.238, loss: 2597.043945\n",
      "Train: step:  66620, time: 0.249, loss: 1914.211670\n",
      "Train: step:  66630, time: 0.267, loss: 995.815857\n",
      "Train: step:  66640, time: 0.236, loss: 2384.935303\n",
      "Train: step:  66650, time: 0.234, loss: 1567.846436\n",
      "Train: step:  66660, time: 0.244, loss: 3092.041504\n",
      "Train: step:  66670, time: 0.240, loss: 366.746521\n",
      "Train: step:  66680, time: 0.243, loss: 2696.416992\n",
      "Train: step:  66690, time: 0.261, loss: 2704.289062\n",
      "Train: step:  66700, time: 0.239, loss: 2030.672485\n",
      "Train: step:  66710, time: 0.240, loss: 2244.459473\n",
      "Train: step:  66720, time: 0.243, loss: 1806.904419\n",
      "Train: step:  66730, time: 0.264, loss: 2336.836914\n",
      "Train: step:  66740, time: 0.240, loss: 735.213989\n",
      "Train: step:  66750, time: 0.241, loss: 1331.911987\n",
      "Train: step:  66760, time: 0.243, loss: 359.129303\n",
      "Train: step:  66770, time: 0.240, loss: 4102.832031\n",
      "Train: step:  66780, time: 0.244, loss: 906.254150\n",
      "Train: step:  66790, time: 0.258, loss: 581.071289\n",
      "Train: step:  66800, time: 0.243, loss: 649.186707\n",
      "Train: step:  66810, time: 0.250, loss: 2445.347900\n",
      "Train: step:  66820, time: 0.247, loss: 1931.261108\n",
      "Train: step:  66830, time: 0.237, loss: 885.371765\n",
      "Train: step:  66840, time: 0.245, loss: 1515.061035\n",
      "Train: step:  66850, time: 0.274, loss: 1215.681274\n",
      "Train: step:  66860, time: 0.269, loss: 1077.210449\n",
      "Train: step:  66870, time: 0.250, loss: 1336.679565\n",
      "Train: step:  66880, time: 0.248, loss: 560.075073\n",
      "Train: step:  66890, time: 0.244, loss: 1443.617676\n",
      "Train: step:  66900, time: 0.273, loss: 765.109924\n",
      "Train: step:  66910, time: 0.239, loss: 1135.347046\n",
      "Train: step:  66920, time: 0.237, loss: 1829.606934\n",
      "Train: step:  66930, time: 0.243, loss: 705.810242\n",
      "Train: step:  66940, time: 0.240, loss: 820.667603\n",
      "Train: step:  66950, time: 0.250, loss: 1489.703125\n",
      "Train: step:  66960, time: 0.240, loss: 2152.128906\n",
      "Train: step:  66970, time: 0.246, loss: 2135.839111\n",
      "Train: step:  66980, time: 0.240, loss: 2757.340332\n",
      "Train: step:  66990, time: 0.239, loss: 1238.361328\n",
      "Train: step:  67000, time: 0.235, loss: 1374.596680\n",
      "Train: step:  67010, time: 0.264, loss: 1752.254517\n",
      "Train: step:  67020, time: 0.239, loss: 1732.860229\n",
      "Train: step:  67030, time: 0.236, loss: 2325.744141\n",
      "Train: step:  67040, time: 0.236, loss: 1210.871338\n",
      "Train: step:  67050, time: 0.280, loss: 1666.425781\n",
      "Train: step:  67060, time: 0.258, loss: 1163.377441\n",
      "Train: step:  67070, time: 0.244, loss: 3500.726074\n",
      "Train: step:  67080, time: 0.265, loss: 3000.004150\n",
      "Train: step:  67090, time: 0.242, loss: 3256.441895\n",
      "Train: step:  67100, time: 0.240, loss: 210.158676\n",
      "Train: step:  67110, time: 0.253, loss: 1740.214233\n",
      "Train: step:  67120, time: 0.240, loss: 412.392700\n",
      "Train: step:  67130, time: 0.244, loss: 3095.196289\n",
      "Train: step:  67140, time: 0.259, loss: 392.070099\n",
      "Train: step:  67150, time: 0.249, loss: 2828.039551\n",
      "Train: step:  67160, time: 0.239, loss: 1267.297241\n",
      "Train: step:  67170, time: 0.253, loss: 2344.947998\n",
      "Train: step:  67180, time: 0.244, loss: 1333.404663\n",
      "Train: step:  67190, time: 0.245, loss: 1441.166748\n",
      "Train: step:  67200, time: 0.239, loss: 2367.642334\n",
      "Train: step:  67210, time: 0.243, loss: 1724.893555\n",
      "Train: step:  67220, time: 0.244, loss: 1310.962402\n",
      "Train: step:  67230, time: 0.246, loss: 2771.573486\n",
      "Train: step:  67240, time: 0.242, loss: 2943.589600\n",
      "Train: step:  67250, time: 0.239, loss: 3044.005859\n",
      "Train: step:  67260, time: 0.250, loss: 2299.834961\n",
      "Train: step:  67270, time: 0.241, loss: 2597.067139\n",
      "Train: step:  67280, time: 0.230, loss: 966.398743\n",
      "Train: step:  67290, time: 0.285, loss: 632.973022\n",
      "Train: step:  67300, time: 0.263, loss: 3094.876953\n",
      "Train: step:  67310, time: 0.244, loss: 1832.052002\n",
      "Train: step:  67320, time: 0.235, loss: 2979.222656\n",
      "Train: step:  67330, time: 0.240, loss: 2452.322510\n",
      "Train: step:  67340, time: 0.233, loss: 1115.781250\n",
      "Train: step:  67350, time: 0.242, loss: 2677.497559\n",
      "Train: step:  67360, time: 0.237, loss: 568.207947\n",
      "Train: step:  67370, time: 0.234, loss: 1973.501343\n",
      "Train: step:  67380, time: 0.261, loss: 2529.071777\n",
      "Train: step:  67390, time: 0.238, loss: 746.583435\n",
      "Train: step:  67400, time: 0.253, loss: 2073.834473\n",
      "Train: step:  67410, time: 0.232, loss: 2361.798340\n",
      "Train: step:  67420, time: 0.230, loss: 727.138428\n",
      "Train: step:  67430, time: 0.232, loss: 1569.251221\n",
      "Train: step:  67440, time: 0.230, loss: 2125.015625\n",
      "Train: step:  67450, time: 0.253, loss: 1740.361206\n",
      "Train: step:  67460, time: 0.231, loss: 3028.309814\n",
      "Train: step:  67470, time: 0.254, loss: 3098.397217\n",
      "Train: step:  67480, time: 0.258, loss: 1240.339722\n",
      "Train: step:  67490, time: 0.228, loss: 1613.609741\n",
      "Train: step:  67500, time: 0.230, loss: 808.718872\n",
      "Train: step:  67510, time: 0.273, loss: 2168.986084\n",
      "Train: step:  67520, time: 0.230, loss: 743.911072\n",
      "Train: step:  67530, time: 0.240, loss: 3124.270996\n",
      "Train: step:  67540, time: 0.259, loss: 1245.275146\n",
      "Train: step:  67550, time: 0.239, loss: 857.330017\n",
      "Train: step:  67560, time: 0.229, loss: 3032.923096\n",
      "Train: step:  67570, time: 0.235, loss: 1725.442871\n",
      "Train: step:  67580, time: 0.230, loss: 1965.570435\n",
      "Train: step:  67590, time: 0.235, loss: 2398.542480\n",
      "Train: step:  67600, time: 0.240, loss: 1621.631958\n",
      "Train: step:  67610, time: 0.236, loss: 1021.197205\n",
      "Train: step:  67620, time: 0.259, loss: 2150.316406\n",
      "Train: step:  67630, time: 0.236, loss: 1967.501831\n",
      "Train: step:  67640, time: 0.239, loss: 717.526794\n",
      "Train: step:  67650, time: 0.235, loss: 2743.465820\n",
      "Train: step:  67660, time: 0.253, loss: 3311.782227\n",
      "Train: step:  67670, time: 0.252, loss: 2030.842041\n",
      "Train: step:  67680, time: 0.281, loss: 561.548584\n",
      "Train: step:  67690, time: 0.256, loss: 1485.286255\n",
      "Train: step:  67700, time: 0.271, loss: 610.680847\n",
      "Train: step:  67710, time: 0.235, loss: 1337.661987\n",
      "Train: step:  67720, time: 0.233, loss: 3614.941406\n",
      "Train: step:  67730, time: 0.238, loss: 935.602295\n",
      "Train: step:  67740, time: 0.256, loss: 478.187439\n",
      "Train: step:  67750, time: 0.229, loss: 2000.144897\n",
      "Train: step:  67760, time: 0.244, loss: 1750.838501\n",
      "Train: step:  67770, time: 0.236, loss: 2345.499756\n",
      "Train: step:  67780, time: 0.261, loss: 2075.728760\n",
      "Train: step:  67790, time: 0.243, loss: 2002.301880\n",
      "Train: step:  67800, time: 0.246, loss: 1991.404907\n",
      "Train: step:  67810, time: 0.229, loss: 831.873413\n",
      "Train: step:  67820, time: 0.233, loss: 1548.299072\n",
      "Train: step:  67830, time: 0.266, loss: 484.727539\n",
      "Train: step:  67840, time: 0.242, loss: 1788.124146\n",
      "Train: step:  67850, time: 0.228, loss: 2464.738770\n",
      "Train: step:  67860, time: 0.259, loss: 658.467651\n",
      "Train: step:  67870, time: 0.232, loss: 571.936523\n",
      "Train: step:  67880, time: 0.239, loss: 2052.677490\n",
      "Train: step:  67890, time: 0.240, loss: 1560.652832\n",
      "Train: step:  67900, time: 0.232, loss: 1620.578247\n",
      "Train: step:  67910, time: 0.253, loss: 1934.425049\n",
      "Train: step:  67920, time: 0.264, loss: 2743.966309\n",
      "Train: step:  67930, time: 0.231, loss: 1904.503418\n",
      "Train: step:  67940, time: 0.243, loss: 2919.793945\n",
      "Train: step:  67950, time: 0.229, loss: 942.126282\n",
      "Train: step:  67960, time: 0.231, loss: 3043.553955\n",
      "Train: step:  67970, time: 0.232, loss: 197.473831\n",
      "Train: step:  67980, time: 0.239, loss: 2190.823486\n",
      "Train: step:  67990, time: 0.242, loss: 2795.210693\n",
      "Train: step:  68000, time: 0.256, loss: 2968.338379\n",
      "Train: step:  68010, time: 0.228, loss: 1631.557617\n",
      "Train: step:  68020, time: 0.229, loss: 1449.862915\n",
      "Train: step:  68030, time: 0.233, loss: 2088.603271\n",
      "Train: step:  68040, time: 0.225, loss: 3313.073486\n",
      "Train: step:  68050, time: 0.231, loss: 1745.695923\n",
      "Train: step:  68060, time: 0.237, loss: 2644.810303\n",
      "Train: step:  68070, time: 0.259, loss: 989.407471\n",
      "Train: step:  68080, time: 0.236, loss: 1222.824219\n",
      "Train: step:  68090, time: 0.238, loss: 2369.807373\n",
      "Train: step:  68100, time: 0.271, loss: 348.090790\n",
      "Train: step:  68110, time: 0.242, loss: 1988.289917\n",
      "Train: step:  68120, time: 0.228, loss: 1332.502319\n",
      "Train: step:  68130, time: 0.232, loss: 2246.558105\n",
      "Train: step:  68140, time: 0.232, loss: 2024.301514\n",
      "Train: step:  68150, time: 0.247, loss: 2178.872314\n",
      "Train: step:  68160, time: 0.236, loss: 925.536194\n",
      "Train: step:  68170, time: 0.262, loss: 1327.839111\n",
      "Train: step:  68180, time: 0.274, loss: 422.543793\n",
      "Train: step:  68190, time: 0.246, loss: 853.620056\n",
      "Train: step:  68200, time: 0.237, loss: 1592.510864\n",
      "Train: step:  68210, time: 0.245, loss: 1385.569092\n",
      "Train: step:  68220, time: 0.234, loss: 3215.848633\n",
      "Train: step:  68230, time: 0.239, loss: 2397.099609\n",
      "Train: step:  68240, time: 0.240, loss: 1994.828613\n",
      "Train: step:  68250, time: 0.232, loss: 809.633545\n",
      "Train: step:  68260, time: 0.265, loss: 1690.977051\n",
      "Train: step:  68270, time: 0.236, loss: 1707.915771\n",
      "Train: step:  68280, time: 0.225, loss: 2121.464844\n",
      "Train: step:  68290, time: 0.233, loss: 412.547516\n",
      "Train: step:  68300, time: 0.232, loss: 2884.542969\n",
      "Train: step:  68310, time: 0.233, loss: 1266.307495\n",
      "Train: step:  68320, time: 0.262, loss: 1002.373535\n",
      "Train: step:  68330, time: 0.231, loss: 3444.974609\n",
      "Train: step:  68340, time: 0.218, loss: 1624.508667\n",
      "Train: step:  68350, time: 0.232, loss: 2193.239014\n",
      "Train: step:  68360, time: 0.244, loss: 1706.445435\n",
      "Train: step:  68370, time: 0.232, loss: 2511.250244\n",
      "Train: step:  68380, time: 0.232, loss: 486.373199\n",
      "Train: step:  68390, time: 0.238, loss: 2608.828125\n",
      "Train: step:  68400, time: 0.232, loss: 1106.043457\n",
      "Train: step:  68410, time: 0.252, loss: 933.164307\n",
      "Train: step:  68420, time: 0.236, loss: 1891.215332\n",
      "Train: step:  68430, time: 0.222, loss: 2414.592773\n",
      "Train: step:  68440, time: 0.251, loss: 1217.053711\n",
      "Train: step:  68450, time: 0.257, loss: 2771.800049\n",
      "Train: step:  68460, time: 0.244, loss: 1257.067993\n",
      "Train: step:  68470, time: 0.243, loss: 937.692932\n",
      "Train: step:  68480, time: 0.230, loss: 733.593323\n",
      "Train: step:  68490, time: 0.224, loss: 2230.266602\n",
      "Train: step:  68500, time: 0.222, loss: 408.886749\n",
      "Train: step:  68510, time: 0.231, loss: 1571.438354\n",
      "Train: step:  68520, time: 0.229, loss: 1846.811279\n",
      "Train: step:  68530, time: 0.223, loss: 2301.596191\n",
      "Train: step:  68540, time: 0.251, loss: 2174.040039\n",
      "Train: step:  68550, time: 0.237, loss: 1866.073730\n",
      "Train: step:  68560, time: 0.211, loss: 657.588257\n",
      "Train: step:  68570, time: 0.219, loss: 1176.575562\n",
      "Train: step:  68580, time: 0.238, loss: 1219.690063\n",
      "Train: step:  68590, time: 0.230, loss: 2256.208496\n",
      "Train: step:  68600, time: 0.223, loss: 1458.864868\n",
      "Train: step:  68610, time: 0.219, loss: 2376.023926\n",
      "Train: step:  68620, time: 0.224, loss: 2129.565430\n",
      "Train: step:  68630, time: 0.225, loss: 2138.049561\n",
      "Train: step:  68640, time: 0.224, loss: 253.264664\n",
      "Train: step:  68650, time: 0.220, loss: 2535.666016\n",
      "Train: step:  68660, time: 0.274, loss: 2623.520264\n",
      "Train: step:  68670, time: 0.226, loss: 1307.875610\n",
      "Train: step:  68680, time: 0.221, loss: 3574.384766\n",
      "Train: step:  68690, time: 0.230, loss: 1941.413330\n",
      "Train: step:  68700, time: 0.237, loss: 1274.525513\n",
      "Train: step:  68710, time: 0.231, loss: 1035.384888\n",
      "Train: step:  68720, time: 0.241, loss: 2182.479980\n",
      "Train: step:  68730, time: 0.232, loss: 1852.714966\n",
      "Train: step:  68740, time: 0.246, loss: 362.403656\n",
      "Train: step:  68750, time: 0.260, loss: 1648.659424\n",
      "Train: step:  68760, time: 0.229, loss: 676.755493\n",
      "Train: step:  68770, time: 0.226, loss: 3138.443604\n",
      "Train: step:  68780, time: 0.235, loss: 1452.051025\n",
      "Train: step:  68790, time: 0.224, loss: 1548.388550\n",
      "Train: step:  68800, time: 0.221, loss: 1766.793091\n",
      "Train: step:  68810, time: 0.222, loss: 2570.317627\n",
      "Train: step:  68820, time: 0.224, loss: 2859.926514\n",
      "Train: step:  68830, time: 0.254, loss: 2688.859863\n",
      "Train: step:  68840, time: 0.220, loss: 2163.484863\n",
      "Train: step:  68850, time: 0.220, loss: 2246.161865\n",
      "Train: step:  68860, time: 0.223, loss: 1357.301147\n",
      "Train: step:  68870, time: 0.229, loss: 2167.044434\n",
      "Train: step:  68880, time: 0.256, loss: 702.252991\n",
      "Train: step:  68890, time: 0.235, loss: 505.229187\n",
      "Train: step:  68900, time: 0.223, loss: 790.380920\n",
      "Train: step:  68910, time: 0.234, loss: 2352.931396\n",
      "Train: step:  68920, time: 0.226, loss: 1868.949829\n",
      "Train: step:  68930, time: 0.223, loss: 2493.370361\n",
      "Train: step:  68940, time: 0.229, loss: 1475.370605\n",
      "Train: step:  68950, time: 0.213, loss: 1677.888794\n",
      "Train: step:  68960, time: 0.272, loss: 2090.308838\n",
      "Train: step:  68970, time: 0.224, loss: 2850.432617\n",
      "Train: step:  68980, time: 0.218, loss: 3692.988037\n",
      "Train: step:  68990, time: 0.229, loss: 1367.062500\n",
      "Train: step:  69000, time: 0.229, loss: 1445.652832\n",
      "Train: step:  69010, time: 0.226, loss: 1142.443604\n",
      "Train: step:  69020, time: 0.235, loss: 1980.715454\n",
      "Train: step:  69030, time: 0.221, loss: 1657.546631\n",
      "Train: step:  69040, time: 0.232, loss: 1153.707764\n",
      "Train: step:  69050, time: 0.229, loss: 4542.365234\n",
      "Train: step:  69060, time: 0.226, loss: 1759.462280\n",
      "Train: step:  69070, time: 0.219, loss: 997.900146\n",
      "Train: step:  69080, time: 0.247, loss: 2261.756348\n",
      "Train: step:  69090, time: 0.252, loss: 2150.747559\n",
      "Train: step:  69100, time: 0.223, loss: 2357.919922\n",
      "Train: step:  69110, time: 0.228, loss: 3250.642578\n",
      "Train: step:  69120, time: 0.295, loss: 3965.318604\n",
      "Train: step:  69130, time: 0.231, loss: 2434.079834\n",
      "Train: step:  69140, time: 0.234, loss: 1708.280518\n",
      "Train: step:  69150, time: 0.222, loss: 2536.392334\n",
      "Train: step:  69160, time: 0.248, loss: 4002.408447\n",
      "Train: step:  69170, time: 0.225, loss: 1636.463501\n",
      "Train: step:  69180, time: 0.256, loss: 2318.418945\n",
      "Train: step:  69190, time: 0.225, loss: 1019.946960\n",
      "Train: step:  69200, time: 0.263, loss: 2706.188721\n",
      "Train: step:  69210, time: 0.239, loss: 1990.370850\n",
      "Train: step:  69220, time: 0.232, loss: 1273.856567\n",
      "Train: step:  69230, time: 0.237, loss: 1807.002686\n",
      "Train: step:  69240, time: 0.227, loss: 330.920380\n",
      "Train: step:  69250, time: 0.230, loss: 2987.548340\n",
      "Train: step:  69260, time: 0.221, loss: 2568.448242\n",
      "Train: step:  69270, time: 0.219, loss: 2469.373047\n",
      "Train: step:  69280, time: 0.223, loss: 1232.974365\n",
      "Train: step:  69290, time: 0.222, loss: 1340.543701\n",
      "Train: step:  69300, time: 0.292, loss: 2964.829346\n",
      "Train: step:  69310, time: 0.258, loss: 1973.797363\n",
      "Train: step:  69320, time: 0.256, loss: 1751.637939\n",
      "Train: step:  69330, time: 0.252, loss: 841.640198\n",
      "Train: step:  69340, time: 0.225, loss: 3705.027588\n",
      "Train: step:  69350, time: 0.230, loss: 1906.630493\n",
      "Train: step:  69360, time: 0.227, loss: 2190.055664\n",
      "Train: step:  69370, time: 0.252, loss: 2816.876953\n",
      "Train: step:  69380, time: 0.222, loss: 1739.252441\n",
      "Train: step:  69390, time: 0.218, loss: 811.345764\n",
      "Train: step:  69400, time: 0.221, loss: 1473.599243\n",
      "Train: step:  69410, time: 0.225, loss: 1353.476807\n",
      "Train: step:  69420, time: 0.222, loss: 4102.124512\n",
      "Train: step:  69430, time: 0.249, loss: 4882.108398\n",
      "Train: step:  69440, time: 0.251, loss: 2052.212646\n",
      "Train: step:  69450, time: 0.219, loss: 3019.758301\n",
      "Train: step:  69460, time: 0.260, loss: 2372.624023\n",
      "Train: step:  69470, time: 0.222, loss: 1657.384521\n",
      "Train: step:  69480, time: 0.234, loss: 3575.848877\n",
      "Train: step:  69490, time: 0.360, loss: 2925.707520\n",
      "Train: step:  69500, time: 0.257, loss: 2361.416260\n",
      "Train: step:  69510, time: 0.228, loss: 1475.175537\n",
      "Train: step:  69520, time: 0.223, loss: 994.390015\n",
      "Train: step:  69530, time: 0.216, loss: 1558.386230\n",
      "Train: step:  69540, time: 0.218, loss: 2442.994873\n",
      "Train: step:  69550, time: 0.222, loss: 263.629944\n",
      "Train: step:  69560, time: 0.225, loss: 1951.506836\n",
      "Train: step:  69570, time: 0.210, loss: 2651.114258\n",
      "Train: step:  69580, time: 0.228, loss: 1045.230225\n",
      "Train: step:  69590, time: 0.254, loss: 1827.604980\n",
      "Train: step:  69600, time: 0.222, loss: 928.928101\n",
      "Train: step:  69610, time: 0.222, loss: 721.399170\n",
      "Train: step:  69620, time: 0.225, loss: 233.896286\n",
      "Train: step:  69630, time: 0.220, loss: 2319.597900\n",
      "Train: step:  69640, time: 0.218, loss: 2105.451172\n",
      "Train: step:  69650, time: 0.219, loss: 1897.372803\n",
      "Train: step:  69660, time: 0.276, loss: 2116.630859\n",
      "Train: step:  69670, time: 0.220, loss: 1949.985596\n",
      "Train: step:  69680, time: 0.250, loss: 2249.621826\n",
      "Train: step:  69690, time: 0.219, loss: 1804.649780\n",
      "Train: step:  69700, time: 0.249, loss: 2531.566650\n",
      "Train: step:  69710, time: 0.251, loss: 3410.958252\n",
      "Train: step:  69720, time: 0.277, loss: 2556.230957\n",
      "Train: step:  69730, time: 0.271, loss: 410.737427\n",
      "Train: step:  69740, time: 0.260, loss: 393.391968\n",
      "Train: step:  69750, time: 0.234, loss: 950.900818\n",
      "Train: step:  69760, time: 0.233, loss: 2310.068848\n",
      "Train: step:  69770, time: 0.258, loss: 1002.829163\n",
      "Train: step:  69780, time: 0.224, loss: 291.264008\n",
      "Train: step:  69790, time: 0.233, loss: 1243.523071\n",
      "Train: step:  69800, time: 0.252, loss: 1147.337891\n",
      "Train: step:  69810, time: 0.260, loss: 2251.533447\n",
      "Train: step:  69820, time: 0.232, loss: 2558.237549\n",
      "Train: step:  69830, time: 0.249, loss: 206.025803\n",
      "Train: step:  69840, time: 0.261, loss: 2114.679688\n",
      "Train: step:  69850, time: 0.220, loss: 2339.317627\n",
      "Train: step:  69860, time: 0.223, loss: 1157.437256\n",
      "Train: step:  69870, time: 0.234, loss: 4466.588379\n",
      "Train: step:  69880, time: 0.251, loss: 1885.458130\n",
      "Train: step:  69890, time: 0.271, loss: 2856.503418\n",
      "Train: step:  69900, time: 0.250, loss: 2467.666504\n",
      "Train: step:  69910, time: 0.226, loss: 2362.844971\n",
      "Train: step:  69920, time: 0.227, loss: 535.153931\n",
      "Train: step:  69930, time: 0.219, loss: 2638.776611\n",
      "Train: step:  69940, time: 0.220, loss: 1714.646484\n",
      "Train: step:  69950, time: 0.217, loss: 1135.398682\n",
      "Train: step:  69960, time: 0.292, loss: 2971.148193\n",
      "Train: step:  69970, time: 0.215, loss: 2379.833496\n",
      "Train: step:  69980, time: 0.209, loss: 3003.155518\n",
      "Train: step:  69990, time: 0.236, loss: 2969.466553\n",
      "Train: step:  70000, time: 0.218, loss: 3314.358643\n",
      "Train: step:  70010, time: 0.221, loss: 3944.345215\n",
      "Train: step:  70020, time: 0.223, loss: 210.202271\n",
      "Train: step:  70030, time: 0.210, loss: 2412.664795\n",
      "Train: step:  70040, time: 0.217, loss: 2332.227295\n",
      "Train: step:  70050, time: 0.225, loss: 402.894501\n",
      "Train: step:  70060, time: 0.256, loss: 2167.916748\n",
      "Train: step:  70070, time: 0.233, loss: 2724.914551\n",
      "Train: step:  70080, time: 0.236, loss: 749.782043\n",
      "Train: step:  70090, time: 0.223, loss: 2782.252441\n",
      "Train: step:  70100, time: 0.261, loss: 1532.366577\n",
      "Train: step:  70110, time: 0.226, loss: 2673.555908\n",
      "Train: step:  70120, time: 0.221, loss: 1432.690796\n",
      "Train: step:  70130, time: 0.227, loss: 1214.750366\n",
      "Train: step:  70140, time: 0.227, loss: 479.497772\n",
      "Train: step:  70150, time: 0.233, loss: 1185.017212\n",
      "Train: step:  70160, time: 0.253, loss: 1403.241943\n",
      "Train: step:  70170, time: 0.234, loss: 1507.311523\n",
      "Train: step:  70180, time: 0.240, loss: 1992.399902\n",
      "Train: step:  70190, time: 0.262, loss: 2363.647461\n",
      "Train: step:  70200, time: 0.226, loss: 1214.518188\n",
      "Train: step:  70210, time: 0.259, loss: 2980.129150\n",
      "Train: step:  70220, time: 0.244, loss: 2778.883545\n",
      "Train: step:  70230, time: 0.221, loss: 3071.547363\n",
      "Train: step:  70240, time: 0.215, loss: 1933.074951\n",
      "Train: step:  70250, time: 0.233, loss: 1477.730469\n",
      "Train: step:  70260, time: 0.222, loss: 230.504730\n",
      "Train: step:  70270, time: 0.243, loss: 604.818848\n",
      "Train: step:  70280, time: 0.254, loss: 832.695740\n",
      "Train: step:  70290, time: 0.230, loss: 825.734558\n",
      "Train: step:  70300, time: 0.217, loss: 1792.240723\n",
      "Train: step:  70310, time: 0.227, loss: 2379.290039\n",
      "Train: step:  70320, time: 0.242, loss: 1476.662476\n",
      "Train: step:  70330, time: 0.226, loss: 1406.066406\n",
      "Train: step:  70340, time: 0.230, loss: 2274.530029\n",
      "Train: step:  70350, time: 0.226, loss: 2588.472412\n",
      "Train: step:  70360, time: 0.254, loss: 1505.270386\n",
      "Train: step:  70370, time: 0.233, loss: 1141.309814\n",
      "Train: step:  70380, time: 0.236, loss: 3219.514893\n",
      "Train: step:  70390, time: 0.258, loss: 2109.756348\n",
      "Train: step:  70400, time: 0.275, loss: 4041.655762\n",
      "Train: step:  70410, time: 0.232, loss: 1523.589233\n",
      "Train: step:  70420, time: 0.284, loss: 2169.280273\n",
      "Train: step:  70430, time: 0.247, loss: 1943.659058\n",
      "Train: step:  70440, time: 0.233, loss: 2977.718262\n",
      "Train: step:  70450, time: 0.256, loss: 731.113342\n",
      "Train: step:  70460, time: 0.233, loss: 2254.300537\n",
      "Train: step:  70470, time: 0.269, loss: 211.108490\n",
      "Train: step:  70480, time: 0.231, loss: 2786.546875\n",
      "Train: step:  70490, time: 0.228, loss: 927.224487\n",
      "Train: step:  70500, time: 0.231, loss: 1760.334106\n",
      "Train: step:  70510, time: 0.292, loss: 3406.438232\n",
      "Train: step:  70520, time: 0.235, loss: 2146.719482\n",
      "Train: step:  70530, time: 0.263, loss: 1634.480469\n",
      "Train: step:  70540, time: 0.231, loss: 1883.015747\n",
      "Train: step:  70550, time: 0.231, loss: 959.270874\n",
      "Train: step:  70560, time: 0.234, loss: 4175.089355\n",
      "Train: step:  70570, time: 0.242, loss: 945.683716\n",
      "Train: step:  70580, time: 0.233, loss: 360.078278\n",
      "Train: step:  70590, time: 0.280, loss: 835.481689\n",
      "Train: step:  70600, time: 0.233, loss: 1255.540771\n",
      "Train: step:  70610, time: 0.235, loss: 908.343628\n",
      "Train: step:  70620, time: 0.245, loss: 2275.997559\n",
      "Train: step:  70630, time: 0.228, loss: 1669.355225\n",
      "Train: step:  70640, time: 0.267, loss: 1357.372437\n",
      "Train: step:  70650, time: 0.229, loss: 1360.871948\n",
      "Train: step:  70660, time: 0.255, loss: 1267.406738\n",
      "Train: step:  70670, time: 0.251, loss: 1363.327148\n",
      "Train: step:  70680, time: 0.232, loss: 2539.983154\n",
      "Train: step:  70690, time: 0.232, loss: 2159.059570\n",
      "Train: step:  70700, time: 0.230, loss: 2703.000244\n",
      "Train: step:  70710, time: 0.238, loss: 3324.022949\n",
      "Train: step:  70720, time: 0.223, loss: 1952.682373\n",
      "Train: step:  70730, time: 0.230, loss: 5039.833008\n",
      "Train: step:  70740, time: 0.227, loss: 3628.145020\n",
      "Train: step:  70750, time: 0.244, loss: 1082.907104\n",
      "Train: step:  70760, time: 0.220, loss: 1450.518066\n",
      "Train: step:  70770, time: 0.244, loss: 1309.327515\n",
      "Train: step:  70780, time: 0.252, loss: 514.216614\n",
      "Train: step:  70790, time: 0.227, loss: 2793.131836\n",
      "Train: step:  70800, time: 0.223, loss: 1417.639160\n",
      "Train: step:  70810, time: 0.225, loss: 1380.350586\n",
      "Train: step:  70820, time: 0.252, loss: 1079.564697\n",
      "Train: step:  70830, time: 0.253, loss: 765.786133\n",
      "Train: step:  70840, time: 0.221, loss: 1553.102417\n",
      "Train: step:  70850, time: 0.229, loss: 2867.295166\n",
      "Train: step:  70860, time: 0.251, loss: 2066.092041\n",
      "Train: step:  70870, time: 0.227, loss: 2624.202881\n",
      "Train: step:  70880, time: 0.250, loss: 1303.518433\n",
      "Train: step:  70890, time: 0.242, loss: 2171.577393\n",
      "Train: step:  70900, time: 0.246, loss: 3264.133301\n",
      "Train: step:  70910, time: 0.258, loss: 1125.076294\n",
      "Train: step:  70920, time: 0.254, loss: 2360.758057\n",
      "Train: step:  70930, time: 0.239, loss: 2676.358154\n",
      "Train: step:  70940, time: 0.260, loss: 2101.382568\n",
      "Train: step:  70950, time: 0.230, loss: 2402.833984\n",
      "Train: step:  70960, time: 0.224, loss: 2607.691650\n",
      "Train: step:  70970, time: 0.231, loss: 911.564819\n",
      "Train: step:  70980, time: 0.234, loss: 1701.905640\n",
      "Train: step:  70990, time: 0.228, loss: 1553.508545\n",
      "Train: step:  71000, time: 0.257, loss: 797.445984\n",
      "Train: step:  71010, time: 0.221, loss: 879.164001\n",
      "Train: step:  71020, time: 0.228, loss: 2924.443848\n",
      "Train: step:  71030, time: 0.229, loss: 1354.481812\n",
      "Train: step:  71040, time: 0.235, loss: 980.789673\n",
      "Train: step:  71050, time: 0.229, loss: 1323.980591\n",
      "Train: step:  71060, time: 0.246, loss: 2962.554932\n",
      "Train: step:  71070, time: 0.229, loss: 3138.951172\n",
      "Train: step:  71080, time: 0.222, loss: 492.466461\n",
      "Train: step:  71090, time: 0.289, loss: 1869.479736\n",
      "Train: step:  71100, time: 0.254, loss: 2913.424561\n",
      "Train: step:  71110, time: 0.228, loss: 1536.933228\n",
      "Train: step:  71120, time: 0.221, loss: 2770.084717\n",
      "Train: step:  71130, time: 0.255, loss: 498.073853\n",
      "Train: step:  71140, time: 0.233, loss: 744.013184\n",
      "Train: step:  71150, time: 0.227, loss: 2046.918579\n",
      "Train: step:  71160, time: 0.222, loss: 2453.469482\n",
      "Train: step:  71170, time: 0.225, loss: 4124.979980\n",
      "Train: step:  71180, time: 0.245, loss: 1686.522339\n",
      "Train: step:  71190, time: 0.229, loss: 515.273987\n",
      "Train: step:  71200, time: 0.221, loss: 679.936279\n",
      "Train: step:  71210, time: 0.241, loss: 1650.344971\n",
      "Train: step:  71220, time: 0.225, loss: 1029.814819\n",
      "Train: step:  71230, time: 0.239, loss: 2670.671387\n",
      "Train: step:  71240, time: 0.230, loss: 1188.278320\n",
      "Train: step:  71250, time: 0.270, loss: 1066.441284\n",
      "Train: step:  71260, time: 0.231, loss: 3678.794189\n",
      "Train: step:  71270, time: 0.250, loss: 2808.856689\n",
      "Train: step:  71280, time: 0.229, loss: 5006.022461\n",
      "Train: step:  71290, time: 0.228, loss: 1554.500732\n",
      "Train: step:  71300, time: 0.229, loss: 3125.627686\n",
      "Train: step:  71310, time: 0.261, loss: 1883.025146\n",
      "Train: step:  71320, time: 0.250, loss: 673.448303\n",
      "Train: step:  71330, time: 0.236, loss: 501.721313\n",
      "Train: step:  71340, time: 0.257, loss: 2211.930908\n",
      "Train: step:  71350, time: 0.223, loss: 2178.427979\n",
      "Train: step:  71360, time: 0.225, loss: 1590.918823\n",
      "Train: step:  71370, time: 0.253, loss: 869.844788\n",
      "Train: step:  71380, time: 0.232, loss: 1712.708130\n",
      "Train: step:  71390, time: 0.231, loss: 1523.979492\n",
      "Train: step:  71400, time: 0.229, loss: 1956.637939\n",
      "Train: step:  71410, time: 0.225, loss: 1170.967163\n",
      "Train: step:  71420, time: 0.231, loss: 3383.950195\n",
      "Train: step:  71430, time: 0.264, loss: 2212.989990\n",
      "Train: step:  71440, time: 0.232, loss: 1771.557739\n",
      "Train: step:  71450, time: 0.226, loss: 1795.115234\n",
      "Train: step:  71460, time: 0.230, loss: 750.573853\n",
      "Train: step:  71470, time: 0.246, loss: 1836.188110\n",
      "Train: step:  71480, time: 0.227, loss: 933.853699\n",
      "Train: step:  71490, time: 0.236, loss: 1819.770508\n",
      "Train: step:  71500, time: 0.225, loss: 1091.566284\n",
      "Train: step:  71510, time: 0.224, loss: 1758.714600\n",
      "Train: step:  71520, time: 0.236, loss: 475.904388\n",
      "Train: step:  71530, time: 0.224, loss: 614.559692\n",
      "Train: step:  71540, time: 0.226, loss: 1933.162842\n",
      "Train: step:  71550, time: 0.262, loss: 2632.171631\n",
      "Train: step:  71560, time: 0.225, loss: 312.528107\n",
      "Train: step:  71570, time: 0.230, loss: 1918.858887\n",
      "Train: step:  71580, time: 0.231, loss: 1043.154175\n",
      "Train: step:  71590, time: 0.250, loss: 1233.762817\n",
      "Train: step:  71600, time: 0.259, loss: 3967.561523\n",
      "Train: step:  71610, time: 0.231, loss: 716.327515\n",
      "Train: step:  71620, time: 0.233, loss: 1374.060181\n",
      "Train: step:  71630, time: 0.237, loss: 522.535889\n",
      "Train: step:  71640, time: 0.225, loss: 2212.233154\n",
      "Train: step:  71650, time: 0.254, loss: 2221.379883\n",
      "Train: step:  71660, time: 0.226, loss: 1516.666504\n",
      "Train: step:  71670, time: 0.222, loss: 3528.877930\n",
      "Train: step:  71680, time: 0.273, loss: 1108.224365\n",
      "Train: step:  71690, time: 0.245, loss: 1191.304321\n",
      "Train: step:  71700, time: 0.265, loss: 2032.281128\n",
      "Train: step:  71710, time: 0.222, loss: 2953.978516\n",
      "Train: step:  71720, time: 0.230, loss: 3340.557861\n",
      "Train: step:  71730, time: 0.265, loss: 651.524048\n",
      "Train: step:  71740, time: 0.266, loss: 2039.966797\n",
      "Train: step:  71750, time: 0.231, loss: 3997.287842\n",
      "Train: step:  71760, time: 0.228, loss: 2880.547852\n",
      "Train: step:  71770, time: 0.230, loss: 1100.997314\n",
      "Train: step:  71780, time: 0.253, loss: 1575.807373\n",
      "Train: step:  71790, time: 0.271, loss: 2068.813721\n",
      "Train: step:  71800, time: 0.255, loss: 1834.531860\n",
      "Train: step:  71810, time: 0.228, loss: 3166.123047\n",
      "Train: step:  71820, time: 0.253, loss: 1484.247925\n",
      "Train: step:  71830, time: 0.250, loss: 3599.376953\n",
      "Train: step:  71840, time: 0.236, loss: 1776.507324\n",
      "Train: step:  71850, time: 0.227, loss: 827.760376\n",
      "Train: step:  71860, time: 0.229, loss: 1882.844727\n",
      "Train: step:  71870, time: 0.262, loss: 2343.249512\n",
      "Train: step:  71880, time: 0.231, loss: 872.665710\n",
      "Train: step:  71890, time: 0.239, loss: 918.702576\n",
      "Train: step:  71900, time: 0.229, loss: 4700.686035\n",
      "Train: step:  71910, time: 0.247, loss: 536.473267\n",
      "Train: step:  71920, time: 0.246, loss: 1958.527588\n",
      "Train: step:  71930, time: 0.251, loss: 2161.604736\n",
      "Train: step:  71940, time: 0.227, loss: 1651.370117\n",
      "Train: step:  71950, time: 0.229, loss: 1211.393311\n",
      "Train: step:  71960, time: 0.264, loss: 523.242126\n",
      "Train: step:  71970, time: 0.225, loss: 2746.858398\n",
      "Train: step:  71980, time: 0.259, loss: 1203.602905\n",
      "Train: step:  71990, time: 0.228, loss: 1879.197021\n",
      "Train: step:  72000, time: 0.278, loss: 2067.148926\n",
      "Train: step:  72010, time: 0.224, loss: 752.453247\n",
      "Train: step:  72020, time: 0.223, loss: 2151.451172\n",
      "Train: step:  72030, time: 0.225, loss: 2658.153809\n",
      "Train: step:  72040, time: 0.251, loss: 2515.402100\n",
      "Train: step:  72050, time: 0.224, loss: 1424.685791\n",
      "Train: step:  72060, time: 0.280, loss: 767.372742\n",
      "Train: step:  72070, time: 0.278, loss: 2403.123291\n",
      "Train: step:  72080, time: 0.260, loss: 1373.595825\n",
      "Train: step:  72090, time: 0.224, loss: 3353.327148\n",
      "Train: step:  72100, time: 0.223, loss: 652.328796\n",
      "Train: step:  72110, time: 0.222, loss: 3139.595947\n",
      "Train: step:  72120, time: 0.270, loss: 2395.578857\n",
      "Train: step:  72130, time: 0.220, loss: 3073.321777\n",
      "Train: step:  72140, time: 0.256, loss: 2359.601807\n",
      "Train: step:  72150, time: 0.232, loss: 2330.370850\n",
      "Train: step:  72160, time: 0.237, loss: 2665.592285\n",
      "Train: step:  72170, time: 0.231, loss: 1412.029419\n",
      "Train: step:  72180, time: 0.226, loss: 2575.521484\n",
      "Train: step:  72190, time: 0.232, loss: 3839.549561\n",
      "Train: step:  72200, time: 0.236, loss: 2041.737915\n",
      "Train: step:  72210, time: 0.237, loss: 1161.176025\n",
      "Train: step:  72220, time: 0.234, loss: 2640.985840\n",
      "Train: step:  72230, time: 0.227, loss: 2289.270020\n",
      "Train: step:  72240, time: 0.258, loss: 1561.380981\n",
      "Train: step:  72250, time: 0.256, loss: 238.586685\n",
      "Train: step:  72260, time: 0.259, loss: 1261.890015\n",
      "Train: step:  72270, time: 0.223, loss: 2504.739502\n",
      "Train: step:  72280, time: 0.227, loss: 2267.922363\n",
      "Train: step:  72290, time: 0.229, loss: 2614.818115\n",
      "Train: step:  72300, time: 0.231, loss: 2617.592773\n",
      "Train: step:  72310, time: 0.228, loss: 2042.105347\n",
      "Train: step:  72320, time: 0.225, loss: 2290.990967\n",
      "Train: step:  72330, time: 0.227, loss: 980.066833\n",
      "Train: step:  72340, time: 0.236, loss: 2298.581299\n",
      "Train: step:  72350, time: 0.233, loss: 1352.505371\n",
      "Train: step:  72360, time: 0.229, loss: 908.826294\n",
      "Train: step:  72370, time: 0.230, loss: 2111.543701\n",
      "Train: step:  72380, time: 0.260, loss: 910.541748\n",
      "Train: step:  72390, time: 0.230, loss: 3208.493408\n",
      "Train: step:  72400, time: 0.231, loss: 4697.614258\n",
      "Train: step:  72410, time: 0.239, loss: 1215.508545\n",
      "Train: step:  72420, time: 0.234, loss: 1238.165771\n",
      "Train: step:  72430, time: 0.229, loss: 1104.338013\n",
      "Train: step:  72440, time: 0.231, loss: 393.518372\n",
      "Train: step:  72450, time: 0.253, loss: 353.100922\n",
      "Train: step:  72460, time: 0.284, loss: 436.409637\n",
      "Train: step:  72470, time: 0.233, loss: 2846.850098\n",
      "Train: step:  72480, time: 0.251, loss: 3360.104736\n",
      "Train: step:  72490, time: 0.228, loss: 1950.246338\n",
      "Train: step:  72500, time: 0.260, loss: 2072.912598\n",
      "Train: step:  72510, time: 0.230, loss: 414.062103\n",
      "Train: step:  72520, time: 0.224, loss: 1346.656616\n",
      "Train: step:  72530, time: 0.227, loss: 1461.491821\n",
      "Train: step:  72540, time: 0.227, loss: 2621.300049\n",
      "Train: step:  72550, time: 0.226, loss: 1863.224609\n",
      "Train: step:  72560, time: 0.235, loss: 1645.471680\n",
      "Train: step:  72570, time: 0.227, loss: 1816.898926\n",
      "Train: step:  72580, time: 0.226, loss: 3044.102295\n",
      "Train: step:  72590, time: 0.229, loss: 2250.897949\n",
      "Train: step:  72600, time: 0.229, loss: 2357.723389\n",
      "Train: step:  72610, time: 0.268, loss: 2468.437500\n",
      "Train: step:  72620, time: 0.230, loss: 1164.427002\n",
      "Train: step:  72630, time: 0.232, loss: 987.230042\n",
      "Train: step:  72640, time: 0.251, loss: 2872.833252\n",
      "Train: step:  72650, time: 0.234, loss: 2085.936279\n",
      "Train: step:  72660, time: 0.263, loss: 1354.974609\n",
      "Train: step:  72670, time: 0.228, loss: 194.572556\n",
      "Train: step:  72680, time: 0.231, loss: 1576.733276\n",
      "Train: step:  72690, time: 0.224, loss: 1090.308594\n",
      "Train: step:  72700, time: 0.225, loss: 897.126038\n",
      "Train: step:  72710, time: 0.227, loss: 2283.642578\n",
      "Train: step:  72720, time: 0.228, loss: 2736.466309\n",
      "Train: step:  72730, time: 0.261, loss: 1139.827759\n",
      "Train: step:  72740, time: 0.262, loss: 1432.221313\n",
      "Train: step:  72750, time: 0.231, loss: 398.514984\n",
      "Train: step:  72760, time: 0.259, loss: 1436.816772\n",
      "Train: step:  72770, time: 0.252, loss: 1713.330566\n",
      "Train: step:  72780, time: 0.231, loss: 2869.153809\n",
      "Train: step:  72790, time: 0.248, loss: 2442.673096\n",
      "Train: step:  72800, time: 0.230, loss: 742.308167\n",
      "Train: step:  72810, time: 0.226, loss: 2554.393799\n",
      "Train: step:  72820, time: 0.241, loss: 1989.122681\n",
      "Train: step:  72830, time: 0.227, loss: 2535.758789\n",
      "Train: step:  72840, time: 0.227, loss: 1003.937378\n",
      "Train: step:  72850, time: 0.239, loss: 1436.964844\n",
      "Train: step:  72860, time: 0.256, loss: 1922.278564\n",
      "Train: step:  72870, time: 0.235, loss: 1199.355591\n",
      "Train: step:  72880, time: 0.229, loss: 1455.510742\n",
      "Train: step:  72890, time: 0.225, loss: 279.699219\n",
      "Train: step:  72900, time: 0.232, loss: 1566.336914\n",
      "Train: step:  72910, time: 0.252, loss: 1380.969360\n",
      "Train: step:  72920, time: 0.230, loss: 987.569946\n",
      "Train: step:  72930, time: 0.228, loss: 341.490417\n",
      "Train: step:  72940, time: 0.257, loss: 462.129700\n",
      "Train: step:  72950, time: 0.230, loss: 1364.546509\n",
      "Train: step:  72960, time: 0.228, loss: 2025.547363\n",
      "Train: step:  72970, time: 0.261, loss: 666.511719\n",
      "Train: step:  72980, time: 0.274, loss: 1535.858887\n",
      "Train: step:  72990, time: 0.240, loss: 999.218384\n",
      "Train: step:  73000, time: 0.239, loss: 863.197327\n",
      "Train: step:  73010, time: 0.225, loss: 1578.039917\n",
      "Train: step:  73020, time: 0.237, loss: 804.925842\n",
      "Train: step:  73030, time: 0.232, loss: 829.805054\n",
      "Train: step:  73040, time: 0.255, loss: 2541.033447\n",
      "Train: step:  73050, time: 0.236, loss: 2625.925781\n",
      "Train: step:  73060, time: 0.222, loss: 1111.691406\n",
      "Train: step:  73070, time: 0.256, loss: 2771.946777\n",
      "Train: step:  73080, time: 0.265, loss: 1369.980591\n",
      "Train: step:  73090, time: 0.228, loss: 934.290283\n",
      "Train: step:  73100, time: 0.271, loss: 1284.087524\n",
      "Train: step:  73110, time: 0.240, loss: 739.737427\n",
      "Train: step:  73120, time: 0.274, loss: 2906.257812\n",
      "Train: step:  73130, time: 0.228, loss: 3153.335449\n",
      "Train: step:  73140, time: 0.280, loss: 3644.009521\n",
      "Train: step:  73150, time: 0.237, loss: 1918.985107\n",
      "Train: step:  73160, time: 0.229, loss: 3017.629639\n",
      "Train: step:  73170, time: 0.238, loss: 1956.976318\n",
      "Train: step:  73180, time: 0.236, loss: 631.116394\n",
      "Train: step:  73190, time: 0.230, loss: 1078.537598\n",
      "Train: step:  73200, time: 0.230, loss: 1303.510376\n",
      "Train: step:  73210, time: 0.225, loss: 1750.791504\n",
      "Train: step:  73220, time: 0.259, loss: 1878.944946\n",
      "Train: step:  73230, time: 0.224, loss: 1949.466309\n",
      "Train: step:  73240, time: 0.229, loss: 2081.136230\n",
      "Train: step:  73250, time: 0.226, loss: 3311.648682\n",
      "Train: step:  73260, time: 0.252, loss: 1052.529297\n",
      "Train: step:  73270, time: 0.258, loss: 1440.072510\n",
      "Train: step:  73280, time: 0.252, loss: 1053.750488\n",
      "Train: step:  73290, time: 0.224, loss: 675.869446\n",
      "Train: step:  73300, time: 0.226, loss: 608.777954\n",
      "Train: step:  73310, time: 0.228, loss: 457.799713\n",
      "Train: step:  73320, time: 0.228, loss: 1086.313965\n",
      "Train: step:  73330, time: 0.229, loss: 1929.467407\n",
      "Train: step:  73340, time: 0.232, loss: 1342.284668\n",
      "Train: step:  73350, time: 0.272, loss: 3146.291992\n",
      "Train: step:  73360, time: 0.227, loss: 2524.795410\n",
      "Train: step:  73370, time: 0.252, loss: 263.326782\n",
      "Train: step:  73380, time: 0.228, loss: 1896.981689\n",
      "Train: step:  73390, time: 0.256, loss: 776.428284\n",
      "Train: step:  73400, time: 0.235, loss: 2647.120850\n",
      "Train: step:  73410, time: 0.220, loss: 871.558838\n",
      "Train: step:  73420, time: 0.229, loss: 2359.782959\n",
      "Train: step:  73430, time: 0.236, loss: 2503.347168\n",
      "Train: step:  73440, time: 0.258, loss: 1504.208984\n",
      "Train: step:  73450, time: 0.264, loss: 2348.935303\n",
      "Train: step:  73460, time: 0.263, loss: 1384.491943\n",
      "Train: step:  73470, time: 0.220, loss: 3389.707275\n",
      "Train: step:  73480, time: 0.261, loss: 2324.525635\n",
      "Train: step:  73490, time: 0.259, loss: 3131.146729\n",
      "Train: step:  73500, time: 0.226, loss: 2121.161865\n",
      "Train: step:  73510, time: 0.261, loss: 307.607269\n",
      "Train: step:  73520, time: 0.257, loss: 2699.897217\n",
      "Train: step:  73530, time: 0.226, loss: 1215.178833\n",
      "Train: step:  73540, time: 0.224, loss: 1386.128662\n",
      "Train: step:  73550, time: 0.234, loss: 1039.141846\n",
      "Train: step:  73560, time: 0.238, loss: 2031.814087\n",
      "Train: step:  73570, time: 0.232, loss: 903.749207\n",
      "Train: step:  73580, time: 0.258, loss: 1029.815063\n",
      "Train: step:  73590, time: 0.233, loss: 1164.496704\n",
      "Train: step:  73600, time: 0.231, loss: 2680.835205\n",
      "Train: step:  73610, time: 0.233, loss: 1250.816772\n",
      "Train: step:  73620, time: 0.225, loss: 2731.282227\n",
      "Train: step:  73630, time: 0.226, loss: 2886.433838\n",
      "Train: step:  73640, time: 0.223, loss: 634.038818\n",
      "Train: step:  73650, time: 0.253, loss: 1074.463623\n",
      "Train: step:  73660, time: 0.227, loss: 1207.790527\n",
      "Train: step:  73670, time: 0.233, loss: 554.379395\n",
      "Train: step:  73680, time: 0.237, loss: 237.546234\n",
      "Train: step:  73690, time: 0.228, loss: 1673.127563\n",
      "Train: step:  73700, time: 0.252, loss: 732.410034\n",
      "Train: step:  73710, time: 0.231, loss: 2156.871094\n",
      "Train: step:  73720, time: 0.229, loss: 972.901855\n",
      "Train: step:  73730, time: 0.236, loss: 1882.588013\n",
      "Train: step:  73740, time: 0.232, loss: 1262.146362\n",
      "Train: step:  73750, time: 0.225, loss: 1572.356689\n",
      "Train: step:  73760, time: 0.233, loss: 406.466980\n",
      "Train: step:  73770, time: 0.231, loss: 954.322632\n",
      "Train: step:  73780, time: 0.229, loss: 1279.713501\n",
      "Train: step:  73790, time: 0.231, loss: 2387.409180\n",
      "Train: step:  73800, time: 0.238, loss: 2396.585693\n",
      "Train: step:  73810, time: 0.225, loss: 949.545288\n",
      "Train: step:  73820, time: 0.257, loss: 1698.908203\n",
      "Train: step:  73830, time: 0.236, loss: 2033.138916\n",
      "Train: step:  73840, time: 0.252, loss: 861.482178\n",
      "Train: step:  73850, time: 0.223, loss: 1358.153442\n",
      "Train: step:  73860, time: 0.223, loss: 2102.831055\n",
      "Train: step:  73870, time: 0.224, loss: 2223.384521\n",
      "Train: step:  73880, time: 0.224, loss: 2446.042969\n",
      "Train: step:  73890, time: 0.233, loss: 1794.546753\n",
      "Train: step:  73900, time: 0.231, loss: 1920.272461\n",
      "Train: step:  73910, time: 0.258, loss: 1985.709106\n",
      "Train: step:  73920, time: 0.232, loss: 1160.574219\n",
      "Train: step:  73930, time: 0.225, loss: 1135.860718\n",
      "Train: step:  73940, time: 0.255, loss: 1435.689331\n",
      "Train: step:  73950, time: 0.234, loss: 2234.266602\n",
      "Train: step:  73960, time: 0.255, loss: 3044.262207\n",
      "Train: step:  73970, time: 0.276, loss: 2264.355957\n",
      "Train: step:  73980, time: 0.232, loss: 2393.238037\n",
      "Train: step:  73990, time: 0.231, loss: 1409.888550\n",
      "Train: step:  74000, time: 0.223, loss: 2477.321289\n",
      "Train: step:  74010, time: 0.244, loss: 3155.833984\n",
      "Train: step:  74020, time: 0.226, loss: 1140.477783\n",
      "Train: step:  74030, time: 0.230, loss: 2045.473267\n",
      "Train: step:  74040, time: 0.251, loss: 1891.149658\n",
      "Train: step:  74050, time: 0.249, loss: 577.686646\n",
      "Train: step:  74060, time: 0.229, loss: 2832.501953\n",
      "Train: step:  74070, time: 0.221, loss: 2012.518677\n",
      "Train: step:  74080, time: 0.244, loss: 3745.642822\n",
      "Train: step:  74090, time: 0.227, loss: 2294.356201\n",
      "Train: step:  74100, time: 0.224, loss: 2470.482910\n",
      "Train: step:  74110, time: 0.252, loss: 471.340027\n",
      "Train: step:  74120, time: 0.270, loss: 1980.208984\n",
      "Train: step:  74130, time: 0.227, loss: 2969.728760\n",
      "Train: step:  74140, time: 0.254, loss: 2618.976318\n",
      "Train: step:  74150, time: 0.247, loss: 2592.571777\n",
      "Train: step:  74160, time: 0.229, loss: 1107.534546\n",
      "Train: step:  74170, time: 0.269, loss: 1503.473267\n",
      "Train: step:  74180, time: 0.275, loss: 1444.040527\n",
      "Train: step:  74190, time: 0.235, loss: 1395.792236\n",
      "Train: step:  74200, time: 0.245, loss: 618.239746\n",
      "Train: step:  74210, time: 0.227, loss: 2076.555908\n",
      "Train: step:  74220, time: 0.225, loss: 4090.527344\n",
      "Train: step:  74230, time: 0.229, loss: 1629.219482\n",
      "Train: step:  74240, time: 0.235, loss: 2063.611572\n",
      "Train: step:  74250, time: 0.223, loss: 2250.426514\n",
      "Train: step:  74260, time: 0.257, loss: 1844.391113\n",
      "Train: step:  74270, time: 0.259, loss: 1184.799194\n",
      "Train: step:  74280, time: 0.223, loss: 3666.533936\n",
      "Train: step:  74290, time: 0.255, loss: 2599.390137\n",
      "Train: step:  74300, time: 0.228, loss: 1098.424805\n",
      "Train: step:  74310, time: 0.241, loss: 891.068420\n",
      "Train: step:  74320, time: 0.231, loss: 1072.663208\n",
      "Train: step:  74330, time: 0.258, loss: 1057.368652\n",
      "Train: step:  74340, time: 0.229, loss: 1765.165649\n",
      "Train: step:  74350, time: 0.232, loss: 1640.427002\n",
      "Train: step:  74360, time: 0.235, loss: 1350.729248\n",
      "Train: step:  74370, time: 0.257, loss: 947.619873\n",
      "Train: step:  74380, time: 0.249, loss: 2396.073242\n",
      "Train: step:  74390, time: 0.231, loss: 644.124329\n",
      "Train: step:  74400, time: 0.235, loss: 1090.226318\n",
      "Train: step:  74410, time: 0.228, loss: 2195.188477\n",
      "Train: step:  74420, time: 0.233, loss: 228.251465\n",
      "Train: step:  74430, time: 0.256, loss: 1907.247925\n",
      "Train: step:  74440, time: 0.246, loss: 688.353394\n",
      "Train: step:  74450, time: 0.228, loss: 542.903931\n",
      "Train: step:  74460, time: 0.226, loss: 2383.587646\n",
      "Train: step:  74470, time: 0.224, loss: 489.959686\n",
      "Train: step:  74480, time: 0.230, loss: 2699.421631\n",
      "Train: step:  74490, time: 0.234, loss: 2586.739502\n",
      "Train: step:  74500, time: 0.234, loss: 1540.199219\n",
      "Train: step:  74510, time: 0.280, loss: 2377.762451\n",
      "Train: step:  74520, time: 0.263, loss: 1823.128784\n",
      "Train: step:  74530, time: 0.231, loss: 637.742737\n",
      "Train: step:  74540, time: 0.264, loss: 2244.013916\n",
      "Train: step:  74550, time: 0.263, loss: 2928.632324\n",
      "Train: step:  74560, time: 0.250, loss: 1073.051636\n",
      "Train: step:  74570, time: 0.231, loss: 1814.537720\n",
      "Train: step:  74580, time: 0.250, loss: 2611.254639\n",
      "Train: step:  74590, time: 0.223, loss: 1221.968262\n",
      "Train: step:  74600, time: 0.228, loss: 799.437012\n",
      "Train: step:  74610, time: 0.267, loss: 1008.422729\n",
      "Train: step:  74620, time: 0.249, loss: 3039.130127\n",
      "Train: step:  74630, time: 0.223, loss: 3139.400391\n",
      "Train: step:  74640, time: 0.230, loss: 3207.824707\n",
      "Train: step:  74650, time: 0.239, loss: 3084.794922\n",
      "Train: step:  74660, time: 0.257, loss: 1357.830811\n",
      "Train: step:  74670, time: 0.229, loss: 905.912964\n",
      "Train: step:  74680, time: 0.247, loss: 2005.478394\n",
      "Train: step:  74690, time: 0.227, loss: 792.236755\n",
      "Train: step:  74700, time: 0.225, loss: 1792.144409\n",
      "Train: step:  74710, time: 0.221, loss: 260.014893\n",
      "Train: step:  74720, time: 0.242, loss: 4561.025391\n",
      "Train: step:  74730, time: 0.231, loss: 926.899414\n",
      "Train: step:  74740, time: 0.229, loss: 1947.346680\n",
      "Train: step:  74750, time: 0.228, loss: 2016.396973\n",
      "Train: step:  74760, time: 0.218, loss: 3284.100342\n",
      "Train: step:  74770, time: 0.237, loss: 2394.596924\n",
      "Train: step:  74780, time: 0.225, loss: 1286.919312\n",
      "Train: step:  74790, time: 0.234, loss: 950.216064\n",
      "Train: step:  74800, time: 0.252, loss: 878.184753\n",
      "Train: step:  74810, time: 0.230, loss: 1364.277100\n",
      "Train: step:  74820, time: 0.229, loss: 2083.730713\n",
      "Train: step:  74830, time: 0.226, loss: 783.177002\n",
      "Train: step:  74840, time: 0.226, loss: 2375.105713\n",
      "Train: step:  74850, time: 0.231, loss: 3055.611084\n",
      "Train: step:  74860, time: 0.230, loss: 2660.735840\n",
      "Train: step:  74870, time: 0.225, loss: 1368.765625\n",
      "Train: step:  74880, time: 0.239, loss: 902.303833\n",
      "Train: step:  74890, time: 0.234, loss: 1444.107178\n",
      "Train: step:  74900, time: 0.225, loss: 1177.446533\n",
      "Train: step:  74910, time: 0.253, loss: 2277.665771\n",
      "Train: step:  74920, time: 0.254, loss: 3438.185547\n",
      "Train: step:  74930, time: 0.251, loss: 2028.475220\n",
      "Train: step:  74940, time: 0.229, loss: 1968.337769\n",
      "Train: step:  74950, time: 0.232, loss: 997.284790\n",
      "Train: step:  74960, time: 0.233, loss: 2001.983887\n",
      "Train: step:  74970, time: 0.255, loss: 1384.945068\n",
      "Train: step:  74980, time: 0.241, loss: 2659.333252\n",
      "Train: step:  74990, time: 0.224, loss: 1846.027344\n",
      "Train: step:  75000, time: 0.239, loss: 2981.219482\n",
      "Train: step:  75010, time: 0.233, loss: 417.133972\n",
      "Train: step:  75020, time: 0.251, loss: 2391.669434\n",
      "Train: step:  75030, time: 0.229, loss: 336.095490\n",
      "Train: step:  75040, time: 0.242, loss: 1364.989624\n",
      "Train: step:  75050, time: 0.263, loss: 2420.743164\n",
      "Train: step:  75060, time: 0.229, loss: 2181.372803\n",
      "Train: step:  75070, time: 0.229, loss: 1019.851318\n",
      "Train: step:  75080, time: 0.253, loss: 2467.760498\n",
      "Train: step:  75090, time: 0.241, loss: 954.373352\n",
      "Train: step:  75100, time: 0.225, loss: 1728.823120\n",
      "Train: step:  75110, time: 0.223, loss: 1939.804443\n",
      "Train: step:  75120, time: 0.225, loss: 2570.160889\n",
      "Train: step:  75130, time: 0.228, loss: 2233.890869\n",
      "Train: step:  75140, time: 0.229, loss: 4060.055908\n",
      "Train: step:  75150, time: 0.272, loss: 1890.009521\n",
      "Train: step:  75160, time: 0.227, loss: 1815.913208\n",
      "Train: step:  75170, time: 0.229, loss: 2050.027588\n",
      "Train: step:  75180, time: 0.223, loss: 1198.038086\n",
      "Train: step:  75190, time: 0.230, loss: 768.072815\n",
      "Train: step:  75200, time: 0.225, loss: 2518.646484\n",
      "Train: step:  75210, time: 0.264, loss: 3335.754150\n",
      "Train: step:  75220, time: 0.252, loss: 2951.962891\n",
      "Train: step:  75230, time: 0.252, loss: 1852.691528\n",
      "Train: step:  75240, time: 0.259, loss: 2405.767578\n",
      "Train: step:  75250, time: 0.247, loss: 778.126587\n",
      "Train: step:  75260, time: 0.267, loss: 1296.065186\n",
      "Train: step:  75270, time: 0.251, loss: 908.093018\n",
      "Train: step:  75280, time: 0.239, loss: 2164.857178\n",
      "Train: step:  75290, time: 0.229, loss: 3414.093262\n",
      "Train: step:  75300, time: 0.236, loss: 1720.581177\n",
      "Train: step:  75310, time: 0.225, loss: 879.642334\n",
      "Train: step:  75320, time: 0.226, loss: 1826.822021\n",
      "Train: step:  75330, time: 0.237, loss: 1821.476685\n",
      "Train: step:  75340, time: 0.225, loss: 1339.416626\n",
      "Train: step:  75350, time: 0.224, loss: 627.496216\n",
      "Train: step:  75360, time: 0.236, loss: 1401.571899\n",
      "Train: step:  75370, time: 0.235, loss: 962.704529\n",
      "Train: step:  75380, time: 0.225, loss: 1619.126465\n",
      "Train: step:  75390, time: 0.226, loss: 556.882996\n",
      "Train: step:  75400, time: 0.230, loss: 1657.096802\n",
      "Train: step:  75410, time: 0.230, loss: 1951.194580\n",
      "Train: step:  75420, time: 0.228, loss: 2719.624512\n",
      "Train: step:  75430, time: 0.264, loss: 800.310364\n",
      "Train: step:  75440, time: 0.254, loss: 1897.792603\n",
      "Train: step:  75450, time: 0.253, loss: 305.717041\n",
      "Train: step:  75460, time: 0.226, loss: 2206.724121\n",
      "Train: step:  75470, time: 0.249, loss: 1703.933350\n",
      "Train: step:  75480, time: 0.268, loss: 2030.800293\n",
      "Train: step:  75490, time: 0.239, loss: 3475.556641\n",
      "Train: step:  75500, time: 0.231, loss: 3756.150879\n",
      "Train: step:  75510, time: 0.247, loss: 2197.628174\n",
      "Train: step:  75520, time: 0.233, loss: 904.327759\n",
      "Train: step:  75530, time: 0.234, loss: 1870.741577\n",
      "Train: step:  75540, time: 0.261, loss: 2402.712402\n",
      "Train: step:  75550, time: 0.228, loss: 1227.452515\n",
      "Train: step:  75560, time: 0.255, loss: 1771.089478\n",
      "Train: step:  75570, time: 0.231, loss: 4651.304688\n",
      "Train: step:  75580, time: 0.242, loss: 479.116638\n",
      "Train: step:  75590, time: 0.259, loss: 1239.342529\n",
      "Train: step:  75600, time: 0.253, loss: 1769.464722\n",
      "Train: step:  75610, time: 0.230, loss: 1397.869751\n",
      "Train: step:  75620, time: 0.225, loss: 1877.528076\n",
      "Train: step:  75630, time: 0.232, loss: 2814.760010\n",
      "Train: step:  75640, time: 0.219, loss: 3015.583008\n",
      "Train: step:  75650, time: 0.219, loss: 1105.591064\n",
      "Train: step:  75660, time: 0.243, loss: 763.921021\n",
      "Train: step:  75670, time: 0.232, loss: 2994.573975\n",
      "Train: step:  75680, time: 0.227, loss: 673.979065\n",
      "Train: step:  75690, time: 0.255, loss: 1635.110596\n",
      "Train: step:  75700, time: 0.271, loss: 1609.874268\n",
      "Train: step:  75710, time: 0.235, loss: 2597.379150\n",
      "Train: step:  75720, time: 0.238, loss: 1460.224731\n",
      "Train: step:  75730, time: 0.232, loss: 2641.639893\n",
      "Train: step:  75740, time: 0.222, loss: 620.256714\n",
      "Train: step:  75750, time: 0.252, loss: 2933.370850\n",
      "Train: step:  75760, time: 0.263, loss: 1973.732666\n",
      "Train: step:  75770, time: 0.229, loss: 2079.578857\n",
      "Train: step:  75780, time: 0.221, loss: 1561.110107\n",
      "Train: step:  75790, time: 0.260, loss: 3084.333008\n",
      "Train: step:  75800, time: 0.228, loss: 1386.204834\n",
      "Train: step:  75810, time: 0.230, loss: 2159.189941\n",
      "Train: step:  75820, time: 0.233, loss: 3169.613037\n",
      "Train: step:  75830, time: 0.247, loss: 2942.121582\n",
      "Train: step:  75840, time: 0.238, loss: 1474.322632\n",
      "Train: step:  75850, time: 0.227, loss: 446.428192\n",
      "Train: step:  75860, time: 0.260, loss: 1077.870483\n",
      "Train: step:  75870, time: 0.225, loss: 3886.526123\n",
      "Train: step:  75880, time: 0.256, loss: 844.356812\n",
      "Train: step:  75890, time: 0.229, loss: 2021.304321\n",
      "Train: step:  75900, time: 0.231, loss: 988.278198\n",
      "Train: step:  75910, time: 0.224, loss: 342.246552\n",
      "Train: step:  75920, time: 0.227, loss: 3129.999512\n",
      "Train: step:  75930, time: 0.238, loss: 4056.131104\n",
      "Train: step:  75940, time: 0.227, loss: 663.843323\n",
      "Train: step:  75950, time: 0.222, loss: 562.831970\n",
      "Train: step:  75960, time: 0.252, loss: 2612.129395\n",
      "Train: step:  75970, time: 0.253, loss: 2317.805420\n",
      "Train: step:  75980, time: 0.228, loss: 1810.213379\n",
      "Train: step:  75990, time: 0.240, loss: 1425.597046\n",
      "Train: step:  76000, time: 0.237, loss: 1950.913818\n",
      "Train: step:  76010, time: 0.257, loss: 1403.251587\n",
      "Train: step:  76020, time: 0.235, loss: 1748.358276\n",
      "Train: step:  76030, time: 0.237, loss: 2108.933105\n",
      "Train: step:  76040, time: 0.239, loss: 3679.359375\n",
      "Train: step:  76050, time: 0.224, loss: 3406.429443\n",
      "Train: step:  76060, time: 0.250, loss: 2432.643799\n",
      "Train: step:  76070, time: 0.227, loss: 2407.881836\n",
      "Train: step:  76080, time: 0.254, loss: 2311.283936\n",
      "Train: step:  76090, time: 0.230, loss: 398.016357\n",
      "Train: step:  76100, time: 0.251, loss: 2757.633057\n",
      "Train: step:  76110, time: 0.222, loss: 1090.955322\n",
      "Train: step:  76120, time: 0.227, loss: 1456.035034\n",
      "Train: step:  76130, time: 0.228, loss: 2636.632080\n",
      "Train: step:  76140, time: 0.222, loss: 509.501709\n",
      "Train: step:  76150, time: 0.218, loss: 2009.299805\n",
      "Train: step:  76160, time: 0.223, loss: 1771.490234\n",
      "Train: step:  76170, time: 0.253, loss: 1481.180664\n",
      "Train: step:  76180, time: 0.229, loss: 2101.654053\n",
      "Train: step:  76190, time: 0.264, loss: 1023.512329\n",
      "Train: step:  76200, time: 0.230, loss: 2414.928955\n",
      "Train: step:  76210, time: 0.250, loss: 1395.950195\n",
      "Train: step:  76220, time: 0.235, loss: 1063.364990\n",
      "Train: step:  76230, time: 0.230, loss: 2286.994873\n",
      "Train: step:  76240, time: 0.256, loss: 1762.805420\n",
      "Train: step:  76250, time: 0.228, loss: 2750.781982\n",
      "Train: step:  76260, time: 0.221, loss: 1693.880493\n",
      "Train: step:  76270, time: 0.223, loss: 2523.583008\n",
      "Train: step:  76280, time: 0.228, loss: 1769.938232\n",
      "Train: step:  76290, time: 0.254, loss: 670.341614\n",
      "Train: step:  76300, time: 0.229, loss: 1211.805298\n",
      "Train: step:  76310, time: 0.244, loss: 1138.038086\n",
      "Train: step:  76320, time: 0.238, loss: 1513.374146\n",
      "Train: step:  76330, time: 0.232, loss: 2208.211670\n",
      "Train: step:  76340, time: 0.222, loss: 1018.949890\n",
      "Train: step:  76350, time: 0.228, loss: 3510.629639\n",
      "Train: step:  76360, time: 0.234, loss: 2481.980469\n",
      "Train: step:  76370, time: 0.229, loss: 1816.141113\n",
      "Train: step:  76380, time: 0.239, loss: 3393.989746\n",
      "Train: step:  76390, time: 0.234, loss: 2186.235352\n",
      "Train: step:  76400, time: 0.254, loss: 1447.140503\n",
      "Train: step:  76410, time: 0.255, loss: 1774.180298\n",
      "Train: step:  76420, time: 0.232, loss: 1754.943237\n",
      "Train: step:  76430, time: 0.259, loss: 2352.353760\n",
      "Train: step:  76440, time: 0.258, loss: 2313.054443\n",
      "Train: step:  76450, time: 0.235, loss: 1816.441284\n",
      "Train: step:  76460, time: 0.235, loss: 1484.356567\n",
      "Train: step:  76470, time: 0.227, loss: 1929.492554\n",
      "Train: step:  76480, time: 0.260, loss: 2796.501221\n",
      "Train: step:  76490, time: 0.267, loss: 2649.091064\n",
      "Train: step:  76500, time: 0.255, loss: 1450.373779\n",
      "Train: step:  76510, time: 0.258, loss: 2468.649414\n",
      "Train: step:  76520, time: 0.229, loss: 1200.094727\n",
      "Train: step:  76530, time: 0.226, loss: 2560.205078\n",
      "Train: step:  76540, time: 0.257, loss: 901.912354\n",
      "Train: step:  76550, time: 0.255, loss: 1390.379028\n",
      "Train: step:  76560, time: 0.225, loss: 1601.885986\n",
      "Train: step:  76570, time: 0.222, loss: 1505.616577\n",
      "Train: step:  76580, time: 0.269, loss: 1017.084045\n",
      "Train: step:  76590, time: 0.223, loss: 2338.699219\n",
      "Train: step:  76600, time: 0.237, loss: 2097.033936\n",
      "Train: step:  76610, time: 0.256, loss: 2069.598145\n",
      "Train: step:  76620, time: 0.228, loss: 874.427307\n",
      "Train: step:  76630, time: 0.226, loss: 602.041260\n",
      "Train: step:  76640, time: 0.229, loss: 2641.830566\n",
      "Train: step:  76650, time: 0.223, loss: 2114.877197\n",
      "Train: step:  76660, time: 0.252, loss: 1623.579346\n",
      "Train: step:  76670, time: 0.225, loss: 410.111877\n",
      "Train: step:  76680, time: 0.234, loss: 314.982300\n",
      "Train: step:  76690, time: 0.230, loss: 2838.656982\n",
      "Train: step:  76700, time: 0.233, loss: 1577.186646\n",
      "Train: step:  76710, time: 0.224, loss: 2944.553711\n",
      "Train: step:  76720, time: 0.230, loss: 2325.575439\n",
      "Train: step:  76730, time: 0.227, loss: 1603.211304\n",
      "Train: step:  76740, time: 0.240, loss: 3893.865479\n",
      "Train: step:  76750, time: 0.268, loss: 1729.641357\n",
      "Train: step:  76760, time: 0.225, loss: 2112.858887\n",
      "Train: step:  76770, time: 0.253, loss: 2097.318359\n",
      "Train: step:  76780, time: 0.225, loss: 351.270233\n",
      "Train: step:  76790, time: 0.233, loss: 2906.167236\n",
      "Train: step:  76800, time: 0.233, loss: 956.791077\n",
      "Train: step:  76810, time: 0.247, loss: 1411.529297\n",
      "Train: step:  76820, time: 0.265, loss: 1488.722290\n",
      "Train: step:  76830, time: 0.235, loss: 2465.570801\n",
      "Train: step:  76840, time: 0.227, loss: 2754.798584\n",
      "Train: step:  76850, time: 0.220, loss: 2658.571045\n",
      "Train: step:  76860, time: 0.232, loss: 2578.624512\n",
      "Train: step:  76870, time: 0.256, loss: 1231.263184\n",
      "Train: step:  76880, time: 0.225, loss: 2204.970703\n",
      "Train: step:  76890, time: 0.234, loss: 2891.933838\n",
      "Train: step:  76900, time: 0.235, loss: 1006.634949\n",
      "Train: step:  76910, time: 0.237, loss: 1171.933838\n",
      "Train: step:  76920, time: 0.237, loss: 3388.427979\n",
      "Train: step:  76930, time: 0.234, loss: 994.061340\n",
      "Train: step:  76940, time: 0.227, loss: 3686.819824\n",
      "Train: step:  76950, time: 0.237, loss: 2379.823730\n",
      "Train: step:  76960, time: 0.234, loss: 2279.294189\n",
      "Train: step:  76970, time: 0.232, loss: 1807.082642\n",
      "Train: step:  76980, time: 0.241, loss: 985.764221\n",
      "Train: step:  76990, time: 0.229, loss: 880.929321\n",
      "Train: step:  77000, time: 0.237, loss: 563.737671\n",
      "Train: step:  77010, time: 0.230, loss: 3087.859619\n",
      "Train: step:  77020, time: 0.244, loss: 2750.268799\n",
      "Train: step:  77030, time: 0.232, loss: 2999.707520\n",
      "Train: step:  77040, time: 0.234, loss: 1386.907593\n",
      "Train: step:  77050, time: 0.226, loss: 1301.930420\n",
      "Train: step:  77060, time: 0.225, loss: 520.118591\n",
      "Train: step:  77070, time: 0.223, loss: 1572.229126\n",
      "Train: step:  77080, time: 0.236, loss: 2028.549805\n",
      "Train: step:  77090, time: 0.239, loss: 2289.479736\n",
      "Train: step:  77100, time: 0.258, loss: 1869.942017\n",
      "Train: step:  77110, time: 0.226, loss: 1948.262573\n",
      "Train: step:  77120, time: 0.226, loss: 949.045410\n",
      "Train: step:  77130, time: 0.231, loss: 1152.895752\n",
      "Train: step:  77140, time: 0.229, loss: 689.327087\n",
      "Train: step:  77150, time: 0.226, loss: 460.938538\n",
      "Train: step:  77160, time: 0.244, loss: 3207.355713\n",
      "Train: step:  77170, time: 0.229, loss: 3544.013916\n",
      "Train: step:  77180, time: 0.228, loss: 529.337280\n",
      "Train: step:  77190, time: 0.260, loss: 1522.811890\n",
      "Train: step:  77200, time: 0.227, loss: 1107.307129\n",
      "Train: step:  77210, time: 0.225, loss: 1341.173218\n",
      "Train: step:  77220, time: 0.235, loss: 963.223206\n",
      "Train: step:  77230, time: 0.219, loss: 3578.619629\n",
      "Train: step:  77240, time: 0.235, loss: 3528.714355\n",
      "Train: step:  77250, time: 0.228, loss: 3918.575928\n",
      "Train: step:  77260, time: 0.231, loss: 994.607544\n",
      "Train: step:  77270, time: 0.224, loss: 1168.098999\n",
      "Train: step:  77280, time: 0.235, loss: 837.245850\n",
      "Train: step:  77290, time: 0.234, loss: 3037.426758\n",
      "Train: step:  77300, time: 0.232, loss: 1650.408813\n",
      "Train: step:  77310, time: 0.250, loss: 1826.521362\n",
      "Train: step:  77320, time: 0.234, loss: 2707.198242\n",
      "Train: step:  77330, time: 0.229, loss: 695.561401\n",
      "Train: step:  77340, time: 0.251, loss: 2966.214844\n",
      "Train: step:  77350, time: 0.223, loss: 589.685486\n",
      "Train: step:  77360, time: 0.223, loss: 3303.904541\n",
      "Train: step:  77370, time: 0.238, loss: 1082.616089\n",
      "Train: step:  77380, time: 0.261, loss: 1699.183105\n",
      "Train: step:  77390, time: 0.262, loss: 1884.222168\n",
      "Train: step:  77400, time: 0.236, loss: 3481.779297\n",
      "Train: step:  77410, time: 0.253, loss: 479.581757\n",
      "Train: step:  77420, time: 0.237, loss: 2943.779785\n",
      "Train: step:  77430, time: 0.237, loss: 2411.498779\n",
      "Train: step:  77440, time: 0.225, loss: 1426.897095\n",
      "Train: step:  77450, time: 0.226, loss: 1363.870239\n",
      "Train: step:  77460, time: 0.229, loss: 3293.872070\n",
      "Train: step:  77470, time: 0.227, loss: 1906.537598\n",
      "Train: step:  77480, time: 0.231, loss: 852.205750\n",
      "Train: step:  77490, time: 0.237, loss: 2138.769043\n",
      "Train: step:  77500, time: 0.230, loss: 4494.369629\n",
      "Train: step:  77510, time: 0.273, loss: 1159.001587\n",
      "Train: step:  77520, time: 0.234, loss: 4530.699707\n",
      "Train: step:  77530, time: 0.239, loss: 1377.032227\n",
      "Train: step:  77540, time: 0.229, loss: 1899.874023\n",
      "Train: step:  77550, time: 0.246, loss: 441.141968\n",
      "Train: step:  77560, time: 0.273, loss: 743.261230\n",
      "Train: step:  77570, time: 0.272, loss: 1754.142578\n",
      "Train: step:  77580, time: 0.271, loss: 731.355591\n",
      "Train: step:  77590, time: 0.253, loss: 1374.608398\n",
      "Train: step:  77600, time: 0.252, loss: 2845.166260\n",
      "Train: step:  77610, time: 0.231, loss: 1039.700439\n",
      "Train: step:  77620, time: 0.224, loss: 1928.440430\n",
      "Train: step:  77630, time: 0.229, loss: 1438.255249\n",
      "Train: step:  77640, time: 0.229, loss: 1168.888916\n",
      "Train: step:  77650, time: 0.235, loss: 2974.525391\n",
      "Train: step:  77660, time: 0.237, loss: 1018.226379\n",
      "Train: step:  77670, time: 0.224, loss: 580.588562\n",
      "Train: step:  77680, time: 0.272, loss: 872.765747\n",
      "Train: step:  77690, time: 0.223, loss: 190.542313\n",
      "Train: step:  77700, time: 0.228, loss: 733.636963\n",
      "Train: step:  77710, time: 0.229, loss: 407.120239\n",
      "Train: step:  77720, time: 0.230, loss: 1362.652344\n",
      "Train: step:  77730, time: 0.236, loss: 1815.799072\n",
      "Train: step:  77740, time: 0.259, loss: 2250.023682\n",
      "Train: step:  77750, time: 0.253, loss: 446.751587\n",
      "Train: step:  77760, time: 0.267, loss: 2818.525146\n",
      "Train: step:  77770, time: 0.230, loss: 1281.122070\n",
      "Train: step:  77780, time: 0.225, loss: 1326.718628\n",
      "Train: step:  77790, time: 0.234, loss: 564.355957\n",
      "Train: step:  77800, time: 0.223, loss: 801.258667\n",
      "Train: step:  77810, time: 0.263, loss: 1824.670532\n",
      "Train: step:  77820, time: 0.226, loss: 1469.054199\n",
      "Train: step:  77830, time: 0.231, loss: 3386.001953\n",
      "Train: step:  77840, time: 0.225, loss: 566.838806\n",
      "Train: step:  77850, time: 0.232, loss: 1707.925537\n",
      "Train: step:  77860, time: 0.227, loss: 1558.739258\n",
      "Train: step:  77870, time: 0.281, loss: 1671.142456\n",
      "Train: step:  77880, time: 0.223, loss: 647.630493\n",
      "Train: step:  77890, time: 0.227, loss: 1228.916748\n",
      "Train: step:  77900, time: 0.271, loss: 2911.266602\n",
      "Train: step:  77910, time: 0.228, loss: 2822.221191\n",
      "Train: step:  77920, time: 0.267, loss: 2220.909424\n",
      "Train: step:  77930, time: 0.227, loss: 660.390869\n",
      "Train: step:  77940, time: 0.243, loss: 2250.470459\n",
      "Train: step:  77950, time: 0.258, loss: 1741.442017\n",
      "Train: step:  77960, time: 0.227, loss: 454.171478\n",
      "Train: step:  77970, time: 0.237, loss: 298.222382\n",
      "Train: step:  77980, time: 0.230, loss: 953.866943\n",
      "Train: step:  77990, time: 0.385, loss: 1115.207642\n",
      "Train: step:  78000, time: 0.233, loss: 440.989563\n",
      "Train: step:  78010, time: 0.264, loss: 2401.508057\n",
      "Train: step:  78020, time: 0.271, loss: 942.864075\n",
      "Train: step:  78030, time: 0.254, loss: 723.884216\n",
      "Train: step:  78040, time: 0.255, loss: 884.913940\n",
      "Train: step:  78050, time: 0.231, loss: 1041.269043\n",
      "Train: step:  78060, time: 0.231, loss: 1894.700684\n",
      "Train: step:  78070, time: 0.240, loss: 1957.736450\n",
      "Train: step:  78080, time: 0.227, loss: 2694.425537\n",
      "Train: step:  78090, time: 0.230, loss: 2050.533936\n",
      "Train: step:  78100, time: 0.229, loss: 1921.707031\n",
      "Train: step:  78110, time: 0.218, loss: 2746.886475\n",
      "Train: step:  78120, time: 0.230, loss: 437.423004\n",
      "Train: step:  78130, time: 0.226, loss: 358.069733\n",
      "Train: step:  78140, time: 0.234, loss: 1625.903442\n",
      "Train: step:  78150, time: 0.225, loss: 740.460327\n",
      "Train: step:  78160, time: 0.228, loss: 555.628601\n",
      "Train: step:  78170, time: 0.236, loss: 1290.874023\n",
      "Train: step:  78180, time: 0.262, loss: 3027.271484\n",
      "Train: step:  78190, time: 0.222, loss: 1507.044434\n",
      "Train: step:  78200, time: 0.224, loss: 1262.913208\n",
      "Train: step:  78210, time: 0.248, loss: 2008.201172\n",
      "Train: step:  78220, time: 0.237, loss: 881.498596\n",
      "Train: step:  78230, time: 0.228, loss: 802.979248\n",
      "Train: step:  78240, time: 0.262, loss: 3017.514648\n",
      "Train: step:  78250, time: 0.230, loss: 2760.573730\n",
      "Train: step:  78260, time: 0.256, loss: 1259.236694\n",
      "Train: step:  78270, time: 0.229, loss: 1214.898071\n",
      "Train: step:  78280, time: 0.258, loss: 979.770935\n",
      "Train: step:  78290, time: 0.228, loss: 3280.880859\n",
      "Train: step:  78300, time: 0.227, loss: 3357.723633\n",
      "Train: step:  78310, time: 0.229, loss: 956.080505\n",
      "Train: step:  78320, time: 0.225, loss: 2925.568115\n",
      "Train: step:  78330, time: 0.232, loss: 829.810791\n",
      "Train: step:  78340, time: 0.219, loss: 2143.833252\n",
      "Train: step:  78350, time: 0.244, loss: 479.638275\n",
      "Train: step:  78360, time: 0.243, loss: 2073.471680\n",
      "Train: step:  78370, time: 0.255, loss: 2903.601807\n",
      "Train: step:  78380, time: 0.233, loss: 837.324158\n",
      "Train: step:  78390, time: 0.228, loss: 4360.598145\n",
      "Train: step:  78400, time: 0.224, loss: 1536.023071\n",
      "Train: step:  78410, time: 0.222, loss: 2260.233643\n",
      "Train: step:  78420, time: 0.226, loss: 538.243103\n",
      "Train: step:  78430, time: 0.244, loss: 1969.294189\n",
      "Train: step:  78440, time: 0.276, loss: 750.242615\n",
      "Train: step:  78450, time: 0.249, loss: 3898.321045\n",
      "Train: step:  78460, time: 0.226, loss: 2142.958496\n",
      "Train: step:  78470, time: 0.229, loss: 2004.322876\n",
      "Train: step:  78480, time: 0.239, loss: 2439.076660\n",
      "Train: step:  78490, time: 0.250, loss: 1224.777344\n",
      "Train: step:  78500, time: 0.223, loss: 1442.337280\n",
      "Train: step:  78510, time: 0.228, loss: 987.330200\n",
      "Train: step:  78520, time: 0.232, loss: 1639.900635\n",
      "Train: step:  78530, time: 0.225, loss: 881.112122\n",
      "Train: step:  78540, time: 0.232, loss: 707.479004\n",
      "Train: step:  78550, time: 0.228, loss: 1676.094849\n",
      "Train: step:  78560, time: 0.258, loss: 1763.525635\n",
      "Train: step:  78570, time: 0.256, loss: 1374.351685\n",
      "Train: step:  78580, time: 0.231, loss: 1742.726929\n",
      "Train: step:  78590, time: 0.228, loss: 905.917358\n",
      "Train: step:  78600, time: 0.226, loss: 2089.201416\n",
      "Train: step:  78610, time: 0.255, loss: 2755.478760\n",
      "Train: step:  78620, time: 0.231, loss: 2208.517578\n",
      "Train: step:  78630, time: 0.236, loss: 2644.114258\n",
      "Train: step:  78640, time: 0.234, loss: 2320.978760\n",
      "Train: step:  78650, time: 0.229, loss: 925.787415\n",
      "Train: step:  78660, time: 0.231, loss: 874.370544\n",
      "Train: step:  78670, time: 0.225, loss: 2591.946533\n",
      "Train: step:  78680, time: 0.237, loss: 1097.313843\n",
      "Train: step:  78690, time: 0.235, loss: 1940.205322\n",
      "Train: step:  78700, time: 0.229, loss: 1694.418701\n",
      "Train: step:  78710, time: 0.231, loss: 479.544861\n",
      "Train: step:  78720, time: 0.258, loss: 486.753418\n",
      "Train: step:  78730, time: 0.258, loss: 573.625000\n",
      "Train: step:  78740, time: 0.253, loss: 841.466675\n",
      "Train: step:  78750, time: 0.272, loss: 4776.869629\n",
      "Train: step:  78760, time: 0.244, loss: 2628.629395\n",
      "Train: step:  78770, time: 0.232, loss: 2998.280029\n",
      "Train: step:  78780, time: 0.228, loss: 1429.055786\n",
      "Train: step:  78790, time: 0.236, loss: 1801.288208\n",
      "Train: step:  78800, time: 0.266, loss: 2096.698730\n",
      "Train: step:  78810, time: 0.233, loss: 1612.038330\n",
      "Train: step:  78820, time: 0.230, loss: 1973.533081\n",
      "Train: step:  78830, time: 0.222, loss: 2401.751221\n",
      "Train: step:  78840, time: 0.237, loss: 777.315735\n",
      "Train: step:  78850, time: 0.253, loss: 1225.097412\n",
      "Train: step:  78860, time: 0.232, loss: 562.399170\n",
      "Train: step:  78870, time: 0.229, loss: 1874.195435\n",
      "Train: step:  78880, time: 0.217, loss: 1102.027832\n",
      "Train: step:  78890, time: 0.231, loss: 989.020447\n",
      "Train: step:  78900, time: 0.225, loss: 2819.321289\n",
      "Train: step:  78910, time: 0.231, loss: 1091.402588\n",
      "Train: step:  78920, time: 0.233, loss: 980.827271\n",
      "Train: step:  78930, time: 0.231, loss: 671.804138\n",
      "Train: step:  78940, time: 0.223, loss: 3092.727783\n",
      "Train: step:  78950, time: 0.235, loss: 2101.432617\n",
      "Train: step:  78960, time: 0.254, loss: 2085.416992\n",
      "Train: step:  78970, time: 0.231, loss: 1623.323364\n",
      "Train: step:  78980, time: 0.235, loss: 2158.897461\n",
      "Train: step:  78990, time: 0.224, loss: 1332.227539\n",
      "Train: step:  79000, time: 0.254, loss: 473.375488\n",
      "Train: step:  79010, time: 0.261, loss: 1360.828369\n",
      "Train: step:  79020, time: 0.236, loss: 1300.130859\n",
      "Train: step:  79030, time: 0.234, loss: 553.716614\n",
      "Train: step:  79040, time: 0.269, loss: 1788.949707\n",
      "Train: step:  79050, time: 0.263, loss: 2006.322388\n",
      "Train: step:  79060, time: 0.230, loss: 1091.025024\n",
      "Train: step:  79070, time: 0.228, loss: 2021.642700\n",
      "Train: step:  79080, time: 0.235, loss: 1683.027954\n",
      "Train: step:  79090, time: 0.234, loss: 1431.779541\n",
      "Train: step:  79100, time: 0.233, loss: 1143.017944\n",
      "Train: step:  79110, time: 0.230, loss: 1305.506958\n",
      "Train: step:  79120, time: 0.224, loss: 1004.625000\n",
      "Train: step:  79130, time: 0.223, loss: 2333.001221\n",
      "Train: step:  79140, time: 0.256, loss: 3062.131104\n",
      "Train: step:  79150, time: 0.221, loss: 1666.451172\n",
      "Train: step:  79160, time: 0.227, loss: 1557.496216\n",
      "Train: step:  79170, time: 0.254, loss: 4880.798340\n",
      "Train: step:  79180, time: 0.228, loss: 2506.690918\n",
      "Train: step:  79190, time: 0.220, loss: 2031.847412\n",
      "Train: step:  79200, time: 0.228, loss: 1403.114868\n",
      "Train: step:  79210, time: 0.226, loss: 1220.705200\n",
      "Train: step:  79220, time: 0.221, loss: 2105.605957\n",
      "Train: step:  79230, time: 0.230, loss: 4247.288086\n",
      "Train: step:  79240, time: 0.266, loss: 1441.451294\n",
      "Train: step:  79250, time: 0.227, loss: 1877.090332\n",
      "Train: step:  79260, time: 0.232, loss: 280.603027\n",
      "Train: step:  79270, time: 0.263, loss: 2130.774414\n",
      "Train: step:  79280, time: 0.231, loss: 655.070984\n",
      "Train: step:  79290, time: 0.233, loss: 1606.140625\n",
      "Train: step:  79300, time: 0.226, loss: 980.811890\n",
      "Train: step:  79310, time: 0.247, loss: 421.253693\n",
      "Train: step:  79320, time: 0.223, loss: 710.104309\n",
      "Train: step:  79330, time: 0.264, loss: 1608.187988\n",
      "Train: step:  79340, time: 0.224, loss: 3058.590576\n",
      "Train: step:  79350, time: 0.220, loss: 2123.462891\n",
      "Train: step:  79360, time: 0.228, loss: 427.140656\n",
      "Train: step:  79370, time: 0.227, loss: 2353.763184\n",
      "Train: step:  79380, time: 0.255, loss: 3444.955078\n",
      "Train: step:  79390, time: 0.229, loss: 2502.356689\n",
      "Train: step:  79400, time: 0.227, loss: 471.674896\n",
      "Train: step:  79410, time: 0.258, loss: 2235.013428\n",
      "Train: step:  79420, time: 0.228, loss: 1845.238403\n",
      "Train: step:  79430, time: 0.223, loss: 1926.338623\n",
      "Train: step:  79440, time: 0.235, loss: 1430.562622\n",
      "Train: step:  79450, time: 0.223, loss: 1953.609131\n",
      "Train: step:  79460, time: 0.223, loss: 2248.369629\n",
      "Train: step:  79470, time: 0.220, loss: 1651.238770\n",
      "Train: step:  79480, time: 0.273, loss: 2087.726807\n",
      "Train: step:  79490, time: 0.225, loss: 1745.740234\n",
      "Train: step:  79500, time: 0.233, loss: 2959.858154\n",
      "Train: step:  79510, time: 0.222, loss: 3304.298340\n",
      "Train: step:  79520, time: 0.225, loss: 2653.438477\n",
      "Train: step:  79530, time: 0.249, loss: 3179.162598\n",
      "Train: step:  79540, time: 0.226, loss: 3227.454590\n",
      "Train: step:  79550, time: 0.238, loss: 2431.315430\n",
      "Train: step:  79560, time: 0.231, loss: 1485.902466\n",
      "Train: step:  79570, time: 0.219, loss: 1032.361206\n",
      "Train: step:  79580, time: 0.231, loss: 1321.091064\n",
      "Train: step:  79590, time: 0.273, loss: 3424.491455\n",
      "Train: step:  79600, time: 0.227, loss: 1519.111084\n",
      "Train: step:  79610, time: 0.232, loss: 1013.336548\n",
      "Train: step:  79620, time: 0.225, loss: 2017.076538\n",
      "Train: step:  79630, time: 0.259, loss: 1704.242920\n",
      "Train: step:  79640, time: 0.231, loss: 772.714111\n",
      "Train: step:  79650, time: 0.256, loss: 2255.028809\n",
      "Train: step:  79660, time: 0.239, loss: 664.664673\n",
      "Train: step:  79670, time: 0.254, loss: 1756.361572\n",
      "Train: step:  79680, time: 0.230, loss: 3594.696289\n",
      "Train: step:  79690, time: 0.233, loss: 2548.213623\n",
      "Train: step:  79700, time: 0.256, loss: 2631.691650\n",
      "Train: step:  79710, time: 0.234, loss: 481.847321\n",
      "Train: step:  79720, time: 0.225, loss: 1448.386963\n",
      "Train: step:  79730, time: 0.243, loss: 2376.891602\n",
      "Train: step:  79740, time: 0.236, loss: 4249.560059\n",
      "Train: step:  79750, time: 0.230, loss: 1832.577393\n",
      "Train: step:  79760, time: 0.267, loss: 782.594849\n",
      "Train: step:  79770, time: 0.226, loss: 2401.873291\n",
      "Train: step:  79780, time: 0.251, loss: 1787.205933\n",
      "Train: step:  79790, time: 0.253, loss: 1127.726318\n",
      "Train: step:  79800, time: 0.223, loss: 2127.585693\n",
      "Train: step:  79810, time: 0.223, loss: 1487.787842\n",
      "Train: step:  79820, time: 0.230, loss: 2324.446289\n",
      "Train: step:  79830, time: 0.269, loss: 1592.314209\n",
      "Train: step:  79840, time: 0.222, loss: 1432.170654\n",
      "Train: step:  79850, time: 0.226, loss: 2333.112793\n",
      "Train: step:  79860, time: 0.222, loss: 2732.583496\n",
      "Train: step:  79870, time: 0.232, loss: 3005.536865\n",
      "Train: step:  79880, time: 0.223, loss: 468.901794\n",
      "Train: step:  79890, time: 0.267, loss: 1264.961670\n",
      "Train: step:  79900, time: 0.226, loss: 2386.186279\n",
      "Train: step:  79910, time: 0.219, loss: 1832.060303\n",
      "Train: step:  79920, time: 0.228, loss: 2028.374390\n",
      "Train: step:  79930, time: 0.261, loss: 1360.198853\n",
      "Train: step:  79940, time: 0.227, loss: 1828.670044\n",
      "Train: step:  79950, time: 0.235, loss: 810.633240\n",
      "Train: step:  79960, time: 0.226, loss: 400.965881\n",
      "Train: step:  79970, time: 0.254, loss: 906.399475\n",
      "Train: step:  79980, time: 0.233, loss: 2782.063721\n",
      "Train: step:  79990, time: 0.228, loss: 2873.349121\n",
      "Train: step:  80000, time: 0.220, loss: 364.750183\n",
      "Train: step:  80010, time: 0.224, loss: 2669.664062\n",
      "Train: step:  80020, time: 0.237, loss: 1916.196289\n",
      "Train: step:  80030, time: 0.254, loss: 1768.910156\n",
      "Train: step:  80040, time: 0.232, loss: 1630.703125\n",
      "Train: step:  80050, time: 0.252, loss: 2224.568604\n",
      "Train: step:  80060, time: 0.231, loss: 3293.831055\n",
      "Train: step:  80070, time: 0.228, loss: 792.974548\n",
      "Train: step:  80080, time: 0.222, loss: 1408.729248\n",
      "Train: step:  80090, time: 0.227, loss: 2245.200439\n",
      "Train: step:  80100, time: 0.221, loss: 1858.111938\n",
      "Train: step:  80110, time: 0.220, loss: 1603.352905\n",
      "Train: step:  80120, time: 0.224, loss: 1685.427734\n",
      "Train: step:  80130, time: 0.233, loss: 1877.337891\n",
      "Train: step:  80140, time: 0.227, loss: 1626.012329\n",
      "Train: step:  80150, time: 0.224, loss: 1176.924438\n",
      "Train: step:  80160, time: 0.230, loss: 1120.026489\n",
      "Train: step:  80170, time: 0.233, loss: 992.346863\n",
      "Train: step:  80180, time: 0.223, loss: 355.161987\n",
      "Train: step:  80190, time: 0.253, loss: 641.913147\n",
      "Train: step:  80200, time: 0.257, loss: 1419.110107\n",
      "Train: step:  80210, time: 0.249, loss: 2081.937500\n",
      "Train: step:  80220, time: 0.223, loss: 1654.629639\n",
      "Train: step:  80230, time: 0.229, loss: 380.903595\n",
      "Train: step:  80240, time: 0.250, loss: 850.151855\n",
      "Train: step:  80250, time: 0.226, loss: 1141.363281\n",
      "Train: step:  80260, time: 0.222, loss: 2415.284180\n",
      "Train: step:  80270, time: 0.229, loss: 3207.100830\n",
      "Train: step:  80280, time: 0.240, loss: 2968.528320\n",
      "Train: step:  80290, time: 0.226, loss: 991.430908\n",
      "Train: step:  80300, time: 0.225, loss: 2598.141113\n",
      "Train: step:  80310, time: 0.232, loss: 2201.429932\n",
      "Train: step:  80320, time: 0.249, loss: 2363.636475\n",
      "Train: step:  80330, time: 0.240, loss: 2803.407227\n",
      "Train: step:  80340, time: 0.259, loss: 2677.556396\n",
      "Train: step:  80350, time: 0.224, loss: 2088.078857\n",
      "Train: step:  80360, time: 0.247, loss: 4237.267578\n",
      "Train: step:  80370, time: 0.251, loss: 2598.875732\n",
      "Train: step:  80380, time: 0.264, loss: 3331.258545\n",
      "Train: step:  80390, time: 0.222, loss: 668.097046\n",
      "Train: step:  80400, time: 0.244, loss: 3713.947998\n",
      "Train: step:  80410, time: 0.227, loss: 2410.467773\n",
      "Train: step:  80420, time: 0.235, loss: 2127.516113\n",
      "Train: step:  80430, time: 0.221, loss: 1542.805908\n",
      "Train: step:  80440, time: 0.298, loss: 777.790222\n",
      "Train: step:  80450, time: 0.262, loss: 698.712708\n",
      "Train: step:  80460, time: 0.222, loss: 317.732361\n",
      "Train: step:  80470, time: 0.222, loss: 1299.832031\n",
      "Train: step:  80480, time: 0.235, loss: 2451.967285\n",
      "Train: step:  80490, time: 0.223, loss: 1223.647095\n",
      "Train: step:  80500, time: 0.250, loss: 1783.312500\n",
      "Train: step:  80510, time: 0.231, loss: 1792.911011\n",
      "Train: step:  80520, time: 0.228, loss: 2202.543945\n",
      "Train: step:  80530, time: 0.226, loss: 2645.253662\n",
      "Train: step:  80540, time: 0.221, loss: 1232.947876\n",
      "Train: step:  80550, time: 0.259, loss: 1770.757324\n",
      "Train: step:  80560, time: 0.232, loss: 3165.896973\n",
      "Train: step:  80570, time: 0.244, loss: 2651.993652\n",
      "Train: step:  80580, time: 0.279, loss: 2469.232422\n",
      "Train: step:  80590, time: 0.229, loss: 1546.603760\n",
      "Train: step:  80600, time: 0.239, loss: 1801.240601\n",
      "Train: step:  80610, time: 0.256, loss: 2645.276855\n",
      "Train: step:  80620, time: 0.230, loss: 1389.521362\n",
      "Train: step:  80630, time: 0.276, loss: 4008.448486\n",
      "Train: step:  80640, time: 0.239, loss: 1566.667236\n",
      "Train: step:  80650, time: 0.221, loss: 1373.334473\n",
      "Train: step:  80660, time: 0.228, loss: 2254.799805\n",
      "Train: step:  80670, time: 0.233, loss: 3199.038330\n",
      "Train: step:  80680, time: 0.241, loss: 3181.261230\n",
      "Train: step:  80690, time: 0.234, loss: 4627.658203\n",
      "Train: step:  80700, time: 0.254, loss: 558.753967\n",
      "Train: step:  80710, time: 0.225, loss: 1503.734131\n",
      "Train: step:  80720, time: 0.263, loss: 2522.425537\n",
      "Train: step:  80730, time: 0.270, loss: 683.105225\n",
      "Train: step:  80740, time: 0.229, loss: 759.586243\n",
      "Train: step:  80750, time: 0.224, loss: 1381.006714\n",
      "Train: step:  80760, time: 0.227, loss: 2356.401855\n",
      "Train: step:  80770, time: 0.231, loss: 1880.499023\n",
      "Train: step:  80780, time: 0.230, loss: 3310.917725\n",
      "Train: step:  80790, time: 0.226, loss: 1337.120605\n",
      "Train: step:  80800, time: 0.257, loss: 658.786255\n",
      "Train: step:  80810, time: 0.233, loss: 946.410889\n",
      "Train: step:  80820, time: 0.265, loss: 1062.562012\n",
      "Train: step:  80830, time: 0.236, loss: 1896.203003\n",
      "Train: step:  80840, time: 0.238, loss: 2281.509033\n",
      "Train: step:  80850, time: 0.253, loss: 2197.406494\n",
      "Train: step:  80860, time: 0.269, loss: 2700.687988\n",
      "Train: step:  80870, time: 0.238, loss: 1647.655762\n",
      "Train: step:  80880, time: 0.244, loss: 1313.155273\n",
      "Train: step:  80890, time: 0.225, loss: 2001.498169\n",
      "Train: step:  80900, time: 0.259, loss: 395.694244\n",
      "Train: step:  80910, time: 0.220, loss: 2248.968506\n",
      "Train: step:  80920, time: 0.229, loss: 2321.049072\n",
      "Train: step:  80930, time: 0.273, loss: 4992.951660\n",
      "Train: step:  80940, time: 0.252, loss: 2999.844482\n",
      "Train: step:  80950, time: 0.230, loss: 1829.308594\n",
      "Train: step:  80960, time: 0.240, loss: 3363.257080\n",
      "Train: step:  80970, time: 0.231, loss: 3988.728516\n",
      "Train: step:  80980, time: 0.221, loss: 2499.167480\n",
      "Train: step:  80990, time: 0.301, loss: 852.824646\n",
      "Train: step:  81000, time: 0.229, loss: 1550.720825\n",
      "Train: step:  81010, time: 0.233, loss: 2002.835938\n",
      "Train: step:  81020, time: 0.226, loss: 2178.597900\n",
      "Train: step:  81030, time: 0.227, loss: 1896.568604\n",
      "Train: step:  81040, time: 0.230, loss: 1971.418213\n",
      "Train: step:  81050, time: 0.226, loss: 1299.368164\n",
      "Train: step:  81060, time: 0.233, loss: 2926.746338\n",
      "Train: step:  81070, time: 0.258, loss: 1083.071777\n",
      "Train: step:  81080, time: 0.262, loss: 3570.283691\n",
      "Train: step:  81090, time: 0.222, loss: 1690.502563\n",
      "Train: step:  81100, time: 0.230, loss: 3665.448242\n",
      "Train: step:  81110, time: 0.232, loss: 4066.145264\n",
      "Train: step:  81120, time: 0.231, loss: 801.898804\n",
      "Train: step:  81130, time: 0.224, loss: 1712.268066\n",
      "Train: step:  81140, time: 0.234, loss: 1076.107300\n",
      "Train: step:  81150, time: 0.228, loss: 1016.145264\n",
      "Train: step:  81160, time: 0.263, loss: 1332.834717\n",
      "Train: step:  81170, time: 0.227, loss: 298.446960\n",
      "Train: step:  81180, time: 0.263, loss: 2205.282959\n",
      "Train: step:  81190, time: 0.253, loss: 1384.534058\n",
      "Train: step:  81200, time: 0.227, loss: 1436.602173\n",
      "Train: step:  81210, time: 0.229, loss: 1590.833130\n",
      "Train: step:  81220, time: 0.224, loss: 1707.448975\n",
      "Train: step:  81230, time: 0.240, loss: 1061.702026\n",
      "Train: step:  81240, time: 0.226, loss: 2381.741211\n",
      "Train: step:  81250, time: 0.220, loss: 2319.864258\n",
      "Train: step:  81260, time: 0.236, loss: 1632.192261\n",
      "Train: step:  81270, time: 0.253, loss: 3322.377930\n",
      "Train: step:  81280, time: 0.228, loss: 2059.373779\n",
      "Train: step:  81290, time: 0.221, loss: 2146.012939\n",
      "Train: step:  81300, time: 0.228, loss: 2868.230957\n",
      "Train: step:  81310, time: 0.222, loss: 879.609741\n",
      "Train: step:  81320, time: 0.232, loss: 679.602356\n",
      "Train: step:  81330, time: 0.268, loss: 951.687317\n",
      "Train: step:  81340, time: 0.227, loss: 2974.909668\n",
      "Train: step:  81350, time: 0.224, loss: 3219.811279\n",
      "Train: step:  81360, time: 0.227, loss: 707.872681\n",
      "Train: step:  81370, time: 0.223, loss: 2004.754150\n",
      "Train: step:  81380, time: 0.218, loss: 664.753540\n",
      "Train: step:  81390, time: 0.223, loss: 2436.532471\n",
      "Train: step:  81400, time: 0.232, loss: 2457.032959\n",
      "Train: step:  81410, time: 0.251, loss: 575.715332\n",
      "Train: step:  81420, time: 0.224, loss: 2767.043701\n",
      "Train: step:  81430, time: 0.228, loss: 2280.319580\n",
      "Train: step:  81440, time: 0.233, loss: 5951.772949\n",
      "Train: step:  81450, time: 0.239, loss: 302.211182\n",
      "Train: step:  81460, time: 0.226, loss: 669.041138\n",
      "Train: step:  81470, time: 0.259, loss: 2446.148193\n",
      "Train: step:  81480, time: 0.233, loss: 2322.376221\n",
      "Train: step:  81490, time: 0.242, loss: 749.796814\n",
      "Train: step:  81500, time: 0.231, loss: 3180.952637\n",
      "Train: step:  81510, time: 0.229, loss: 1717.029541\n",
      "Train: step:  81520, time: 0.238, loss: 1029.250000\n",
      "Train: step:  81530, time: 0.230, loss: 2324.048828\n",
      "Train: step:  81540, time: 0.259, loss: 1380.268066\n",
      "Train: step:  81550, time: 0.275, loss: 874.384888\n",
      "Train: step:  81560, time: 0.226, loss: 2195.910889\n",
      "Train: step:  81570, time: 0.248, loss: 1360.267212\n",
      "Train: step:  81580, time: 0.234, loss: 2243.048096\n",
      "Train: step:  81590, time: 0.231, loss: 1209.212646\n",
      "Train: step:  81600, time: 0.228, loss: 832.103638\n",
      "Train: step:  81610, time: 0.231, loss: 285.059021\n",
      "Train: step:  81620, time: 0.237, loss: 3502.790771\n",
      "Train: step:  81630, time: 0.275, loss: 569.913330\n",
      "Train: step:  81640, time: 0.246, loss: 1065.757690\n",
      "Train: step:  81650, time: 0.243, loss: 2553.839111\n",
      "Train: step:  81660, time: 0.243, loss: 2492.787354\n",
      "Train: step:  81670, time: 0.237, loss: 2110.750488\n",
      "Train: step:  81680, time: 0.266, loss: 2025.271973\n",
      "Train: step:  81690, time: 0.243, loss: 413.528381\n",
      "Train: step:  81700, time: 0.239, loss: 3587.131104\n",
      "Train: step:  81710, time: 0.247, loss: 643.178833\n",
      "Train: step:  81720, time: 0.239, loss: 3292.738037\n",
      "Train: step:  81730, time: 0.270, loss: 1569.108032\n",
      "Train: step:  81740, time: 0.266, loss: 2016.419312\n",
      "Train: step:  81750, time: 0.243, loss: 1771.358154\n",
      "Train: step:  81760, time: 0.264, loss: 2583.080566\n",
      "Train: step:  81770, time: 0.250, loss: 1627.198853\n",
      "Train: step:  81780, time: 0.258, loss: 2398.099365\n",
      "Train: step:  81790, time: 0.272, loss: 1771.372314\n",
      "Train: step:  81800, time: 0.267, loss: 1304.495117\n",
      "Train: step:  81810, time: 0.237, loss: 525.509583\n",
      "Train: step:  81820, time: 0.280, loss: 2426.612061\n",
      "Train: step:  81830, time: 0.245, loss: 3475.222656\n",
      "Train: step:  81840, time: 0.247, loss: 1162.515625\n",
      "Train: step:  81850, time: 0.250, loss: 2861.515869\n",
      "Train: step:  81860, time: 0.245, loss: 3318.418701\n",
      "Train: step:  81870, time: 0.280, loss: 2207.502930\n",
      "Train: step:  81880, time: 0.251, loss: 2548.162842\n",
      "Train: step:  81890, time: 0.250, loss: 979.099487\n",
      "Train: step:  81900, time: 0.246, loss: 1104.132935\n",
      "Train: step:  81910, time: 0.242, loss: 2271.031250\n",
      "Train: step:  81920, time: 0.237, loss: 2089.140869\n",
      "Train: step:  81930, time: 0.244, loss: 2699.213379\n",
      "Train: step:  81940, time: 0.241, loss: 1351.393311\n",
      "Train: step:  81950, time: 0.270, loss: 3098.039551\n",
      "Train: step:  81960, time: 0.237, loss: 1775.663574\n",
      "Train: step:  81970, time: 0.246, loss: 409.238495\n",
      "Train: step:  81980, time: 0.240, loss: 901.374451\n",
      "Train: step:  81990, time: 0.239, loss: 3868.076172\n",
      "Train: step:  82000, time: 0.240, loss: 1128.677490\n",
      "Train: step:  82010, time: 0.226, loss: 3292.388916\n",
      "Train: step:  82020, time: 0.266, loss: 1954.644165\n",
      "Train: step:  82030, time: 0.253, loss: 453.229523\n",
      "Train: step:  82040, time: 0.229, loss: 4094.856445\n",
      "Train: step:  82050, time: 0.232, loss: 2718.444336\n",
      "Train: step:  82060, time: 0.235, loss: 1997.891235\n",
      "Train: step:  82070, time: 0.230, loss: 1144.284180\n",
      "Train: step:  82080, time: 0.277, loss: 2959.350342\n",
      "Train: step:  82090, time: 0.237, loss: 2080.694092\n",
      "Train: step:  82100, time: 0.229, loss: 3304.650391\n",
      "Train: step:  82110, time: 0.235, loss: 1044.953491\n",
      "Train: step:  82120, time: 0.278, loss: 2779.162598\n",
      "Train: step:  82130, time: 0.236, loss: 567.427063\n",
      "Train: step:  82140, time: 0.255, loss: 643.970276\n",
      "Train: step:  82150, time: 0.269, loss: 2067.853516\n",
      "Train: step:  82160, time: 0.269, loss: 1215.890503\n",
      "Train: step:  82170, time: 0.256, loss: 2698.950928\n",
      "Train: step:  82180, time: 0.250, loss: 1049.045288\n",
      "Train: step:  82190, time: 0.249, loss: 3592.095459\n",
      "Train: step:  82200, time: 0.288, loss: 6596.286621\n",
      "Train: step:  82210, time: 0.249, loss: 3265.381592\n",
      "Train: step:  82220, time: 0.239, loss: 620.906982\n",
      "Train: step:  82230, time: 0.256, loss: 2166.707764\n",
      "Train: step:  82240, time: 0.264, loss: 1830.664429\n",
      "Train: step:  82250, time: 0.269, loss: 2013.236206\n",
      "Train: step:  82260, time: 0.236, loss: 2913.376953\n",
      "Train: step:  82270, time: 0.245, loss: 2574.926270\n",
      "Train: step:  82280, time: 0.240, loss: 1926.509766\n",
      "Train: step:  82290, time: 0.248, loss: 2563.943604\n",
      "Train: step:  82300, time: 0.265, loss: 498.078979\n",
      "Train: step:  82310, time: 0.243, loss: 3398.718262\n",
      "Train: step:  82320, time: 0.254, loss: 774.596436\n",
      "Train: step:  82330, time: 0.270, loss: 2450.894531\n",
      "Train: step:  82340, time: 0.240, loss: 3708.920166\n",
      "Train: step:  82350, time: 0.279, loss: 1708.494751\n",
      "Train: step:  82360, time: 0.238, loss: 1685.932495\n",
      "Train: step:  82370, time: 0.238, loss: 2221.470947\n",
      "Train: step:  82380, time: 0.238, loss: 542.092773\n",
      "Train: step:  82390, time: 0.238, loss: 1544.107666\n",
      "Train: step:  82400, time: 0.239, loss: 1164.810303\n",
      "Train: step:  82410, time: 0.242, loss: 2399.801514\n",
      "Train: step:  82420, time: 0.284, loss: 1846.564453\n",
      "Train: step:  82430, time: 0.263, loss: 1731.975708\n",
      "Train: step:  82440, time: 0.278, loss: 2347.504150\n",
      "Train: step:  82450, time: 0.243, loss: 567.915955\n",
      "Train: step:  82460, time: 0.244, loss: 1458.452881\n",
      "Train: step:  82470, time: 0.239, loss: 901.344666\n",
      "Train: step:  82480, time: 0.249, loss: 1154.045898\n",
      "Train: step:  82490, time: 0.234, loss: 566.361694\n",
      "Train: step:  82500, time: 0.233, loss: 1762.762085\n",
      "Train: step:  82510, time: 0.233, loss: 2113.485596\n",
      "Train: step:  82520, time: 0.244, loss: 2215.029785\n",
      "Train: step:  82530, time: 0.238, loss: 2616.336670\n",
      "Train: step:  82540, time: 0.240, loss: 934.795532\n",
      "Train: step:  82550, time: 0.234, loss: 1810.758057\n",
      "Train: step:  82560, time: 0.255, loss: 338.320679\n",
      "Train: step:  82570, time: 0.240, loss: 2074.664062\n",
      "Train: step:  82580, time: 0.239, loss: 2274.767090\n",
      "Train: step:  82590, time: 0.246, loss: 1517.438843\n",
      "Train: step:  82600, time: 0.240, loss: 2844.020020\n",
      "Train: step:  82610, time: 0.233, loss: 923.965332\n",
      "Train: step:  82620, time: 0.266, loss: 1344.148804\n",
      "Train: step:  82630, time: 0.234, loss: 1167.503296\n",
      "Train: step:  82640, time: 0.240, loss: 1003.737305\n",
      "Train: step:  82650, time: 0.245, loss: 3059.410889\n",
      "Train: step:  82660, time: 0.284, loss: 2114.834229\n",
      "Train: step:  82670, time: 0.242, loss: 3534.651367\n",
      "Train: step:  82680, time: 0.244, loss: 1170.950439\n",
      "Train: step:  82690, time: 0.242, loss: 2312.493408\n",
      "Train: step:  82700, time: 0.279, loss: 2837.461426\n",
      "Train: step:  82710, time: 0.249, loss: 2705.057373\n",
      "Train: step:  82720, time: 0.241, loss: 1990.506348\n",
      "Train: step:  82730, time: 0.241, loss: 2514.049805\n",
      "Train: step:  82740, time: 0.247, loss: 1091.033447\n",
      "Train: step:  82750, time: 0.239, loss: 1499.678467\n",
      "Train: step:  82760, time: 0.276, loss: 3349.701660\n",
      "Train: step:  82770, time: 0.275, loss: 1461.874878\n",
      "Train: step:  82780, time: 0.297, loss: 2986.684326\n",
      "Train: step:  82790, time: 0.239, loss: 1311.945190\n",
      "Train: step:  82800, time: 0.258, loss: 1314.642822\n",
      "Train: step:  82810, time: 0.255, loss: 2155.724609\n",
      "Train: step:  82820, time: 0.272, loss: 1485.183105\n",
      "Train: step:  82830, time: 0.234, loss: 1291.483765\n",
      "Train: step:  82840, time: 0.238, loss: 501.295776\n",
      "Train: step:  82850, time: 0.265, loss: 227.276962\n",
      "Train: step:  82860, time: 0.243, loss: 1587.677124\n",
      "Train: step:  82870, time: 0.250, loss: 1017.052124\n",
      "Train: step:  82880, time: 0.248, loss: 1371.191040\n",
      "Train: step:  82890, time: 0.233, loss: 1826.111084\n",
      "Train: step:  82900, time: 0.252, loss: 729.332520\n",
      "Train: step:  82910, time: 0.243, loss: 1830.975098\n",
      "Train: step:  82920, time: 0.245, loss: 835.417419\n",
      "Train: step:  82930, time: 0.245, loss: 2764.081055\n",
      "Train: step:  82940, time: 0.237, loss: 2927.517334\n",
      "Train: step:  82950, time: 0.247, loss: 1648.111328\n",
      "Train: step:  82960, time: 0.236, loss: 1099.199463\n",
      "Train: step:  82970, time: 0.238, loss: 2192.491699\n",
      "Train: step:  82980, time: 0.265, loss: 2051.644043\n",
      "Train: step:  82990, time: 0.240, loss: 1310.285889\n",
      "Train: step:  83000, time: 0.232, loss: 730.307495\n",
      "Train: step:  83010, time: 0.240, loss: 2714.208008\n",
      "Train: step:  83020, time: 0.253, loss: 1797.756470\n",
      "Train: step:  83030, time: 0.264, loss: 1734.296509\n",
      "Train: step:  83040, time: 0.243, loss: 2750.945801\n",
      "Train: step:  83050, time: 0.243, loss: 1281.471313\n",
      "Train: step:  83060, time: 0.248, loss: 2969.450439\n",
      "Train: step:  83070, time: 0.279, loss: 2160.805420\n",
      "Train: step:  83080, time: 0.249, loss: 2026.628296\n",
      "Train: step:  83090, time: 0.264, loss: 912.127441\n",
      "Train: step:  83100, time: 0.278, loss: 3442.777100\n",
      "Train: step:  83110, time: 0.240, loss: 1851.260376\n",
      "Train: step:  83120, time: 0.260, loss: 486.650421\n",
      "Train: step:  83130, time: 0.245, loss: 1438.372803\n",
      "Train: step:  83140, time: 0.245, loss: 2601.427734\n",
      "Train: step:  83150, time: 0.238, loss: 2267.143799\n",
      "Train: step:  83160, time: 0.250, loss: 908.982727\n",
      "Train: step:  83170, time: 0.236, loss: 1381.696167\n",
      "Train: step:  83180, time: 0.232, loss: 517.869385\n",
      "Train: step:  83190, time: 0.238, loss: 1764.381836\n",
      "Train: step:  83200, time: 0.240, loss: 2201.766602\n",
      "Train: step:  83210, time: 0.258, loss: 1617.548706\n",
      "Train: step:  83220, time: 0.262, loss: 1944.056396\n",
      "Train: step:  83230, time: 0.239, loss: 1445.255493\n",
      "Train: step:  83240, time: 0.241, loss: 2842.381836\n",
      "Train: step:  83250, time: 0.243, loss: 913.252380\n",
      "Train: step:  83260, time: 0.245, loss: 1375.143799\n",
      "Train: step:  83270, time: 0.245, loss: 2601.878418\n",
      "Train: step:  83280, time: 0.266, loss: 877.688721\n",
      "Train: step:  83290, time: 0.247, loss: 2986.538086\n",
      "Train: step:  83300, time: 0.246, loss: 622.361084\n",
      "Train: step:  83310, time: 0.244, loss: 2493.266846\n",
      "Train: step:  83320, time: 0.281, loss: 2140.645508\n",
      "Train: step:  83330, time: 0.237, loss: 1394.301636\n",
      "Train: step:  83340, time: 0.242, loss: 1201.250488\n",
      "Train: step:  83350, time: 0.278, loss: 3426.362061\n",
      "Train: step:  83360, time: 0.261, loss: 659.055786\n",
      "Train: step:  83370, time: 0.243, loss: 977.145691\n",
      "Train: step:  83380, time: 0.234, loss: 2338.280029\n",
      "Train: step:  83390, time: 0.239, loss: 1820.256348\n",
      "Train: step:  83400, time: 0.243, loss: 2912.496338\n",
      "Train: step:  83410, time: 0.262, loss: 550.268982\n",
      "Train: step:  83420, time: 0.243, loss: 716.545044\n",
      "Train: step:  83430, time: 0.249, loss: 1207.463257\n",
      "Train: step:  83440, time: 0.310, loss: 771.242188\n",
      "Train: step:  83450, time: 0.265, loss: 729.879578\n",
      "Train: step:  83460, time: 0.247, loss: 295.914734\n",
      "Train: step:  83470, time: 0.229, loss: 2257.518555\n",
      "Train: step:  83480, time: 0.266, loss: 2715.076172\n",
      "Train: step:  83490, time: 0.241, loss: 864.957520\n",
      "Train: step:  83500, time: 0.238, loss: 886.082520\n",
      "Train: step:  83510, time: 0.275, loss: 1493.706787\n",
      "Train: step:  83520, time: 0.239, loss: 3914.413574\n",
      "Train: step:  83530, time: 0.262, loss: 1388.353027\n",
      "Train: step:  83540, time: 0.231, loss: 1739.665771\n",
      "Train: step:  83550, time: 0.236, loss: 1148.563354\n",
      "Train: step:  83560, time: 0.239, loss: 584.899536\n",
      "Train: step:  83570, time: 0.238, loss: 1311.031982\n",
      "Train: step:  83580, time: 0.265, loss: 1206.883057\n",
      "Train: step:  83590, time: 0.247, loss: 4028.901367\n",
      "Train: step:  83600, time: 0.278, loss: 2688.124512\n",
      "Train: step:  83610, time: 0.265, loss: 2772.785889\n",
      "Train: step:  83620, time: 0.242, loss: 3137.732178\n",
      "Train: step:  83630, time: 0.248, loss: 1425.130981\n",
      "Train: step:  83640, time: 0.236, loss: 3030.963623\n",
      "Train: step:  83650, time: 0.282, loss: 1989.031738\n",
      "Train: step:  83660, time: 0.244, loss: 329.108215\n",
      "Train: step:  83670, time: 0.250, loss: 2279.583496\n",
      "Train: step:  83680, time: 0.239, loss: 2877.722412\n",
      "Train: step:  83690, time: 0.262, loss: 3093.397461\n",
      "Train: step:  83700, time: 0.230, loss: 985.196045\n",
      "Train: step:  83710, time: 0.244, loss: 1994.151367\n",
      "Train: step:  83720, time: 0.273, loss: 1128.280762\n",
      "Train: step:  83730, time: 0.246, loss: 2399.326416\n",
      "Train: step:  83740, time: 0.280, loss: 2731.022217\n",
      "Train: step:  83750, time: 0.251, loss: 1159.715454\n",
      "Train: step:  83760, time: 0.240, loss: 1748.297119\n",
      "Train: step:  83770, time: 0.255, loss: 1136.367676\n",
      "Train: step:  83780, time: 0.239, loss: 1275.819702\n",
      "Train: step:  83790, time: 0.236, loss: 1831.297241\n",
      "Train: step:  83800, time: 0.274, loss: 2674.051514\n",
      "Train: step:  83810, time: 0.266, loss: 1276.069092\n",
      "Train: step:  83820, time: 0.243, loss: 3461.086670\n",
      "Train: step:  83830, time: 0.249, loss: 2650.129395\n",
      "Train: step:  83840, time: 0.262, loss: 807.434509\n",
      "Train: step:  83850, time: 0.231, loss: 776.239868\n",
      "Train: step:  83860, time: 0.240, loss: 1122.640991\n",
      "Train: step:  83870, time: 0.240, loss: 1008.330200\n",
      "Train: step:  83880, time: 0.263, loss: 1313.876709\n",
      "Train: step:  83890, time: 0.231, loss: 1585.365479\n",
      "Train: step:  83900, time: 0.242, loss: 2661.790283\n",
      "Train: step:  83910, time: 0.236, loss: 1097.294312\n",
      "Train: step:  83920, time: 0.234, loss: 1677.387329\n",
      "Train: step:  83930, time: 0.232, loss: 4359.366699\n",
      "Train: step:  83940, time: 0.275, loss: 2592.313721\n",
      "Train: step:  83950, time: 0.234, loss: 900.342712\n",
      "Train: step:  83960, time: 0.256, loss: 1836.225586\n",
      "Train: step:  83970, time: 0.236, loss: 1709.978394\n",
      "Train: step:  83980, time: 0.239, loss: 2136.469238\n",
      "Train: step:  83990, time: 0.243, loss: 2838.349854\n",
      "Train: step:  84000, time: 0.294, loss: 2192.794434\n",
      "Train: step:  84010, time: 0.280, loss: 1655.564331\n",
      "Train: step:  84020, time: 0.264, loss: 1301.532104\n",
      "Train: step:  84030, time: 0.240, loss: 1990.925903\n",
      "Train: step:  84040, time: 0.260, loss: 1935.904785\n",
      "Train: step:  84050, time: 0.236, loss: 2557.921387\n",
      "Train: step:  84060, time: 0.290, loss: 2026.491821\n",
      "Train: step:  84070, time: 0.248, loss: 2332.019775\n",
      "Train: step:  84080, time: 0.246, loss: 809.735962\n",
      "Train: step:  84090, time: 0.271, loss: 978.054260\n",
      "Train: step:  84100, time: 0.239, loss: 2744.075439\n",
      "Train: step:  84110, time: 0.238, loss: 377.880646\n",
      "Train: step:  84120, time: 0.282, loss: 2257.045410\n",
      "Train: step:  84130, time: 0.243, loss: 2107.798584\n",
      "Train: step:  84140, time: 0.243, loss: 2503.741699\n",
      "Train: step:  84150, time: 0.245, loss: 1629.730103\n",
      "Train: step:  84160, time: 0.241, loss: 1527.192749\n",
      "Train: step:  84170, time: 0.267, loss: 3521.872314\n",
      "Train: step:  84180, time: 0.246, loss: 3873.580566\n",
      "Train: step:  84190, time: 0.244, loss: 3552.751221\n",
      "Train: step:  84200, time: 0.269, loss: 1310.740601\n",
      "Train: step:  84210, time: 0.271, loss: 1597.011230\n",
      "Train: step:  84220, time: 0.237, loss: 3283.350586\n",
      "Train: step:  84230, time: 0.237, loss: 2239.376221\n",
      "Train: step:  84240, time: 0.238, loss: 1578.334229\n",
      "Train: step:  84250, time: 0.270, loss: 1220.545654\n",
      "Train: step:  84260, time: 0.244, loss: 2030.966919\n",
      "Train: step:  84270, time: 0.246, loss: 2011.504395\n",
      "Train: step:  84280, time: 0.266, loss: 1137.992798\n",
      "Train: step:  84290, time: 0.266, loss: 3077.355469\n",
      "Train: step:  84300, time: 0.272, loss: 1059.117310\n",
      "Train: step:  84310, time: 0.272, loss: 441.753021\n",
      "Train: step:  84320, time: 0.238, loss: 698.736572\n",
      "Train: step:  84330, time: 0.243, loss: 2061.085205\n",
      "Train: step:  84340, time: 0.243, loss: 1496.624512\n",
      "Train: step:  84350, time: 0.269, loss: 2175.896729\n",
      "Train: step:  84360, time: 0.235, loss: 2637.119629\n",
      "Train: step:  84370, time: 0.280, loss: 1207.290527\n",
      "Train: step:  84380, time: 0.266, loss: 1073.464844\n",
      "Train: step:  84390, time: 0.245, loss: 1671.172974\n",
      "Train: step:  84400, time: 0.240, loss: 2734.850586\n",
      "Train: step:  84410, time: 0.232, loss: 1508.348755\n",
      "Train: step:  84420, time: 0.262, loss: 2204.910400\n",
      "Train: step:  84430, time: 0.274, loss: 2345.416748\n",
      "Train: step:  84440, time: 0.274, loss: 1875.115356\n",
      "Train: step:  84450, time: 0.245, loss: 1134.591797\n",
      "Train: step:  84460, time: 0.246, loss: 2014.212280\n",
      "Train: step:  84470, time: 0.252, loss: 3884.612305\n",
      "Train: step:  84480, time: 0.258, loss: 2755.223633\n",
      "Train: step:  84490, time: 0.241, loss: 3040.872070\n",
      "Train: step:  84500, time: 0.242, loss: 570.355896\n",
      "Train: step:  84510, time: 0.250, loss: 1881.322754\n",
      "Train: step:  84520, time: 0.237, loss: 2575.166748\n",
      "Train: step:  84530, time: 0.243, loss: 3130.863281\n",
      "Train: step:  84540, time: 0.248, loss: 1647.629150\n",
      "Train: step:  84550, time: 0.240, loss: 786.544495\n",
      "Train: step:  84560, time: 0.241, loss: 815.871521\n",
      "Train: step:  84570, time: 0.240, loss: 572.750977\n",
      "Train: step:  84580, time: 0.223, loss: 1725.384888\n",
      "Train: step:  84590, time: 0.242, loss: 2765.439697\n",
      "Train: step:  84600, time: 0.237, loss: 3247.039062\n",
      "Train: step:  84610, time: 0.274, loss: 2181.449707\n",
      "Train: step:  84620, time: 0.244, loss: 1235.546753\n",
      "Train: step:  84630, time: 0.233, loss: 2094.831787\n",
      "Train: step:  84640, time: 0.246, loss: 1348.663818\n",
      "Train: step:  84650, time: 0.247, loss: 2547.240234\n",
      "Train: step:  84660, time: 0.257, loss: 187.692856\n",
      "Train: step:  84670, time: 0.290, loss: 2872.361572\n",
      "Train: step:  84680, time: 0.242, loss: 3229.686035\n",
      "Train: step:  84690, time: 0.243, loss: 1151.345215\n",
      "Train: step:  84700, time: 0.246, loss: 1418.963257\n",
      "Train: step:  84710, time: 0.244, loss: 1288.884155\n",
      "Train: step:  84720, time: 0.265, loss: 1615.628174\n",
      "Train: step:  84730, time: 0.235, loss: 1665.426636\n",
      "Train: step:  84740, time: 0.231, loss: 738.009338\n",
      "Train: step:  84750, time: 0.273, loss: 1760.396362\n",
      "Train: step:  84760, time: 0.261, loss: 1537.935669\n",
      "Train: step:  84770, time: 0.264, loss: 2855.442383\n",
      "Train: step:  84780, time: 0.261, loss: 494.987396\n",
      "Train: step:  84790, time: 0.232, loss: 1720.740479\n",
      "Train: step:  84800, time: 0.238, loss: 2100.354980\n",
      "Train: step:  84810, time: 0.275, loss: 1423.586548\n",
      "Train: step:  84820, time: 0.239, loss: 2217.374512\n",
      "Train: step:  84830, time: 0.240, loss: 2198.993408\n",
      "Train: step:  84840, time: 0.239, loss: 1204.887817\n",
      "Train: step:  84850, time: 0.261, loss: 1506.169556\n",
      "Train: step:  84860, time: 0.265, loss: 396.986877\n",
      "Train: step:  84870, time: 0.237, loss: 655.200195\n",
      "Train: step:  84880, time: 0.238, loss: 640.348572\n",
      "Train: step:  84890, time: 0.239, loss: 2172.075928\n",
      "Train: step:  84900, time: 0.273, loss: 3527.829102\n",
      "Train: step:  84910, time: 0.278, loss: 1138.507812\n",
      "Train: step:  84920, time: 0.233, loss: 829.768555\n",
      "Train: step:  84930, time: 0.240, loss: 278.130707\n",
      "Train: step:  84940, time: 0.259, loss: 731.077698\n",
      "Train: step:  84950, time: 0.271, loss: 1558.379028\n",
      "Train: step:  84960, time: 0.244, loss: 2888.598877\n",
      "Train: step:  84970, time: 0.247, loss: 2243.906738\n",
      "Train: step:  84980, time: 0.240, loss: 1774.846313\n",
      "Train: step:  84990, time: 0.241, loss: 1408.416382\n",
      "Train: step:  85000, time: 0.233, loss: 1153.376221\n",
      "Train: step:  85010, time: 0.247, loss: 720.459656\n",
      "Train: step:  85020, time: 0.240, loss: 2247.699219\n",
      "Train: step:  85030, time: 0.276, loss: 783.083618\n",
      "Train: step:  85040, time: 0.241, loss: 1726.648926\n",
      "Train: step:  85050, time: 0.276, loss: 2373.491455\n",
      "Train: step:  85060, time: 0.253, loss: 1651.479126\n",
      "Train: step:  85070, time: 0.282, loss: 2738.444336\n",
      "Train: step:  85080, time: 0.273, loss: 2584.648438\n",
      "Train: step:  85090, time: 0.243, loss: 723.433350\n",
      "Train: step:  85100, time: 0.262, loss: 1132.350464\n",
      "Train: step:  85110, time: 0.238, loss: 3274.016602\n",
      "Train: step:  85120, time: 0.240, loss: 2671.135010\n",
      "Train: step:  85130, time: 0.237, loss: 1470.211060\n",
      "Train: step:  85140, time: 0.236, loss: 1218.520996\n",
      "Train: step:  85150, time: 0.235, loss: 1252.866699\n",
      "Train: step:  85160, time: 0.241, loss: 2703.377686\n",
      "Train: step:  85170, time: 0.240, loss: 1769.824707\n",
      "Train: step:  85180, time: 0.280, loss: 1373.477783\n",
      "Train: step:  85190, time: 0.240, loss: 1428.855713\n",
      "Train: step:  85200, time: 0.255, loss: 2939.803711\n",
      "Train: step:  85210, time: 0.245, loss: 1450.307617\n",
      "Train: step:  85220, time: 0.262, loss: 2314.251221\n",
      "Train: step:  85230, time: 0.242, loss: 1966.168579\n",
      "Train: step:  85240, time: 0.244, loss: 2222.082764\n",
      "Train: step:  85250, time: 0.241, loss: 3013.916016\n",
      "Train: step:  85260, time: 0.242, loss: 2406.433838\n",
      "Train: step:  85270, time: 0.234, loss: 2429.120361\n",
      "Train: step:  85280, time: 0.257, loss: 2902.262939\n",
      "Train: step:  85290, time: 0.264, loss: 2358.370117\n",
      "Train: step:  85300, time: 0.263, loss: 386.276398\n",
      "Train: step:  85310, time: 0.243, loss: 3091.251953\n",
      "Train: step:  85320, time: 0.239, loss: 1724.035522\n",
      "Train: step:  85330, time: 0.245, loss: 1771.113770\n",
      "Train: step:  85340, time: 0.241, loss: 1840.671753\n",
      "Train: step:  85350, time: 0.232, loss: 2362.707764\n",
      "Train: step:  85360, time: 0.235, loss: 1038.170166\n",
      "Train: step:  85370, time: 0.260, loss: 3079.329834\n",
      "Train: step:  85380, time: 0.237, loss: 2086.578857\n",
      "Train: step:  85390, time: 0.277, loss: 1209.017456\n",
      "Train: step:  85400, time: 0.228, loss: 1967.595337\n",
      "Train: step:  85410, time: 0.234, loss: 2500.595703\n",
      "Train: step:  85420, time: 0.231, loss: 3072.465332\n",
      "Train: step:  85430, time: 0.252, loss: 2331.855225\n",
      "Train: step:  85440, time: 0.241, loss: 2986.693604\n",
      "Train: step:  85450, time: 0.242, loss: 1435.051025\n",
      "Train: step:  85460, time: 0.234, loss: 2534.240234\n",
      "Train: step:  85470, time: 0.260, loss: 2656.022217\n",
      "Train: step:  85480, time: 0.246, loss: 1786.805664\n",
      "Train: step:  85490, time: 0.239, loss: 1901.142700\n",
      "Train: step:  85500, time: 0.271, loss: 1841.976074\n",
      "Train: step:  85510, time: 0.280, loss: 2010.811523\n",
      "Train: step:  85520, time: 0.236, loss: 1584.357544\n",
      "Train: step:  85530, time: 0.238, loss: 3215.610352\n",
      "Train: step:  85540, time: 0.241, loss: 2535.289551\n",
      "Train: step:  85550, time: 0.236, loss: 2284.890137\n",
      "Train: step:  85560, time: 0.255, loss: 3000.835693\n",
      "Train: step:  85570, time: 0.267, loss: 798.772156\n",
      "Train: step:  85580, time: 0.265, loss: 413.389923\n",
      "Train: step:  85590, time: 0.250, loss: 1180.161499\n",
      "Train: step:  85600, time: 0.263, loss: 502.734558\n",
      "Train: step:  85610, time: 0.265, loss: 2014.049927\n",
      "Train: step:  85620, time: 0.267, loss: 1819.042847\n",
      "Train: step:  85630, time: 0.237, loss: 1956.522461\n",
      "Train: step:  85640, time: 0.237, loss: 2679.599609\n",
      "Train: step:  85650, time: 0.236, loss: 400.321289\n",
      "Train: step:  85660, time: 0.225, loss: 1155.766724\n",
      "Train: step:  85670, time: 0.236, loss: 1959.719727\n",
      "Train: step:  85680, time: 0.247, loss: 860.134338\n",
      "Train: step:  85690, time: 0.262, loss: 1396.379761\n",
      "Train: step:  85700, time: 0.234, loss: 2393.706787\n",
      "Train: step:  85710, time: 0.259, loss: 401.133423\n",
      "Train: step:  85720, time: 0.243, loss: 2986.849121\n",
      "Train: step:  85730, time: 0.260, loss: 1853.289551\n",
      "Train: step:  85740, time: 0.237, loss: 809.174988\n",
      "Train: step:  85750, time: 0.278, loss: 948.565369\n",
      "Train: step:  85760, time: 0.239, loss: 2869.221436\n",
      "Train: step:  85770, time: 0.240, loss: 1952.494385\n",
      "Train: step:  85780, time: 0.239, loss: 1109.110352\n",
      "Train: step:  85790, time: 0.228, loss: 2082.304199\n",
      "Train: step:  85800, time: 0.244, loss: 1428.147339\n",
      "Train: step:  85810, time: 0.232, loss: 785.645508\n",
      "Train: step:  85820, time: 0.241, loss: 2803.088379\n",
      "Train: step:  85830, time: 0.249, loss: 2817.900635\n",
      "Train: step:  85840, time: 0.262, loss: 2409.928711\n",
      "Train: step:  85850, time: 0.239, loss: 1670.308838\n",
      "Train: step:  85860, time: 0.267, loss: 1355.991943\n",
      "Train: step:  85870, time: 0.244, loss: 1041.928467\n",
      "Train: step:  85880, time: 0.235, loss: 1024.639282\n",
      "Train: step:  85890, time: 0.260, loss: 1546.551025\n",
      "Train: step:  85900, time: 0.240, loss: 1669.656006\n",
      "Train: step:  85910, time: 0.247, loss: 192.787857\n",
      "Train: step:  85920, time: 0.240, loss: 1666.533447\n",
      "Train: step:  85930, time: 0.239, loss: 663.392944\n",
      "Train: step:  85940, time: 0.237, loss: 1382.420410\n",
      "Train: step:  85950, time: 0.261, loss: 559.340088\n",
      "Train: step:  85960, time: 0.271, loss: 2241.490479\n",
      "Train: step:  85970, time: 0.236, loss: 3097.953857\n",
      "Train: step:  85980, time: 0.251, loss: 2977.297852\n",
      "Train: step:  85990, time: 0.234, loss: 1453.887207\n",
      "Train: step:  86000, time: 0.268, loss: 1295.631226\n",
      "Train: step:  86010, time: 0.236, loss: 516.082031\n",
      "Train: step:  86020, time: 0.236, loss: 977.293579\n",
      "Train: step:  86030, time: 0.254, loss: 3327.568115\n",
      "Train: step:  86040, time: 0.242, loss: 2461.648193\n",
      "Train: step:  86050, time: 0.273, loss: 1802.922241\n",
      "Train: step:  86060, time: 0.281, loss: 2036.597656\n",
      "Train: step:  86070, time: 0.241, loss: 762.646729\n",
      "Train: step:  86080, time: 0.247, loss: 1813.091797\n",
      "Train: step:  86090, time: 0.265, loss: 1538.156128\n",
      "Train: step:  86100, time: 0.262, loss: 805.764038\n",
      "Train: step:  86110, time: 0.235, loss: 2438.240723\n",
      "Train: step:  86120, time: 0.246, loss: 1506.271973\n",
      "Train: step:  86130, time: 0.235, loss: 2929.360352\n",
      "Train: step:  86140, time: 0.245, loss: 2459.911133\n",
      "Train: step:  86150, time: 0.261, loss: 1752.893555\n",
      "Train: step:  86160, time: 0.243, loss: 3676.148438\n",
      "Train: step:  86170, time: 0.245, loss: 1451.972534\n",
      "Train: step:  86180, time: 0.233, loss: 1431.082764\n",
      "Train: step:  86190, time: 0.235, loss: 1374.376221\n",
      "Train: step:  86200, time: 0.258, loss: 1673.810181\n",
      "Train: step:  86210, time: 0.251, loss: 1937.465698\n",
      "Train: step:  86220, time: 0.256, loss: 974.518738\n",
      "Train: step:  86230, time: 0.234, loss: 2884.106689\n",
      "Train: step:  86240, time: 0.233, loss: 970.669922\n",
      "Train: step:  86250, time: 0.232, loss: 2538.472168\n",
      "Train: step:  86260, time: 0.231, loss: 1601.891479\n",
      "Train: step:  86270, time: 0.231, loss: 534.730591\n",
      "Train: step:  86280, time: 0.252, loss: 2312.571289\n",
      "Train: step:  86290, time: 0.262, loss: 388.898346\n",
      "Train: step:  86300, time: 0.227, loss: 1901.638184\n",
      "Train: step:  86310, time: 0.232, loss: 3476.942139\n",
      "Train: step:  86320, time: 0.224, loss: 1408.675049\n",
      "Train: step:  86330, time: 0.241, loss: 1727.598267\n",
      "Train: step:  86340, time: 0.243, loss: 1966.297119\n",
      "Train: step:  86350, time: 0.240, loss: 1548.105347\n",
      "Train: step:  86360, time: 0.270, loss: 4541.411621\n",
      "Train: step:  86370, time: 0.240, loss: 2633.988037\n",
      "Train: step:  86380, time: 0.258, loss: 2554.756348\n",
      "Train: step:  86390, time: 0.246, loss: 1051.281616\n",
      "Train: step:  86400, time: 0.234, loss: 746.872864\n",
      "Train: step:  86410, time: 0.233, loss: 1580.187134\n",
      "Train: step:  86420, time: 0.257, loss: 1353.091919\n",
      "Train: step:  86430, time: 0.238, loss: 1991.818115\n",
      "Train: step:  86440, time: 0.233, loss: 722.827698\n",
      "Train: step:  86450, time: 0.231, loss: 2264.372314\n",
      "Train: step:  86460, time: 0.229, loss: 668.243774\n",
      "Train: step:  86470, time: 0.234, loss: 1854.129272\n",
      "Train: step:  86480, time: 0.226, loss: 651.517944\n",
      "Train: step:  86490, time: 0.226, loss: 2157.260254\n",
      "Train: step:  86500, time: 0.315, loss: 1833.964844\n",
      "Train: step:  86510, time: 0.276, loss: 1525.307129\n",
      "Train: step:  86520, time: 0.267, loss: 2690.778076\n",
      "Train: step:  86530, time: 0.273, loss: 1259.655640\n",
      "Train: step:  86540, time: 0.244, loss: 1835.163208\n",
      "Train: step:  86550, time: 0.245, loss: 2562.375732\n",
      "Train: step:  86560, time: 0.240, loss: 1816.466553\n",
      "Train: step:  86570, time: 0.232, loss: 1061.970825\n",
      "Train: step:  86580, time: 0.239, loss: 484.691833\n",
      "Train: step:  86590, time: 0.279, loss: 3596.195801\n",
      "Train: step:  86600, time: 0.235, loss: 1909.806885\n",
      "Train: step:  86610, time: 0.258, loss: 3331.866699\n",
      "Train: step:  86620, time: 0.269, loss: 2567.978760\n",
      "Train: step:  86630, time: 0.237, loss: 2824.356201\n",
      "Train: step:  86640, time: 0.241, loss: 1109.787964\n",
      "Train: step:  86650, time: 0.230, loss: 2231.754395\n",
      "Train: step:  86660, time: 0.248, loss: 3574.622803\n",
      "Train: step:  86670, time: 0.232, loss: 688.301270\n",
      "Train: step:  86680, time: 0.234, loss: 638.813354\n",
      "Train: step:  86690, time: 0.226, loss: 1980.642334\n",
      "Train: step:  86700, time: 0.230, loss: 2268.246826\n",
      "Train: step:  86710, time: 0.231, loss: 1528.131958\n",
      "Train: step:  86720, time: 0.238, loss: 896.560730\n",
      "Train: step:  86730, time: 0.232, loss: 1191.206787\n",
      "Train: step:  86740, time: 0.231, loss: 464.654480\n",
      "Train: step:  86750, time: 0.227, loss: 3261.465820\n",
      "Train: step:  86760, time: 0.253, loss: 456.340149\n",
      "Train: step:  86770, time: 0.232, loss: 2618.033691\n",
      "Train: step:  86780, time: 0.261, loss: 1750.615234\n",
      "Train: step:  86790, time: 0.228, loss: 2567.091309\n",
      "Train: step:  86800, time: 0.269, loss: 2961.769775\n",
      "Train: step:  86810, time: 0.240, loss: 2410.652344\n",
      "Train: step:  86820, time: 0.222, loss: 2211.645996\n",
      "Train: step:  86830, time: 0.224, loss: 1196.649170\n",
      "Train: step:  86840, time: 0.224, loss: 2184.485840\n",
      "Train: step:  86850, time: 0.256, loss: 2679.802979\n",
      "Train: step:  86860, time: 0.419, loss: 2009.856079\n",
      "Train: step:  86870, time: 0.235, loss: 2046.135376\n",
      "Train: step:  86880, time: 0.260, loss: 1645.730835\n",
      "Train: step:  86890, time: 0.229, loss: 3002.886475\n",
      "Train: step:  86900, time: 0.269, loss: 676.774597\n",
      "Train: step:  86910, time: 0.270, loss: 1687.794189\n",
      "Train: step:  86920, time: 0.232, loss: 1910.477783\n",
      "Train: step:  86930, time: 0.233, loss: 949.321167\n",
      "Train: step:  86940, time: 0.269, loss: 1157.016968\n",
      "Train: step:  86950, time: 0.229, loss: 2080.850586\n",
      "Train: step:  86960, time: 0.228, loss: 976.950623\n",
      "Train: step:  86970, time: 0.233, loss: 2338.594727\n",
      "Train: step:  86980, time: 0.255, loss: 2425.565186\n",
      "Train: step:  86990, time: 0.228, loss: 1840.073608\n",
      "Train: step:  87000, time: 0.229, loss: 925.010193\n",
      "Train: step:  87010, time: 0.220, loss: 1321.956787\n",
      "Train: step:  87020, time: 0.276, loss: 1786.280273\n",
      "Train: step:  87030, time: 0.259, loss: 999.959167\n",
      "Train: step:  87040, time: 0.251, loss: 3322.667236\n",
      "Train: step:  87050, time: 0.227, loss: 927.650818\n",
      "Train: step:  87060, time: 0.228, loss: 1123.274170\n",
      "Train: step:  87070, time: 0.227, loss: 930.631287\n",
      "Train: step:  87080, time: 0.234, loss: 3096.561768\n",
      "Train: step:  87090, time: 0.229, loss: 2545.905762\n",
      "Train: step:  87100, time: 0.227, loss: 1674.454956\n",
      "Train: step:  87110, time: 0.230, loss: 2826.071045\n",
      "Train: step:  87120, time: 0.220, loss: 1239.838867\n",
      "Train: step:  87130, time: 0.229, loss: 2084.934326\n",
      "Train: step:  87140, time: 0.225, loss: 176.835403\n",
      "Train: step:  87150, time: 0.247, loss: 1862.072998\n",
      "Train: step:  87160, time: 0.235, loss: 3373.722412\n",
      "Train: step:  87170, time: 0.239, loss: 2518.988037\n",
      "Train: step:  87180, time: 0.261, loss: 3059.808594\n",
      "Train: step:  87190, time: 0.253, loss: 1133.138062\n",
      "Train: step:  87200, time: 0.234, loss: 1617.785278\n",
      "Train: step:  87210, time: 0.225, loss: 1069.166504\n",
      "Train: step:  87220, time: 0.283, loss: 1232.256958\n",
      "Train: step:  87230, time: 0.225, loss: 472.522217\n",
      "Train: step:  87240, time: 0.222, loss: 2324.514893\n",
      "Train: step:  87250, time: 0.219, loss: 962.831360\n",
      "Train: step:  87260, time: 0.230, loss: 215.997208\n",
      "Train: step:  87270, time: 0.229, loss: 2106.940918\n",
      "Train: step:  87280, time: 0.221, loss: 1589.934692\n",
      "Train: step:  87290, time: 0.259, loss: 1478.092163\n",
      "Train: step:  87300, time: 0.225, loss: 923.340027\n",
      "Train: step:  87310, time: 0.268, loss: 3115.233398\n",
      "Train: step:  87320, time: 0.228, loss: 1240.637329\n",
      "Train: step:  87330, time: 0.228, loss: 3440.354492\n",
      "Train: step:  87340, time: 0.248, loss: 192.544266\n",
      "Train: step:  87350, time: 0.226, loss: 3584.786377\n",
      "Train: step:  87360, time: 0.228, loss: 1716.221069\n",
      "Train: step:  87370, time: 0.224, loss: 2453.518799\n",
      "Train: step:  87380, time: 0.234, loss: 664.461426\n",
      "Train: step:  87390, time: 0.233, loss: 1870.875488\n",
      "Train: step:  87400, time: 0.230, loss: 3220.810303\n",
      "Train: step:  87410, time: 0.222, loss: 1091.841797\n",
      "Train: step:  87420, time: 0.230, loss: 2184.567383\n",
      "Train: step:  87430, time: 0.225, loss: 3867.651611\n",
      "Train: step:  87440, time: 0.233, loss: 3454.526123\n",
      "Train: step:  87450, time: 0.226, loss: 540.477783\n",
      "Train: step:  87460, time: 0.249, loss: 870.644348\n",
      "Train: step:  87470, time: 0.227, loss: 872.022766\n",
      "Train: step:  87480, time: 0.226, loss: 2185.795166\n",
      "Train: step:  87490, time: 0.242, loss: 1925.721191\n",
      "Train: step:  87500, time: 0.233, loss: 1586.311035\n",
      "Train: step:  87510, time: 0.266, loss: 2962.699951\n",
      "Train: step:  87520, time: 0.251, loss: 460.753052\n",
      "Train: step:  87530, time: 0.233, loss: 2788.879883\n",
      "Train: step:  87540, time: 0.246, loss: 1163.663696\n",
      "Train: step:  87550, time: 0.249, loss: 3012.325195\n",
      "Train: step:  87560, time: 0.233, loss: 2275.890869\n",
      "Train: step:  87570, time: 0.225, loss: 2219.758789\n",
      "Train: step:  87580, time: 0.253, loss: 1870.471313\n",
      "Train: step:  87590, time: 0.261, loss: 1960.916626\n",
      "Train: step:  87600, time: 0.256, loss: 1026.523315\n",
      "Train: step:  87610, time: 0.268, loss: 3201.786621\n",
      "Train: step:  87620, time: 0.235, loss: 270.525543\n",
      "Train: step:  87630, time: 0.298, loss: 771.186768\n",
      "Train: step:  87640, time: 0.234, loss: 1287.913208\n",
      "Train: step:  87650, time: 0.269, loss: 444.623077\n",
      "Train: step:  87660, time: 0.255, loss: 1519.569702\n",
      "Train: step:  87670, time: 0.255, loss: 2301.797852\n",
      "Train: step:  87680, time: 0.241, loss: 1593.441040\n",
      "Train: step:  87690, time: 0.238, loss: 1118.407104\n",
      "Train: step:  87700, time: 0.249, loss: 670.029480\n",
      "Train: step:  87710, time: 0.282, loss: 278.063904\n",
      "Train: step:  87720, time: 0.249, loss: 919.222168\n",
      "Train: step:  87730, time: 0.238, loss: 2226.648193\n",
      "Train: step:  87740, time: 0.241, loss: 2439.820801\n",
      "Train: step:  87750, time: 0.245, loss: 2186.178467\n",
      "Train: step:  87760, time: 0.256, loss: 1882.757935\n",
      "Train: step:  87770, time: 0.259, loss: 292.154358\n",
      "Train: step:  87780, time: 0.250, loss: 2010.854370\n",
      "Train: step:  87790, time: 0.293, loss: 2011.513550\n",
      "Train: step:  87800, time: 0.236, loss: 1025.668823\n",
      "Train: step:  87810, time: 0.243, loss: 523.759155\n",
      "Train: step:  87820, time: 0.269, loss: 1698.691650\n",
      "Train: step:  87830, time: 0.264, loss: 1111.562256\n",
      "Train: step:  87840, time: 0.282, loss: 1843.323608\n",
      "Train: step:  87850, time: 0.251, loss: 1485.729492\n",
      "Train: step:  87860, time: 0.250, loss: 4151.759277\n",
      "Train: step:  87870, time: 0.240, loss: 1643.332520\n",
      "Train: step:  87880, time: 0.266, loss: 758.272217\n",
      "Train: step:  87890, time: 0.249, loss: 2318.423340\n",
      "Train: step:  87900, time: 0.237, loss: 3137.909424\n",
      "Train: step:  87910, time: 0.257, loss: 2152.741455\n",
      "Train: step:  87920, time: 0.245, loss: 1600.572876\n",
      "Train: step:  87930, time: 0.241, loss: 986.446899\n",
      "Train: step:  87940, time: 0.244, loss: 1833.109009\n",
      "Train: step:  87950, time: 0.244, loss: 402.108368\n",
      "Train: step:  87960, time: 0.236, loss: 2401.020508\n",
      "Train: step:  87970, time: 0.249, loss: 3186.804443\n",
      "Train: step:  87980, time: 0.278, loss: 1352.267334\n",
      "Train: step:  87990, time: 0.228, loss: 685.694885\n",
      "Train: step:  88000, time: 0.243, loss: 2240.919434\n",
      "Train: step:  88010, time: 0.236, loss: 751.437378\n",
      "Train: step:  88020, time: 0.258, loss: 1222.128052\n",
      "Train: step:  88030, time: 0.239, loss: 2118.201660\n",
      "Train: step:  88040, time: 0.242, loss: 1662.602539\n",
      "Train: step:  88050, time: 0.247, loss: 1388.349731\n",
      "Train: step:  88060, time: 0.277, loss: 2209.047607\n",
      "Train: step:  88070, time: 0.257, loss: 1421.805176\n",
      "Train: step:  88080, time: 0.246, loss: 213.389801\n",
      "Train: step:  88090, time: 0.259, loss: 3457.988037\n",
      "Train: step:  88100, time: 0.236, loss: 611.817261\n",
      "Train: step:  88110, time: 0.268, loss: 376.212128\n",
      "Train: step:  88120, time: 0.242, loss: 1838.103271\n",
      "Train: step:  88130, time: 0.287, loss: 2096.974854\n",
      "Train: step:  88140, time: 0.242, loss: 591.634827\n",
      "Train: step:  88150, time: 0.249, loss: 2679.069824\n",
      "Train: step:  88160, time: 0.254, loss: 433.822968\n",
      "Train: step:  88170, time: 0.271, loss: 1051.877930\n",
      "Train: step:  88180, time: 0.259, loss: 869.944031\n",
      "Train: step:  88190, time: 0.249, loss: 452.548584\n",
      "Train: step:  88200, time: 0.241, loss: 1047.285645\n",
      "Train: step:  88210, time: 0.245, loss: 1257.101318\n",
      "Train: step:  88220, time: 0.241, loss: 2518.805664\n",
      "Train: step:  88230, time: 0.252, loss: 1020.013916\n",
      "Train: step:  88240, time: 0.246, loss: 1514.531128\n",
      "Train: step:  88250, time: 0.245, loss: 2182.873291\n",
      "Train: step:  88260, time: 0.264, loss: 1419.579956\n",
      "Train: step:  88270, time: 0.243, loss: 1412.947266\n",
      "Train: step:  88280, time: 0.269, loss: 1501.965820\n",
      "Train: step:  88290, time: 0.251, loss: 2979.034180\n",
      "Train: step:  88300, time: 0.245, loss: 444.261230\n",
      "Train: step:  88310, time: 0.274, loss: 401.904449\n",
      "Train: step:  88320, time: 0.240, loss: 1268.763672\n",
      "Train: step:  88330, time: 0.278, loss: 4450.054688\n",
      "Train: step:  88340, time: 0.251, loss: 1393.128662\n",
      "Train: step:  88350, time: 0.310, loss: 1894.568359\n",
      "Train: step:  88360, time: 0.262, loss: 1009.654968\n",
      "Train: step:  88370, time: 0.266, loss: 440.591431\n",
      "Train: step:  88380, time: 0.261, loss: 2552.427979\n",
      "Train: step:  88390, time: 0.238, loss: 1026.493530\n",
      "Train: step:  88400, time: 0.246, loss: 694.393188\n",
      "Train: step:  88410, time: 0.241, loss: 2285.942871\n",
      "Train: step:  88420, time: 0.268, loss: 3340.111816\n",
      "Train: step:  88430, time: 0.236, loss: 1151.125122\n",
      "Train: step:  88440, time: 0.244, loss: 2509.047852\n",
      "Train: step:  88450, time: 0.249, loss: 1490.795166\n",
      "Train: step:  88460, time: 0.240, loss: 1047.454834\n",
      "Train: step:  88470, time: 0.281, loss: 382.468567\n",
      "Train: step:  88480, time: 0.247, loss: 822.208435\n",
      "Train: step:  88490, time: 0.243, loss: 3680.178223\n",
      "Train: step:  88500, time: 0.259, loss: 3080.656982\n",
      "Train: step:  88510, time: 0.248, loss: 1938.159302\n",
      "Train: step:  88520, time: 0.240, loss: 3311.560303\n",
      "Train: step:  88530, time: 0.272, loss: 2695.833984\n",
      "Train: step:  88540, time: 0.264, loss: 2537.187500\n",
      "Train: step:  88550, time: 0.260, loss: 2585.022949\n",
      "Train: step:  88560, time: 0.239, loss: 1440.401489\n",
      "Train: step:  88570, time: 0.237, loss: 1868.680664\n",
      "Train: step:  88580, time: 0.238, loss: 1025.912842\n",
      "Train: step:  88590, time: 0.244, loss: 1814.290527\n",
      "Train: step:  88600, time: 0.253, loss: 3554.122559\n",
      "Train: step:  88610, time: 0.277, loss: 1649.850586\n",
      "Train: step:  88620, time: 0.245, loss: 2169.637695\n",
      "Train: step:  88630, time: 0.243, loss: 2888.255371\n",
      "Train: step:  88640, time: 0.268, loss: 560.130981\n",
      "Train: step:  88650, time: 0.244, loss: 1136.968750\n",
      "Train: step:  88660, time: 0.244, loss: 2703.067871\n",
      "Train: step:  88670, time: 0.244, loss: 1523.027832\n",
      "Train: step:  88680, time: 0.243, loss: 2877.151855\n",
      "Train: step:  88690, time: 0.245, loss: 3373.283936\n",
      "Train: step:  88700, time: 0.246, loss: 3171.535156\n",
      "Train: step:  88710, time: 0.253, loss: 1810.481323\n",
      "Train: step:  88720, time: 0.249, loss: 2398.323730\n",
      "Train: step:  88730, time: 0.243, loss: 955.845154\n",
      "Train: step:  88740, time: 0.250, loss: 1088.188477\n",
      "Train: step:  88750, time: 0.265, loss: 2168.288330\n",
      "Train: step:  88760, time: 0.269, loss: 2480.411133\n",
      "Train: step:  88770, time: 0.267, loss: 2479.854248\n",
      "Train: step:  88780, time: 0.266, loss: 2019.897339\n",
      "Train: step:  88790, time: 0.245, loss: 2513.326172\n",
      "Train: step:  88800, time: 0.271, loss: 1482.428955\n",
      "Train: step:  88810, time: 0.244, loss: 1882.307617\n",
      "Train: step:  88820, time: 0.238, loss: 1286.080688\n",
      "Train: step:  88830, time: 0.243, loss: 2508.979248\n",
      "Train: step:  88840, time: 0.278, loss: 2191.183594\n",
      "Train: step:  88850, time: 0.242, loss: 650.673584\n",
      "Train: step:  88860, time: 0.238, loss: 2636.106201\n",
      "Train: step:  88870, time: 0.239, loss: 983.795105\n",
      "Train: step:  88880, time: 0.245, loss: 2063.366699\n",
      "Train: step:  88890, time: 0.239, loss: 1939.594604\n",
      "Train: step:  88900, time: 0.281, loss: 1103.904541\n",
      "Train: step:  88910, time: 0.258, loss: 463.765228\n",
      "Train: step:  88920, time: 0.263, loss: 1450.534790\n",
      "Train: step:  88930, time: 0.241, loss: 1974.793213\n",
      "Train: step:  88940, time: 0.246, loss: 885.031311\n",
      "Train: step:  88950, time: 0.245, loss: 2135.401367\n",
      "Train: step:  88960, time: 0.239, loss: 2910.644043\n",
      "Train: step:  88970, time: 0.238, loss: 827.271484\n",
      "Train: step:  88980, time: 0.264, loss: 2270.159912\n",
      "Train: step:  88990, time: 0.232, loss: 311.135559\n",
      "Train: step:  89000, time: 0.263, loss: 1642.878052\n",
      "Train: step:  89010, time: 0.245, loss: 2099.487061\n",
      "Train: step:  89020, time: 0.248, loss: 1705.251709\n",
      "Train: step:  89030, time: 0.241, loss: 1615.152588\n",
      "Train: step:  89040, time: 0.247, loss: 2003.227783\n",
      "Train: step:  89050, time: 0.245, loss: 2166.441406\n",
      "Train: step:  89060, time: 0.263, loss: 1659.793579\n",
      "Train: step:  89070, time: 0.269, loss: 2021.795532\n",
      "Train: step:  89080, time: 0.245, loss: 4036.537354\n",
      "Train: step:  89090, time: 0.242, loss: 582.451172\n",
      "Train: step:  89100, time: 0.236, loss: 2573.398926\n",
      "Train: step:  89110, time: 0.247, loss: 2910.672607\n",
      "Train: step:  89120, time: 0.242, loss: 1648.062256\n",
      "Train: step:  89130, time: 0.247, loss: 2374.474365\n",
      "Train: step:  89140, time: 0.245, loss: 2665.257812\n",
      "Train: step:  89150, time: 0.242, loss: 3689.700439\n",
      "Train: step:  89160, time: 0.244, loss: 2245.183350\n",
      "Train: step:  89170, time: 0.252, loss: 1852.577393\n",
      "Train: step:  89180, time: 0.244, loss: 3480.060059\n",
      "Train: step:  89190, time: 0.241, loss: 2280.545166\n",
      "Train: step:  89200, time: 0.251, loss: 2462.827393\n",
      "Train: step:  89210, time: 0.238, loss: 1096.572021\n",
      "Train: step:  89220, time: 0.283, loss: 372.968994\n",
      "Train: step:  89230, time: 0.240, loss: 1451.480103\n",
      "Train: step:  89240, time: 0.242, loss: 1792.700562\n",
      "Train: step:  89250, time: 0.237, loss: 2608.678711\n",
      "Train: step:  89260, time: 0.266, loss: 1634.468384\n",
      "Train: step:  89270, time: 0.266, loss: 738.687500\n",
      "Train: step:  89280, time: 0.243, loss: 3190.890137\n",
      "Train: step:  89290, time: 0.273, loss: 2917.528809\n",
      "Train: step:  89300, time: 0.232, loss: 1020.132812\n",
      "Train: step:  89310, time: 0.259, loss: 1894.469482\n",
      "Train: step:  89320, time: 0.245, loss: 771.990356\n",
      "Train: step:  89330, time: 0.231, loss: 1024.845093\n",
      "Train: step:  89340, time: 0.239, loss: 2317.653320\n",
      "Train: step:  89350, time: 0.238, loss: 1276.723022\n",
      "Train: step:  89360, time: 0.234, loss: 1675.055420\n",
      "Train: step:  89370, time: 0.232, loss: 904.665955\n",
      "Train: step:  89380, time: 0.251, loss: 210.895798\n",
      "Train: step:  89390, time: 0.255, loss: 2528.071045\n",
      "Train: step:  89400, time: 0.245, loss: 3607.118896\n",
      "Train: step:  89410, time: 0.272, loss: 721.895752\n",
      "Train: step:  89420, time: 0.279, loss: 1846.281494\n",
      "Train: step:  89430, time: 0.283, loss: 1551.453613\n",
      "Train: step:  89440, time: 0.237, loss: 1038.633301\n",
      "Train: step:  89450, time: 0.240, loss: 2005.875732\n",
      "Train: step:  89460, time: 0.244, loss: 2499.705566\n",
      "Train: step:  89470, time: 0.249, loss: 855.960693\n",
      "Train: step:  89480, time: 0.243, loss: 1069.152466\n",
      "Train: step:  89490, time: 0.243, loss: 950.236389\n",
      "Train: step:  89500, time: 0.250, loss: 3310.167725\n",
      "Train: step:  89510, time: 0.273, loss: 1341.689819\n",
      "Train: step:  89520, time: 0.241, loss: 2568.478516\n",
      "Train: step:  89530, time: 0.268, loss: 1448.635132\n",
      "Train: step:  89540, time: 0.281, loss: 1257.453735\n",
      "Train: step:  89550, time: 0.251, loss: 1953.617554\n",
      "Train: step:  89560, time: 0.270, loss: 1309.143066\n",
      "Train: step:  89570, time: 0.240, loss: 1554.172241\n",
      "Train: step:  89580, time: 0.248, loss: 2629.863037\n",
      "Train: step:  89590, time: 0.245, loss: 1713.889526\n",
      "Train: step:  89600, time: 0.273, loss: 520.049500\n",
      "Train: step:  89610, time: 0.246, loss: 2462.623047\n",
      "Train: step:  89620, time: 0.283, loss: 2274.761963\n",
      "Train: step:  89630, time: 0.243, loss: 2100.447266\n",
      "Train: step:  89640, time: 0.242, loss: 795.809265\n",
      "Train: step:  89650, time: 0.239, loss: 3616.838867\n",
      "Train: step:  89660, time: 0.242, loss: 2585.989258\n",
      "Train: step:  89670, time: 0.267, loss: 1053.912354\n",
      "Train: step:  89680, time: 0.251, loss: 1847.710693\n",
      "Train: step:  89690, time: 0.274, loss: 1410.732178\n",
      "Train: step:  89700, time: 0.266, loss: 1692.357544\n",
      "Train: step:  89710, time: 0.244, loss: 665.677124\n",
      "Train: step:  89720, time: 0.276, loss: 778.777832\n",
      "Train: step:  89730, time: 0.237, loss: 3242.261719\n",
      "Train: step:  89740, time: 0.241, loss: 2395.286377\n",
      "Train: step:  89750, time: 0.256, loss: 945.551392\n",
      "Train: step:  89760, time: 0.257, loss: 2748.863037\n",
      "Train: step:  89770, time: 0.244, loss: 3087.067383\n",
      "Train: step:  89780, time: 0.239, loss: 245.804245\n",
      "Train: step:  89790, time: 0.240, loss: 1990.562622\n",
      "Train: step:  89800, time: 0.241, loss: 2298.657227\n",
      "Train: step:  89810, time: 0.232, loss: 2163.917969\n",
      "Train: step:  89820, time: 0.235, loss: 1628.667236\n",
      "Train: step:  89830, time: 0.258, loss: 1353.398071\n",
      "Train: step:  89840, time: 0.276, loss: 2121.959961\n",
      "Train: step:  89850, time: 0.250, loss: 2023.036865\n",
      "Train: step:  89860, time: 0.243, loss: 960.431946\n",
      "Train: step:  89870, time: 0.246, loss: 1256.732056\n",
      "Train: step:  89880, time: 0.242, loss: 3192.673828\n",
      "Train: step:  89890, time: 0.252, loss: 335.641205\n",
      "Train: step:  89900, time: 0.273, loss: 1098.276978\n",
      "Train: step:  89910, time: 0.285, loss: 2333.667969\n",
      "Train: step:  89920, time: 0.252, loss: 1253.419434\n",
      "Train: step:  89930, time: 0.251, loss: 1516.462769\n",
      "Train: step:  89940, time: 0.237, loss: 2693.572998\n",
      "Train: step:  89950, time: 0.242, loss: 1175.691284\n",
      "Train: step:  89960, time: 0.248, loss: 2187.665527\n",
      "Train: step:  89970, time: 0.297, loss: 2972.531494\n",
      "Train: step:  89980, time: 0.249, loss: 2470.106445\n",
      "Train: step:  89990, time: 0.291, loss: 3282.528809\n",
      "Train: step:  90000, time: 0.232, loss: 2266.301270\n",
      "Train: step:  90010, time: 0.273, loss: 1913.027100\n",
      "Train: step:  90020, time: 0.270, loss: 3952.573486\n",
      "Train: step:  90030, time: 0.284, loss: 2146.668701\n",
      "Train: step:  90040, time: 0.254, loss: 1152.887329\n",
      "Train: step:  90050, time: 0.239, loss: 2456.316895\n",
      "Train: step:  90060, time: 0.259, loss: 2065.007568\n",
      "Train: step:  90070, time: 0.264, loss: 2531.274902\n",
      "Train: step:  90080, time: 0.276, loss: 3026.000000\n",
      "Train: step:  90090, time: 0.264, loss: 2620.766357\n",
      "Train: step:  90100, time: 0.241, loss: 2287.049072\n",
      "Train: step:  90110, time: 0.272, loss: 2911.569092\n",
      "Train: step:  90120, time: 0.253, loss: 1085.730225\n",
      "Train: step:  90130, time: 0.250, loss: 2151.647705\n",
      "Train: step:  90140, time: 0.248, loss: 1269.451416\n",
      "Train: step:  90150, time: 0.258, loss: 4830.156738\n",
      "Train: step:  90160, time: 0.243, loss: 1842.437256\n",
      "Train: step:  90170, time: 0.283, loss: 2939.725098\n",
      "Train: step:  90180, time: 0.246, loss: 892.281799\n",
      "Train: step:  90190, time: 0.246, loss: 2536.562500\n",
      "Train: step:  90200, time: 0.242, loss: 896.787659\n",
      "Train: step:  90210, time: 0.245, loss: 1013.695007\n",
      "Train: step:  90220, time: 0.289, loss: 2598.804443\n",
      "Train: step:  90230, time: 0.243, loss: 2314.853271\n",
      "Train: step:  90240, time: 0.243, loss: 931.024048\n",
      "Train: step:  90250, time: 0.244, loss: 1386.203491\n",
      "Train: step:  90260, time: 0.240, loss: 825.075928\n",
      "Train: step:  90270, time: 0.253, loss: 1966.881958\n",
      "Train: step:  90280, time: 0.238, loss: 756.910278\n",
      "Train: step:  90290, time: 0.241, loss: 1574.703857\n",
      "Train: step:  90300, time: 0.281, loss: 4378.234863\n",
      "Train: step:  90310, time: 0.241, loss: 1040.302246\n",
      "Train: step:  90320, time: 0.245, loss: 2565.010254\n",
      "Train: step:  90330, time: 0.285, loss: 876.951477\n",
      "Train: step:  90340, time: 0.276, loss: 2736.375244\n",
      "Train: step:  90350, time: 0.242, loss: 3764.792480\n",
      "Train: step:  90360, time: 0.241, loss: 766.367981\n",
      "Train: step:  90370, time: 0.241, loss: 1072.353638\n",
      "Train: step:  90380, time: 0.242, loss: 342.093353\n",
      "Train: step:  90390, time: 0.234, loss: 1789.438843\n",
      "Train: step:  90400, time: 0.242, loss: 1808.365723\n",
      "Train: step:  90410, time: 0.242, loss: 2666.396484\n",
      "Train: step:  90420, time: 0.247, loss: 1315.292358\n",
      "Train: step:  90430, time: 0.241, loss: 906.383362\n",
      "Train: step:  90440, time: 0.269, loss: 985.385071\n",
      "Train: step:  90450, time: 0.266, loss: 2242.205078\n",
      "Train: step:  90460, time: 0.244, loss: 1121.845581\n",
      "Train: step:  90470, time: 0.236, loss: 1120.554932\n",
      "Train: step:  90480, time: 0.239, loss: 4008.701904\n",
      "Train: step:  90490, time: 0.266, loss: 2716.438232\n",
      "Train: step:  90500, time: 0.230, loss: 2068.262695\n",
      "Train: step:  90510, time: 0.263, loss: 2965.921387\n",
      "Train: step:  90520, time: 0.247, loss: 1032.553223\n",
      "Train: step:  90530, time: 0.244, loss: 843.538147\n",
      "Train: step:  90540, time: 0.253, loss: 1661.905762\n",
      "Train: step:  90550, time: 0.240, loss: 1273.519531\n",
      "Train: step:  90560, time: 0.243, loss: 1484.281128\n",
      "Train: step:  90570, time: 0.246, loss: 1783.757080\n",
      "Train: step:  90580, time: 0.236, loss: 1466.122925\n",
      "Train: step:  90590, time: 0.243, loss: 1215.445190\n",
      "Train: step:  90600, time: 0.246, loss: 887.327332\n",
      "Train: step:  90610, time: 0.236, loss: 2407.629395\n",
      "Train: step:  90620, time: 0.241, loss: 753.169922\n",
      "Train: step:  90630, time: 0.240, loss: 800.703003\n",
      "Train: step:  90640, time: 0.239, loss: 2877.936768\n",
      "Train: step:  90650, time: 0.244, loss: 893.549011\n",
      "Train: step:  90660, time: 0.240, loss: 2444.837891\n",
      "Train: step:  90670, time: 0.250, loss: 1544.936523\n",
      "Train: step:  90680, time: 0.277, loss: 1554.474609\n",
      "Train: step:  90690, time: 0.243, loss: 2879.319824\n",
      "Train: step:  90700, time: 0.264, loss: 1941.593506\n",
      "Train: step:  90710, time: 0.271, loss: 668.576355\n",
      "Train: step:  90720, time: 0.241, loss: 1757.217651\n",
      "Train: step:  90730, time: 0.264, loss: 2074.664307\n",
      "Train: step:  90740, time: 0.267, loss: 2430.914307\n",
      "Train: step:  90750, time: 0.266, loss: 2398.805420\n",
      "Train: step:  90760, time: 0.237, loss: 3069.506348\n",
      "Train: step:  90770, time: 0.238, loss: 2766.285156\n",
      "Train: step:  90780, time: 0.251, loss: 1108.023560\n",
      "Train: step:  90790, time: 0.246, loss: 1584.136597\n",
      "Train: step:  90800, time: 0.243, loss: 943.693481\n",
      "Train: step:  90810, time: 0.241, loss: 2131.619873\n",
      "Train: step:  90820, time: 0.247, loss: 718.331238\n",
      "Train: step:  90830, time: 0.283, loss: 1646.967285\n",
      "Train: step:  90840, time: 0.243, loss: 1488.521362\n",
      "Train: step:  90850, time: 0.242, loss: 692.042786\n",
      "Train: step:  90860, time: 0.289, loss: 708.223145\n",
      "Train: step:  90870, time: 0.236, loss: 2113.965576\n",
      "Train: step:  90880, time: 0.278, loss: 915.104065\n",
      "Train: step:  90890, time: 0.244, loss: 1534.856323\n",
      "Train: step:  90900, time: 0.240, loss: 2878.429199\n",
      "Train: step:  90910, time: 0.258, loss: 1477.538208\n",
      "Train: step:  90920, time: 0.270, loss: 2627.727539\n",
      "Train: step:  90930, time: 0.270, loss: 2571.085449\n",
      "Train: step:  90940, time: 0.243, loss: 899.240967\n",
      "Train: step:  90950, time: 0.245, loss: 2135.022461\n",
      "Train: step:  90960, time: 0.234, loss: 3065.899902\n",
      "Train: step:  90970, time: 0.246, loss: 363.811157\n",
      "Train: step:  90980, time: 0.262, loss: 1745.304321\n",
      "Train: step:  90990, time: 0.240, loss: 1466.272461\n",
      "Train: step:  91000, time: 0.230, loss: 2452.897461\n",
      "Train: step:  91010, time: 0.245, loss: 2192.071777\n",
      "Train: step:  91020, time: 0.237, loss: 709.375183\n",
      "Train: step:  91030, time: 0.244, loss: 338.216675\n",
      "Train: step:  91040, time: 0.246, loss: 3092.295654\n",
      "Train: step:  91050, time: 0.270, loss: 2100.267822\n",
      "Train: step:  91060, time: 0.248, loss: 1924.449585\n",
      "Train: step:  91070, time: 0.258, loss: 1246.574585\n",
      "Train: step:  91080, time: 0.267, loss: 857.646790\n",
      "Train: step:  91090, time: 0.240, loss: 929.787354\n",
      "Train: step:  91100, time: 0.259, loss: 2474.156494\n",
      "Train: step:  91110, time: 0.280, loss: 1021.802856\n",
      "Train: step:  91120, time: 0.265, loss: 767.243225\n",
      "Train: step:  91130, time: 0.242, loss: 2175.655518\n",
      "Train: step:  91140, time: 0.241, loss: 2147.280273\n",
      "Train: step:  91150, time: 0.246, loss: 4477.333008\n",
      "Train: step:  91160, time: 0.252, loss: 175.750473\n",
      "Train: step:  91170, time: 0.253, loss: 1448.896118\n",
      "Train: step:  91180, time: 0.282, loss: 616.602783\n",
      "Train: step:  91190, time: 0.295, loss: 1366.598999\n",
      "Train: step:  91200, time: 0.245, loss: 483.587830\n",
      "Train: step:  91210, time: 0.242, loss: 423.103546\n",
      "Train: step:  91220, time: 0.237, loss: 1288.228638\n",
      "Train: step:  91230, time: 0.284, loss: 2269.827637\n",
      "Train: step:  91240, time: 0.259, loss: 858.921509\n",
      "Train: step:  91250, time: 0.251, loss: 1739.410156\n",
      "Train: step:  91260, time: 0.247, loss: 2361.268311\n",
      "Train: step:  91270, time: 0.235, loss: 1126.586548\n",
      "Train: step:  91280, time: 0.265, loss: 5058.190918\n",
      "Train: step:  91290, time: 0.254, loss: 260.208435\n",
      "Train: step:  91300, time: 0.240, loss: 2151.725830\n",
      "Train: step:  91310, time: 0.247, loss: 1845.721191\n",
      "Train: step:  91320, time: 0.257, loss: 3946.290527\n",
      "Train: step:  91330, time: 0.242, loss: 2145.018311\n",
      "Train: step:  91340, time: 0.238, loss: 3366.844238\n",
      "Train: step:  91350, time: 0.261, loss: 2441.048096\n",
      "Train: step:  91360, time: 0.239, loss: 3239.382080\n",
      "Train: step:  91370, time: 0.243, loss: 998.756470\n",
      "Train: step:  91380, time: 0.241, loss: 1351.290405\n",
      "Train: step:  91390, time: 0.251, loss: 3008.176025\n",
      "Train: step:  91400, time: 0.242, loss: 1684.362061\n",
      "Train: step:  91410, time: 0.241, loss: 1240.314087\n",
      "Train: step:  91420, time: 0.251, loss: 1715.800781\n",
      "Train: step:  91430, time: 0.246, loss: 3263.733643\n",
      "Train: step:  91440, time: 0.279, loss: 1395.733032\n",
      "Train: step:  91450, time: 0.281, loss: 1980.625854\n",
      "Train: step:  91460, time: 0.241, loss: 2194.465332\n",
      "Train: step:  91470, time: 0.260, loss: 2226.868652\n",
      "Train: step:  91480, time: 0.244, loss: 2644.549072\n",
      "Train: step:  91490, time: 0.229, loss: 1551.265137\n",
      "Train: step:  91500, time: 0.220, loss: 1000.305603\n",
      "Train: step:  91510, time: 0.260, loss: 2483.799805\n",
      "Train: step:  91520, time: 0.234, loss: 2657.932129\n",
      "Train: step:  91530, time: 0.223, loss: 1530.197998\n",
      "Train: step:  91540, time: 0.220, loss: 2310.514648\n",
      "Train: step:  91550, time: 0.223, loss: 2457.845703\n",
      "Train: step:  91560, time: 0.234, loss: 2069.468506\n",
      "Train: step:  91570, time: 0.250, loss: 1562.500977\n",
      "Train: step:  91580, time: 0.231, loss: 2427.990967\n",
      "Train: step:  91590, time: 0.226, loss: 2372.687500\n",
      "Train: step:  91600, time: 0.253, loss: 2252.717285\n",
      "Train: step:  91610, time: 0.226, loss: 1514.360474\n",
      "Train: step:  91620, time: 0.271, loss: 1481.802856\n",
      "Train: step:  91630, time: 0.240, loss: 773.377747\n",
      "Train: step:  91640, time: 0.234, loss: 554.275818\n",
      "Train: step:  91650, time: 0.224, loss: 1556.390137\n",
      "Train: step:  91660, time: 0.230, loss: 1622.660645\n",
      "Train: step:  91670, time: 0.226, loss: 1248.023438\n",
      "Train: step:  91680, time: 0.233, loss: 478.894592\n",
      "Train: step:  91690, time: 0.232, loss: 574.888916\n",
      "Train: step:  91700, time: 0.234, loss: 3553.624512\n",
      "Train: step:  91710, time: 0.226, loss: 2422.856445\n",
      "Train: step:  91720, time: 0.230, loss: 2459.241699\n",
      "Train: step:  91730, time: 0.229, loss: 1263.372314\n",
      "Train: step:  91740, time: 0.229, loss: 2755.299072\n",
      "Train: step:  91750, time: 0.260, loss: 1038.110474\n",
      "Train: step:  91760, time: 0.222, loss: 2679.082031\n",
      "Train: step:  91770, time: 0.230, loss: 2396.809082\n",
      "Train: step:  91780, time: 0.228, loss: 1813.750610\n",
      "Train: step:  91790, time: 0.250, loss: 1076.766724\n",
      "Train: step:  91800, time: 0.220, loss: 2110.849854\n",
      "Train: step:  91810, time: 0.229, loss: 2769.186523\n",
      "Train: step:  91820, time: 0.225, loss: 2244.670166\n",
      "Train: step:  91830, time: 0.222, loss: 269.359070\n",
      "Train: step:  91840, time: 0.228, loss: 2471.384766\n",
      "Train: step:  91850, time: 0.254, loss: 1544.939453\n",
      "Train: step:  91860, time: 0.227, loss: 1378.897461\n",
      "Train: step:  91870, time: 0.227, loss: 825.982544\n",
      "Train: step:  91880, time: 0.216, loss: 1126.569092\n",
      "Train: step:  91890, time: 0.230, loss: 263.236328\n",
      "Train: step:  91900, time: 0.231, loss: 1818.462036\n",
      "Train: step:  91910, time: 0.228, loss: 1450.782104\n",
      "Train: step:  91920, time: 0.222, loss: 3364.087402\n",
      "Train: step:  91930, time: 0.225, loss: 726.025818\n",
      "Train: step:  91940, time: 0.226, loss: 225.129669\n",
      "Train: step:  91950, time: 0.225, loss: 2149.502197\n",
      "Train: step:  91960, time: 0.227, loss: 2275.568359\n",
      "Train: step:  91970, time: 0.221, loss: 1823.705688\n",
      "Train: step:  91980, time: 0.223, loss: 2166.346924\n",
      "Train: step:  91990, time: 0.225, loss: 2841.241455\n",
      "Train: step:  92000, time: 0.223, loss: 2427.665527\n",
      "Train: step:  92010, time: 0.258, loss: 2096.656738\n",
      "Train: step:  92020, time: 0.222, loss: 1537.094116\n",
      "Train: step:  92030, time: 0.229, loss: 2004.778076\n",
      "Train: step:  92040, time: 0.228, loss: 3635.196533\n",
      "Train: step:  92050, time: 0.225, loss: 327.317261\n",
      "Train: step:  92060, time: 0.227, loss: 3752.168945\n",
      "Train: step:  92070, time: 0.261, loss: 2593.298584\n",
      "Train: step:  92080, time: 0.223, loss: 1309.681641\n",
      "Train: step:  92090, time: 0.235, loss: 4183.289062\n",
      "Train: step:  92100, time: 0.224, loss: 2719.208984\n",
      "Train: step:  92110, time: 0.252, loss: 769.416321\n",
      "Train: step:  92120, time: 0.225, loss: 977.571472\n",
      "Train: step:  92130, time: 0.264, loss: 1021.199646\n",
      "Train: step:  92140, time: 0.220, loss: 1041.076172\n",
      "Train: step:  92150, time: 0.229, loss: 2437.878418\n",
      "Train: step:  92160, time: 0.245, loss: 1005.210999\n",
      "Train: step:  92170, time: 0.248, loss: 2423.001953\n",
      "Train: step:  92180, time: 0.260, loss: 1482.460083\n",
      "Train: step:  92190, time: 0.231, loss: 1933.115845\n",
      "Train: step:  92200, time: 0.225, loss: 418.119751\n",
      "Train: step:  92210, time: 0.286, loss: 4735.180664\n",
      "Train: step:  92220, time: 0.228, loss: 2350.339355\n",
      "Train: step:  92230, time: 0.232, loss: 1531.802002\n",
      "Train: step:  92240, time: 0.255, loss: 3866.676270\n",
      "Train: step:  92250, time: 0.228, loss: 1577.544556\n",
      "Train: step:  92260, time: 0.265, loss: 433.969666\n",
      "Train: step:  92270, time: 0.270, loss: 543.325867\n",
      "Train: step:  92280, time: 0.226, loss: 1741.802979\n",
      "Train: step:  92290, time: 0.235, loss: 768.921448\n",
      "Train: step:  92300, time: 0.268, loss: 1035.476562\n",
      "Train: step:  92310, time: 0.231, loss: 3385.453613\n",
      "Train: step:  92320, time: 0.229, loss: 542.191162\n",
      "Train: step:  92330, time: 0.226, loss: 2003.043579\n",
      "Train: step:  92340, time: 0.230, loss: 939.825989\n",
      "Train: step:  92350, time: 0.234, loss: 3414.191162\n",
      "Train: step:  92360, time: 0.232, loss: 978.935608\n",
      "Train: step:  92370, time: 0.272, loss: 1121.401611\n",
      "Train: step:  92380, time: 0.238, loss: 799.466797\n",
      "Train: step:  92390, time: 0.237, loss: 1762.684326\n",
      "Train: step:  92400, time: 0.270, loss: 3772.056641\n",
      "Train: step:  92410, time: 0.259, loss: 1224.295898\n",
      "Train: step:  92420, time: 0.240, loss: 3414.002686\n",
      "Train: step:  92430, time: 0.262, loss: 1817.101074\n",
      "Train: step:  92440, time: 0.238, loss: 2697.829346\n",
      "Train: step:  92450, time: 0.232, loss: 2089.352051\n",
      "Train: step:  92460, time: 0.256, loss: 2165.760742\n",
      "Train: step:  92470, time: 0.259, loss: 1390.613037\n",
      "Train: step:  92480, time: 0.266, loss: 1872.405029\n",
      "Train: step:  92490, time: 0.235, loss: 2705.315918\n",
      "Train: step:  92500, time: 0.230, loss: 1613.627197\n",
      "Train: step:  92510, time: 0.233, loss: 2421.431641\n",
      "Train: step:  92520, time: 0.240, loss: 871.578735\n",
      "Train: step:  92530, time: 0.233, loss: 560.510986\n",
      "Train: step:  92540, time: 0.233, loss: 2071.796631\n",
      "Train: step:  92550, time: 0.228, loss: 2374.418213\n",
      "Train: step:  92560, time: 0.229, loss: 1770.396851\n",
      "Train: step:  92570, time: 0.239, loss: 829.179688\n",
      "Train: step:  92580, time: 0.233, loss: 1192.641724\n",
      "Train: step:  92590, time: 0.233, loss: 2036.076538\n",
      "Train: step:  92600, time: 0.238, loss: 1682.538818\n",
      "Train: step:  92610, time: 0.236, loss: 801.166321\n",
      "Train: step:  92620, time: 0.239, loss: 2227.328125\n",
      "Train: step:  92630, time: 0.273, loss: 4936.772461\n",
      "Train: step:  92640, time: 0.242, loss: 1345.394653\n",
      "Train: step:  92650, time: 0.243, loss: 2689.380371\n",
      "Train: step:  92660, time: 0.241, loss: 3017.278076\n",
      "Train: step:  92670, time: 0.236, loss: 2577.200195\n",
      "Train: step:  92680, time: 0.280, loss: 775.531616\n",
      "Train: step:  92690, time: 0.243, loss: 2448.571045\n",
      "Train: step:  92700, time: 0.260, loss: 733.301331\n",
      "Train: step:  92710, time: 0.245, loss: 5308.056641\n",
      "Train: step:  92720, time: 0.243, loss: 2257.051025\n",
      "Train: step:  92730, time: 0.269, loss: 2865.256104\n",
      "Train: step:  92740, time: 0.265, loss: 2518.976807\n",
      "Train: step:  92750, time: 0.287, loss: 727.814575\n",
      "Train: step:  92760, time: 0.240, loss: 2233.054199\n",
      "Train: step:  92770, time: 0.244, loss: 1068.789429\n",
      "Train: step:  92780, time: 0.248, loss: 2126.155762\n",
      "Train: step:  92790, time: 0.255, loss: 1618.795166\n",
      "Train: step:  92800, time: 0.246, loss: 2595.362793\n",
      "Train: step:  92810, time: 0.267, loss: 1279.118896\n",
      "Train: step:  92820, time: 0.244, loss: 1007.473816\n",
      "Train: step:  92830, time: 0.250, loss: 1490.983765\n",
      "Train: step:  92840, time: 0.262, loss: 1526.228149\n",
      "Train: step:  92850, time: 0.269, loss: 1811.955444\n",
      "Train: step:  92860, time: 0.278, loss: 1164.507202\n",
      "Train: step:  92870, time: 0.310, loss: 1707.915527\n",
      "Train: step:  92880, time: 0.247, loss: 648.083069\n",
      "Train: step:  92890, time: 0.265, loss: 1773.948730\n",
      "Train: step:  92900, time: 0.243, loss: 1779.709229\n",
      "Train: step:  92910, time: 0.244, loss: 2262.619873\n",
      "Train: step:  92920, time: 0.240, loss: 301.440277\n",
      "Train: step:  92930, time: 0.243, loss: 751.481445\n",
      "Train: step:  92940, time: 0.270, loss: 2284.404053\n",
      "Train: step:  92950, time: 0.271, loss: 2567.480957\n",
      "Train: step:  92960, time: 0.232, loss: 1906.239746\n",
      "Train: step:  92970, time: 0.238, loss: 3290.318359\n",
      "Train: step:  92980, time: 0.251, loss: 2982.061035\n",
      "Train: step:  92990, time: 0.290, loss: 2044.439453\n",
      "Train: step:  93000, time: 0.244, loss: 2471.594727\n",
      "Train: step:  93010, time: 0.278, loss: 2676.469482\n",
      "Train: step:  93020, time: 0.241, loss: 1232.339233\n",
      "Train: step:  93030, time: 0.269, loss: 2185.802979\n",
      "Train: step:  93040, time: 0.244, loss: 2371.199219\n",
      "Train: step:  93050, time: 0.242, loss: 2123.712158\n",
      "Train: step:  93060, time: 0.243, loss: 1876.375610\n",
      "Train: step:  93070, time: 0.284, loss: 4098.850098\n",
      "Train: step:  93080, time: 0.245, loss: 2815.886475\n",
      "Train: step:  93090, time: 0.252, loss: 286.175934\n",
      "Train: step:  93100, time: 0.255, loss: 413.357605\n",
      "Train: step:  93110, time: 0.279, loss: 1416.275024\n",
      "Train: step:  93120, time: 0.248, loss: 3094.113281\n",
      "Train: step:  93130, time: 0.247, loss: 1814.962891\n",
      "Train: step:  93140, time: 0.292, loss: 2489.263672\n",
      "Train: step:  93150, time: 0.245, loss: 1558.332153\n",
      "Train: step:  93160, time: 0.246, loss: 500.184387\n",
      "Train: step:  93170, time: 0.240, loss: 956.574524\n",
      "Train: step:  93180, time: 0.241, loss: 1715.170654\n",
      "Train: step:  93190, time: 0.247, loss: 1743.884521\n",
      "Train: step:  93200, time: 0.253, loss: 1579.995483\n",
      "Train: step:  93210, time: 0.246, loss: 3883.068604\n",
      "Train: step:  93220, time: 0.268, loss: 2883.521973\n",
      "Train: step:  93230, time: 0.266, loss: 2345.559082\n",
      "Train: step:  93240, time: 0.245, loss: 2058.124512\n",
      "Train: step:  93250, time: 0.267, loss: 1575.789917\n",
      "Train: step:  93260, time: 0.266, loss: 1205.779663\n",
      "Train: step:  93270, time: 0.265, loss: 1531.722534\n",
      "Train: step:  93280, time: 0.242, loss: 3296.293945\n",
      "Train: step:  93290, time: 0.241, loss: 1753.280029\n",
      "Train: step:  93300, time: 0.242, loss: 1491.643921\n",
      "Train: step:  93310, time: 0.275, loss: 1392.745117\n",
      "Train: step:  93320, time: 0.242, loss: 255.959274\n",
      "Train: step:  93330, time: 0.239, loss: 3512.207520\n",
      "Train: step:  93340, time: 0.251, loss: 867.350464\n",
      "Train: step:  93350, time: 0.288, loss: 4062.879883\n",
      "Train: step:  93360, time: 0.268, loss: 2434.991943\n",
      "Train: step:  93370, time: 0.270, loss: 2096.424561\n",
      "Train: step:  93380, time: 0.232, loss: 1639.256714\n",
      "Train: step:  93390, time: 0.233, loss: 695.885559\n",
      "Train: step:  93400, time: 0.239, loss: 923.411011\n",
      "Train: step:  93410, time: 0.271, loss: 1177.005981\n",
      "Train: step:  93420, time: 0.234, loss: 3921.741699\n",
      "Train: step:  93430, time: 0.257, loss: 2398.506836\n",
      "Train: step:  93440, time: 0.233, loss: 1704.740356\n",
      "Train: step:  93450, time: 0.246, loss: 1146.149536\n",
      "Train: step:  93460, time: 0.229, loss: 2178.605957\n",
      "Train: step:  93470, time: 0.260, loss: 991.330261\n",
      "Train: step:  93480, time: 0.231, loss: 1676.330566\n",
      "Train: step:  93490, time: 0.241, loss: 314.161682\n",
      "Train: step:  93500, time: 0.234, loss: 1667.954468\n",
      "Train: step:  93510, time: 0.252, loss: 792.591553\n",
      "Train: step:  93520, time: 0.246, loss: 2680.458740\n",
      "Train: step:  93530, time: 0.264, loss: 504.032074\n",
      "Train: step:  93540, time: 0.252, loss: 992.175598\n",
      "Train: step:  93550, time: 0.234, loss: 1763.021240\n",
      "Train: step:  93560, time: 0.228, loss: 2918.962646\n",
      "Train: step:  93570, time: 0.239, loss: 3470.760010\n",
      "Train: step:  93580, time: 0.253, loss: 1495.446899\n",
      "Train: step:  93590, time: 0.274, loss: 1328.618896\n",
      "Train: step:  93600, time: 0.230, loss: 1697.861450\n",
      "Train: step:  93610, time: 0.236, loss: 1352.673462\n",
      "Train: step:  93620, time: 0.226, loss: 2044.184570\n",
      "Train: step:  93630, time: 0.251, loss: 1846.851685\n",
      "Train: step:  93640, time: 0.235, loss: 1232.007812\n",
      "Train: step:  93650, time: 0.267, loss: 1965.853638\n",
      "Train: step:  93660, time: 0.245, loss: 2065.029785\n",
      "Train: step:  93670, time: 0.230, loss: 1545.887329\n",
      "Train: step:  93680, time: 0.239, loss: 2300.779541\n",
      "Train: step:  93690, time: 0.252, loss: 637.206848\n",
      "Train: step:  93700, time: 0.272, loss: 2654.268066\n",
      "Train: step:  93710, time: 0.264, loss: 2397.352783\n",
      "Train: step:  93720, time: 0.228, loss: 1313.067749\n",
      "Train: step:  93730, time: 0.241, loss: 2484.903320\n",
      "Train: step:  93740, time: 0.241, loss: 3970.749756\n",
      "Train: step:  93750, time: 0.221, loss: 910.223938\n",
      "Train: step:  93760, time: 0.231, loss: 1133.787476\n",
      "Train: step:  93770, time: 0.257, loss: 3417.292236\n",
      "Train: step:  93780, time: 0.239, loss: 755.472046\n",
      "Train: step:  93790, time: 0.235, loss: 2178.866699\n",
      "Train: step:  93800, time: 0.234, loss: 977.469666\n",
      "Train: step:  93810, time: 0.248, loss: 1403.730835\n",
      "Train: step:  93820, time: 0.272, loss: 836.716003\n",
      "Train: step:  93830, time: 0.228, loss: 1759.199951\n",
      "Train: step:  93840, time: 0.228, loss: 2469.856934\n",
      "Train: step:  93850, time: 0.234, loss: 2705.932617\n",
      "Train: step:  93860, time: 0.239, loss: 781.193176\n",
      "Train: step:  93870, time: 0.278, loss: 3541.544678\n",
      "Train: step:  93880, time: 0.287, loss: 510.494873\n",
      "Train: step:  93890, time: 0.230, loss: 1331.513794\n",
      "Train: step:  93900, time: 0.255, loss: 2955.866943\n",
      "Train: step:  93910, time: 0.229, loss: 1050.548828\n",
      "Train: step:  93920, time: 0.229, loss: 1920.750610\n",
      "Train: step:  93930, time: 0.242, loss: 2900.103516\n",
      "Train: step:  93940, time: 0.235, loss: 2108.033203\n",
      "Train: step:  93950, time: 0.238, loss: 621.700439\n",
      "Train: step:  93960, time: 0.229, loss: 817.804749\n",
      "Train: step:  93970, time: 0.230, loss: 2754.781006\n",
      "Train: step:  93980, time: 0.236, loss: 936.047852\n",
      "Train: step:  93990, time: 0.237, loss: 1671.011353\n",
      "Train: step:  94000, time: 0.225, loss: 1218.234131\n",
      "Train: step:  94010, time: 0.223, loss: 2250.477051\n",
      "Train: step:  94020, time: 0.271, loss: 2008.552124\n",
      "Train: step:  94030, time: 0.231, loss: 2561.825928\n",
      "Train: step:  94040, time: 0.238, loss: 1701.226562\n",
      "Train: step:  94050, time: 0.253, loss: 1389.187744\n",
      "Train: step:  94060, time: 0.226, loss: 295.119080\n",
      "Train: step:  94070, time: 0.243, loss: 1762.469482\n",
      "Train: step:  94080, time: 0.227, loss: 288.002533\n",
      "Train: step:  94090, time: 0.230, loss: 1235.977539\n",
      "Train: step:  94100, time: 0.249, loss: 1753.406982\n",
      "Train: step:  94110, time: 0.235, loss: 342.764648\n",
      "Train: step:  94120, time: 0.221, loss: 2034.187256\n",
      "Train: step:  94130, time: 0.239, loss: 2722.884277\n",
      "Train: step:  94140, time: 0.238, loss: 1828.544678\n",
      "Train: step:  94150, time: 0.231, loss: 4199.385742\n",
      "Train: step:  94160, time: 0.241, loss: 1304.010864\n",
      "Train: step:  94170, time: 0.226, loss: 528.258179\n",
      "Train: step:  94180, time: 0.254, loss: 1016.798218\n",
      "Train: step:  94190, time: 0.234, loss: 678.881775\n",
      "Train: step:  94200, time: 0.230, loss: 2992.702637\n",
      "Train: step:  94210, time: 0.227, loss: 2954.216309\n",
      "Train: step:  94220, time: 0.233, loss: 3240.017578\n",
      "Train: step:  94230, time: 0.225, loss: 405.202423\n",
      "Train: step:  94240, time: 0.236, loss: 2209.693604\n",
      "Train: step:  94250, time: 0.232, loss: 1315.969849\n",
      "Train: step:  94260, time: 0.228, loss: 2080.594482\n",
      "Train: step:  94270, time: 0.236, loss: 2967.871826\n",
      "Train: step:  94280, time: 0.230, loss: 2212.852295\n",
      "Train: step:  94290, time: 0.257, loss: 376.209290\n",
      "Train: step:  94300, time: 0.252, loss: 3343.106934\n",
      "Train: step:  94310, time: 0.228, loss: 1405.045654\n",
      "Train: step:  94320, time: 0.225, loss: 1883.524658\n",
      "Train: step:  94330, time: 0.230, loss: 1831.481812\n",
      "Train: step:  94340, time: 0.228, loss: 703.873596\n",
      "Train: step:  94350, time: 0.252, loss: 1386.497314\n",
      "Train: step:  94360, time: 0.248, loss: 842.082520\n",
      "Train: step:  94370, time: 0.224, loss: 3216.802979\n",
      "Train: step:  94380, time: 0.223, loss: 1277.115234\n",
      "Train: step:  94390, time: 0.217, loss: 3024.751221\n",
      "Train: step:  94400, time: 0.308, loss: 3022.671631\n",
      "Train: step:  94410, time: 0.238, loss: 1893.222534\n",
      "Train: step:  94420, time: 0.222, loss: 1747.888428\n",
      "Train: step:  94430, time: 0.231, loss: 1778.778931\n",
      "Train: step:  94440, time: 0.224, loss: 2736.640869\n",
      "Train: step:  94450, time: 0.269, loss: 2769.328613\n",
      "Train: step:  94460, time: 0.252, loss: 2381.440674\n",
      "Train: step:  94470, time: 0.225, loss: 4071.361816\n",
      "Train: step:  94480, time: 0.231, loss: 898.800537\n",
      "Train: step:  94490, time: 0.233, loss: 920.506897\n",
      "Train: step:  94500, time: 0.267, loss: 1683.237305\n",
      "Train: step:  94510, time: 0.247, loss: 3549.541504\n",
      "Train: step:  94520, time: 0.265, loss: 1601.024048\n",
      "Train: step:  94530, time: 0.233, loss: 3696.492676\n",
      "Train: step:  94540, time: 0.273, loss: 2127.741943\n",
      "Train: step:  94550, time: 0.266, loss: 1651.846313\n",
      "Train: step:  94560, time: 0.244, loss: 2134.727783\n",
      "Train: step:  94570, time: 0.236, loss: 924.567383\n",
      "Train: step:  94580, time: 0.256, loss: 1863.452393\n",
      "Train: step:  94590, time: 0.245, loss: 1827.928711\n",
      "Train: step:  94600, time: 0.246, loss: 5042.474609\n",
      "Train: step:  94610, time: 0.242, loss: 1018.524658\n",
      "Train: step:  94620, time: 0.265, loss: 690.611694\n",
      "Train: step:  94630, time: 0.247, loss: 3108.065918\n",
      "Train: step:  94640, time: 0.249, loss: 2342.510498\n",
      "Train: step:  94650, time: 0.241, loss: 1857.847412\n",
      "Train: step:  94660, time: 0.260, loss: 568.604431\n",
      "Train: step:  94670, time: 0.275, loss: 1475.308960\n",
      "Train: step:  94680, time: 0.239, loss: 2435.818604\n",
      "Train: step:  94690, time: 0.252, loss: 734.317017\n",
      "Train: step:  94700, time: 0.260, loss: 2372.946045\n",
      "Train: step:  94710, time: 0.242, loss: 2446.608398\n",
      "Train: step:  94720, time: 0.225, loss: 3585.165039\n",
      "Train: step:  94730, time: 0.235, loss: 4183.048828\n",
      "Train: step:  94740, time: 0.240, loss: 1150.595825\n",
      "Train: step:  94750, time: 0.265, loss: 1157.293701\n",
      "Train: step:  94760, time: 0.232, loss: 1520.479736\n",
      "Train: step:  94770, time: 0.238, loss: 1525.853882\n",
      "Train: step:  94780, time: 0.238, loss: 1720.904297\n",
      "Train: step:  94790, time: 0.249, loss: 2258.350098\n",
      "Train: step:  94800, time: 0.256, loss: 2683.741455\n",
      "Train: step:  94810, time: 0.257, loss: 1974.461304\n",
      "Train: step:  94820, time: 0.253, loss: 1359.718506\n",
      "Train: step:  94830, time: 0.236, loss: 854.040283\n",
      "Train: step:  94840, time: 0.264, loss: 1010.540100\n",
      "Train: step:  94850, time: 0.238, loss: 2181.911377\n",
      "Train: step:  94860, time: 0.243, loss: 872.848755\n",
      "Train: step:  94870, time: 0.224, loss: 1211.302612\n",
      "Train: step:  94880, time: 0.225, loss: 2293.357178\n",
      "Train: step:  94890, time: 0.241, loss: 2968.982666\n",
      "Train: step:  94900, time: 0.258, loss: 1616.206909\n",
      "Train: step:  94910, time: 0.262, loss: 1763.301270\n",
      "Train: step:  94920, time: 0.238, loss: 1175.727905\n",
      "Train: step:  94930, time: 0.254, loss: 725.720276\n",
      "Train: step:  94940, time: 0.247, loss: 1189.947632\n",
      "Train: step:  94950, time: 0.252, loss: 1795.380981\n",
      "Train: step:  94960, time: 0.240, loss: 1478.847778\n",
      "Train: step:  94970, time: 0.263, loss: 358.674103\n",
      "Train: step:  94980, time: 0.252, loss: 1510.412231\n",
      "Train: step:  94990, time: 0.234, loss: 3072.714111\n",
      "Train: step:  95000, time: 0.245, loss: 785.512695\n",
      "Train: step:  95010, time: 0.242, loss: 2025.120483\n",
      "Train: step:  95020, time: 0.231, loss: 2567.063232\n",
      "Train: step:  95030, time: 0.224, loss: 906.102722\n",
      "Train: step:  95040, time: 0.224, loss: 1986.404907\n",
      "Train: step:  95050, time: 0.253, loss: 1372.980347\n",
      "Train: step:  95060, time: 0.240, loss: 1354.433228\n",
      "Train: step:  95070, time: 0.243, loss: 1206.404907\n",
      "Train: step:  95080, time: 0.234, loss: 2171.261963\n",
      "Train: step:  95090, time: 0.254, loss: 1132.665405\n",
      "Train: step:  95100, time: 0.236, loss: 700.357239\n",
      "Train: step:  95110, time: 0.227, loss: 4406.871094\n",
      "Train: step:  95120, time: 0.235, loss: 1078.979370\n",
      "Train: step:  95130, time: 0.240, loss: 2354.718018\n",
      "Train: step:  95140, time: 0.239, loss: 290.923798\n",
      "Train: step:  95150, time: 0.233, loss: 1652.095093\n",
      "Train: step:  95160, time: 0.233, loss: 1085.518555\n",
      "Train: step:  95170, time: 0.262, loss: 2484.982666\n",
      "Train: step:  95180, time: 0.234, loss: 928.175476\n",
      "Train: step:  95190, time: 0.232, loss: 3194.376221\n",
      "Train: step:  95200, time: 0.240, loss: 882.073669\n",
      "Train: step:  95210, time: 0.219, loss: 1064.216064\n",
      "Train: step:  95220, time: 0.223, loss: 3791.083984\n",
      "Train: step:  95230, time: 0.231, loss: 2524.053711\n",
      "Train: step:  95240, time: 0.225, loss: 2696.153809\n",
      "Train: step:  95250, time: 0.259, loss: 2990.935059\n",
      "Train: step:  95260, time: 0.255, loss: 2392.587891\n",
      "Train: step:  95270, time: 0.247, loss: 2589.220459\n",
      "Train: step:  95280, time: 0.250, loss: 2510.881104\n",
      "Train: step:  95290, time: 0.274, loss: 1776.262085\n",
      "Train: step:  95300, time: 0.251, loss: 1746.907349\n",
      "Train: step:  95310, time: 0.229, loss: 370.455872\n",
      "Train: step:  95320, time: 0.247, loss: 249.230057\n",
      "Train: step:  95330, time: 0.253, loss: 998.787964\n",
      "Train: step:  95340, time: 0.224, loss: 2029.538086\n",
      "Train: step:  95350, time: 0.268, loss: 1213.585205\n",
      "Train: step:  95360, time: 0.235, loss: 2862.603760\n",
      "Train: step:  95370, time: 0.263, loss: 1401.911499\n",
      "Train: step:  95380, time: 0.259, loss: 2069.849854\n",
      "Train: step:  95390, time: 0.240, loss: 854.374817\n",
      "Train: step:  95400, time: 0.224, loss: 1258.803223\n",
      "Train: step:  95410, time: 0.227, loss: 4141.317871\n",
      "Train: step:  95420, time: 0.242, loss: 3338.515137\n",
      "Train: step:  95430, time: 0.230, loss: 2652.505127\n",
      "Train: step:  95440, time: 0.235, loss: 2779.537109\n",
      "Train: step:  95450, time: 0.243, loss: 1383.257080\n",
      "Train: step:  95460, time: 0.241, loss: 2619.797363\n",
      "Train: step:  95470, time: 0.281, loss: 2422.080811\n",
      "Train: step:  95480, time: 0.222, loss: 3340.338623\n",
      "Train: step:  95490, time: 0.233, loss: 842.464111\n",
      "Train: step:  95500, time: 0.242, loss: 2036.940063\n",
      "Train: step:  95510, time: 0.229, loss: 1203.192627\n",
      "Train: step:  95520, time: 0.235, loss: 2274.222168\n",
      "Train: step:  95530, time: 0.257, loss: 1321.838257\n",
      "Train: step:  95540, time: 0.255, loss: 1345.641846\n",
      "Train: step:  95550, time: 0.245, loss: 1331.142456\n",
      "Train: step:  95560, time: 0.224, loss: 720.498352\n",
      "Train: step:  95570, time: 0.219, loss: 1631.945435\n",
      "Train: step:  95580, time: 0.231, loss: 919.855713\n",
      "Train: step:  95590, time: 0.234, loss: 1967.071045\n",
      "Train: step:  95600, time: 0.233, loss: 1980.989136\n",
      "Train: step:  95610, time: 0.235, loss: 3848.688232\n",
      "Train: step:  95620, time: 0.287, loss: 664.956848\n",
      "Train: step:  95630, time: 0.229, loss: 2022.534546\n",
      "Train: step:  95640, time: 0.240, loss: 227.318558\n",
      "Train: step:  95650, time: 0.243, loss: 2694.422607\n",
      "Train: step:  95660, time: 0.253, loss: 1288.290039\n",
      "Train: step:  95670, time: 0.257, loss: 1362.864014\n",
      "Train: step:  95680, time: 0.234, loss: 1508.253540\n",
      "Train: step:  95690, time: 0.248, loss: 2027.940430\n",
      "Train: step:  95700, time: 0.254, loss: 1949.818237\n",
      "Train: step:  95710, time: 0.260, loss: 2081.778564\n",
      "Train: step:  95720, time: 0.226, loss: 1510.964233\n",
      "Train: step:  95730, time: 0.243, loss: 232.639236\n",
      "Train: step:  95740, time: 0.238, loss: 1322.367554\n",
      "Train: step:  95750, time: 0.232, loss: 1696.475464\n",
      "Train: step:  95760, time: 0.221, loss: 2683.153809\n",
      "Train: step:  95770, time: 0.227, loss: 2849.718750\n",
      "Train: step:  95780, time: 0.261, loss: 2322.894775\n",
      "Train: step:  95790, time: 0.253, loss: 1624.046021\n",
      "Train: step:  95800, time: 0.218, loss: 1538.792969\n",
      "Train: step:  95810, time: 0.224, loss: 3478.096680\n",
      "Train: step:  95820, time: 0.237, loss: 1234.784912\n",
      "Train: step:  95830, time: 0.222, loss: 2186.799316\n",
      "Train: step:  95840, time: 0.224, loss: 2817.809082\n",
      "Train: step:  95850, time: 0.254, loss: 1805.204712\n",
      "Train: step:  95860, time: 0.220, loss: 2275.392822\n",
      "Train: step:  95870, time: 0.227, loss: 2413.555908\n",
      "Train: step:  95880, time: 0.244, loss: 2528.510742\n",
      "Train: step:  95890, time: 0.255, loss: 3510.493652\n",
      "Train: step:  95900, time: 0.229, loss: 2123.665283\n",
      "Train: step:  95910, time: 0.239, loss: 527.321533\n",
      "Train: step:  95920, time: 0.244, loss: 2575.337891\n",
      "Train: step:  95930, time: 0.230, loss: 1919.511841\n",
      "Train: step:  95940, time: 0.226, loss: 2436.110840\n",
      "Train: step:  95950, time: 0.250, loss: 1232.173462\n",
      "Train: step:  95960, time: 0.239, loss: 1282.051270\n",
      "Train: step:  95970, time: 0.253, loss: 1383.558350\n",
      "Train: step:  95980, time: 0.251, loss: 3832.445801\n",
      "Train: step:  95990, time: 0.260, loss: 1183.497681\n",
      "Train: step:  96000, time: 0.243, loss: 3368.462891\n",
      "Train: step:  96010, time: 0.249, loss: 1196.557251\n",
      "Train: step:  96020, time: 0.247, loss: 2153.477051\n",
      "Train: step:  96030, time: 0.255, loss: 1507.077148\n",
      "Train: step:  96040, time: 0.269, loss: 1102.178589\n",
      "Train: step:  96050, time: 0.224, loss: 1590.606079\n",
      "Train: step:  96060, time: 0.253, loss: 915.674011\n",
      "Train: step:  96070, time: 0.228, loss: 3312.538818\n",
      "Train: step:  96080, time: 0.253, loss: 552.453430\n",
      "Train: step:  96090, time: 0.223, loss: 3797.909424\n",
      "Train: step:  96100, time: 0.258, loss: 2421.351074\n",
      "Train: step:  96110, time: 0.230, loss: 731.412415\n",
      "Train: step:  96120, time: 0.227, loss: 669.741943\n",
      "Train: step:  96130, time: 0.233, loss: 1754.425171\n",
      "Train: step:  96140, time: 0.225, loss: 2184.169189\n",
      "Train: step:  96150, time: 0.257, loss: 2023.119507\n",
      "Train: step:  96160, time: 0.272, loss: 1319.385132\n",
      "Train: step:  96170, time: 0.231, loss: 1706.239380\n",
      "Train: step:  96180, time: 0.236, loss: 1497.241577\n",
      "Train: step:  96190, time: 0.263, loss: 3501.956055\n",
      "Train: step:  96200, time: 0.225, loss: 846.453003\n",
      "Train: step:  96210, time: 0.251, loss: 2083.550293\n",
      "Train: step:  96220, time: 0.233, loss: 1356.054565\n",
      "Train: step:  96230, time: 0.255, loss: 1382.890381\n",
      "Train: step:  96240, time: 0.241, loss: 947.053101\n",
      "Train: step:  96250, time: 0.224, loss: 1855.081055\n",
      "Train: step:  96260, time: 0.222, loss: 2088.755371\n",
      "Train: step:  96270, time: 0.264, loss: 3024.906494\n",
      "Train: step:  96280, time: 0.257, loss: 938.416382\n",
      "Train: step:  96290, time: 0.238, loss: 1639.698486\n",
      "Train: step:  96300, time: 0.279, loss: 2385.393555\n",
      "Train: step:  96310, time: 0.237, loss: 2526.295166\n",
      "Train: step:  96320, time: 0.234, loss: 1629.234497\n",
      "Train: step:  96330, time: 0.271, loss: 2973.613770\n",
      "Train: step:  96340, time: 0.223, loss: 2326.569336\n",
      "Train: step:  96350, time: 0.226, loss: 1211.026367\n",
      "Train: step:  96360, time: 0.223, loss: 1815.226685\n",
      "Train: step:  96370, time: 0.224, loss: 930.474487\n",
      "Train: step:  96380, time: 0.260, loss: 972.904358\n",
      "Train: step:  96390, time: 0.236, loss: 920.894470\n",
      "Train: step:  96400, time: 0.225, loss: 1047.025879\n",
      "Train: step:  96410, time: 0.258, loss: 1566.260864\n",
      "Train: step:  96420, time: 0.222, loss: 532.901306\n",
      "Train: step:  96430, time: 0.253, loss: 962.801575\n",
      "Train: step:  96440, time: 0.232, loss: 3832.299805\n",
      "Train: step:  96450, time: 0.222, loss: 2336.110596\n",
      "Train: step:  96460, time: 0.224, loss: 1502.675659\n",
      "Train: step:  96470, time: 0.250, loss: 1937.111084\n",
      "Train: step:  96480, time: 0.272, loss: 1871.740112\n",
      "Train: step:  96490, time: 0.255, loss: 2374.526123\n",
      "Train: step:  96500, time: 0.263, loss: 1210.820679\n",
      "Train: step:  96510, time: 0.233, loss: 2387.201660\n",
      "Train: step:  96520, time: 0.223, loss: 2442.558350\n",
      "Train: step:  96530, time: 0.265, loss: 3766.295410\n",
      "Train: step:  96540, time: 0.250, loss: 1539.245972\n",
      "Train: step:  96550, time: 0.267, loss: 1138.626221\n",
      "Train: step:  96560, time: 0.234, loss: 998.862427\n",
      "Train: step:  96570, time: 0.234, loss: 1293.889526\n",
      "Train: step:  96580, time: 0.225, loss: 3541.071777\n",
      "Train: step:  96590, time: 0.262, loss: 2595.609375\n",
      "Train: step:  96600, time: 0.258, loss: 2405.166992\n",
      "Train: step:  96610, time: 0.240, loss: 548.023682\n",
      "Train: step:  96620, time: 0.218, loss: 2962.882568\n",
      "Train: step:  96630, time: 0.228, loss: 250.494812\n",
      "Train: step:  96640, time: 0.229, loss: 2702.779785\n",
      "Train: step:  96650, time: 0.272, loss: 834.822021\n",
      "Train: step:  96660, time: 0.263, loss: 1043.867432\n",
      "Train: step:  96670, time: 0.237, loss: 711.102600\n",
      "Train: step:  96680, time: 0.259, loss: 2291.533447\n",
      "Train: step:  96690, time: 0.239, loss: 1853.304077\n",
      "Train: step:  96700, time: 0.225, loss: 981.582214\n",
      "Train: step:  96710, time: 0.276, loss: 2377.061279\n",
      "Train: step:  96720, time: 0.228, loss: 2118.471924\n",
      "Train: step:  96730, time: 0.265, loss: 3909.149170\n",
      "Train: step:  96740, time: 0.225, loss: 1676.487671\n",
      "Train: step:  96750, time: 0.252, loss: 2454.288574\n",
      "Train: step:  96760, time: 0.229, loss: 3088.751465\n",
      "Train: step:  96770, time: 0.232, loss: 2782.586670\n",
      "Train: step:  96780, time: 0.253, loss: 3600.959961\n",
      "Train: step:  96790, time: 0.263, loss: 1411.219116\n",
      "Train: step:  96800, time: 0.225, loss: 352.094757\n",
      "Train: step:  96810, time: 0.227, loss: 2758.138184\n",
      "Train: step:  96820, time: 0.242, loss: 2817.110596\n",
      "Train: step:  96830, time: 0.259, loss: 2985.822998\n",
      "Train: step:  96840, time: 0.256, loss: 1667.050781\n",
      "Train: step:  96850, time: 0.231, loss: 4757.932617\n",
      "Train: step:  96860, time: 0.221, loss: 2625.719727\n",
      "Train: step:  96870, time: 0.255, loss: 903.557861\n",
      "Train: step:  96880, time: 0.230, loss: 1303.433350\n",
      "Train: step:  96890, time: 0.229, loss: 2270.942383\n",
      "Train: step:  96900, time: 0.228, loss: 3155.111084\n",
      "Train: step:  96910, time: 0.229, loss: 3495.716797\n",
      "Train: step:  96920, time: 0.246, loss: 2961.842773\n",
      "Train: step:  96930, time: 0.232, loss: 408.647675\n",
      "Train: step:  96940, time: 0.241, loss: 659.213379\n",
      "Train: step:  96950, time: 0.263, loss: 1912.805542\n",
      "Train: step:  96960, time: 0.238, loss: 3274.011719\n",
      "Train: step:  96970, time: 0.253, loss: 674.300354\n",
      "Train: step:  96980, time: 0.257, loss: 2619.406250\n",
      "Train: step:  96990, time: 0.232, loss: 2255.465820\n",
      "Train: step:  97000, time: 0.222, loss: 2236.487061\n",
      "Train: step:  97010, time: 0.245, loss: 619.190247\n",
      "Train: step:  97020, time: 0.222, loss: 3328.145020\n",
      "Train: step:  97030, time: 0.261, loss: 2823.332764\n",
      "Train: step:  97040, time: 0.216, loss: 1482.505859\n",
      "Train: step:  97050, time: 0.263, loss: 1350.768799\n",
      "Train: step:  97060, time: 0.265, loss: 1628.527466\n",
      "Train: step:  97070, time: 0.220, loss: 890.558289\n",
      "Train: step:  97080, time: 0.222, loss: 3025.586426\n",
      "Train: step:  97090, time: 0.228, loss: 1208.229614\n",
      "Train: step:  97100, time: 0.230, loss: 2100.264404\n",
      "Train: step:  97110, time: 0.220, loss: 2192.141602\n",
      "Train: step:  97120, time: 0.225, loss: 4542.161621\n",
      "Train: step:  97130, time: 0.272, loss: 2507.521484\n",
      "Train: step:  97140, time: 0.253, loss: 625.321899\n",
      "Train: step:  97150, time: 0.267, loss: 1943.906738\n",
      "Train: step:  97160, time: 0.254, loss: 1701.608765\n",
      "Train: step:  97170, time: 0.252, loss: 2982.559082\n",
      "Train: step:  97180, time: 0.260, loss: 774.366577\n",
      "Train: step:  97190, time: 0.259, loss: 2757.468506\n",
      "Train: step:  97200, time: 0.270, loss: 864.179749\n",
      "Train: step:  97210, time: 0.263, loss: 2534.720703\n",
      "Train: step:  97220, time: 0.235, loss: 1219.371460\n",
      "Train: step:  97230, time: 0.226, loss: 1417.117310\n",
      "Train: step:  97240, time: 0.229, loss: 2336.836670\n",
      "Train: step:  97250, time: 0.232, loss: 1847.939575\n",
      "Train: step:  97260, time: 0.223, loss: 353.481415\n",
      "Train: step:  97270, time: 0.224, loss: 1625.244751\n",
      "Train: step:  97280, time: 0.261, loss: 1789.705811\n",
      "Train: step:  97290, time: 0.221, loss: 1788.538086\n",
      "Train: step:  97300, time: 0.236, loss: 1955.303345\n",
      "Train: step:  97310, time: 0.234, loss: 1492.740845\n",
      "Train: step:  97320, time: 0.226, loss: 4552.371582\n",
      "Train: step:  97330, time: 0.251, loss: 2323.607666\n",
      "Train: step:  97340, time: 0.227, loss: 363.638000\n",
      "Train: step:  97350, time: 0.241, loss: 2319.463623\n",
      "Train: step:  97360, time: 0.242, loss: 1338.777344\n",
      "Train: step:  97370, time: 0.228, loss: 1716.413696\n",
      "Train: step:  97380, time: 0.234, loss: 1463.985474\n",
      "Train: step:  97390, time: 0.224, loss: 4075.441650\n",
      "Train: step:  97400, time: 0.230, loss: 1670.324585\n",
      "Train: step:  97410, time: 0.252, loss: 569.100647\n",
      "Train: step:  97420, time: 0.264, loss: 2248.133545\n",
      "Train: step:  97430, time: 0.226, loss: 1075.495605\n",
      "Train: step:  97440, time: 0.224, loss: 974.671326\n",
      "Train: step:  97450, time: 0.252, loss: 1946.444946\n",
      "Train: step:  97460, time: 0.240, loss: 2660.569824\n",
      "Train: step:  97470, time: 0.277, loss: 2808.508301\n",
      "Train: step:  97480, time: 0.268, loss: 1145.728394\n",
      "Train: step:  97490, time: 0.231, loss: 2244.632568\n",
      "Train: step:  97500, time: 0.227, loss: 1898.536743\n",
      "Train: step:  97510, time: 0.228, loss: 583.931580\n",
      "Train: step:  97520, time: 0.223, loss: 1420.882446\n",
      "Train: step:  97530, time: 0.256, loss: 1150.849487\n",
      "Train: step:  97540, time: 0.244, loss: 1431.217285\n",
      "Train: step:  97550, time: 0.226, loss: 1554.443237\n",
      "Train: step:  97560, time: 0.238, loss: 2583.517822\n",
      "Train: step:  97570, time: 0.220, loss: 3006.455811\n",
      "Train: step:  97580, time: 0.227, loss: 2024.046143\n",
      "Train: step:  97590, time: 0.238, loss: 3438.696045\n",
      "Train: step:  97600, time: 0.242, loss: 3808.124268\n",
      "Train: step:  97610, time: 0.260, loss: 2034.429688\n",
      "Train: step:  97620, time: 0.246, loss: 2819.416016\n",
      "Train: step:  97630, time: 0.250, loss: 2371.653564\n",
      "Train: step:  97640, time: 0.218, loss: 1290.911255\n",
      "Train: step:  97650, time: 0.285, loss: 2692.815430\n",
      "Train: step:  97660, time: 0.229, loss: 1314.390137\n",
      "Train: step:  97670, time: 0.223, loss: 1431.661377\n",
      "Train: step:  97680, time: 0.218, loss: 672.648926\n",
      "Train: step:  97690, time: 0.252, loss: 1110.576416\n",
      "Train: step:  97700, time: 0.251, loss: 2041.531006\n",
      "Train: step:  97710, time: 0.244, loss: 1994.809814\n",
      "Train: step:  97720, time: 0.229, loss: 3711.120850\n",
      "Train: step:  97730, time: 0.236, loss: 3289.529785\n",
      "Train: step:  97740, time: 0.255, loss: 1545.343994\n",
      "Train: step:  97750, time: 0.238, loss: 1355.838867\n",
      "Train: step:  97760, time: 0.234, loss: 1802.709717\n",
      "Train: step:  97770, time: 0.233, loss: 2385.787354\n",
      "Train: step:  97780, time: 0.292, loss: 401.098846\n",
      "Train: step:  97790, time: 0.269, loss: 758.130920\n",
      "Train: step:  97800, time: 0.232, loss: 2866.960693\n",
      "Train: step:  97810, time: 0.232, loss: 1108.917725\n",
      "Train: step:  97820, time: 0.274, loss: 901.340942\n",
      "Train: step:  97830, time: 0.241, loss: 1246.918091\n",
      "Train: step:  97840, time: 0.230, loss: 2711.097900\n",
      "Train: step:  97850, time: 0.233, loss: 1661.023193\n",
      "Train: step:  97860, time: 0.229, loss: 1696.432495\n",
      "Train: step:  97870, time: 0.227, loss: 3113.813965\n",
      "Train: step:  97880, time: 0.231, loss: 1767.985107\n",
      "Train: step:  97890, time: 0.221, loss: 3165.400879\n",
      "Train: step:  97900, time: 0.232, loss: 671.624268\n",
      "Train: step:  97910, time: 0.224, loss: 1515.184814\n",
      "Train: step:  97920, time: 0.252, loss: 2338.269531\n",
      "Train: step:  97930, time: 0.271, loss: 683.703613\n",
      "Train: step:  97940, time: 0.268, loss: 1350.475830\n",
      "Train: step:  97950, time: 0.243, loss: 2050.132080\n",
      "Train: step:  97960, time: 0.271, loss: 2184.978516\n",
      "Train: step:  97970, time: 0.244, loss: 2315.688232\n",
      "Train: step:  97980, time: 0.254, loss: 1193.763916\n",
      "Train: step:  97990, time: 0.259, loss: 2710.619385\n",
      "Train: step:  98000, time: 0.244, loss: 1777.058716\n",
      "Train: step:  98010, time: 0.254, loss: 1870.375488\n",
      "Train: step:  98020, time: 0.255, loss: 1589.619873\n",
      "Train: step:  98030, time: 0.252, loss: 1261.077637\n",
      "Train: step:  98040, time: 0.230, loss: 2684.814941\n",
      "Train: step:  98050, time: 0.224, loss: 3113.158447\n",
      "Train: step:  98060, time: 0.261, loss: 2779.316895\n",
      "Train: step:  98070, time: 0.238, loss: 313.149933\n",
      "Train: step:  98080, time: 0.239, loss: 3452.685059\n",
      "Train: step:  98090, time: 0.257, loss: 911.177795\n",
      "Train: step:  98100, time: 0.265, loss: 2341.143311\n",
      "Train: step:  98110, time: 0.261, loss: 1972.378174\n",
      "Train: step:  98120, time: 0.266, loss: 1409.132080\n",
      "Train: step:  98130, time: 0.226, loss: 1111.668579\n",
      "Train: step:  98140, time: 0.246, loss: 267.777191\n",
      "Train: step:  98150, time: 0.263, loss: 1611.263306\n",
      "Train: step:  98160, time: 0.230, loss: 1296.510254\n",
      "Train: step:  98170, time: 0.226, loss: 526.839417\n",
      "Train: step:  98180, time: 0.231, loss: 1323.045776\n",
      "Train: step:  98190, time: 0.270, loss: 2199.889404\n",
      "Train: step:  98200, time: 0.243, loss: 1494.288696\n",
      "Train: step:  98210, time: 0.228, loss: 1835.138794\n",
      "Train: step:  98220, time: 0.248, loss: 2039.408813\n",
      "Train: step:  98230, time: 0.269, loss: 1023.139771\n",
      "Train: step:  98240, time: 0.255, loss: 570.277588\n",
      "Train: step:  98250, time: 0.221, loss: 1166.158447\n",
      "Train: step:  98260, time: 0.230, loss: 1544.800537\n",
      "Train: step:  98270, time: 0.233, loss: 3104.012207\n",
      "Train: step:  98280, time: 0.230, loss: 260.590454\n",
      "Train: step:  98290, time: 0.237, loss: 1167.500610\n",
      "Train: step:  98300, time: 0.235, loss: 1811.700684\n",
      "Train: step:  98310, time: 0.225, loss: 1240.922241\n",
      "Train: step:  98320, time: 0.269, loss: 4393.471191\n",
      "Train: step:  98330, time: 0.219, loss: 1514.099609\n",
      "Train: step:  98340, time: 0.230, loss: 4871.222656\n",
      "Train: step:  98350, time: 0.242, loss: 2145.056396\n",
      "Train: step:  98360, time: 0.251, loss: 2142.771729\n",
      "Train: step:  98370, time: 0.238, loss: 1349.588989\n",
      "Train: step:  98380, time: 0.250, loss: 484.780975\n",
      "Train: step:  98390, time: 0.222, loss: 1713.299072\n",
      "Train: step:  98400, time: 0.249, loss: 1144.316284\n",
      "Train: step:  98410, time: 0.228, loss: 598.618103\n",
      "Train: step:  98420, time: 0.227, loss: 2449.331543\n",
      "Train: step:  98430, time: 0.238, loss: 3359.680664\n",
      "Train: step:  98440, time: 0.227, loss: 1989.487305\n",
      "Train: step:  98450, time: 0.218, loss: 2100.656738\n",
      "Train: step:  98460, time: 0.226, loss: 142.095795\n",
      "Train: step:  98470, time: 0.258, loss: 2856.695312\n",
      "Train: step:  98480, time: 0.239, loss: 962.406860\n",
      "Train: step:  98490, time: 0.233, loss: 2051.897949\n",
      "Train: step:  98500, time: 0.248, loss: 1284.147095\n",
      "Train: step:  98510, time: 0.230, loss: 2462.125488\n",
      "Train: step:  98520, time: 0.227, loss: 3512.320557\n",
      "Train: step:  98530, time: 0.258, loss: 1304.493530\n",
      "Train: step:  98540, time: 0.233, loss: 2428.599609\n",
      "Train: step:  98550, time: 0.270, loss: 4120.704590\n",
      "Train: step:  98560, time: 0.272, loss: 3918.799072\n",
      "Train: step:  98570, time: 0.234, loss: 2015.645142\n",
      "Train: step:  98580, time: 0.228, loss: 342.996704\n",
      "Train: step:  98590, time: 0.251, loss: 493.467560\n",
      "Train: step:  98600, time: 0.251, loss: 2259.921875\n",
      "Train: step:  98610, time: 0.227, loss: 937.973816\n",
      "Train: step:  98620, time: 0.223, loss: 2959.196289\n",
      "Train: step:  98630, time: 0.237, loss: 2566.941162\n",
      "Train: step:  98640, time: 0.226, loss: 2911.586670\n",
      "Train: step:  98650, time: 0.230, loss: 2948.345459\n",
      "Train: step:  98660, time: 0.275, loss: 1406.136475\n",
      "Train: step:  98670, time: 0.283, loss: 2689.389893\n",
      "Train: step:  98680, time: 0.256, loss: 2986.659912\n",
      "Train: step:  98690, time: 0.253, loss: 2512.050293\n",
      "Train: step:  98700, time: 0.263, loss: 681.666199\n",
      "Train: step:  98710, time: 0.243, loss: 2389.429443\n",
      "Train: step:  98720, time: 0.262, loss: 907.318298\n",
      "Train: step:  98730, time: 0.248, loss: 1413.137451\n",
      "Train: step:  98740, time: 0.240, loss: 2424.721436\n",
      "Train: step:  98750, time: 0.229, loss: 1231.128662\n",
      "Train: step:  98760, time: 0.254, loss: 1402.035645\n",
      "Train: step:  98770, time: 0.228, loss: 3389.387451\n",
      "Train: step:  98780, time: 0.256, loss: 3331.482178\n",
      "Train: step:  98790, time: 0.237, loss: 3419.507080\n",
      "Train: step:  98800, time: 0.273, loss: 3167.594482\n",
      "Train: step:  98810, time: 0.223, loss: 789.151306\n",
      "Train: step:  98820, time: 0.258, loss: 279.119080\n",
      "Train: step:  98830, time: 0.226, loss: 3714.421875\n",
      "Train: step:  98840, time: 0.248, loss: 1226.393921\n",
      "Train: step:  98850, time: 0.229, loss: 2964.691406\n",
      "Train: step:  98860, time: 0.230, loss: 1821.412109\n",
      "Train: step:  98870, time: 0.230, loss: 2046.690430\n",
      "Train: step:  98880, time: 0.224, loss: 3756.137207\n",
      "Train: step:  98890, time: 0.233, loss: 2576.654541\n",
      "Train: step:  98900, time: 0.229, loss: 1147.452393\n",
      "Train: step:  98910, time: 0.242, loss: 1726.202271\n",
      "Train: step:  98920, time: 0.220, loss: 2192.337158\n",
      "Train: step:  98930, time: 0.253, loss: 1138.340576\n",
      "Train: step:  98940, time: 0.235, loss: 860.606323\n",
      "Train: step:  98950, time: 0.228, loss: 2310.323975\n",
      "Train: step:  98960, time: 0.221, loss: 1914.193481\n",
      "Train: step:  98970, time: 0.240, loss: 766.929810\n",
      "Train: step:  98980, time: 0.235, loss: 733.986694\n",
      "Train: step:  98990, time: 0.226, loss: 1654.695312\n",
      "Train: step:  99000, time: 0.224, loss: 2714.963379\n",
      "Train: step:  99010, time: 0.248, loss: 700.378357\n",
      "Train: step:  99020, time: 0.229, loss: 1277.308838\n",
      "Train: step:  99030, time: 0.240, loss: 2363.480957\n",
      "Train: step:  99040, time: 0.233, loss: 2506.711182\n",
      "Train: step:  99050, time: 0.265, loss: 337.694214\n",
      "Train: step:  99060, time: 0.254, loss: 2477.781738\n",
      "Train: step:  99070, time: 0.226, loss: 409.737488\n",
      "Train: step:  99080, time: 0.233, loss: 1301.590576\n",
      "Train: step:  99090, time: 0.226, loss: 1966.078613\n",
      "Train: step:  99100, time: 0.226, loss: 1601.071411\n",
      "Train: step:  99110, time: 0.244, loss: 866.002319\n",
      "Train: step:  99120, time: 0.227, loss: 880.531189\n",
      "Train: step:  99130, time: 0.290, loss: 1586.590942\n",
      "Train: step:  99140, time: 0.230, loss: 2130.687256\n",
      "Train: step:  99150, time: 0.262, loss: 2776.311279\n",
      "Train: step:  99160, time: 0.249, loss: 1569.257568\n",
      "Train: step:  99170, time: 0.259, loss: 2521.288574\n",
      "Train: step:  99180, time: 0.233, loss: 2185.869385\n",
      "Train: step:  99190, time: 0.235, loss: 3272.130859\n",
      "Train: step:  99200, time: 0.222, loss: 3185.943359\n",
      "Train: step:  99210, time: 0.281, loss: 1681.766602\n",
      "Train: step:  99220, time: 0.265, loss: 4692.782715\n",
      "Train: step:  99230, time: 0.268, loss: 1807.740723\n",
      "Train: step:  99240, time: 0.233, loss: 2059.285889\n",
      "Train: step:  99250, time: 0.229, loss: 1943.247314\n",
      "Train: step:  99260, time: 0.253, loss: 3618.361572\n",
      "Train: step:  99270, time: 0.254, loss: 2606.747803\n",
      "Train: step:  99280, time: 0.232, loss: 2040.502197\n",
      "Train: step:  99290, time: 0.227, loss: 4194.882324\n",
      "Train: step:  99300, time: 0.275, loss: 3195.835938\n",
      "Train: step:  99310, time: 0.256, loss: 1394.761841\n",
      "Train: step:  99320, time: 0.265, loss: 2213.654053\n",
      "Train: step:  99330, time: 0.224, loss: 387.613678\n",
      "Train: step:  99340, time: 0.270, loss: 2120.272217\n",
      "Train: step:  99350, time: 0.228, loss: 3500.950684\n",
      "Train: step:  99360, time: 0.224, loss: 502.154785\n",
      "Train: step:  99370, time: 0.266, loss: 2915.871094\n",
      "Train: step:  99380, time: 0.235, loss: 1284.506592\n",
      "Train: step:  99390, time: 0.247, loss: 887.110168\n",
      "Train: step:  99400, time: 0.261, loss: 1941.998291\n",
      "Train: step:  99410, time: 0.225, loss: 1854.743652\n",
      "Train: step:  99420, time: 0.247, loss: 2185.294678\n",
      "Train: step:  99430, time: 0.254, loss: 1783.388794\n",
      "Train: step:  99440, time: 0.220, loss: 1116.304688\n",
      "Train: step:  99450, time: 0.267, loss: 1090.516235\n",
      "Train: step:  99460, time: 0.223, loss: 2381.980225\n",
      "Train: step:  99470, time: 0.227, loss: 3923.023193\n",
      "Train: step:  99480, time: 0.250, loss: 2811.378174\n",
      "Train: step:  99490, time: 0.268, loss: 3389.218262\n",
      "Train: step:  99500, time: 0.258, loss: 1035.461548\n",
      "Train: step:  99510, time: 0.269, loss: 1979.078491\n",
      "Train: step:  99520, time: 0.252, loss: 2160.062256\n",
      "Train: step:  99530, time: 0.224, loss: 3382.750732\n",
      "Train: step:  99540, time: 0.224, loss: 2024.937500\n",
      "Train: step:  99550, time: 0.225, loss: 3495.702148\n",
      "Train: step:  99560, time: 0.221, loss: 2256.266113\n",
      "Train: step:  99570, time: 0.253, loss: 2156.978516\n",
      "Train: step:  99580, time: 0.235, loss: 1051.260498\n",
      "Train: step:  99590, time: 0.234, loss: 939.167114\n",
      "Train: step:  99600, time: 0.222, loss: 1912.088379\n",
      "Train: step:  99610, time: 0.225, loss: 2173.264648\n",
      "Train: step:  99620, time: 0.253, loss: 2375.164307\n",
      "Train: step:  99630, time: 0.242, loss: 2527.581543\n",
      "Train: step:  99640, time: 0.221, loss: 220.735672\n",
      "Train: step:  99650, time: 0.227, loss: 1531.525146\n",
      "Train: step:  99660, time: 0.247, loss: 2111.666504\n",
      "Train: step:  99670, time: 0.252, loss: 332.471375\n",
      "Train: step:  99680, time: 0.262, loss: 768.574463\n",
      "Train: step:  99690, time: 0.235, loss: 771.343323\n",
      "Train: step:  99700, time: 0.257, loss: 1084.400391\n",
      "Train: step:  99710, time: 0.269, loss: 696.913879\n",
      "Train: step:  99720, time: 0.245, loss: 2804.906494\n",
      "Train: step:  99730, time: 0.246, loss: 1001.694824\n",
      "Train: step:  99740, time: 0.244, loss: 2983.806885\n",
      "Train: step:  99750, time: 0.236, loss: 3075.037598\n",
      "Train: step:  99760, time: 0.231, loss: 2120.147705\n",
      "Train: step:  99770, time: 0.217, loss: 822.876404\n",
      "Train: step:  99780, time: 0.233, loss: 1184.865112\n",
      "Train: step:  99790, time: 0.235, loss: 843.717468\n",
      "Train: step:  99800, time: 0.241, loss: 1599.912109\n",
      "Train: step:  99810, time: 0.248, loss: 1134.918213\n",
      "Train: step:  99820, time: 0.253, loss: 665.136230\n",
      "Train: step:  99830, time: 0.254, loss: 1858.515015\n",
      "Train: step:  99840, time: 0.251, loss: 420.767731\n",
      "Train: step:  99850, time: 0.221, loss: 1470.862183\n",
      "Train: step:  99860, time: 0.228, loss: 2421.742188\n",
      "Train: step:  99870, time: 0.230, loss: 3014.664551\n",
      "Train: step:  99880, time: 0.213, loss: 1615.223511\n",
      "Train: step:  99890, time: 0.217, loss: 1576.676270\n",
      "Train: step:  99900, time: 0.216, loss: 1561.938721\n",
      "Train: step:  99910, time: 0.264, loss: 801.290466\n",
      "Train: step:  99920, time: 0.224, loss: 2404.969238\n",
      "Train: step:  99930, time: 0.246, loss: 2141.601074\n",
      "Train: step:  99940, time: 0.222, loss: 1737.370483\n",
      "Train: step:  99950, time: 0.219, loss: 3186.755127\n",
      "Train: step:  99960, time: 0.251, loss: 1313.836914\n",
      "Train: step:  99970, time: 0.226, loss: 932.121765\n",
      "Train: step:  99980, time: 0.221, loss: 1710.673340\n",
      "Train: step:  99990, time: 0.231, loss: 1746.276123\n",
      "Train: step: 100000, time: 0.216, loss: 2262.586182\n",
      "Train: step: 100010, time: 0.252, loss: 3192.321777\n",
      "Train: step: 100020, time: 0.225, loss: 2082.907959\n",
      "Train: step: 100030, time: 0.230, loss: 3190.461914\n",
      "Train: step: 100040, time: 0.245, loss: 3731.960449\n",
      "Train: step: 100050, time: 0.224, loss: 1103.237671\n",
      "Train: step: 100060, time: 0.223, loss: 2962.687500\n",
      "Train: step: 100070, time: 0.240, loss: 3996.151855\n",
      "Train: step: 100080, time: 0.264, loss: 938.936157\n",
      "Train: step: 100090, time: 0.218, loss: 462.769897\n",
      "Train: step: 100100, time: 0.253, loss: 1635.736938\n",
      "Train: step: 100110, time: 0.270, loss: 339.217621\n",
      "Train: step: 100120, time: 0.250, loss: 2352.399902\n",
      "Train: step: 100130, time: 0.220, loss: 1487.538818\n",
      "Train: step: 100140, time: 0.254, loss: 1540.735718\n",
      "Train: step: 100150, time: 0.236, loss: 815.230530\n",
      "Train: step: 100160, time: 0.232, loss: 2129.258301\n",
      "Train: step: 100170, time: 0.245, loss: 3801.354248\n",
      "Train: step: 100180, time: 0.231, loss: 697.852844\n",
      "Train: step: 100190, time: 0.231, loss: 2669.977539\n",
      "Train: step: 100200, time: 0.259, loss: 1008.525757\n",
      "Train: step: 100210, time: 0.228, loss: 626.158997\n",
      "Train: step: 100220, time: 0.267, loss: 1914.812866\n",
      "Train: step: 100230, time: 0.230, loss: 2168.726807\n",
      "Train: step: 100240, time: 0.253, loss: 3350.973389\n",
      "Train: step: 100250, time: 0.268, loss: 3144.652344\n",
      "Train: step: 100260, time: 0.234, loss: 2504.333496\n",
      "Train: step: 100270, time: 0.229, loss: 888.647461\n",
      "Train: step: 100280, time: 0.225, loss: 4211.887207\n",
      "Train: step: 100290, time: 0.266, loss: 446.406464\n",
      "Train: step: 100300, time: 0.227, loss: 2884.898438\n",
      "Train: step: 100310, time: 0.257, loss: 2473.555664\n",
      "Train: step: 100320, time: 0.252, loss: 714.165283\n",
      "Train: step: 100330, time: 0.223, loss: 2520.660156\n",
      "Train: step: 100340, time: 0.276, loss: 2498.326660\n",
      "Train: step: 100350, time: 0.239, loss: 3845.871826\n",
      "Train: step: 100360, time: 0.229, loss: 1668.173950\n",
      "Train: step: 100370, time: 0.231, loss: 893.706482\n",
      "Train: step: 100380, time: 0.232, loss: 1810.976318\n",
      "Train: step: 100390, time: 0.232, loss: 407.056091\n",
      "Train: step: 100400, time: 0.260, loss: 954.605835\n",
      "Train: step: 100410, time: 0.238, loss: 1368.735229\n",
      "Train: step: 100420, time: 0.268, loss: 2296.691162\n",
      "Train: step: 100430, time: 0.224, loss: 1672.399658\n",
      "Train: step: 100440, time: 0.228, loss: 1093.399170\n",
      "Train: step: 100450, time: 0.234, loss: 2183.773682\n",
      "Train: step: 100460, time: 0.236, loss: 1825.671021\n",
      "Train: step: 100470, time: 0.235, loss: 3072.859131\n",
      "Train: step: 100480, time: 0.225, loss: 1411.472290\n",
      "Train: step: 100490, time: 0.264, loss: 1277.114990\n",
      "Train: step: 100500, time: 0.230, loss: 795.123108\n",
      "Train: step: 100510, time: 0.283, loss: 1119.756348\n",
      "Train: step: 100520, time: 0.247, loss: 2229.718506\n",
      "Train: step: 100530, time: 0.220, loss: 4427.791016\n",
      "Train: step: 100540, time: 0.222, loss: 1789.048340\n",
      "Train: step: 100550, time: 0.259, loss: 2574.654297\n",
      "Train: step: 100560, time: 0.222, loss: 1543.445679\n",
      "Train: step: 100570, time: 0.254, loss: 2621.740479\n",
      "Train: step: 100580, time: 0.232, loss: 891.795349\n",
      "Train: step: 100590, time: 0.233, loss: 2614.847412\n",
      "Train: step: 100600, time: 0.252, loss: 3260.271729\n",
      "Train: step: 100610, time: 0.250, loss: 1480.142212\n",
      "Train: step: 100620, time: 0.227, loss: 621.831421\n",
      "Train: step: 100630, time: 0.231, loss: 1334.035767\n",
      "Train: step: 100640, time: 0.268, loss: 2534.637207\n",
      "Train: step: 100650, time: 0.261, loss: 1282.482300\n",
      "Train: step: 100660, time: 0.253, loss: 943.444946\n",
      "Train: step: 100670, time: 0.225, loss: 1178.693848\n",
      "Train: step: 100680, time: 0.228, loss: 1295.117798\n",
      "Train: step: 100690, time: 0.219, loss: 660.635010\n",
      "Train: step: 100700, time: 0.226, loss: 1098.825684\n",
      "Train: step: 100710, time: 0.236, loss: 1568.062134\n",
      "Train: step: 100720, time: 0.217, loss: 322.959290\n",
      "Train: step: 100730, time: 0.263, loss: 1106.204224\n",
      "Train: step: 100740, time: 0.227, loss: 1157.110840\n",
      "Train: step: 100750, time: 0.236, loss: 1288.346313\n",
      "Train: step: 100760, time: 0.227, loss: 723.377136\n",
      "Train: step: 100770, time: 0.230, loss: 633.522095\n",
      "Train: step: 100780, time: 0.235, loss: 1990.070190\n",
      "Train: step: 100790, time: 0.241, loss: 2777.938477\n",
      "Train: step: 100800, time: 0.222, loss: 439.732391\n",
      "Train: step: 100810, time: 0.223, loss: 1983.540649\n",
      "Train: step: 100820, time: 0.227, loss: 1405.833374\n",
      "Train: step: 100830, time: 0.231, loss: 2368.656738\n",
      "Train: step: 100840, time: 0.254, loss: 1506.800049\n",
      "Train: step: 100850, time: 0.237, loss: 2155.608154\n",
      "Train: step: 100860, time: 0.233, loss: 2639.008545\n",
      "Train: step: 100870, time: 0.274, loss: 3387.148682\n",
      "Train: step: 100880, time: 0.229, loss: 2969.825439\n",
      "Train: step: 100890, time: 0.227, loss: 2581.369385\n",
      "Train: step: 100900, time: 0.240, loss: 1902.753296\n",
      "Train: step: 100910, time: 0.233, loss: 1297.883179\n",
      "Train: step: 100920, time: 0.233, loss: 3022.604980\n",
      "Train: step: 100930, time: 0.231, loss: 2088.708252\n",
      "Train: step: 100940, time: 0.237, loss: 2044.921631\n",
      "Train: step: 100950, time: 0.251, loss: 3145.500244\n",
      "Train: step: 100960, time: 0.234, loss: 1863.044434\n",
      "Train: step: 100970, time: 0.227, loss: 2195.195801\n",
      "Train: step: 100980, time: 0.226, loss: 436.577209\n",
      "Train: step: 100990, time: 0.259, loss: 1210.558960\n",
      "Train: step: 101000, time: 0.221, loss: 856.165466\n",
      "Train: step: 101010, time: 0.225, loss: 2483.589111\n",
      "Train: step: 101020, time: 0.217, loss: 736.392883\n",
      "Train: step: 101030, time: 0.228, loss: 2037.802856\n",
      "Train: step: 101040, time: 0.222, loss: 2832.258545\n",
      "Train: step: 101050, time: 0.218, loss: 2000.938110\n",
      "Train: step: 101060, time: 0.233, loss: 2832.163086\n",
      "Train: step: 101070, time: 0.269, loss: 1839.606812\n",
      "Train: step: 101080, time: 0.269, loss: 900.125488\n",
      "Train: step: 101090, time: 0.227, loss: 2983.656738\n",
      "Train: step: 101100, time: 0.244, loss: 1979.584839\n",
      "Train: step: 101110, time: 0.235, loss: 428.018951\n",
      "Train: step: 101120, time: 0.223, loss: 2269.625488\n",
      "Train: step: 101130, time: 0.267, loss: 2937.324463\n",
      "Train: step: 101140, time: 0.251, loss: 2926.348389\n",
      "Train: step: 101150, time: 0.228, loss: 224.965607\n",
      "Train: step: 101160, time: 0.255, loss: 2823.014160\n",
      "Train: step: 101170, time: 0.255, loss: 2003.650391\n",
      "Train: step: 101180, time: 0.234, loss: 345.118073\n",
      "Train: step: 101190, time: 0.237, loss: 2278.054932\n",
      "Train: step: 101200, time: 0.237, loss: 825.264893\n",
      "Train: step: 101210, time: 0.229, loss: 1451.569336\n",
      "Train: step: 101220, time: 0.230, loss: 1568.732666\n",
      "Train: step: 101230, time: 0.257, loss: 1365.628784\n",
      "Train: step: 101240, time: 0.248, loss: 1584.005127\n",
      "Train: step: 101250, time: 0.226, loss: 3004.163818\n",
      "Train: step: 101260, time: 0.224, loss: 1561.465454\n",
      "Train: step: 101270, time: 0.231, loss: 2354.245117\n",
      "Train: step: 101280, time: 0.235, loss: 2280.569580\n",
      "Train: step: 101290, time: 0.230, loss: 1928.750488\n",
      "Train: step: 101300, time: 0.233, loss: 3983.620605\n",
      "Train: step: 101310, time: 0.251, loss: 2927.138184\n",
      "Train: step: 101320, time: 0.236, loss: 3231.699219\n",
      "Train: step: 101330, time: 0.227, loss: 3269.397949\n",
      "Train: step: 101340, time: 0.227, loss: 1910.790894\n",
      "Train: step: 101350, time: 0.239, loss: 1819.293701\n",
      "Train: step: 101360, time: 0.230, loss: 236.862259\n",
      "Train: step: 101370, time: 0.240, loss: 2268.129150\n",
      "Train: step: 101380, time: 0.230, loss: 1421.191040\n",
      "Train: step: 101390, time: 0.238, loss: 2572.229492\n",
      "Train: step: 101400, time: 0.225, loss: 4449.425781\n",
      "Train: step: 101410, time: 0.228, loss: 2502.261475\n",
      "Train: step: 101420, time: 0.234, loss: 2439.916260\n",
      "Train: step: 101430, time: 0.254, loss: 3370.726562\n",
      "Train: step: 101440, time: 0.230, loss: 1891.052124\n",
      "Train: step: 101450, time: 0.225, loss: 1702.829834\n",
      "Train: step: 101460, time: 0.254, loss: 1111.590698\n",
      "Train: step: 101470, time: 0.254, loss: 885.574890\n",
      "Train: step: 101480, time: 0.249, loss: 1919.305908\n",
      "Train: step: 101490, time: 0.242, loss: 3087.973389\n",
      "Train: step: 101500, time: 0.251, loss: 2771.926514\n",
      "Train: step: 101510, time: 0.255, loss: 1369.448486\n",
      "Train: step: 101520, time: 0.234, loss: 1401.333130\n",
      "Train: step: 101530, time: 0.271, loss: 3298.986816\n",
      "Train: step: 101540, time: 0.230, loss: 1360.657715\n",
      "Train: step: 101550, time: 0.235, loss: 1664.150391\n",
      "Train: step: 101560, time: 0.228, loss: 1926.613281\n",
      "Train: step: 101570, time: 0.232, loss: 1611.017334\n",
      "Train: step: 101580, time: 0.219, loss: 1874.314453\n",
      "Train: step: 101590, time: 0.253, loss: 2931.976318\n",
      "Train: step: 101600, time: 0.251, loss: 1204.743652\n",
      "Train: step: 101610, time: 0.253, loss: 1565.214233\n",
      "Train: step: 101620, time: 0.265, loss: 2865.993652\n",
      "Train: step: 101630, time: 0.257, loss: 2379.450684\n",
      "Train: step: 101640, time: 0.228, loss: 2711.946533\n",
      "Train: step: 101650, time: 0.220, loss: 1139.610229\n",
      "Train: step: 101660, time: 0.257, loss: 1219.814453\n",
      "Train: step: 101670, time: 0.254, loss: 1353.580688\n",
      "Train: step: 101680, time: 0.265, loss: 1218.612915\n",
      "Train: step: 101690, time: 0.265, loss: 2877.728271\n",
      "Train: step: 101700, time: 0.241, loss: 1831.386841\n",
      "Train: step: 101710, time: 0.238, loss: 1156.059814\n",
      "Train: step: 101720, time: 0.226, loss: 826.431519\n",
      "Train: step: 101730, time: 0.246, loss: 1680.775146\n",
      "Train: step: 101740, time: 0.224, loss: 993.841064\n",
      "Train: step: 101750, time: 0.224, loss: 560.127563\n",
      "Train: step: 101760, time: 0.218, loss: 2152.146729\n",
      "Train: step: 101770, time: 0.255, loss: 894.170471\n",
      "Train: step: 101780, time: 0.246, loss: 2372.114258\n",
      "Train: step: 101790, time: 0.230, loss: 1667.976074\n",
      "Train: step: 101800, time: 0.237, loss: 2430.851318\n",
      "Train: step: 101810, time: 0.235, loss: 1171.111938\n",
      "Train: step: 101820, time: 0.229, loss: 1486.969727\n",
      "Train: step: 101830, time: 0.255, loss: 1956.405396\n",
      "Train: step: 101840, time: 0.239, loss: 2149.298096\n",
      "Train: step: 101850, time: 0.286, loss: 899.135437\n",
      "Train: step: 101860, time: 0.229, loss: 1319.318359\n",
      "Train: step: 101870, time: 0.237, loss: 2233.978271\n",
      "Train: step: 101880, time: 0.248, loss: 2469.710449\n",
      "Train: step: 101890, time: 0.285, loss: 2662.252441\n",
      "Train: step: 101900, time: 0.243, loss: 3946.177002\n",
      "Train: step: 101910, time: 0.258, loss: 2682.470947\n",
      "Train: step: 101920, time: 0.227, loss: 1245.556030\n",
      "Train: step: 101930, time: 0.224, loss: 2897.122314\n",
      "Train: step: 101940, time: 0.237, loss: 2229.343506\n",
      "Train: step: 101950, time: 0.261, loss: 2671.814941\n",
      "Train: step: 101960, time: 0.244, loss: 1392.722778\n",
      "Train: step: 101970, time: 0.242, loss: 822.829956\n",
      "Train: step: 101980, time: 0.235, loss: 2447.857178\n",
      "Train: step: 101990, time: 0.234, loss: 2072.979492\n",
      "Train: step: 102000, time: 0.241, loss: 1227.410156\n",
      "Train: step: 102010, time: 0.269, loss: 3091.885986\n",
      "Train: step: 102020, time: 0.232, loss: 3397.237061\n",
      "Train: step: 102030, time: 0.229, loss: 1550.931885\n",
      "Train: step: 102040, time: 0.229, loss: 1870.540771\n",
      "Train: step: 102050, time: 0.257, loss: 1704.237183\n",
      "Train: step: 102060, time: 0.262, loss: 2557.722900\n",
      "Train: step: 102070, time: 0.253, loss: 503.463654\n",
      "Train: step: 102080, time: 0.228, loss: 1582.630737\n",
      "Train: step: 102090, time: 0.234, loss: 742.854309\n",
      "Train: step: 102100, time: 0.254, loss: 2390.892578\n",
      "Train: step: 102110, time: 0.256, loss: 1481.586304\n",
      "Train: step: 102120, time: 0.230, loss: 3935.362549\n",
      "Train: step: 102130, time: 0.270, loss: 274.309326\n",
      "Train: step: 102140, time: 0.272, loss: 186.010681\n",
      "Train: step: 102150, time: 0.242, loss: 1841.009888\n",
      "Train: step: 102160, time: 0.225, loss: 351.936340\n",
      "Train: step: 102170, time: 0.240, loss: 163.373093\n",
      "Train: step: 102180, time: 0.243, loss: 2536.260010\n",
      "Train: step: 102190, time: 0.243, loss: 822.290100\n",
      "Train: step: 102200, time: 0.256, loss: 1969.761230\n",
      "Train: step: 102210, time: 0.249, loss: 1482.542603\n",
      "Train: step: 102220, time: 0.229, loss: 1754.048584\n",
      "Train: step: 102230, time: 0.238, loss: 546.503357\n",
      "Train: step: 102240, time: 0.241, loss: 690.759155\n",
      "Train: step: 102250, time: 0.255, loss: 2398.563965\n",
      "Train: step: 102260, time: 0.256, loss: 1557.305542\n",
      "Train: step: 102270, time: 0.228, loss: 2331.578125\n",
      "Train: step: 102280, time: 0.225, loss: 2428.458496\n",
      "Train: step: 102290, time: 0.217, loss: 772.333374\n",
      "Train: step: 102300, time: 0.254, loss: 1867.942993\n",
      "Train: step: 102310, time: 0.257, loss: 1878.970947\n",
      "Train: step: 102320, time: 0.240, loss: 2001.568848\n",
      "Train: step: 102330, time: 0.226, loss: 2405.387695\n",
      "Train: step: 102340, time: 0.224, loss: 3127.574463\n",
      "Train: step: 102350, time: 0.264, loss: 647.031921\n",
      "Train: step: 102360, time: 0.229, loss: 2788.114502\n",
      "Train: step: 102370, time: 0.234, loss: 460.677063\n",
      "Train: step: 102380, time: 0.257, loss: 2015.328125\n",
      "Train: step: 102390, time: 0.251, loss: 1377.493164\n",
      "Train: step: 102400, time: 0.223, loss: 2630.529053\n",
      "Train: step: 102410, time: 0.254, loss: 2488.270508\n",
      "Train: step: 102420, time: 0.248, loss: 1485.484497\n",
      "Train: step: 102430, time: 0.258, loss: 2308.774902\n",
      "Train: step: 102440, time: 0.264, loss: 3568.789795\n",
      "Train: step: 102450, time: 0.229, loss: 575.215515\n",
      "Train: step: 102460, time: 0.240, loss: 2817.072998\n",
      "Train: step: 102470, time: 0.274, loss: 2476.368896\n",
      "Train: step: 102480, time: 0.221, loss: 1550.576416\n",
      "Train: step: 102490, time: 0.232, loss: 629.312500\n",
      "Train: step: 102500, time: 0.236, loss: 2886.707764\n",
      "Train: step: 102510, time: 0.255, loss: 2854.445557\n",
      "Train: step: 102520, time: 0.224, loss: 4403.795410\n",
      "Train: step: 102530, time: 0.232, loss: 1605.871338\n",
      "Train: step: 102540, time: 0.260, loss: 2902.844727\n",
      "Train: step: 102550, time: 0.231, loss: 1149.632690\n",
      "Train: step: 102560, time: 0.244, loss: 655.620850\n",
      "Train: step: 102570, time: 0.256, loss: 1119.165894\n",
      "Train: step: 102580, time: 0.237, loss: 1493.630127\n",
      "Train: step: 102590, time: 0.223, loss: 1035.232910\n",
      "Train: step: 102600, time: 0.217, loss: 963.830688\n",
      "Train: step: 102610, time: 0.226, loss: 1296.562378\n",
      "Train: step: 102620, time: 0.256, loss: 1362.295044\n",
      "Train: step: 102630, time: 0.224, loss: 1660.631104\n",
      "Train: step: 102640, time: 0.229, loss: 1401.210327\n",
      "Train: step: 102650, time: 0.231, loss: 2267.619141\n",
      "Train: step: 102660, time: 0.233, loss: 2424.242432\n",
      "Train: step: 102670, time: 0.222, loss: 2362.237305\n",
      "Train: step: 102680, time: 0.231, loss: 1093.738892\n",
      "Train: step: 102690, time: 0.224, loss: 729.230774\n",
      "Train: step: 102700, time: 0.289, loss: 1775.139893\n",
      "Train: step: 102710, time: 0.281, loss: 2962.980957\n",
      "Train: step: 102720, time: 0.268, loss: 1727.145142\n",
      "Train: step: 102730, time: 0.231, loss: 4020.356201\n",
      "Train: step: 102740, time: 0.257, loss: 2446.329102\n",
      "Train: step: 102750, time: 0.226, loss: 2139.121582\n",
      "Train: step: 102760, time: 0.226, loss: 2328.483643\n",
      "Train: step: 102770, time: 0.272, loss: 1388.206665\n",
      "Train: step: 102780, time: 0.227, loss: 618.140625\n",
      "Train: step: 102790, time: 0.228, loss: 1130.637207\n",
      "Train: step: 102800, time: 0.253, loss: 1690.370361\n",
      "Train: step: 102810, time: 0.251, loss: 3638.628662\n",
      "Train: step: 102820, time: 0.237, loss: 2245.167236\n",
      "Train: step: 102830, time: 0.222, loss: 1469.932739\n",
      "Train: step: 102840, time: 0.255, loss: 2199.990723\n",
      "Train: step: 102850, time: 0.259, loss: 1529.260864\n",
      "Train: step: 102860, time: 0.223, loss: 306.404419\n",
      "Train: step: 102870, time: 0.262, loss: 1753.162476\n",
      "Train: step: 102880, time: 0.232, loss: 3108.434814\n",
      "Train: step: 102890, time: 0.253, loss: 1053.319580\n",
      "Train: step: 102900, time: 0.222, loss: 953.262939\n",
      "Train: step: 102910, time: 0.236, loss: 1451.667603\n",
      "Train: step: 102920, time: 0.258, loss: 1011.573303\n",
      "Train: step: 102930, time: 0.271, loss: 752.220398\n",
      "Train: step: 102940, time: 0.265, loss: 2194.657959\n",
      "Train: step: 102950, time: 0.240, loss: 2840.318115\n",
      "Train: step: 102960, time: 0.239, loss: 2650.050049\n",
      "Train: step: 102970, time: 0.246, loss: 2174.083984\n",
      "Train: step: 102980, time: 0.293, loss: 2593.385742\n",
      "Train: step: 102990, time: 0.252, loss: 1900.124878\n",
      "Train: step: 103000, time: 0.251, loss: 231.177277\n",
      "Train: step: 103010, time: 0.245, loss: 2364.924805\n",
      "Train: step: 103020, time: 0.247, loss: 913.793091\n",
      "Train: step: 103030, time: 0.217, loss: 2540.779053\n",
      "Train: step: 103040, time: 0.224, loss: 2242.883789\n",
      "Train: step: 103050, time: 0.222, loss: 371.385345\n",
      "Train: step: 103060, time: 0.270, loss: 816.359070\n",
      "Train: step: 103070, time: 0.229, loss: 1117.191650\n",
      "Train: step: 103080, time: 0.228, loss: 1538.200562\n",
      "Train: step: 103090, time: 0.218, loss: 1431.505005\n",
      "Train: step: 103100, time: 0.252, loss: 2095.313232\n",
      "Train: step: 103110, time: 0.227, loss: 1454.829346\n",
      "Train: step: 103120, time: 0.226, loss: 2468.178955\n",
      "Train: step: 103130, time: 0.242, loss: 566.946594\n",
      "Train: step: 103140, time: 0.236, loss: 1164.075562\n",
      "Train: step: 103150, time: 0.250, loss: 3211.184814\n",
      "Train: step: 103160, time: 0.227, loss: 608.996948\n",
      "Train: step: 103170, time: 0.259, loss: 1292.581421\n",
      "Train: step: 103180, time: 0.251, loss: 953.244751\n",
      "Train: step: 103190, time: 0.230, loss: 608.399902\n",
      "Train: step: 103200, time: 0.224, loss: 2674.312988\n",
      "Train: step: 103210, time: 0.230, loss: 959.273499\n",
      "Train: step: 103220, time: 0.228, loss: 2923.573730\n",
      "Train: step: 103230, time: 0.218, loss: 809.294434\n",
      "Train: step: 103240, time: 0.229, loss: 2103.151611\n",
      "Train: step: 103250, time: 0.224, loss: 1309.312866\n",
      "Train: step: 103260, time: 0.231, loss: 3047.553955\n",
      "Train: step: 103270, time: 0.219, loss: 2966.749268\n",
      "Train: step: 103280, time: 0.232, loss: 2600.790039\n",
      "Train: step: 103290, time: 0.220, loss: 870.797119\n",
      "Train: step: 103300, time: 0.227, loss: 1410.070068\n",
      "Train: step: 103310, time: 0.226, loss: 982.125793\n",
      "Train: step: 103320, time: 0.218, loss: 1819.043945\n",
      "Train: step: 103330, time: 0.228, loss: 1372.212280\n",
      "Train: step: 103340, time: 0.219, loss: 672.126892\n",
      "Train: step: 103350, time: 0.217, loss: 1174.471924\n",
      "Train: step: 103360, time: 0.221, loss: 2184.933838\n",
      "Train: step: 103370, time: 0.225, loss: 2156.283936\n",
      "Train: step: 103380, time: 0.226, loss: 1918.534302\n",
      "Train: step: 103390, time: 0.215, loss: 720.280945\n",
      "Train: step: 103400, time: 0.257, loss: 3820.772705\n",
      "Train: step: 103410, time: 0.224, loss: 2525.351074\n",
      "Train: step: 103420, time: 0.228, loss: 2028.232300\n",
      "Train: step: 103430, time: 0.228, loss: 1751.436890\n",
      "Train: step: 103440, time: 0.220, loss: 2399.017090\n",
      "Train: step: 103450, time: 0.261, loss: 1241.187256\n",
      "Train: step: 103460, time: 0.234, loss: 320.029877\n",
      "Train: step: 103470, time: 0.261, loss: 1031.665405\n",
      "Train: step: 103480, time: 0.228, loss: 1121.012207\n",
      "Train: step: 103490, time: 0.238, loss: 1892.932129\n",
      "Train: step: 103500, time: 0.221, loss: 850.476746\n",
      "Train: step: 103510, time: 0.232, loss: 1012.117615\n",
      "Train: step: 103520, time: 0.227, loss: 1222.610352\n",
      "Train: step: 103530, time: 0.221, loss: 2324.788574\n",
      "Train: step: 103540, time: 0.229, loss: 1074.330688\n",
      "Train: step: 103550, time: 0.224, loss: 2184.042725\n",
      "Train: step: 103560, time: 0.226, loss: 1584.066772\n",
      "Train: step: 103570, time: 0.255, loss: 1712.530518\n",
      "Train: step: 103580, time: 0.223, loss: 842.681030\n",
      "Train: step: 103590, time: 0.224, loss: 963.058044\n",
      "Train: step: 103600, time: 0.220, loss: 587.550476\n",
      "Train: step: 103610, time: 0.227, loss: 2303.225830\n",
      "Train: step: 103620, time: 0.251, loss: 1229.788208\n",
      "Train: step: 103630, time: 0.253, loss: 731.493958\n",
      "Train: step: 103640, time: 0.254, loss: 316.379242\n",
      "Train: step: 103650, time: 0.236, loss: 967.061340\n",
      "Train: step: 103660, time: 0.252, loss: 2024.683960\n",
      "Train: step: 103670, time: 0.219, loss: 1411.599487\n",
      "Train: step: 103680, time: 0.229, loss: 1984.601807\n",
      "Train: step: 103690, time: 0.226, loss: 1735.285522\n",
      "Train: step: 103700, time: 0.221, loss: 2193.883789\n",
      "Train: step: 103710, time: 0.224, loss: 885.046265\n",
      "Train: step: 103720, time: 0.218, loss: 451.118011\n",
      "Train: step: 103730, time: 0.219, loss: 2301.266113\n",
      "Train: step: 103740, time: 0.229, loss: 2605.432129\n",
      "Train: step: 103750, time: 0.223, loss: 1661.448242\n",
      "Train: step: 103760, time: 0.269, loss: 2719.362549\n",
      "Train: step: 103770, time: 0.247, loss: 2633.534424\n",
      "Train: step: 103780, time: 0.254, loss: 500.178223\n",
      "Train: step: 103790, time: 0.226, loss: 2516.260742\n",
      "Train: step: 103800, time: 0.228, loss: 3861.093994\n",
      "Train: step: 103810, time: 0.253, loss: 1437.534058\n",
      "Train: step: 103820, time: 0.250, loss: 1349.146729\n",
      "Train: step: 103830, time: 0.225, loss: 1321.333862\n",
      "Train: step: 103840, time: 0.254, loss: 967.839905\n",
      "Train: step: 103850, time: 0.230, loss: 2450.297119\n",
      "Train: step: 103860, time: 0.222, loss: 1039.630737\n",
      "Train: step: 103870, time: 0.255, loss: 828.147949\n",
      "Train: step: 103880, time: 0.231, loss: 1054.861694\n",
      "Train: step: 103890, time: 0.216, loss: 1617.858398\n",
      "Train: step: 103900, time: 0.229, loss: 1042.071411\n",
      "Train: step: 103910, time: 0.228, loss: 2634.930664\n",
      "Train: step: 103920, time: 0.230, loss: 1302.053833\n",
      "Train: step: 103930, time: 0.227, loss: 3791.731201\n",
      "Train: step: 103940, time: 0.230, loss: 2617.413086\n",
      "Train: step: 103950, time: 0.222, loss: 2392.943115\n",
      "Train: step: 103960, time: 0.255, loss: 2092.301758\n",
      "Train: step: 103970, time: 0.254, loss: 3567.790283\n",
      "Train: step: 103980, time: 0.275, loss: 866.294189\n",
      "Train: step: 103990, time: 0.226, loss: 527.478455\n",
      "Train: step: 104000, time: 0.257, loss: 2871.471924\n",
      "Train: step: 104010, time: 0.258, loss: 857.117798\n",
      "Train: step: 104020, time: 0.228, loss: 363.392914\n",
      "Train: step: 104030, time: 0.229, loss: 1620.156128\n",
      "Train: step: 104040, time: 0.227, loss: 3863.936279\n",
      "Train: step: 104050, time: 0.248, loss: 1481.458008\n",
      "Train: step: 104060, time: 0.226, loss: 1333.093506\n",
      "Train: step: 104070, time: 0.226, loss: 2840.718750\n",
      "Train: step: 104080, time: 0.255, loss: 849.623779\n",
      "Train: step: 104090, time: 0.221, loss: 1667.547729\n",
      "Train: step: 104100, time: 0.269, loss: 3394.875977\n",
      "Train: step: 104110, time: 0.267, loss: 2130.109131\n",
      "Train: step: 104120, time: 0.227, loss: 2646.552979\n",
      "Train: step: 104130, time: 0.256, loss: 961.643066\n",
      "Train: step: 104140, time: 0.240, loss: 1289.818481\n",
      "Train: step: 104150, time: 0.252, loss: 1294.068115\n",
      "Train: step: 104160, time: 0.219, loss: 1418.528076\n",
      "Train: step: 104170, time: 0.270, loss: 670.645203\n",
      "Train: step: 104180, time: 0.235, loss: 1992.400635\n",
      "Train: step: 104190, time: 0.226, loss: 1177.780640\n",
      "Train: step: 104200, time: 0.233, loss: 1532.084351\n",
      "Train: step: 104210, time: 0.229, loss: 2195.560303\n",
      "Train: step: 104220, time: 0.233, loss: 2007.556396\n",
      "Train: step: 104230, time: 0.222, loss: 1943.061401\n",
      "Train: step: 104240, time: 0.288, loss: 2329.500000\n",
      "Train: step: 104250, time: 0.273, loss: 491.421967\n",
      "Train: step: 104260, time: 0.225, loss: 1403.663086\n",
      "Train: step: 104270, time: 0.278, loss: 4057.357178\n",
      "Train: step: 104280, time: 0.231, loss: 1057.322876\n",
      "Train: step: 104290, time: 0.249, loss: 3595.446289\n",
      "Train: step: 104300, time: 0.259, loss: 2720.260254\n",
      "Train: step: 104310, time: 0.229, loss: 383.193817\n",
      "Train: step: 104320, time: 0.250, loss: 2507.882568\n",
      "Train: step: 104330, time: 0.251, loss: 889.238953\n",
      "Train: step: 104340, time: 0.257, loss: 2499.948486\n",
      "Train: step: 104350, time: 0.231, loss: 1892.519775\n",
      "Train: step: 104360, time: 0.250, loss: 2536.131104\n",
      "Train: step: 104370, time: 0.268, loss: 258.340240\n",
      "Train: step: 104380, time: 0.221, loss: 2153.961914\n",
      "Train: step: 104390, time: 0.229, loss: 1935.690430\n",
      "Train: step: 104400, time: 0.219, loss: 1947.756714\n",
      "Train: step: 104410, time: 0.229, loss: 2437.676514\n",
      "Train: step: 104420, time: 0.229, loss: 432.201935\n",
      "Train: step: 104430, time: 0.239, loss: 1609.062256\n",
      "Train: step: 104440, time: 0.236, loss: 899.887390\n",
      "Train: step: 104450, time: 0.225, loss: 387.904358\n",
      "Train: step: 104460, time: 0.225, loss: 3116.142578\n",
      "Train: step: 104470, time: 0.231, loss: 904.476440\n",
      "Train: step: 104480, time: 0.230, loss: 2733.215332\n",
      "Train: step: 104490, time: 0.239, loss: 454.495422\n",
      "Train: step: 104500, time: 0.244, loss: 3244.394287\n",
      "Train: step: 104510, time: 0.225, loss: 3371.230713\n",
      "Train: step: 104520, time: 0.250, loss: 930.535034\n",
      "Train: step: 104530, time: 0.237, loss: 1981.314331\n",
      "Train: step: 104540, time: 0.220, loss: 1667.824585\n",
      "Train: step: 104550, time: 0.234, loss: 1913.432617\n",
      "Train: step: 104560, time: 0.223, loss: 1350.244507\n",
      "Train: step: 104570, time: 0.257, loss: 2536.906250\n",
      "Train: step: 104580, time: 0.227, loss: 2283.853760\n",
      "Train: step: 104590, time: 0.218, loss: 3140.830322\n",
      "Train: step: 104600, time: 0.236, loss: 1868.757690\n",
      "Train: step: 104610, time: 0.246, loss: 2861.166260\n",
      "Train: step: 104620, time: 0.268, loss: 2262.566650\n",
      "Train: step: 104630, time: 0.241, loss: 2871.954590\n",
      "Train: step: 104640, time: 0.245, loss: 1667.777100\n",
      "Train: step: 104650, time: 0.229, loss: 3191.293457\n",
      "Train: step: 104660, time: 0.242, loss: 472.374176\n",
      "Train: step: 104670, time: 0.231, loss: 3041.816162\n",
      "Train: step: 104680, time: 0.234, loss: 1770.945435\n",
      "Train: step: 104690, time: 0.246, loss: 2461.760254\n",
      "Train: step: 104700, time: 0.237, loss: 2731.779541\n",
      "Train: step: 104710, time: 0.232, loss: 1561.414307\n",
      "Train: step: 104720, time: 0.235, loss: 2930.446533\n",
      "Train: step: 104730, time: 0.228, loss: 1510.402100\n",
      "Train: step: 104740, time: 0.228, loss: 1611.538330\n",
      "Train: step: 104750, time: 0.254, loss: 2223.767822\n",
      "Train: step: 104760, time: 0.236, loss: 2330.209961\n",
      "Train: step: 104770, time: 0.237, loss: 652.658142\n",
      "Train: step: 104780, time: 0.237, loss: 1055.319824\n",
      "Train: step: 104790, time: 0.239, loss: 407.424255\n",
      "Train: step: 104800, time: 0.254, loss: 3325.831543\n",
      "Train: step: 104810, time: 0.222, loss: 2916.360107\n",
      "Train: step: 104820, time: 0.222, loss: 3042.505127\n",
      "Train: step: 104830, time: 0.225, loss: 963.860107\n",
      "Train: step: 104840, time: 0.254, loss: 3652.225830\n",
      "Train: step: 104850, time: 0.219, loss: 976.455811\n",
      "Train: step: 104860, time: 0.260, loss: 941.449768\n",
      "Train: step: 104870, time: 0.220, loss: 2762.559082\n",
      "Train: step: 104880, time: 0.229, loss: 1706.396729\n",
      "Train: step: 104890, time: 0.252, loss: 2796.384766\n",
      "Train: step: 104900, time: 0.256, loss: 5277.236816\n",
      "Train: step: 104910, time: 0.258, loss: 2417.391846\n",
      "Train: step: 104920, time: 0.246, loss: 5198.542969\n",
      "Train: step: 104930, time: 0.281, loss: 1795.591064\n",
      "Train: step: 104940, time: 0.245, loss: 312.483002\n",
      "Train: step: 104950, time: 0.252, loss: 882.985596\n",
      "Train: step: 104960, time: 0.262, loss: 1078.943970\n",
      "Train: step: 104970, time: 0.238, loss: 2717.554688\n",
      "Train: step: 104980, time: 0.239, loss: 3116.433350\n",
      "Train: step: 104990, time: 0.240, loss: 4436.393555\n",
      "Train: step: 105000, time: 0.237, loss: 1396.741455\n",
      "Train: step: 105010, time: 0.235, loss: 1247.640991\n",
      "Train: step: 105020, time: 0.261, loss: 2127.954590\n",
      "Train: step: 105030, time: 0.225, loss: 1842.081787\n",
      "Train: step: 105040, time: 0.228, loss: 1923.014160\n",
      "Train: step: 105050, time: 0.237, loss: 179.834900\n",
      "Train: step: 105060, time: 0.240, loss: 3549.447266\n",
      "Train: step: 105070, time: 0.227, loss: 678.937805\n",
      "Train: step: 105080, time: 0.239, loss: 1569.528931\n",
      "Train: step: 105090, time: 0.230, loss: 799.806030\n",
      "Train: step: 105100, time: 0.265, loss: 198.483917\n",
      "Train: step: 105110, time: 0.232, loss: 891.832336\n",
      "Train: step: 105120, time: 0.231, loss: 741.386169\n",
      "Train: step: 105130, time: 0.220, loss: 472.784515\n",
      "Train: step: 105140, time: 0.219, loss: 1200.867798\n",
      "Train: step: 105150, time: 0.223, loss: 1851.877441\n",
      "Train: step: 105160, time: 0.223, loss: 1451.580322\n",
      "Train: step: 105170, time: 0.217, loss: 2321.127441\n",
      "Train: step: 105180, time: 0.226, loss: 3839.647217\n",
      "Train: step: 105190, time: 0.255, loss: 1465.169067\n",
      "Train: step: 105200, time: 0.243, loss: 1889.314331\n",
      "Train: step: 105210, time: 0.221, loss: 2669.143311\n",
      "Train: step: 105220, time: 0.216, loss: 1726.455566\n",
      "Train: step: 105230, time: 0.243, loss: 1690.369507\n",
      "Train: step: 105240, time: 0.222, loss: 1952.588867\n",
      "Train: step: 105250, time: 0.221, loss: 3047.070801\n",
      "Train: step: 105260, time: 0.270, loss: 1594.454712\n",
      "Train: step: 105270, time: 0.251, loss: 1704.003540\n",
      "Train: step: 105280, time: 0.253, loss: 1172.763794\n",
      "Train: step: 105290, time: 0.243, loss: 3131.768066\n",
      "Train: step: 105300, time: 0.252, loss: 1908.723389\n",
      "Train: step: 105310, time: 0.251, loss: 584.077271\n",
      "Train: step: 105320, time: 0.244, loss: 684.669556\n",
      "Train: step: 105330, time: 0.242, loss: 3008.614990\n",
      "Train: step: 105340, time: 0.254, loss: 347.768494\n",
      "Train: step: 105350, time: 0.271, loss: 2551.656738\n",
      "Train: step: 105360, time: 0.263, loss: 1898.502075\n",
      "Train: step: 105370, time: 0.278, loss: 2285.363281\n",
      "Train: step: 105380, time: 0.245, loss: 1337.804565\n",
      "Train: step: 105390, time: 0.240, loss: 2069.352539\n",
      "Train: step: 105400, time: 0.243, loss: 1292.946533\n",
      "Train: step: 105410, time: 0.240, loss: 3294.410645\n",
      "Train: step: 105420, time: 0.246, loss: 1409.844604\n",
      "Train: step: 105430, time: 0.255, loss: 2526.066895\n",
      "Train: step: 105440, time: 0.246, loss: 2370.036621\n",
      "Train: step: 105450, time: 0.257, loss: 2906.613281\n",
      "Train: step: 105460, time: 0.240, loss: 5301.302734\n",
      "Train: step: 105470, time: 0.248, loss: 2182.753662\n",
      "Train: step: 105480, time: 0.247, loss: 2237.854492\n",
      "Train: step: 105490, time: 0.251, loss: 1559.911865\n",
      "Train: step: 105500, time: 0.261, loss: 1577.340332\n",
      "Train: step: 105510, time: 0.245, loss: 3330.219971\n",
      "Train: step: 105520, time: 0.295, loss: 894.211853\n",
      "Train: step: 105530, time: 0.246, loss: 592.195129\n",
      "Train: step: 105540, time: 0.246, loss: 396.697388\n",
      "Train: step: 105550, time: 0.255, loss: 908.310852\n",
      "Train: step: 105560, time: 0.275, loss: 2890.137207\n",
      "Train: step: 105570, time: 0.249, loss: 1266.423462\n",
      "Train: step: 105580, time: 0.276, loss: 2754.052490\n",
      "Train: step: 105590, time: 0.222, loss: 874.940063\n",
      "Train: step: 105600, time: 0.230, loss: 1171.880615\n",
      "Train: step: 105610, time: 0.298, loss: 1946.614380\n",
      "Train: step: 105620, time: 0.244, loss: 674.073730\n",
      "Train: step: 105630, time: 0.222, loss: 1527.238037\n",
      "Train: step: 105640, time: 0.226, loss: 4048.675781\n",
      "Train: step: 105650, time: 0.222, loss: 1587.994873\n",
      "Train: step: 105660, time: 0.222, loss: 1986.441406\n",
      "Train: step: 105670, time: 0.224, loss: 1533.106689\n",
      "Train: step: 105680, time: 0.263, loss: 2501.473633\n",
      "Train: step: 105690, time: 0.254, loss: 2224.696289\n",
      "Train: step: 105700, time: 0.252, loss: 3086.493164\n",
      "Train: step: 105710, time: 0.265, loss: 1352.574463\n",
      "Train: step: 105720, time: 0.222, loss: 2991.083496\n",
      "Train: step: 105730, time: 0.255, loss: 2935.479980\n",
      "Train: step: 105740, time: 0.232, loss: 1587.351196\n",
      "Train: step: 105750, time: 0.218, loss: 1966.582520\n",
      "Train: step: 105760, time: 0.215, loss: 1693.799805\n",
      "Train: step: 105770, time: 0.222, loss: 2964.015869\n",
      "Train: step: 105780, time: 0.248, loss: 1811.184448\n",
      "Train: step: 105790, time: 0.222, loss: 1809.869873\n",
      "Train: step: 105800, time: 0.226, loss: 1125.214722\n",
      "Train: step: 105810, time: 0.215, loss: 1396.021240\n",
      "Train: step: 105820, time: 0.222, loss: 825.334412\n",
      "Train: step: 105830, time: 0.213, loss: 2241.823242\n",
      "Train: step: 105840, time: 0.233, loss: 805.127136\n",
      "Train: step: 105850, time: 0.231, loss: 1789.129272\n",
      "Train: step: 105860, time: 0.230, loss: 2456.123047\n",
      "Train: step: 105870, time: 0.244, loss: 871.502502\n",
      "Train: step: 105880, time: 0.217, loss: 2227.543701\n",
      "Train: step: 105890, time: 0.217, loss: 2406.537598\n",
      "Train: step: 105900, time: 0.265, loss: 2182.749023\n",
      "Train: step: 105910, time: 0.226, loss: 2184.784668\n",
      "Train: step: 105920, time: 0.241, loss: 1420.428833\n",
      "Train: step: 105930, time: 0.227, loss: 1507.641113\n",
      "Train: step: 105940, time: 0.234, loss: 902.922791\n",
      "Train: step: 105950, time: 0.226, loss: 3444.075439\n",
      "Train: step: 105960, time: 0.268, loss: 2328.243164\n",
      "Train: step: 105970, time: 0.230, loss: 2735.157715\n",
      "Train: step: 105980, time: 0.260, loss: 1064.092529\n",
      "Train: step: 105990, time: 0.251, loss: 1121.235107\n",
      "Train: step: 106000, time: 0.268, loss: 1190.280762\n",
      "Train: step: 106010, time: 0.247, loss: 1505.965454\n",
      "Train: step: 106020, time: 0.245, loss: 2487.479004\n",
      "Train: step: 106030, time: 0.244, loss: 1953.009033\n",
      "Train: step: 106040, time: 0.228, loss: 1718.095215\n",
      "Train: step: 106050, time: 0.222, loss: 1758.524414\n",
      "Train: step: 106060, time: 0.245, loss: 1781.473999\n",
      "Train: step: 106070, time: 0.224, loss: 2167.631836\n",
      "Train: step: 106080, time: 0.221, loss: 4358.031250\n",
      "Train: step: 106090, time: 0.224, loss: 2420.766357\n",
      "Train: step: 106100, time: 0.236, loss: 1112.738281\n",
      "Train: step: 106110, time: 0.255, loss: 2945.820801\n",
      "Train: step: 106120, time: 0.265, loss: 2888.670654\n",
      "Train: step: 106130, time: 0.220, loss: 1854.257812\n",
      "Train: step: 106140, time: 0.233, loss: 2626.389404\n",
      "Train: step: 106150, time: 0.235, loss: 2883.462158\n",
      "Train: step: 106160, time: 0.222, loss: 3142.367188\n",
      "Train: step: 106170, time: 0.226, loss: 1383.689941\n",
      "Train: step: 106180, time: 0.220, loss: 3003.093994\n",
      "Train: step: 106190, time: 0.216, loss: 3188.091064\n",
      "Train: step: 106200, time: 0.253, loss: 3352.045410\n",
      "Train: step: 106210, time: 0.229, loss: 1717.646118\n",
      "Train: step: 106220, time: 0.225, loss: 552.887939\n",
      "Train: step: 106230, time: 0.252, loss: 2649.092285\n",
      "Train: step: 106240, time: 0.221, loss: 1384.164307\n",
      "Train: step: 106250, time: 0.249, loss: 1182.382324\n",
      "Train: step: 106260, time: 0.222, loss: 2382.565186\n",
      "Train: step: 106270, time: 0.231, loss: 1536.716431\n",
      "Train: step: 106280, time: 0.266, loss: 2293.739258\n",
      "Train: step: 106290, time: 0.228, loss: 2322.320557\n",
      "Train: step: 106300, time: 0.225, loss: 1492.615479\n",
      "Train: step: 106310, time: 0.261, loss: 2551.544922\n",
      "Train: step: 106320, time: 0.229, loss: 2388.572021\n",
      "Train: step: 106330, time: 0.264, loss: 1131.359741\n",
      "Train: step: 106340, time: 0.249, loss: 2653.004639\n",
      "Train: step: 106350, time: 0.234, loss: 2550.101562\n",
      "Train: step: 106360, time: 0.231, loss: 1607.980103\n",
      "Train: step: 106370, time: 0.234, loss: 1137.819336\n",
      "Train: step: 106380, time: 0.223, loss: 3247.307129\n",
      "Train: step: 106390, time: 0.227, loss: 1852.191772\n",
      "Train: step: 106400, time: 0.221, loss: 1446.516357\n",
      "Train: step: 106410, time: 0.228, loss: 1327.388306\n",
      "Train: step: 106420, time: 0.229, loss: 608.270813\n",
      "Train: step: 106430, time: 0.269, loss: 1093.696167\n",
      "Train: step: 106440, time: 0.216, loss: 2180.233398\n",
      "Train: step: 106450, time: 0.214, loss: 1408.771362\n",
      "Train: step: 106460, time: 0.215, loss: 2006.560791\n",
      "Train: step: 106470, time: 0.216, loss: 1849.689087\n",
      "Train: step: 106480, time: 0.212, loss: 3287.057129\n",
      "Train: step: 106490, time: 0.238, loss: 2357.650146\n",
      "Train: step: 106500, time: 0.247, loss: 2705.315430\n",
      "Train: step: 106510, time: 0.248, loss: 2939.557861\n",
      "Train: step: 106520, time: 0.232, loss: 2093.446289\n",
      "Train: step: 106530, time: 0.255, loss: 1857.857544\n",
      "Train: step: 106540, time: 0.228, loss: 1706.693115\n",
      "Train: step: 106550, time: 0.228, loss: 986.017578\n",
      "Train: step: 106560, time: 0.255, loss: 1636.685791\n",
      "Train: step: 106570, time: 0.241, loss: 2329.834229\n",
      "Train: step: 106580, time: 0.224, loss: 1888.273315\n",
      "Train: step: 106590, time: 0.240, loss: 1703.725708\n",
      "Train: step: 106600, time: 0.236, loss: 1591.857666\n",
      "Train: step: 106610, time: 0.261, loss: 642.777344\n",
      "Train: step: 106620, time: 0.225, loss: 1281.316528\n",
      "Train: step: 106630, time: 0.244, loss: 623.060547\n",
      "Train: step: 106640, time: 0.226, loss: 1714.433228\n",
      "Train: step: 106650, time: 0.229, loss: 858.757568\n",
      "Train: step: 106660, time: 0.230, loss: 1001.960205\n",
      "Train: step: 106670, time: 0.253, loss: 2171.320312\n",
      "Train: step: 106680, time: 0.219, loss: 1546.826416\n",
      "Train: step: 106690, time: 0.255, loss: 1023.808594\n",
      "Train: step: 106700, time: 0.220, loss: 364.125092\n",
      "Train: step: 106710, time: 0.242, loss: 2570.903076\n",
      "Train: step: 106720, time: 0.232, loss: 1385.917725\n",
      "Train: step: 106730, time: 0.232, loss: 1104.031494\n",
      "Train: step: 106740, time: 0.258, loss: 1558.691040\n",
      "Train: step: 106750, time: 0.264, loss: 1220.994629\n",
      "Train: step: 106760, time: 0.261, loss: 1352.376221\n",
      "Train: step: 106770, time: 0.251, loss: 2495.533691\n",
      "Train: step: 106780, time: 0.232, loss: 1467.856079\n",
      "Train: step: 106790, time: 0.250, loss: 3477.780029\n",
      "Train: step: 106800, time: 0.239, loss: 1381.789917\n",
      "Train: step: 106810, time: 0.228, loss: 2205.621826\n",
      "Train: step: 106820, time: 0.265, loss: 2132.559814\n",
      "Train: step: 106830, time: 0.244, loss: 1678.582520\n",
      "Train: step: 106840, time: 0.231, loss: 1726.908569\n",
      "Train: step: 106850, time: 0.230, loss: 1846.101440\n",
      "Train: step: 106860, time: 0.225, loss: 2183.362061\n",
      "Train: step: 106870, time: 0.250, loss: 2176.000977\n",
      "Train: step: 106880, time: 0.256, loss: 1031.388306\n",
      "Train: step: 106890, time: 0.222, loss: 3350.117920\n",
      "Train: step: 106900, time: 0.218, loss: 1258.508423\n",
      "Train: step: 106910, time: 0.228, loss: 900.758179\n",
      "Train: step: 106920, time: 0.222, loss: 2770.779541\n",
      "Train: step: 106930, time: 0.254, loss: 3276.056885\n",
      "Train: step: 106940, time: 0.263, loss: 1136.002319\n",
      "Train: step: 106950, time: 0.220, loss: 339.880890\n",
      "Train: step: 106960, time: 0.234, loss: 2573.103027\n",
      "Train: step: 106970, time: 0.269, loss: 1805.811279\n",
      "Train: step: 106980, time: 0.258, loss: 598.196350\n",
      "Train: step: 106990, time: 0.231, loss: 693.963440\n",
      "Train: step: 107000, time: 0.226, loss: 861.806519\n",
      "Train: step: 107010, time: 0.226, loss: 1843.498657\n",
      "Train: step: 107020, time: 0.243, loss: 2376.858154\n",
      "Train: step: 107030, time: 0.225, loss: 827.224304\n",
      "Train: step: 107040, time: 0.234, loss: 1049.362305\n",
      "Train: step: 107050, time: 0.238, loss: 1674.998535\n",
      "Train: step: 107060, time: 0.227, loss: 3755.386963\n",
      "Train: step: 107070, time: 0.264, loss: 392.163788\n",
      "Train: step: 107080, time: 0.264, loss: 1099.151978\n",
      "Train: step: 107090, time: 0.256, loss: 1068.284424\n",
      "Train: step: 107100, time: 0.265, loss: 1466.236450\n",
      "Train: step: 107110, time: 0.222, loss: 541.329651\n",
      "Train: step: 107120, time: 0.248, loss: 1882.645874\n",
      "Train: step: 107130, time: 0.230, loss: 1414.734863\n",
      "Train: step: 107140, time: 0.226, loss: 2941.164307\n",
      "Train: step: 107150, time: 0.224, loss: 746.752747\n",
      "Train: step: 107160, time: 0.222, loss: 2629.473389\n",
      "Train: step: 107170, time: 0.225, loss: 1558.968628\n",
      "Train: step: 107180, time: 0.233, loss: 1079.675903\n",
      "Train: step: 107190, time: 0.228, loss: 2150.385498\n",
      "Train: step: 107200, time: 0.224, loss: 836.624390\n",
      "Train: step: 107210, time: 0.219, loss: 1318.163330\n",
      "Train: step: 107220, time: 0.218, loss: 920.633789\n",
      "Train: step: 107230, time: 0.218, loss: 2058.595947\n",
      "Train: step: 107240, time: 0.241, loss: 1343.901978\n",
      "Train: step: 107250, time: 0.248, loss: 2119.006592\n",
      "Train: step: 107260, time: 0.222, loss: 1619.217407\n",
      "Train: step: 107270, time: 0.261, loss: 1538.608154\n",
      "Train: step: 107280, time: 0.223, loss: 3116.308594\n",
      "Train: step: 107290, time: 0.226, loss: 1381.907837\n",
      "Train: step: 107300, time: 0.264, loss: 2034.211792\n",
      "Train: step: 107310, time: 0.268, loss: 514.571228\n",
      "Train: step: 107320, time: 0.235, loss: 900.018188\n",
      "Train: step: 107330, time: 0.251, loss: 3473.794189\n",
      "Train: step: 107340, time: 0.232, loss: 1983.731323\n",
      "Train: step: 107350, time: 0.257, loss: 1982.609375\n",
      "Train: step: 107360, time: 0.262, loss: 3110.823486\n",
      "Train: step: 107370, time: 0.223, loss: 1352.757080\n",
      "Train: step: 107380, time: 0.220, loss: 994.954956\n",
      "Train: step: 107390, time: 0.221, loss: 1716.030273\n",
      "Train: step: 107400, time: 0.223, loss: 3648.886719\n",
      "Train: step: 107410, time: 0.219, loss: 374.069061\n",
      "Train: step: 107420, time: 0.216, loss: 2081.542236\n",
      "Train: step: 107430, time: 0.226, loss: 1777.914307\n",
      "Train: step: 107440, time: 0.219, loss: 2732.620605\n",
      "Train: step: 107450, time: 0.238, loss: 235.155548\n",
      "Train: step: 107460, time: 0.222, loss: 2289.259277\n",
      "Train: step: 107470, time: 0.239, loss: 1146.421143\n",
      "Train: step: 107480, time: 0.221, loss: 1656.531250\n",
      "Train: step: 107490, time: 0.220, loss: 2185.277832\n",
      "Train: step: 107500, time: 0.215, loss: 2618.440674\n",
      "Train: step: 107510, time: 0.220, loss: 1686.207642\n",
      "Train: step: 107520, time: 0.224, loss: 2694.971436\n",
      "Train: step: 107530, time: 0.240, loss: 1777.826782\n",
      "Train: step: 107540, time: 0.250, loss: 2522.712158\n",
      "Train: step: 107550, time: 0.229, loss: 2773.835449\n",
      "Train: step: 107560, time: 0.231, loss: 1893.502075\n",
      "Train: step: 107570, time: 0.223, loss: 2417.452393\n",
      "Train: step: 107580, time: 0.244, loss: 1650.832153\n",
      "Train: step: 107590, time: 0.234, loss: 2436.949463\n",
      "Train: step: 107600, time: 0.243, loss: 2887.435547\n",
      "Train: step: 107610, time: 0.255, loss: 1027.100830\n",
      "Train: step: 107620, time: 0.248, loss: 2221.760742\n",
      "Train: step: 107630, time: 0.216, loss: 754.093384\n",
      "Train: step: 107640, time: 0.276, loss: 2522.249756\n",
      "Train: step: 107650, time: 0.224, loss: 2551.417969\n",
      "Train: step: 107660, time: 0.228, loss: 277.941742\n",
      "Train: step: 107670, time: 0.232, loss: 2730.985352\n",
      "Train: step: 107680, time: 0.226, loss: 689.460510\n",
      "Train: step: 107690, time: 0.234, loss: 2224.284180\n",
      "Train: step: 107700, time: 0.233, loss: 2803.214600\n",
      "Train: step: 107710, time: 0.247, loss: 2400.469727\n",
      "Train: step: 107720, time: 0.237, loss: 1050.000854\n",
      "Train: step: 107730, time: 0.221, loss: 1215.791138\n",
      "Train: step: 107740, time: 0.232, loss: 2191.213867\n",
      "Train: step: 107750, time: 0.236, loss: 2082.559570\n",
      "Train: step: 107760, time: 0.239, loss: 1435.311768\n",
      "Train: step: 107770, time: 0.255, loss: 763.212036\n",
      "Train: step: 107780, time: 0.229, loss: 750.781555\n",
      "Train: step: 107790, time: 0.233, loss: 642.758179\n",
      "Train: step: 107800, time: 0.267, loss: 451.188354\n",
      "Train: step: 107810, time: 0.240, loss: 1661.761841\n",
      "Train: step: 107820, time: 0.256, loss: 2457.508057\n",
      "Train: step: 107830, time: 0.259, loss: 573.098450\n",
      "Train: step: 107840, time: 0.230, loss: 1316.946899\n",
      "Train: step: 107850, time: 0.266, loss: 2744.033936\n",
      "Train: step: 107860, time: 0.257, loss: 732.933411\n",
      "Train: step: 107870, time: 0.224, loss: 2540.575195\n",
      "Train: step: 107880, time: 0.229, loss: 1698.701294\n",
      "Train: step: 107890, time: 0.224, loss: 1106.957764\n",
      "Train: step: 107900, time: 0.237, loss: 1600.575195\n",
      "Train: step: 107910, time: 0.228, loss: 2658.419189\n",
      "Train: step: 107920, time: 0.219, loss: 391.380859\n",
      "Train: step: 107930, time: 0.250, loss: 781.145508\n",
      "Train: step: 107940, time: 0.254, loss: 416.060303\n",
      "Train: step: 107950, time: 0.264, loss: 2365.677734\n",
      "Train: step: 107960, time: 0.226, loss: 1665.379150\n",
      "Train: step: 107970, time: 0.228, loss: 2996.979736\n",
      "Train: step: 107980, time: 0.227, loss: 2419.986084\n",
      "Train: step: 107990, time: 0.224, loss: 1965.370850\n",
      "Train: step: 108000, time: 0.225, loss: 1245.902344\n",
      "Train: step: 108010, time: 0.259, loss: 2911.120361\n",
      "Train: step: 108020, time: 0.278, loss: 1651.755371\n",
      "Train: step: 108030, time: 0.265, loss: 1173.595093\n",
      "Train: step: 108040, time: 0.255, loss: 3059.130127\n",
      "Train: step: 108050, time: 0.248, loss: 1987.699219\n",
      "Train: step: 108060, time: 0.249, loss: 2674.303955\n",
      "Train: step: 108070, time: 0.249, loss: 2729.034180\n",
      "Train: step: 108080, time: 0.248, loss: 2665.232910\n",
      "Train: step: 108090, time: 0.263, loss: 474.777618\n",
      "Train: step: 108100, time: 0.220, loss: 1773.019287\n",
      "Train: step: 108110, time: 0.234, loss: 1592.247925\n",
      "Train: step: 108120, time: 0.232, loss: 2770.970459\n",
      "Train: step: 108130, time: 0.246, loss: 2464.644287\n",
      "Train: step: 108140, time: 0.244, loss: 497.614441\n",
      "Train: step: 108150, time: 0.274, loss: 3302.275391\n",
      "Train: step: 108160, time: 0.241, loss: 1222.792236\n",
      "Train: step: 108170, time: 0.234, loss: 1663.777344\n",
      "Train: step: 108180, time: 0.271, loss: 2492.901611\n",
      "Train: step: 108190, time: 0.262, loss: 3983.043213\n",
      "Train: step: 108200, time: 0.272, loss: 1085.802002\n",
      "Train: step: 108210, time: 0.257, loss: 1223.571655\n",
      "Train: step: 108220, time: 0.248, loss: 1124.283203\n",
      "Train: step: 108230, time: 0.275, loss: 1908.127930\n",
      "Train: step: 108240, time: 0.256, loss: 1938.163818\n",
      "Train: step: 108250, time: 0.231, loss: 623.528503\n",
      "Train: step: 108260, time: 0.220, loss: 2114.996826\n",
      "Train: step: 108270, time: 0.219, loss: 1537.854736\n",
      "Train: step: 108280, time: 0.225, loss: 1269.806641\n",
      "Train: step: 108290, time: 0.236, loss: 3335.901123\n",
      "Train: step: 108300, time: 0.213, loss: 3278.562744\n",
      "Train: step: 108310, time: 0.218, loss: 2718.187500\n",
      "Train: step: 108320, time: 0.232, loss: 903.824463\n",
      "Train: step: 108330, time: 0.254, loss: 2089.389648\n",
      "Train: step: 108340, time: 0.259, loss: 1432.187988\n",
      "Train: step: 108350, time: 0.244, loss: 571.676819\n",
      "Train: step: 108360, time: 0.258, loss: 2739.069092\n",
      "Train: step: 108370, time: 0.236, loss: 791.656555\n",
      "Train: step: 108380, time: 0.261, loss: 3283.022217\n",
      "Train: step: 108390, time: 0.236, loss: 2776.396973\n",
      "Train: step: 108400, time: 0.263, loss: 2640.015869\n",
      "Train: step: 108410, time: 0.254, loss: 960.395874\n",
      "Train: step: 108420, time: 0.256, loss: 549.359009\n",
      "Train: step: 108430, time: 0.242, loss: 682.661255\n",
      "Train: step: 108440, time: 0.231, loss: 3038.081787\n",
      "Train: step: 108450, time: 0.230, loss: 2137.794678\n",
      "Train: step: 108460, time: 0.239, loss: 1524.479126\n",
      "Train: step: 108470, time: 0.265, loss: 1602.079224\n",
      "Train: step: 108480, time: 0.250, loss: 2950.054688\n",
      "Train: step: 108490, time: 0.238, loss: 1501.980835\n",
      "Train: step: 108500, time: 0.262, loss: 1111.774902\n",
      "Train: step: 108510, time: 0.247, loss: 1923.743896\n",
      "Train: step: 108520, time: 0.244, loss: 1353.848145\n",
      "Train: step: 108530, time: 0.234, loss: 1078.713135\n",
      "Train: step: 108540, time: 0.245, loss: 3007.266846\n",
      "Train: step: 108550, time: 0.234, loss: 2329.156006\n",
      "Train: step: 108560, time: 0.245, loss: 1840.458374\n",
      "Train: step: 108570, time: 0.254, loss: 1888.950684\n",
      "Train: step: 108580, time: 0.256, loss: 2211.707275\n",
      "Train: step: 108590, time: 0.219, loss: 2139.049072\n",
      "Train: step: 108600, time: 0.216, loss: 891.157104\n",
      "Train: step: 108610, time: 0.236, loss: 3007.298828\n",
      "Train: step: 108620, time: 0.230, loss: 2174.429199\n",
      "Train: step: 108630, time: 0.231, loss: 2567.699707\n",
      "Train: step: 108640, time: 0.233, loss: 2686.334473\n",
      "Train: step: 108650, time: 0.265, loss: 2201.297852\n",
      "Train: step: 108660, time: 0.254, loss: 3125.777100\n",
      "Train: step: 108670, time: 0.277, loss: 1279.361084\n",
      "Train: step: 108680, time: 0.278, loss: 4170.851562\n",
      "Train: step: 108690, time: 0.266, loss: 1567.568970\n",
      "Train: step: 108700, time: 0.233, loss: 641.968323\n",
      "Train: step: 108710, time: 0.220, loss: 3349.260498\n",
      "Train: step: 108720, time: 0.236, loss: 2028.577759\n",
      "Train: step: 108730, time: 0.223, loss: 2308.045166\n",
      "Train: step: 108740, time: 0.236, loss: 2159.567383\n",
      "Train: step: 108750, time: 0.222, loss: 1609.820557\n",
      "Train: step: 108760, time: 0.230, loss: 1347.356079\n",
      "Train: step: 108770, time: 0.235, loss: 3185.827881\n",
      "Train: step: 108780, time: 0.245, loss: 1532.545654\n",
      "Train: step: 108790, time: 0.241, loss: 706.413391\n",
      "Train: step: 108800, time: 0.234, loss: 1952.181152\n",
      "Train: step: 108810, time: 0.248, loss: 1284.402100\n",
      "Train: step: 108820, time: 0.245, loss: 3562.785889\n",
      "Train: step: 108830, time: 0.254, loss: 1027.318726\n",
      "Train: step: 108840, time: 0.240, loss: 1316.555298\n",
      "Train: step: 108850, time: 0.233, loss: 2780.703125\n",
      "Train: step: 108860, time: 0.257, loss: 2113.908936\n",
      "Train: step: 108870, time: 0.241, loss: 2625.035156\n",
      "Train: step: 108880, time: 0.244, loss: 1009.377319\n",
      "Train: step: 108890, time: 0.242, loss: 1912.126831\n",
      "Train: step: 108900, time: 0.248, loss: 2090.416260\n",
      "Train: step: 108910, time: 0.244, loss: 1012.695496\n",
      "Train: step: 108920, time: 0.246, loss: 1576.524780\n",
      "Train: step: 108930, time: 0.250, loss: 2736.861084\n",
      "Train: step: 108940, time: 0.245, loss: 1998.096436\n",
      "Train: step: 108950, time: 0.258, loss: 1330.892456\n",
      "Train: step: 108960, time: 0.226, loss: 971.745239\n",
      "Train: step: 108970, time: 0.248, loss: 2164.030762\n",
      "Train: step: 108980, time: 0.222, loss: 1519.678711\n",
      "Train: step: 108990, time: 0.226, loss: 3008.655029\n",
      "Train: step: 109000, time: 0.264, loss: 2530.552490\n",
      "Train: step: 109010, time: 0.228, loss: 3045.863037\n",
      "Train: step: 109020, time: 0.271, loss: 2305.154541\n",
      "Train: step: 109030, time: 0.236, loss: 1149.324951\n",
      "Train: step: 109040, time: 0.224, loss: 1049.414185\n",
      "Train: step: 109050, time: 0.263, loss: 2747.678223\n",
      "Train: step: 109060, time: 0.264, loss: 1317.951660\n",
      "Train: step: 109070, time: 0.222, loss: 529.301636\n",
      "Train: step: 109080, time: 0.242, loss: 1853.947876\n",
      "Train: step: 109090, time: 0.260, loss: 1418.772339\n",
      "Train: step: 109100, time: 0.248, loss: 2677.444092\n",
      "Train: step: 109110, time: 0.222, loss: 1966.261353\n",
      "Train: step: 109120, time: 0.236, loss: 826.817261\n",
      "Train: step: 109130, time: 0.221, loss: 2850.045898\n",
      "Train: step: 109140, time: 0.233, loss: 239.355591\n",
      "Train: step: 109150, time: 0.252, loss: 4625.430664\n",
      "Train: step: 109160, time: 0.235, loss: 2816.520996\n",
      "Train: step: 109170, time: 0.253, loss: 325.632294\n",
      "Train: step: 109180, time: 0.254, loss: 1918.647949\n",
      "Train: step: 109190, time: 0.263, loss: 3095.951904\n",
      "Train: step: 109200, time: 0.264, loss: 3819.739502\n",
      "Train: step: 109210, time: 0.289, loss: 1007.428589\n",
      "Train: step: 109220, time: 0.234, loss: 2950.328369\n",
      "Train: step: 109230, time: 0.285, loss: 2341.348877\n",
      "Train: step: 109240, time: 0.270, loss: 1408.129395\n",
      "Train: step: 109250, time: 0.232, loss: 513.780518\n",
      "Train: step: 109260, time: 0.228, loss: 2418.602295\n",
      "Train: step: 109270, time: 0.249, loss: 1197.067139\n",
      "Train: step: 109280, time: 0.227, loss: 3500.182861\n",
      "Train: step: 109290, time: 0.284, loss: 2087.595459\n",
      "Train: step: 109300, time: 0.221, loss: 2412.003174\n",
      "Train: step: 109310, time: 0.251, loss: 2588.275879\n",
      "Train: step: 109320, time: 0.227, loss: 1561.771851\n",
      "Train: step: 109330, time: 0.235, loss: 2579.584961\n",
      "Train: step: 109340, time: 0.228, loss: 1296.023682\n",
      "Train: step: 109350, time: 0.267, loss: 2628.174316\n",
      "Train: step: 109360, time: 0.218, loss: 2091.593018\n",
      "Train: step: 109370, time: 0.226, loss: 222.546478\n",
      "Train: step: 109380, time: 0.262, loss: 1984.223022\n",
      "Train: step: 109390, time: 0.222, loss: 1691.132935\n",
      "Train: step: 109400, time: 0.258, loss: 1512.265747\n",
      "Train: step: 109410, time: 0.292, loss: 3754.393311\n",
      "Train: step: 109420, time: 0.263, loss: 887.918335\n",
      "Train: step: 109430, time: 0.237, loss: 3043.759521\n",
      "Train: step: 109440, time: 0.254, loss: 1981.803345\n",
      "Train: step: 109450, time: 0.234, loss: 928.125305\n",
      "Train: step: 109460, time: 0.231, loss: 2574.994141\n",
      "Train: step: 109470, time: 0.286, loss: 1126.451172\n",
      "Train: step: 109480, time: 0.268, loss: 1559.974854\n",
      "Train: step: 109490, time: 0.233, loss: 771.741760\n",
      "Train: step: 109500, time: 0.247, loss: 345.887390\n",
      "Train: step: 109510, time: 0.264, loss: 257.133759\n",
      "Train: step: 109520, time: 0.232, loss: 2132.341309\n",
      "Train: step: 109530, time: 0.305, loss: 1117.275146\n",
      "Train: step: 109540, time: 0.224, loss: 1308.957520\n",
      "Train: step: 109550, time: 0.280, loss: 2979.051025\n",
      "Train: step: 109560, time: 0.221, loss: 1296.824707\n",
      "Train: step: 109570, time: 0.253, loss: 1969.060547\n",
      "Train: step: 109580, time: 0.227, loss: 748.500977\n",
      "Train: step: 109590, time: 0.251, loss: 2283.220947\n",
      "Train: step: 109600, time: 0.217, loss: 1557.523438\n",
      "Train: step: 109610, time: 0.234, loss: 2695.689209\n",
      "Train: step: 109620, time: 0.253, loss: 1732.554321\n",
      "Train: step: 109630, time: 0.248, loss: 3030.218506\n",
      "Train: step: 109640, time: 0.241, loss: 2388.882080\n",
      "Train: step: 109650, time: 0.228, loss: 903.762207\n",
      "Train: step: 109660, time: 0.247, loss: 811.557495\n",
      "Train: step: 109670, time: 0.215, loss: 241.693939\n",
      "Train: step: 109680, time: 0.220, loss: 3159.871582\n",
      "Train: step: 109690, time: 0.234, loss: 616.094788\n",
      "Train: step: 109700, time: 0.226, loss: 3151.030518\n",
      "Train: step: 109710, time: 0.228, loss: 714.111633\n",
      "Train: step: 109720, time: 0.230, loss: 1006.268311\n",
      "Train: step: 109730, time: 0.231, loss: 551.368225\n",
      "Train: step: 109740, time: 0.225, loss: 1071.054443\n",
      "Train: step: 109750, time: 0.230, loss: 1645.906128\n",
      "Train: step: 109760, time: 0.226, loss: 1548.444946\n",
      "Train: step: 109770, time: 0.222, loss: 3641.760254\n",
      "Train: step: 109780, time: 0.231, loss: 333.701385\n",
      "Train: step: 109790, time: 0.230, loss: 1748.265869\n",
      "Train: step: 109800, time: 0.243, loss: 1924.471680\n",
      "Train: step: 109810, time: 0.264, loss: 2381.630615\n",
      "Train: step: 109820, time: 0.260, loss: 2533.054199\n",
      "Train: step: 109830, time: 0.223, loss: 3049.996582\n",
      "Train: step: 109840, time: 0.226, loss: 1815.459839\n",
      "Train: step: 109850, time: 0.220, loss: 1482.841064\n",
      "Train: step: 109860, time: 0.261, loss: 2852.030518\n",
      "Train: step: 109870, time: 0.233, loss: 772.820801\n",
      "Train: step: 109880, time: 0.259, loss: 2360.940186\n",
      "Train: step: 109890, time: 0.221, loss: 2423.818604\n",
      "Train: step: 109900, time: 0.221, loss: 1295.640381\n",
      "Train: step: 109910, time: 0.218, loss: 1810.070923\n",
      "Train: step: 109920, time: 0.215, loss: 3089.916992\n",
      "Train: step: 109930, time: 0.219, loss: 2284.137695\n",
      "Train: step: 109940, time: 0.277, loss: 2398.637695\n",
      "Train: step: 109950, time: 0.223, loss: 4061.236572\n",
      "Train: step: 109960, time: 0.233, loss: 1092.258179\n",
      "Train: step: 109970, time: 0.223, loss: 1156.452515\n",
      "Train: step: 109980, time: 0.225, loss: 1315.906006\n",
      "Train: step: 109990, time: 0.236, loss: 3625.883789\n",
      "Train: step: 110000, time: 0.217, loss: 2751.647217\n",
      "Train: step: 110010, time: 0.258, loss: 2982.123535\n",
      "Train: step: 110020, time: 0.230, loss: 584.729614\n",
      "Train: step: 110030, time: 0.223, loss: 576.113525\n",
      "Train: step: 110040, time: 0.239, loss: 1613.007690\n",
      "Train: step: 110050, time: 0.223, loss: 3763.852295\n",
      "Train: step: 110060, time: 0.224, loss: 2761.117432\n",
      "Train: step: 110070, time: 0.227, loss: 2397.895752\n",
      "Train: step: 110080, time: 0.239, loss: 1589.151367\n",
      "Train: step: 110090, time: 0.258, loss: 2609.446533\n",
      "Train: step: 110100, time: 0.262, loss: 2386.912842\n",
      "Train: step: 110110, time: 0.246, loss: 1968.239258\n",
      "Train: step: 110120, time: 0.235, loss: 2864.906250\n",
      "Train: step: 110130, time: 0.232, loss: 1751.398560\n",
      "Train: step: 110140, time: 0.239, loss: 383.218445\n",
      "Train: step: 110150, time: 0.232, loss: 2047.003174\n",
      "Train: step: 110160, time: 0.227, loss: 915.903259\n",
      "Train: step: 110170, time: 0.228, loss: 989.409851\n",
      "Train: step: 110180, time: 0.233, loss: 494.596863\n",
      "Train: step: 110190, time: 0.223, loss: 920.042419\n",
      "Train: step: 110200, time: 0.223, loss: 2207.694336\n",
      "Train: step: 110210, time: 0.227, loss: 3006.150879\n",
      "Train: step: 110220, time: 0.255, loss: 822.365540\n",
      "Train: step: 110230, time: 0.224, loss: 1802.702759\n",
      "Train: step: 110240, time: 0.243, loss: 868.106934\n",
      "Train: step: 110250, time: 0.230, loss: 1507.792847\n",
      "Train: step: 110260, time: 0.244, loss: 763.508240\n",
      "Train: step: 110270, time: 0.256, loss: 424.394836\n",
      "Train: step: 110280, time: 0.290, loss: 1683.397705\n",
      "Train: step: 110290, time: 0.256, loss: 1066.665894\n",
      "Train: step: 110300, time: 0.240, loss: 2408.505615\n",
      "Train: step: 110310, time: 0.259, loss: 2156.839844\n",
      "Train: step: 110320, time: 0.244, loss: 1804.568726\n",
      "Train: step: 110330, time: 0.249, loss: 386.670258\n",
      "Train: step: 110340, time: 0.234, loss: 2192.575684\n",
      "Train: step: 110350, time: 0.242, loss: 475.689117\n",
      "Train: step: 110360, time: 0.253, loss: 2051.038818\n",
      "Train: step: 110370, time: 0.226, loss: 2907.909424\n",
      "Train: step: 110380, time: 0.237, loss: 3174.866211\n",
      "Train: step: 110390, time: 0.265, loss: 2902.103271\n",
      "Train: step: 110400, time: 0.274, loss: 2402.306152\n",
      "Train: step: 110410, time: 0.254, loss: 1519.106445\n",
      "Train: step: 110420, time: 0.224, loss: 3386.394775\n",
      "Train: step: 110430, time: 0.288, loss: 3332.860107\n",
      "Train: step: 110440, time: 0.252, loss: 756.478271\n",
      "Train: step: 110450, time: 0.241, loss: 418.826935\n",
      "Train: step: 110460, time: 0.253, loss: 2554.529785\n",
      "Train: step: 110470, time: 0.251, loss: 3237.022217\n",
      "Train: step: 110480, time: 0.244, loss: 1236.461792\n",
      "Train: step: 110490, time: 0.218, loss: 1750.370239\n",
      "Train: step: 110500, time: 0.241, loss: 3375.660645\n",
      "Train: step: 110510, time: 0.251, loss: 766.528992\n",
      "Train: step: 110520, time: 0.227, loss: 2549.975342\n",
      "Train: step: 110530, time: 0.228, loss: 2152.410889\n",
      "Train: step: 110540, time: 0.228, loss: 188.633865\n",
      "Train: step: 110550, time: 0.227, loss: 2765.521240\n",
      "Train: step: 110560, time: 0.230, loss: 2436.134521\n",
      "Train: step: 110570, time: 0.267, loss: 781.218445\n",
      "Train: step: 110580, time: 0.273, loss: 1484.675415\n",
      "Train: step: 110590, time: 0.257, loss: 1985.680054\n",
      "Train: step: 110600, time: 0.264, loss: 1183.881958\n",
      "Train: step: 110610, time: 0.235, loss: 1917.007202\n",
      "Train: step: 110620, time: 0.263, loss: 1764.274536\n",
      "Train: step: 110630, time: 0.263, loss: 2032.461060\n",
      "Train: step: 110640, time: 0.310, loss: 635.546692\n",
      "Train: step: 110650, time: 0.250, loss: 2561.727295\n",
      "Train: step: 110660, time: 0.239, loss: 2737.769531\n",
      "Train: step: 110670, time: 0.245, loss: 2318.976318\n",
      "Train: step: 110680, time: 0.253, loss: 897.171021\n",
      "Train: step: 110690, time: 0.251, loss: 2601.835938\n",
      "Train: step: 110700, time: 0.237, loss: 3718.714355\n",
      "Train: step: 110710, time: 0.229, loss: 1128.734253\n",
      "Train: step: 110720, time: 0.224, loss: 448.081848\n",
      "Train: step: 110730, time: 0.248, loss: 1754.431030\n",
      "Train: step: 110740, time: 0.243, loss: 2154.752197\n",
      "Train: step: 110750, time: 0.267, loss: 2727.273193\n",
      "Train: step: 110760, time: 0.221, loss: 2726.145020\n",
      "Train: step: 110770, time: 0.229, loss: 639.672485\n",
      "Train: step: 110780, time: 0.243, loss: 1206.314087\n",
      "Train: step: 110790, time: 0.219, loss: 2181.881836\n",
      "Train: step: 110800, time: 0.249, loss: 1300.210571\n",
      "Train: step: 110810, time: 0.217, loss: 2759.123047\n",
      "Train: step: 110820, time: 0.229, loss: 2337.131348\n",
      "Train: step: 110830, time: 0.253, loss: 2325.930908\n",
      "Train: step: 110840, time: 0.258, loss: 1378.503662\n",
      "Train: step: 110850, time: 0.284, loss: 573.485901\n",
      "Train: step: 110860, time: 0.266, loss: 3717.054199\n",
      "Train: step: 110870, time: 0.225, loss: 1894.832031\n",
      "Train: step: 110880, time: 0.261, loss: 3282.879639\n",
      "Train: step: 110890, time: 0.219, loss: 2701.687012\n",
      "Train: step: 110900, time: 0.238, loss: 595.028687\n",
      "Train: step: 110910, time: 0.246, loss: 1810.269897\n",
      "Train: step: 110920, time: 0.224, loss: 2150.368896\n",
      "Train: step: 110930, time: 0.224, loss: 3048.722656\n",
      "Train: step: 110940, time: 0.268, loss: 1545.763306\n",
      "Train: step: 110950, time: 0.227, loss: 2504.979492\n",
      "Train: step: 110960, time: 0.223, loss: 2765.002930\n",
      "Train: step: 110970, time: 0.231, loss: 749.172485\n",
      "Train: step: 110980, time: 0.234, loss: 1162.979126\n",
      "Train: step: 110990, time: 0.234, loss: 2324.494873\n",
      "Train: step: 111000, time: 0.265, loss: 4295.274902\n",
      "Train: step: 111010, time: 0.249, loss: 1336.500244\n",
      "Train: step: 111020, time: 0.255, loss: 1255.957397\n",
      "Train: step: 111030, time: 0.272, loss: 1676.989014\n",
      "Train: step: 111040, time: 0.228, loss: 243.788086\n",
      "Train: step: 111050, time: 0.222, loss: 778.711731\n",
      "Train: step: 111060, time: 0.219, loss: 582.633667\n",
      "Train: step: 111070, time: 0.225, loss: 571.005249\n",
      "Train: step: 111080, time: 0.224, loss: 1290.809448\n",
      "Train: step: 111090, time: 0.267, loss: 2926.260010\n",
      "Train: step: 111100, time: 0.304, loss: 1462.768066\n",
      "Train: step: 111110, time: 0.225, loss: 969.836487\n",
      "Train: step: 111120, time: 0.221, loss: 518.755554\n",
      "Train: step: 111130, time: 0.231, loss: 1938.796143\n",
      "Train: step: 111140, time: 0.281, loss: 3528.842529\n",
      "Train: step: 111150, time: 0.226, loss: 2828.232422\n",
      "Train: step: 111160, time: 0.247, loss: 1121.402710\n",
      "Train: step: 111170, time: 0.226, loss: 1122.745972\n",
      "Train: step: 111180, time: 0.277, loss: 3152.515625\n",
      "Train: step: 111190, time: 0.226, loss: 2433.331787\n",
      "Train: step: 111200, time: 0.266, loss: 1375.266235\n",
      "Train: step: 111210, time: 0.247, loss: 1634.132935\n",
      "Train: step: 111220, time: 0.266, loss: 2252.101807\n",
      "Train: step: 111230, time: 0.249, loss: 498.138000\n",
      "Train: step: 111240, time: 0.225, loss: 2105.696045\n",
      "Train: step: 111250, time: 0.243, loss: 2860.672119\n",
      "Train: step: 111260, time: 0.228, loss: 699.295837\n",
      "Train: step: 111270, time: 0.234, loss: 2326.856201\n",
      "Train: step: 111280, time: 0.234, loss: 2449.098877\n",
      "Train: step: 111290, time: 0.236, loss: 2144.830078\n",
      "Train: step: 111300, time: 0.280, loss: 743.666870\n",
      "Train: step: 111310, time: 0.253, loss: 2221.493408\n",
      "Train: step: 111320, time: 0.246, loss: 3169.500732\n",
      "Train: step: 111330, time: 0.226, loss: 1917.567017\n",
      "Train: step: 111340, time: 0.238, loss: 2521.130127\n",
      "Train: step: 111350, time: 0.226, loss: 1823.123657\n",
      "Train: step: 111360, time: 0.242, loss: 1618.408569\n",
      "Train: step: 111370, time: 0.228, loss: 1698.419312\n",
      "Train: step: 111380, time: 0.245, loss: 1670.225952\n",
      "Train: step: 111390, time: 0.230, loss: 1006.446472\n",
      "Train: step: 111400, time: 0.246, loss: 2083.636230\n",
      "Train: step: 111410, time: 0.230, loss: 1460.546143\n",
      "Train: step: 111420, time: 0.252, loss: 2093.090088\n",
      "Train: step: 111430, time: 0.230, loss: 2188.448975\n",
      "Train: step: 111440, time: 0.239, loss: 2721.384277\n",
      "Train: step: 111450, time: 0.245, loss: 1154.792114\n",
      "Train: step: 111460, time: 0.223, loss: 1846.400635\n",
      "Train: step: 111470, time: 0.228, loss: 1158.593994\n",
      "Train: step: 111480, time: 0.226, loss: 3122.256836\n",
      "Train: step: 111490, time: 0.279, loss: 2412.801270\n",
      "Train: step: 111500, time: 0.251, loss: 834.996704\n",
      "Train: step: 111510, time: 0.262, loss: 2907.617920\n",
      "Train: step: 111520, time: 0.242, loss: 281.566040\n",
      "Train: step: 111530, time: 0.238, loss: 1814.697876\n",
      "Train: step: 111540, time: 0.291, loss: 995.467285\n",
      "Train: step: 111550, time: 0.268, loss: 2573.981201\n",
      "Train: step: 111560, time: 0.230, loss: 972.310669\n",
      "Train: step: 111570, time: 0.231, loss: 2662.842529\n",
      "Train: step: 111580, time: 0.243, loss: 2345.406250\n",
      "Train: step: 111590, time: 0.221, loss: 936.660828\n",
      "Train: step: 111600, time: 0.251, loss: 2730.033691\n",
      "Train: step: 111610, time: 0.220, loss: 3029.792725\n",
      "Train: step: 111620, time: 0.240, loss: 3752.976318\n",
      "Train: step: 111630, time: 0.261, loss: 3129.064209\n",
      "Train: step: 111640, time: 0.226, loss: 1866.115845\n",
      "Train: step: 111650, time: 0.239, loss: 1504.725098\n",
      "Train: step: 111660, time: 0.224, loss: 1731.838379\n",
      "Train: step: 111670, time: 0.270, loss: 2212.628174\n",
      "Train: step: 111680, time: 0.239, loss: 3225.936523\n",
      "Train: step: 111690, time: 0.227, loss: 2103.685547\n",
      "Train: step: 111700, time: 0.231, loss: 1585.992432\n",
      "Train: step: 111710, time: 0.224, loss: 905.819641\n",
      "Train: step: 111720, time: 0.238, loss: 3267.474121\n",
      "Train: step: 111730, time: 0.280, loss: 1769.771973\n",
      "Train: step: 111740, time: 0.229, loss: 269.330597\n",
      "Train: step: 111750, time: 0.260, loss: 3283.933105\n",
      "Train: step: 111760, time: 0.262, loss: 472.820862\n",
      "Train: step: 111770, time: 0.235, loss: 1828.029907\n",
      "Train: step: 111780, time: 0.236, loss: 1674.123047\n",
      "Train: step: 111790, time: 0.231, loss: 877.380981\n",
      "Train: step: 111800, time: 0.260, loss: 999.062683\n",
      "Train: step: 111810, time: 0.238, loss: 1879.737915\n",
      "Train: step: 111820, time: 0.252, loss: 1456.574463\n",
      "Train: step: 111830, time: 0.247, loss: 3201.956299\n",
      "Train: step: 111840, time: 0.258, loss: 1794.910889\n",
      "Train: step: 111850, time: 0.238, loss: 1459.536377\n",
      "Train: step: 111860, time: 0.234, loss: 2361.162109\n",
      "Train: step: 111870, time: 0.238, loss: 1386.223999\n",
      "Train: step: 111880, time: 0.244, loss: 1523.971313\n",
      "Train: step: 111890, time: 0.248, loss: 2192.231689\n",
      "Train: step: 111900, time: 0.239, loss: 2616.704102\n",
      "Train: step: 111910, time: 0.252, loss: 504.538574\n",
      "Train: step: 111920, time: 0.253, loss: 3369.661133\n",
      "Train: step: 111930, time: 0.251, loss: 2028.936768\n",
      "Train: step: 111940, time: 0.247, loss: 802.866150\n",
      "Train: step: 111950, time: 0.249, loss: 401.369293\n",
      "Train: step: 111960, time: 0.267, loss: 2007.746704\n",
      "Train: step: 111970, time: 0.247, loss: 2688.165283\n",
      "Train: step: 111980, time: 0.235, loss: 2659.611328\n",
      "Train: step: 111990, time: 0.249, loss: 2345.892090\n",
      "Train: step: 112000, time: 0.237, loss: 1798.492676\n",
      "Train: step: 112010, time: 0.248, loss: 2302.206543\n",
      "Train: step: 112020, time: 0.229, loss: 913.350952\n",
      "Train: step: 112030, time: 0.250, loss: 2147.116943\n",
      "Train: step: 112040, time: 0.263, loss: 1850.031738\n",
      "Train: step: 112050, time: 0.229, loss: 2579.664062\n",
      "Train: step: 112060, time: 0.213, loss: 1724.068115\n",
      "Train: step: 112070, time: 0.251, loss: 1829.548584\n",
      "Train: step: 112080, time: 0.227, loss: 1886.549927\n",
      "Train: step: 112090, time: 0.231, loss: 1111.724609\n",
      "Train: step: 112100, time: 0.231, loss: 2681.440918\n",
      "Train: step: 112110, time: 0.224, loss: 1523.885986\n",
      "Train: step: 112120, time: 0.231, loss: 1404.720947\n",
      "Train: step: 112130, time: 0.225, loss: 2591.112793\n",
      "Train: step: 112140, time: 0.267, loss: 639.052979\n",
      "Train: step: 112150, time: 0.249, loss: 2512.184326\n",
      "Train: step: 112160, time: 0.239, loss: 1684.677246\n",
      "Train: step: 112170, time: 0.227, loss: 359.870056\n",
      "Train: step: 112180, time: 0.230, loss: 2804.086426\n",
      "Train: step: 112190, time: 0.263, loss: 745.859436\n",
      "Train: step: 112200, time: 0.218, loss: 3661.631348\n",
      "Train: step: 112210, time: 0.230, loss: 2977.200439\n",
      "Train: step: 112220, time: 0.229, loss: 2745.786865\n",
      "Train: step: 112230, time: 0.216, loss: 2081.296875\n",
      "Train: step: 112240, time: 0.233, loss: 1403.084473\n",
      "Train: step: 112250, time: 0.254, loss: 2017.873901\n",
      "Train: step: 112260, time: 0.297, loss: 2333.852051\n",
      "Train: step: 112270, time: 0.251, loss: 997.250366\n",
      "Train: step: 112280, time: 0.232, loss: 1855.398682\n",
      "Train: step: 112290, time: 0.277, loss: 1424.956909\n",
      "Train: step: 112300, time: 0.226, loss: 3437.098389\n",
      "Train: step: 112310, time: 0.237, loss: 438.582031\n",
      "Train: step: 112320, time: 0.265, loss: 2279.154541\n",
      "Train: step: 112330, time: 0.242, loss: 1393.267822\n",
      "Train: step: 112340, time: 0.249, loss: 2030.671265\n",
      "Train: step: 112350, time: 0.231, loss: 758.721252\n",
      "Train: step: 112360, time: 0.250, loss: 1500.973145\n",
      "Train: step: 112370, time: 0.258, loss: 2977.878174\n",
      "Train: step: 112380, time: 0.263, loss: 130.143875\n",
      "Train: step: 112390, time: 0.273, loss: 3353.732422\n",
      "Train: step: 112400, time: 0.247, loss: 2571.123535\n",
      "Train: step: 112410, time: 0.251, loss: 492.268921\n",
      "Train: step: 112420, time: 0.272, loss: 2745.090332\n",
      "Train: step: 112430, time: 0.239, loss: 1409.713867\n",
      "Train: step: 112440, time: 0.248, loss: 989.505432\n",
      "Train: step: 112450, time: 0.289, loss: 3089.150146\n",
      "Train: step: 112460, time: 0.274, loss: 2079.400391\n",
      "Train: step: 112470, time: 0.230, loss: 562.273743\n",
      "Train: step: 112480, time: 0.261, loss: 2137.426270\n",
      "Train: step: 112490, time: 0.287, loss: 2160.119385\n",
      "Train: step: 112500, time: 0.247, loss: 1947.690430\n",
      "Train: step: 112510, time: 0.229, loss: 1924.531738\n",
      "Train: step: 112520, time: 0.274, loss: 4107.681641\n",
      "Train: step: 112530, time: 0.254, loss: 2192.471680\n",
      "Train: step: 112540, time: 0.232, loss: 1625.508057\n",
      "Train: step: 112550, time: 0.259, loss: 437.176849\n",
      "Train: step: 112560, time: 0.224, loss: 1781.684570\n",
      "Train: step: 112570, time: 0.228, loss: 2371.729492\n",
      "Train: step: 112580, time: 0.271, loss: 270.594299\n",
      "Train: step: 112590, time: 0.233, loss: 1642.986694\n",
      "Train: step: 112600, time: 0.222, loss: 1765.360962\n",
      "Train: step: 112610, time: 0.234, loss: 2673.169189\n",
      "Train: step: 112620, time: 0.257, loss: 1821.262207\n",
      "Train: step: 112630, time: 0.254, loss: 565.954224\n",
      "Train: step: 112640, time: 0.254, loss: 446.774872\n",
      "Train: step: 112650, time: 0.234, loss: 3171.147461\n",
      "Train: step: 112660, time: 0.255, loss: 386.832092\n",
      "Train: step: 112670, time: 0.250, loss: 2116.873047\n",
      "Train: step: 112680, time: 0.221, loss: 3092.144043\n",
      "Train: step: 112690, time: 0.226, loss: 1045.988159\n",
      "Train: step: 112700, time: 0.268, loss: 2835.578369\n",
      "Train: step: 112710, time: 0.225, loss: 885.937683\n",
      "Train: step: 112720, time: 0.228, loss: 2326.618164\n",
      "Train: step: 112730, time: 0.217, loss: 1628.614746\n",
      "Train: step: 112740, time: 0.223, loss: 2076.135742\n",
      "Train: step: 112750, time: 0.224, loss: 824.688965\n",
      "Train: step: 112760, time: 0.220, loss: 4314.667480\n",
      "Train: step: 112770, time: 0.224, loss: 3243.128906\n",
      "Train: step: 112780, time: 0.247, loss: 1985.229614\n",
      "Train: step: 112790, time: 0.218, loss: 2064.878174\n",
      "Train: step: 112800, time: 0.220, loss: 1913.039795\n",
      "Train: step: 112810, time: 0.223, loss: 286.749298\n",
      "Train: step: 112820, time: 0.225, loss: 992.523865\n",
      "Train: step: 112830, time: 0.240, loss: 2575.037842\n",
      "Train: step: 112840, time: 0.247, loss: 1925.263428\n",
      "Train: step: 112850, time: 0.269, loss: 1362.991211\n",
      "Train: step: 112860, time: 0.260, loss: 1664.245728\n",
      "Train: step: 112870, time: 0.257, loss: 2628.145996\n",
      "Train: step: 112880, time: 0.221, loss: 1934.344360\n",
      "Train: step: 112890, time: 0.249, loss: 1475.949707\n",
      "Train: step: 112900, time: 0.218, loss: 990.349365\n",
      "Train: step: 112910, time: 0.239, loss: 3062.385498\n",
      "Train: step: 112920, time: 0.262, loss: 579.282043\n",
      "Train: step: 112930, time: 0.230, loss: 1801.939941\n",
      "Train: step: 112940, time: 0.266, loss: 1573.767944\n",
      "Train: step: 112950, time: 0.243, loss: 2390.971924\n",
      "Train: step: 112960, time: 0.234, loss: 996.793884\n",
      "Train: step: 112970, time: 0.279, loss: 2475.229248\n",
      "Train: step: 112980, time: 0.227, loss: 917.329285\n",
      "Train: step: 112990, time: 0.227, loss: 2191.446533\n",
      "Train: step: 113000, time: 0.223, loss: 1224.119263\n",
      "Train: step: 113010, time: 0.287, loss: 1116.867432\n",
      "Train: step: 113020, time: 0.264, loss: 1495.339478\n",
      "Train: step: 113030, time: 0.223, loss: 523.593933\n",
      "Train: step: 113040, time: 0.287, loss: 1927.567017\n",
      "Train: step: 113050, time: 0.231, loss: 3616.400146\n",
      "Train: step: 113060, time: 0.262, loss: 1713.705322\n",
      "Train: step: 113070, time: 0.227, loss: 1940.469360\n",
      "Train: step: 113080, time: 0.267, loss: 2529.790283\n",
      "Train: step: 113090, time: 0.233, loss: 1864.652222\n",
      "Train: step: 113100, time: 0.251, loss: 1699.388550\n",
      "Train: step: 113110, time: 0.220, loss: 876.443176\n",
      "Train: step: 113120, time: 0.229, loss: 1580.523438\n",
      "Train: step: 113130, time: 0.248, loss: 1584.577148\n",
      "Train: step: 113140, time: 0.231, loss: 1428.429199\n",
      "Train: step: 113150, time: 0.239, loss: 673.633850\n",
      "Train: step: 113160, time: 0.218, loss: 2163.920898\n",
      "Train: step: 113170, time: 0.217, loss: 3691.692139\n",
      "Train: step: 113180, time: 0.264, loss: 1089.017822\n",
      "Train: step: 113190, time: 0.252, loss: 973.157104\n",
      "Train: step: 113200, time: 0.257, loss: 829.661072\n",
      "Train: step: 113210, time: 0.213, loss: 469.629242\n",
      "Train: step: 113220, time: 0.265, loss: 2651.788818\n",
      "Train: step: 113230, time: 0.250, loss: 2081.512207\n",
      "Train: step: 113240, time: 0.258, loss: 1668.289062\n",
      "Train: step: 113250, time: 0.222, loss: 1695.683472\n",
      "Train: step: 113260, time: 0.233, loss: 1704.973633\n",
      "Train: step: 113270, time: 0.230, loss: 633.618469\n",
      "Train: step: 113280, time: 0.233, loss: 2548.025391\n",
      "Train: step: 113290, time: 0.223, loss: 2025.095825\n",
      "Train: step: 113300, time: 0.225, loss: 3352.926758\n",
      "Train: step: 113310, time: 0.230, loss: 3842.145020\n",
      "Train: step: 113320, time: 0.264, loss: 456.017517\n",
      "Train: step: 113330, time: 0.224, loss: 1058.447388\n",
      "Train: step: 113340, time: 0.240, loss: 554.821228\n",
      "Train: step: 113350, time: 0.262, loss: 2396.911865\n",
      "Train: step: 113360, time: 0.228, loss: 2226.825928\n",
      "Train: step: 113370, time: 0.262, loss: 2131.872314\n",
      "Train: step: 113380, time: 0.241, loss: 1821.194336\n",
      "Train: step: 113390, time: 0.251, loss: 1674.848022\n",
      "Train: step: 113400, time: 0.218, loss: 830.660217\n",
      "Train: step: 113410, time: 0.253, loss: 2074.963623\n",
      "Train: step: 113420, time: 0.226, loss: 3628.954102\n",
      "Train: step: 113430, time: 0.239, loss: 678.334106\n",
      "Train: step: 113440, time: 0.220, loss: 1113.415649\n",
      "Train: step: 113450, time: 0.225, loss: 2505.793457\n",
      "Train: step: 113460, time: 0.273, loss: 208.679413\n",
      "Train: step: 113470, time: 0.260, loss: 1978.012207\n",
      "Train: step: 113480, time: 0.226, loss: 880.092896\n",
      "Train: step: 113490, time: 0.248, loss: 2490.707031\n",
      "Train: step: 113500, time: 0.233, loss: 1223.948120\n",
      "Train: step: 113510, time: 0.250, loss: 507.044769\n",
      "Train: step: 113520, time: 0.286, loss: 1605.069580\n",
      "Train: step: 113530, time: 0.257, loss: 470.565033\n",
      "Train: step: 113540, time: 0.258, loss: 5168.979004\n",
      "Train: step: 113550, time: 0.273, loss: 4858.015137\n",
      "Train: step: 113560, time: 0.252, loss: 1066.129639\n",
      "Train: step: 113570, time: 0.238, loss: 2345.669678\n",
      "Train: step: 113580, time: 0.249, loss: 2043.179932\n",
      "Train: step: 113590, time: 0.227, loss: 2420.020508\n",
      "Train: step: 113600, time: 0.218, loss: 2174.875732\n",
      "Train: step: 113610, time: 0.235, loss: 265.289307\n",
      "Train: step: 113620, time: 0.246, loss: 1183.019531\n",
      "Train: step: 113630, time: 0.231, loss: 1419.087524\n",
      "Train: step: 113640, time: 0.234, loss: 1432.862671\n",
      "Train: step: 113650, time: 0.237, loss: 1868.489136\n",
      "Train: step: 113660, time: 0.239, loss: 887.160339\n",
      "Train: step: 113670, time: 0.229, loss: 497.468506\n",
      "Train: step: 113680, time: 0.231, loss: 2753.706055\n",
      "Train: step: 113690, time: 0.236, loss: 618.808289\n",
      "Train: step: 113700, time: 0.227, loss: 1826.041748\n",
      "Train: step: 113710, time: 0.243, loss: 3499.511719\n",
      "Train: step: 113720, time: 0.307, loss: 1275.668823\n",
      "Train: step: 113730, time: 0.237, loss: 2920.844727\n",
      "Train: step: 113740, time: 0.246, loss: 804.831238\n",
      "Train: step: 113750, time: 0.254, loss: 2946.249756\n",
      "Train: step: 113760, time: 0.258, loss: 805.928589\n",
      "Train: step: 113770, time: 0.222, loss: 2839.278320\n",
      "Train: step: 113780, time: 0.230, loss: 3182.194336\n",
      "Train: step: 113790, time: 0.254, loss: 2937.007812\n",
      "Train: step: 113800, time: 0.248, loss: 1734.316895\n",
      "Train: step: 113810, time: 0.244, loss: 2346.901367\n",
      "Train: step: 113820, time: 0.237, loss: 3887.156738\n",
      "Train: step: 113830, time: 0.225, loss: 2751.517334\n",
      "Train: step: 113840, time: 0.215, loss: 357.194672\n",
      "Train: step: 113850, time: 0.262, loss: 3026.101318\n",
      "Train: step: 113860, time: 0.258, loss: 1512.783203\n",
      "Train: step: 113870, time: 0.237, loss: 2327.738281\n",
      "Train: step: 113880, time: 0.244, loss: 1167.206421\n",
      "Train: step: 113890, time: 0.264, loss: 1470.718140\n",
      "Train: step: 113900, time: 0.259, loss: 499.347473\n",
      "Train: step: 113910, time: 0.237, loss: 1367.267090\n",
      "Train: step: 113920, time: 0.232, loss: 1314.721313\n",
      "Train: step: 113930, time: 0.252, loss: 2303.261963\n",
      "Train: step: 113940, time: 0.269, loss: 3077.656006\n",
      "Train: step: 113950, time: 0.237, loss: 2013.834473\n",
      "Train: step: 113960, time: 0.221, loss: 2643.393066\n",
      "Train: step: 113970, time: 0.249, loss: 1624.014038\n",
      "Train: step: 113980, time: 0.223, loss: 1481.766357\n",
      "Train: step: 113990, time: 0.223, loss: 1495.978516\n",
      "Train: step: 114000, time: 0.223, loss: 1698.327759\n",
      "Train: step: 114010, time: 0.238, loss: 1934.079712\n",
      "Train: step: 114020, time: 0.238, loss: 866.971008\n",
      "Train: step: 114030, time: 0.229, loss: 1586.018555\n",
      "Train: step: 114040, time: 0.240, loss: 1963.686401\n",
      "Train: step: 114050, time: 0.246, loss: 1194.151001\n",
      "Train: step: 114060, time: 0.272, loss: 2243.327148\n",
      "Train: step: 114070, time: 0.261, loss: 1512.531982\n",
      "Train: step: 114080, time: 0.262, loss: 716.148743\n",
      "Train: step: 114090, time: 0.226, loss: 2066.808350\n",
      "Train: step: 114100, time: 0.220, loss: 2541.625000\n",
      "Train: step: 114110, time: 0.251, loss: 1720.999512\n",
      "Train: step: 114120, time: 0.249, loss: 777.100830\n",
      "Train: step: 114130, time: 0.229, loss: 1534.490723\n",
      "Train: step: 114140, time: 0.225, loss: 647.089355\n",
      "Train: step: 114150, time: 0.245, loss: 2068.627686\n",
      "Train: step: 114160, time: 0.236, loss: 2600.426270\n",
      "Train: step: 114170, time: 0.251, loss: 745.007751\n",
      "Train: step: 114180, time: 0.228, loss: 3180.113281\n",
      "Train: step: 114190, time: 0.236, loss: 2708.680176\n",
      "Train: step: 114200, time: 0.221, loss: 1045.645020\n",
      "Train: step: 114210, time: 0.232, loss: 1178.419922\n",
      "Train: step: 114220, time: 0.259, loss: 713.128235\n",
      "Train: step: 114230, time: 0.213, loss: 477.667999\n",
      "Train: step: 114240, time: 0.219, loss: 3391.912354\n",
      "Train: step: 114250, time: 0.262, loss: 2307.136475\n",
      "Train: step: 114260, time: 0.240, loss: 902.471924\n",
      "Train: step: 114270, time: 0.235, loss: 268.774872\n",
      "Train: step: 114280, time: 0.280, loss: 517.941223\n",
      "Train: step: 114290, time: 0.243, loss: 241.704987\n",
      "Train: step: 114300, time: 0.247, loss: 2411.753418\n",
      "Train: step: 114310, time: 0.263, loss: 2218.862549\n",
      "Train: step: 114320, time: 0.240, loss: 1884.632324\n",
      "Train: step: 114330, time: 0.248, loss: 3037.970215\n",
      "Train: step: 114340, time: 0.247, loss: 657.130005\n",
      "Train: step: 114350, time: 0.252, loss: 4210.271973\n",
      "Train: step: 114360, time: 0.267, loss: 2690.106934\n",
      "Train: step: 114370, time: 0.230, loss: 2096.439941\n",
      "Train: step: 114380, time: 0.238, loss: 1431.590942\n",
      "Train: step: 114390, time: 0.249, loss: 1650.142212\n",
      "Train: step: 114400, time: 0.235, loss: 1674.885864\n",
      "Train: step: 114410, time: 0.295, loss: 2283.579346\n",
      "Train: step: 114420, time: 0.230, loss: 1833.616577\n",
      "Train: step: 114430, time: 0.259, loss: 2194.125732\n",
      "Train: step: 114440, time: 0.261, loss: 365.188934\n",
      "Train: step: 114450, time: 0.254, loss: 2865.340820\n",
      "Train: step: 114460, time: 0.228, loss: 1631.778931\n",
      "Train: step: 114470, time: 0.278, loss: 1506.456421\n",
      "Train: step: 114480, time: 0.276, loss: 3210.136230\n",
      "Train: step: 114490, time: 0.257, loss: 1246.758545\n",
      "Train: step: 114500, time: 0.248, loss: 1946.185913\n",
      "Train: step: 114510, time: 0.238, loss: 416.309723\n",
      "Train: step: 114520, time: 0.238, loss: 4875.936035\n",
      "Train: step: 114530, time: 0.253, loss: 943.726624\n",
      "Train: step: 114540, time: 0.244, loss: 1766.371094\n",
      "Train: step: 114550, time: 0.235, loss: 1823.619507\n",
      "Train: step: 114560, time: 0.220, loss: 1810.401123\n",
      "Train: step: 114570, time: 0.253, loss: 643.228027\n",
      "Train: step: 114580, time: 0.239, loss: 2734.771240\n",
      "Train: step: 114590, time: 0.267, loss: 207.257462\n",
      "Train: step: 114600, time: 0.236, loss: 3673.654785\n",
      "Train: step: 114610, time: 0.227, loss: 1226.614624\n",
      "Train: step: 114620, time: 0.240, loss: 1109.690186\n",
      "Train: step: 114630, time: 0.238, loss: 992.743408\n",
      "Train: step: 114640, time: 0.237, loss: 2454.787109\n",
      "Train: step: 114650, time: 0.241, loss: 2340.281494\n",
      "Train: step: 114660, time: 0.238, loss: 1860.640503\n",
      "Train: step: 114670, time: 0.238, loss: 3039.565430\n",
      "Train: step: 114680, time: 0.238, loss: 1554.817383\n",
      "Train: step: 114690, time: 0.249, loss: 478.183136\n",
      "Train: step: 114700, time: 0.269, loss: 3480.987061\n",
      "Train: step: 114710, time: 0.242, loss: 1451.535889\n",
      "Train: step: 114720, time: 0.216, loss: 521.950317\n",
      "Train: step: 114730, time: 0.254, loss: 2075.623291\n",
      "Train: step: 114740, time: 0.275, loss: 2442.845215\n",
      "Train: step: 114750, time: 0.227, loss: 2214.980957\n",
      "Train: step: 114760, time: 0.224, loss: 1377.248901\n",
      "Train: step: 114770, time: 0.248, loss: 904.397461\n",
      "Train: step: 114780, time: 0.254, loss: 2571.286377\n",
      "Train: step: 114790, time: 0.236, loss: 1299.250488\n",
      "Train: step: 114800, time: 0.262, loss: 1481.787720\n",
      "Train: step: 114810, time: 0.216, loss: 1147.427979\n",
      "Train: step: 114820, time: 0.237, loss: 3601.687256\n",
      "Train: step: 114830, time: 0.239, loss: 3074.205811\n",
      "Train: step: 114840, time: 0.227, loss: 3373.998291\n",
      "Train: step: 114850, time: 0.227, loss: 1206.042969\n",
      "Train: step: 114860, time: 0.259, loss: 3026.598389\n",
      "Train: step: 114870, time: 0.234, loss: 871.129211\n",
      "Train: step: 114880, time: 0.228, loss: 1048.502197\n",
      "Train: step: 114890, time: 0.231, loss: 2988.850586\n",
      "Train: step: 114900, time: 0.222, loss: 2137.570312\n",
      "Train: step: 114910, time: 0.227, loss: 2010.567017\n",
      "Train: step: 114920, time: 0.217, loss: 2017.813843\n",
      "Train: step: 114930, time: 0.234, loss: 1942.290283\n",
      "Train: step: 114940, time: 0.216, loss: 2793.739502\n",
      "Train: step: 114950, time: 0.223, loss: 613.394714\n",
      "Train: step: 114960, time: 0.268, loss: 1295.568604\n",
      "Train: step: 114970, time: 0.246, loss: 907.347900\n",
      "Train: step: 114980, time: 0.269, loss: 1706.995605\n",
      "Train: step: 114990, time: 0.220, loss: 4340.598633\n",
      "Train: step: 115000, time: 0.263, loss: 1977.822266\n",
      "Train: step: 115010, time: 0.230, loss: 1639.783936\n",
      "Train: step: 115020, time: 0.255, loss: 1978.282593\n",
      "Train: step: 115030, time: 0.266, loss: 1027.719849\n",
      "Train: step: 115040, time: 0.240, loss: 2657.754639\n",
      "Train: step: 115050, time: 0.253, loss: 718.470093\n",
      "Train: step: 115060, time: 0.242, loss: 2047.452637\n",
      "Train: step: 115070, time: 0.229, loss: 1673.060303\n",
      "Train: step: 115080, time: 0.241, loss: 3177.113281\n",
      "Train: step: 115090, time: 0.251, loss: 2330.662598\n",
      "Train: step: 115100, time: 0.234, loss: 1862.945923\n",
      "Train: step: 115110, time: 0.231, loss: 1643.821899\n",
      "Train: step: 115120, time: 0.246, loss: 3279.065430\n",
      "Train: step: 115130, time: 0.231, loss: 2710.819580\n",
      "Train: step: 115140, time: 0.262, loss: 3359.458740\n",
      "Train: step: 115150, time: 0.226, loss: 3306.997314\n",
      "Train: step: 115160, time: 0.215, loss: 2048.661377\n",
      "Train: step: 115170, time: 0.263, loss: 2551.739258\n",
      "Train: step: 115180, time: 0.221, loss: 1928.604614\n",
      "Train: step: 115190, time: 0.263, loss: 2419.819824\n",
      "Train: step: 115200, time: 0.233, loss: 421.101013\n",
      "Train: step: 115210, time: 0.254, loss: 1418.857056\n",
      "Train: step: 115220, time: 0.237, loss: 2284.963135\n",
      "Train: step: 115230, time: 0.250, loss: 2817.768311\n",
      "Train: step: 115240, time: 0.228, loss: 3211.301025\n",
      "Train: step: 115250, time: 0.256, loss: 2023.808716\n",
      "Train: step: 115260, time: 0.236, loss: 3315.977783\n",
      "Train: step: 115270, time: 0.244, loss: 2873.956299\n",
      "Train: step: 115280, time: 0.260, loss: 1070.095093\n",
      "Train: step: 115290, time: 0.255, loss: 1817.269409\n",
      "Train: step: 115300, time: 0.237, loss: 1495.299561\n",
      "Train: step: 115310, time: 0.274, loss: 2250.740479\n",
      "Train: step: 115320, time: 0.250, loss: 161.209229\n",
      "Train: step: 115330, time: 0.264, loss: 1164.576660\n",
      "Train: step: 115340, time: 0.260, loss: 2219.534912\n",
      "Train: step: 115350, time: 0.262, loss: 759.599426\n",
      "Train: step: 115360, time: 0.252, loss: 388.480103\n",
      "Train: step: 115370, time: 0.253, loss: 1261.867310\n",
      "Train: step: 115380, time: 0.238, loss: 2838.256836\n",
      "Train: step: 115390, time: 0.239, loss: 2698.151367\n",
      "Train: step: 115400, time: 0.248, loss: 1960.291382\n",
      "Train: step: 115410, time: 0.255, loss: 2390.413574\n",
      "Train: step: 115420, time: 0.241, loss: 476.252991\n",
      "Train: step: 115430, time: 0.234, loss: 1411.465332\n",
      "Train: step: 115440, time: 0.240, loss: 1272.876831\n",
      "Train: step: 115450, time: 0.245, loss: 1463.734741\n",
      "Train: step: 115460, time: 0.244, loss: 3176.083008\n",
      "Train: step: 115470, time: 0.230, loss: 2976.362549\n",
      "Train: step: 115480, time: 0.218, loss: 2420.915771\n",
      "Train: step: 115490, time: 0.221, loss: 1771.255249\n",
      "Train: step: 115500, time: 0.237, loss: 1067.764038\n",
      "Train: step: 115510, time: 0.230, loss: 343.227020\n",
      "Train: step: 115520, time: 0.261, loss: 1523.423706\n",
      "Train: step: 115530, time: 0.232, loss: 1559.291016\n",
      "Train: step: 115540, time: 0.226, loss: 1613.891846\n",
      "Train: step: 115550, time: 0.283, loss: 2165.367920\n",
      "Train: step: 115560, time: 0.246, loss: 3786.092529\n",
      "Train: step: 115570, time: 0.229, loss: 2170.032471\n",
      "Train: step: 115580, time: 0.286, loss: 1104.138062\n",
      "Train: step: 115590, time: 0.232, loss: 1805.160278\n",
      "Train: step: 115600, time: 0.242, loss: 703.871033\n",
      "Train: step: 115610, time: 0.247, loss: 542.213074\n",
      "Train: step: 115620, time: 0.275, loss: 1038.773560\n",
      "Train: step: 115630, time: 0.223, loss: 2427.210938\n",
      "Train: step: 115640, time: 0.267, loss: 3401.553955\n",
      "Train: step: 115650, time: 0.253, loss: 1189.982422\n",
      "Train: step: 115660, time: 0.236, loss: 915.989319\n",
      "Train: step: 115670, time: 0.225, loss: 1685.309326\n",
      "Train: step: 115680, time: 0.231, loss: 2081.552734\n",
      "Train: step: 115690, time: 0.254, loss: 2377.846680\n",
      "Train: step: 115700, time: 0.238, loss: 823.855530\n",
      "Train: step: 115710, time: 0.277, loss: 712.670654\n",
      "Train: step: 115720, time: 0.221, loss: 1320.776367\n",
      "Train: step: 115730, time: 0.256, loss: 5022.777832\n",
      "Train: step: 115740, time: 0.247, loss: 1146.575562\n",
      "Train: step: 115750, time: 0.254, loss: 1742.799561\n",
      "Train: step: 115760, time: 0.223, loss: 1969.861694\n",
      "Train: step: 115770, time: 0.224, loss: 2263.151123\n",
      "Train: step: 115780, time: 0.263, loss: 782.304626\n",
      "Train: step: 115790, time: 0.231, loss: 2326.481445\n",
      "Train: step: 115800, time: 0.273, loss: 872.219543\n",
      "Train: step: 115810, time: 0.260, loss: 2067.292480\n",
      "Train: step: 115820, time: 0.266, loss: 1233.053833\n",
      "Train: step: 115830, time: 0.252, loss: 2227.862305\n",
      "Train: step: 115840, time: 0.298, loss: 1567.060425\n",
      "Train: step: 115850, time: 0.251, loss: 2166.502197\n",
      "Train: step: 115860, time: 0.253, loss: 944.325867\n",
      "Train: step: 115870, time: 0.239, loss: 326.158905\n",
      "Train: step: 115880, time: 0.263, loss: 1395.940796\n",
      "Train: step: 115890, time: 0.245, loss: 2041.384888\n",
      "Train: step: 115900, time: 0.251, loss: 784.439758\n",
      "Train: step: 115910, time: 0.272, loss: 1232.505615\n",
      "Train: step: 115920, time: 0.261, loss: 3280.836426\n",
      "Train: step: 115930, time: 0.277, loss: 2223.331299\n",
      "Train: step: 115940, time: 0.226, loss: 1898.817505\n",
      "Train: step: 115950, time: 0.253, loss: 1215.057373\n",
      "Train: step: 115960, time: 0.237, loss: 1669.148682\n",
      "Train: step: 115970, time: 0.241, loss: 2538.111572\n",
      "Train: step: 115980, time: 0.228, loss: 2652.916016\n",
      "Train: step: 115990, time: 0.228, loss: 1647.307739\n",
      "Train: step: 116000, time: 0.245, loss: 2209.545410\n",
      "Train: step: 116010, time: 0.226, loss: 2259.300049\n",
      "Train: step: 116020, time: 0.254, loss: 340.500885\n",
      "Train: step: 116030, time: 0.253, loss: 2225.775391\n",
      "Train: step: 116040, time: 0.231, loss: 2831.339355\n",
      "Train: step: 116050, time: 0.235, loss: 802.234619\n",
      "Train: step: 116060, time: 0.236, loss: 2164.364502\n",
      "Train: step: 116070, time: 0.252, loss: 1692.440674\n",
      "Train: step: 116080, time: 0.250, loss: 2806.912842\n",
      "Train: step: 116090, time: 0.228, loss: 921.159485\n",
      "Train: step: 116100, time: 0.255, loss: 1753.313721\n",
      "Train: step: 116110, time: 0.231, loss: 1555.474609\n",
      "Train: step: 116120, time: 0.265, loss: 2359.563232\n",
      "Train: step: 116130, time: 0.238, loss: 289.518250\n",
      "Train: step: 116140, time: 0.232, loss: 2916.737061\n",
      "Train: step: 116150, time: 0.220, loss: 2451.740967\n",
      "Train: step: 116160, time: 0.221, loss: 970.841614\n",
      "Train: step: 116170, time: 0.261, loss: 542.084106\n",
      "Train: step: 116180, time: 0.222, loss: 1617.636963\n",
      "Train: step: 116190, time: 0.234, loss: 2243.263672\n",
      "Train: step: 116200, time: 0.222, loss: 361.107391\n",
      "Train: step: 116210, time: 0.263, loss: 2653.608887\n",
      "Train: step: 116220, time: 0.258, loss: 1281.875000\n",
      "Train: step: 116230, time: 0.263, loss: 2632.429932\n",
      "Train: step: 116240, time: 0.220, loss: 1981.673706\n",
      "Train: step: 116250, time: 0.230, loss: 921.648438\n",
      "Train: step: 116260, time: 0.246, loss: 1506.062622\n",
      "Train: step: 116270, time: 0.271, loss: 2139.295654\n",
      "Train: step: 116280, time: 0.232, loss: 2199.354248\n",
      "Train: step: 116290, time: 0.228, loss: 594.244629\n",
      "Train: step: 116300, time: 0.226, loss: 456.764404\n",
      "Train: step: 116310, time: 0.231, loss: 2172.050293\n",
      "Train: step: 116320, time: 0.223, loss: 2527.259766\n",
      "Train: step: 116330, time: 0.228, loss: 2789.528809\n",
      "Train: step: 116340, time: 0.223, loss: 2357.802490\n",
      "Train: step: 116350, time: 0.262, loss: 3929.202637\n",
      "Train: step: 116360, time: 0.277, loss: 3227.716064\n",
      "Train: step: 116370, time: 0.223, loss: 2589.311768\n",
      "Train: step: 116380, time: 0.252, loss: 977.145081\n",
      "Train: step: 116390, time: 0.225, loss: 1721.294434\n",
      "Train: step: 116400, time: 0.235, loss: 689.356628\n",
      "Train: step: 116410, time: 0.225, loss: 753.847473\n",
      "Train: step: 116420, time: 0.232, loss: 2382.854004\n",
      "Train: step: 116430, time: 0.246, loss: 2125.685059\n",
      "Train: step: 116440, time: 0.230, loss: 1611.007690\n",
      "Train: step: 116450, time: 0.231, loss: 971.222717\n",
      "Train: step: 116460, time: 0.234, loss: 2985.113525\n",
      "Train: step: 116470, time: 0.229, loss: 2018.639038\n",
      "Train: step: 116480, time: 0.242, loss: 1842.838257\n",
      "Train: step: 116490, time: 0.228, loss: 1020.111572\n",
      "Train: step: 116500, time: 0.229, loss: 2111.700928\n",
      "Train: step: 116510, time: 0.228, loss: 6098.873535\n",
      "Train: step: 116520, time: 0.225, loss: 1462.290527\n",
      "Train: step: 116530, time: 0.234, loss: 1370.710693\n",
      "Train: step: 116540, time: 0.246, loss: 2121.191162\n",
      "Train: step: 116550, time: 0.228, loss: 2132.173340\n",
      "Train: step: 116560, time: 0.225, loss: 311.037140\n",
      "Train: step: 116570, time: 0.395, loss: 840.606689\n",
      "Train: step: 116580, time: 0.253, loss: 1668.821045\n",
      "Train: step: 116590, time: 0.258, loss: 582.142273\n",
      "Train: step: 116600, time: 0.226, loss: 1404.295776\n",
      "Train: step: 116610, time: 0.249, loss: 1169.592285\n",
      "Train: step: 116620, time: 0.238, loss: 997.827454\n",
      "Train: step: 116630, time: 0.229, loss: 3587.166748\n",
      "Train: step: 116640, time: 0.234, loss: 2146.653076\n",
      "Train: step: 116650, time: 0.222, loss: 1931.324585\n",
      "Train: step: 116660, time: 0.240, loss: 1771.442993\n",
      "Train: step: 116670, time: 0.255, loss: 1096.837158\n",
      "Train: step: 116680, time: 0.221, loss: 1129.897095\n",
      "Train: step: 116690, time: 0.227, loss: 1495.855957\n",
      "Train: step: 116700, time: 0.224, loss: 2166.070557\n",
      "Train: step: 116710, time: 0.253, loss: 2653.344482\n",
      "Train: step: 116720, time: 0.219, loss: 2601.709229\n",
      "Train: step: 116730, time: 0.222, loss: 2833.680420\n",
      "Train: step: 116740, time: 0.230, loss: 2391.587646\n",
      "Train: step: 116750, time: 0.226, loss: 1568.712158\n",
      "Train: step: 116760, time: 0.223, loss: 1860.776978\n",
      "Train: step: 116770, time: 0.228, loss: 4783.461426\n",
      "Train: step: 116780, time: 0.250, loss: 1866.146118\n",
      "Train: step: 116790, time: 0.237, loss: 424.377167\n",
      "Train: step: 116800, time: 0.222, loss: 2699.730713\n",
      "Train: step: 116810, time: 0.233, loss: 2370.192871\n",
      "Train: step: 116820, time: 0.260, loss: 2019.084839\n",
      "Train: step: 116830, time: 0.258, loss: 1470.237305\n",
      "Train: step: 116840, time: 0.250, loss: 1811.794189\n",
      "Train: step: 116850, time: 0.241, loss: 304.636322\n",
      "Train: step: 116860, time: 0.262, loss: 1652.926758\n",
      "Train: step: 116870, time: 0.278, loss: 1266.707275\n",
      "Train: step: 116880, time: 0.250, loss: 3693.638916\n",
      "Train: step: 116890, time: 0.243, loss: 1593.470581\n",
      "Train: step: 116900, time: 0.246, loss: 1027.517090\n",
      "Train: step: 116910, time: 0.252, loss: 2188.819824\n",
      "Train: step: 116920, time: 0.253, loss: 864.160522\n",
      "Train: step: 116930, time: 0.243, loss: 590.976440\n",
      "Train: step: 116940, time: 0.236, loss: 452.348694\n",
      "Train: step: 116950, time: 0.236, loss: 2677.653809\n",
      "Train: step: 116960, time: 0.261, loss: 1808.100098\n",
      "Train: step: 116970, time: 0.251, loss: 1218.186035\n",
      "Train: step: 116980, time: 0.254, loss: 2328.762695\n",
      "Train: step: 116990, time: 0.236, loss: 1374.541870\n",
      "Train: step: 117000, time: 0.256, loss: 2043.578125\n",
      "Train: step: 117010, time: 0.227, loss: 2567.785889\n",
      "Train: step: 117020, time: 0.253, loss: 2011.122559\n",
      "Train: step: 117030, time: 0.255, loss: 1979.124390\n",
      "Train: step: 117040, time: 0.247, loss: 2534.734375\n",
      "Train: step: 117050, time: 0.260, loss: 3094.297119\n",
      "Train: step: 117060, time: 0.226, loss: 2384.335693\n",
      "Train: step: 117070, time: 0.248, loss: 3064.989502\n",
      "Train: step: 117080, time: 0.237, loss: 1070.107422\n",
      "Train: step: 117090, time: 0.227, loss: 3008.995361\n",
      "Train: step: 117100, time: 0.225, loss: 3118.181885\n",
      "Train: step: 117110, time: 0.253, loss: 3154.859619\n",
      "Train: step: 117120, time: 0.228, loss: 1767.386230\n",
      "Train: step: 117130, time: 0.234, loss: 524.508118\n",
      "Train: step: 117140, time: 0.235, loss: 237.207428\n",
      "Train: step: 117150, time: 0.226, loss: 2656.410645\n",
      "Train: step: 117160, time: 0.218, loss: 3472.371338\n",
      "Train: step: 117170, time: 0.225, loss: 3347.197510\n",
      "Train: step: 117180, time: 0.252, loss: 824.848938\n",
      "Train: step: 117190, time: 0.276, loss: 917.814636\n",
      "Train: step: 117200, time: 0.221, loss: 2654.085449\n",
      "Train: step: 117210, time: 0.256, loss: 4478.083984\n",
      "Train: step: 117220, time: 0.275, loss: 2416.451416\n",
      "Train: step: 117230, time: 0.230, loss: 2560.898193\n",
      "Train: step: 117240, time: 0.269, loss: 1020.965820\n",
      "Train: step: 117250, time: 0.222, loss: 1682.492920\n",
      "Train: step: 117260, time: 0.224, loss: 1900.085205\n",
      "Train: step: 117270, time: 0.224, loss: 2421.923096\n",
      "Train: step: 117280, time: 0.264, loss: 1536.617310\n",
      "Train: step: 117290, time: 0.252, loss: 3058.339600\n",
      "Train: step: 117300, time: 0.226, loss: 2079.829102\n",
      "Train: step: 117310, time: 0.239, loss: 2670.615723\n",
      "Train: step: 117320, time: 0.253, loss: 2192.763672\n",
      "Train: step: 117330, time: 0.254, loss: 329.392883\n",
      "Train: step: 117340, time: 0.224, loss: 2191.669434\n",
      "Train: step: 117350, time: 0.227, loss: 2979.172607\n",
      "Train: step: 117360, time: 0.227, loss: 242.695816\n",
      "Train: step: 117370, time: 0.241, loss: 3503.796143\n",
      "Train: step: 117380, time: 0.256, loss: 3223.970703\n",
      "Train: step: 117390, time: 0.247, loss: 2592.080322\n",
      "Train: step: 117400, time: 0.231, loss: 2833.399170\n",
      "Train: step: 117410, time: 0.263, loss: 1555.831421\n",
      "Train: step: 117420, time: 0.230, loss: 969.062500\n",
      "Train: step: 117430, time: 0.272, loss: 3800.779297\n",
      "Train: step: 117440, time: 0.215, loss: 3657.594482\n",
      "Train: step: 117450, time: 0.237, loss: 1122.181396\n",
      "Train: step: 117460, time: 0.227, loss: 757.256836\n",
      "Train: step: 117470, time: 0.234, loss: 2566.392334\n",
      "Train: step: 117480, time: 0.256, loss: 1489.983398\n",
      "Train: step: 117490, time: 0.265, loss: 2141.967773\n",
      "Train: step: 117500, time: 0.229, loss: 2981.174561\n",
      "Train: step: 117510, time: 0.257, loss: 1107.104248\n",
      "Train: step: 117520, time: 0.218, loss: 1534.367065\n",
      "Train: step: 117530, time: 0.232, loss: 3098.032471\n",
      "Train: step: 117540, time: 0.233, loss: 1702.313599\n",
      "Train: step: 117550, time: 0.266, loss: 939.802551\n",
      "Train: step: 117560, time: 0.252, loss: 2015.201538\n",
      "Train: step: 117570, time: 0.228, loss: 2958.480469\n",
      "Train: step: 117580, time: 0.225, loss: 1187.893677\n",
      "Train: step: 117590, time: 0.265, loss: 1994.435913\n",
      "Train: step: 117600, time: 0.225, loss: 3852.723877\n",
      "Train: step: 117610, time: 0.253, loss: 2504.359863\n",
      "Train: step: 117620, time: 0.226, loss: 1568.590454\n",
      "Train: step: 117630, time: 0.237, loss: 1058.967285\n",
      "Train: step: 117640, time: 0.248, loss: 1201.621094\n",
      "Train: step: 117650, time: 0.216, loss: 1900.277588\n",
      "Train: step: 117660, time: 0.242, loss: 1648.520752\n",
      "Train: step: 117670, time: 0.242, loss: 967.897156\n",
      "Train: step: 117680, time: 0.262, loss: 2403.987061\n",
      "Train: step: 117690, time: 0.235, loss: 754.243835\n",
      "Train: step: 117700, time: 0.233, loss: 1601.107788\n",
      "Train: step: 117710, time: 0.230, loss: 2343.016846\n",
      "Train: step: 117720, time: 0.223, loss: 2643.839355\n",
      "Train: step: 117730, time: 0.254, loss: 2268.297852\n",
      "Train: step: 117740, time: 0.227, loss: 3293.356934\n",
      "Train: step: 117750, time: 0.267, loss: 620.713135\n",
      "Train: step: 117760, time: 0.248, loss: 1863.511719\n",
      "Train: step: 117770, time: 0.248, loss: 1390.784302\n",
      "Train: step: 117780, time: 0.260, loss: 1280.984985\n",
      "Train: step: 117790, time: 0.240, loss: 3019.852295\n",
      "Train: step: 117800, time: 0.242, loss: 1658.585938\n",
      "Train: step: 117810, time: 0.246, loss: 2071.288818\n",
      "Train: step: 117820, time: 0.255, loss: 3118.417236\n",
      "Train: step: 117830, time: 0.252, loss: 2103.303711\n",
      "Train: step: 117840, time: 0.246, loss: 624.579407\n",
      "Train: step: 117850, time: 0.258, loss: 2688.115967\n",
      "Train: step: 117860, time: 0.236, loss: 2089.990234\n",
      "Train: step: 117870, time: 0.230, loss: 1160.601562\n",
      "Train: step: 117880, time: 0.233, loss: 231.412369\n",
      "Train: step: 117890, time: 0.230, loss: 1208.438843\n",
      "Train: step: 117900, time: 0.232, loss: 2329.029541\n",
      "Train: step: 117910, time: 0.229, loss: 1156.727173\n",
      "Train: step: 117920, time: 0.251, loss: 1238.840088\n",
      "Train: step: 117930, time: 0.218, loss: 1267.288452\n",
      "Train: step: 117940, time: 0.220, loss: 3227.945557\n",
      "Train: step: 117950, time: 0.277, loss: 2248.298584\n",
      "Train: step: 117960, time: 0.319, loss: 2365.439697\n",
      "Train: step: 117970, time: 0.279, loss: 2855.261719\n",
      "Train: step: 117980, time: 0.227, loss: 1868.604736\n",
      "Train: step: 117990, time: 0.235, loss: 2384.026611\n",
      "Train: step: 118000, time: 0.240, loss: 2172.277832\n",
      "Train: step: 118010, time: 0.222, loss: 1586.268188\n",
      "Train: step: 118020, time: 0.233, loss: 2355.573975\n",
      "Train: step: 118030, time: 0.267, loss: 1488.478760\n",
      "Train: step: 118040, time: 0.226, loss: 2559.593750\n",
      "Train: step: 118050, time: 0.298, loss: 2525.012451\n",
      "Train: step: 118060, time: 0.257, loss: 645.437866\n",
      "Train: step: 118070, time: 0.249, loss: 1438.154297\n",
      "Train: step: 118080, time: 0.242, loss: 1602.764038\n",
      "Train: step: 118090, time: 0.218, loss: 2132.909912\n",
      "Train: step: 118100, time: 0.224, loss: 2734.415527\n",
      "Train: step: 118110, time: 0.221, loss: 1234.985229\n",
      "Train: step: 118120, time: 0.260, loss: 1627.527344\n",
      "Train: step: 118130, time: 0.237, loss: 591.936035\n",
      "Train: step: 118140, time: 0.256, loss: 1211.992188\n",
      "Train: step: 118150, time: 0.215, loss: 879.303284\n",
      "Train: step: 118160, time: 0.230, loss: 1672.797241\n",
      "Train: step: 118170, time: 0.215, loss: 1445.654419\n",
      "Train: step: 118180, time: 0.253, loss: 993.320496\n",
      "Train: step: 118190, time: 0.223, loss: 1494.306885\n",
      "Train: step: 118200, time: 0.251, loss: 2370.183105\n",
      "Train: step: 118210, time: 0.251, loss: 1775.460449\n",
      "Train: step: 118220, time: 0.224, loss: 2155.652832\n",
      "Train: step: 118230, time: 0.252, loss: 2273.449707\n",
      "Train: step: 118240, time: 0.265, loss: 1317.177979\n",
      "Train: step: 118250, time: 0.248, loss: 2475.227783\n",
      "Train: step: 118260, time: 0.228, loss: 1847.557251\n",
      "Train: step: 118270, time: 0.227, loss: 2322.882812\n",
      "Train: step: 118280, time: 0.234, loss: 1262.032959\n",
      "Train: step:     10, time: 0.199, loss: 1281.692749\n",
      "Train: step:     20, time: 0.198, loss: 1525.987305\n",
      "Train: step:     30, time: 0.195, loss: 941.885498\n",
      "Train: step:     40, time: 0.198, loss: 2645.331787\n",
      "Train: step:     50, time: 0.190, loss: 2513.538086\n",
      "Train: step:     60, time: 0.198, loss: 1958.890991\n",
      "Train: step:     70, time: 0.202, loss: 2146.335449\n",
      "Train: step:     80, time: 0.238, loss: 1828.100708\n",
      "Train: step:     90, time: 0.232, loss: 3037.564453\n",
      "Train: step:    100, time: 0.224, loss: 2436.972168\n",
      "Train: step:    110, time: 0.191, loss: 1984.638062\n",
      "Train: step:    120, time: 0.188, loss: 1487.485352\n",
      "Train: step:    130, time: 0.192, loss: 2471.739990\n",
      "Train: step:    140, time: 0.200, loss: 1834.715942\n",
      "Train: step:    150, time: 0.221, loss: 2238.541504\n",
      "Train: step:    160, time: 0.215, loss: 2056.012695\n",
      "Train: step:    170, time: 0.194, loss: 978.042542\n",
      "Train: step:    180, time: 0.188, loss: 1241.364868\n",
      "Train: step:    190, time: 0.216, loss: 762.391663\n",
      "Train: step:    200, time: 0.219, loss: 358.425232\n",
      "Train: step:    210, time: 0.193, loss: 501.360046\n",
      "Train: step:    220, time: 0.239, loss: 3310.635986\n",
      "Train: step:    230, time: 0.216, loss: 2000.550537\n",
      "Train: step:    240, time: 0.216, loss: 1791.141235\n",
      "Train: step:    250, time: 0.204, loss: 1128.664795\n",
      "Train: step:    260, time: 0.189, loss: 782.235962\n",
      "Train: step:    270, time: 0.191, loss: 1689.347534\n",
      "Train: step:    280, time: 0.193, loss: 3633.898438\n",
      "Train: step:    290, time: 0.246, loss: 4259.270508\n",
      "Train: step:    300, time: 0.216, loss: 2465.795654\n",
      "Train: step:    310, time: 0.220, loss: 3124.158691\n",
      "Train: step:    320, time: 0.210, loss: 358.201477\n",
      "Train: step:    330, time: 0.194, loss: 566.969666\n",
      "Train: step:    340, time: 0.196, loss: 810.635986\n",
      "Train: step:    350, time: 0.195, loss: 3140.601074\n",
      "Train: step:    360, time: 0.218, loss: 2186.679688\n",
      "Train: step:    370, time: 0.234, loss: 271.929169\n",
      "Train: step:    380, time: 0.192, loss: 2233.445557\n",
      "Train: step:    390, time: 0.196, loss: 2693.022949\n",
      "Train: step:    400, time: 0.208, loss: 1897.708740\n",
      "Train: step:    410, time: 0.201, loss: 1582.884888\n",
      "Train: step:    420, time: 0.218, loss: 2163.016113\n",
      "Train: step:    430, time: 0.217, loss: 1530.054688\n",
      "Train: step:    440, time: 0.230, loss: 1019.371277\n",
      "Train: step:    450, time: 0.228, loss: 1800.120850\n",
      "Train: step:    460, time: 0.224, loss: 1797.249878\n",
      "Train: step:    470, time: 0.208, loss: 3864.971436\n",
      "Train: step:    480, time: 0.223, loss: 2020.690308\n",
      "Train: step:    490, time: 0.205, loss: 835.815979\n",
      "Train: step:    500, time: 0.222, loss: 1067.711914\n",
      "Train: step:    510, time: 0.222, loss: 451.680511\n",
      "Train: step:    520, time: 0.211, loss: 2604.136475\n",
      "Train: step:    530, time: 0.209, loss: 1990.992432\n",
      "Train: step:    540, time: 0.214, loss: 494.891815\n",
      "Train: step:    550, time: 0.216, loss: 1016.757019\n",
      "Train: step:    560, time: 0.211, loss: 1635.362549\n",
      "Train: step:    570, time: 0.220, loss: 335.296600\n",
      "Train: step:    580, time: 0.218, loss: 598.195618\n",
      "Train: step:    590, time: 0.203, loss: 1775.324097\n",
      "Train: step:    600, time: 0.226, loss: 2054.115234\n",
      "Train: step:    610, time: 0.250, loss: 1866.211426\n",
      "Train: step:    620, time: 0.188, loss: 1535.728638\n",
      "Train: step:    630, time: 0.207, loss: 1750.060547\n",
      "Train: step:    640, time: 0.207, loss: 2555.034912\n",
      "Train: step:    650, time: 0.217, loss: 445.709412\n",
      "Train: step:    660, time: 0.243, loss: 1843.561768\n",
      "Train: step:    670, time: 0.212, loss: 1843.997681\n",
      "Train: step:    680, time: 0.195, loss: 2224.411865\n",
      "Train: step:    690, time: 0.220, loss: 1463.392822\n",
      "Train: step:    700, time: 0.214, loss: 2287.843750\n",
      "Train: step:    710, time: 0.218, loss: 852.397888\n",
      "Train: step:    720, time: 0.242, loss: 466.605225\n",
      "Train: step:    730, time: 0.188, loss: 2167.165039\n",
      "Train: step:    740, time: 0.193, loss: 549.564819\n",
      "Train: step:    750, time: 0.217, loss: 1312.184814\n",
      "Train: step:    760, time: 0.220, loss: 2484.404785\n",
      "Train: step:    770, time: 0.194, loss: 4538.868164\n",
      "Train: step:    780, time: 0.194, loss: 2462.544189\n",
      "Train: step:    790, time: 0.195, loss: 2251.009277\n",
      "Train: step:    800, time: 0.193, loss: 4511.198242\n",
      "Train: step:    810, time: 0.187, loss: 1226.645630\n",
      "Train: step:    820, time: 0.207, loss: 1247.446533\n",
      "Train: step:    830, time: 0.204, loss: 1976.775879\n",
      "Train: step:    840, time: 0.238, loss: 216.975266\n",
      "Train: step:    850, time: 0.190, loss: 1053.377319\n",
      "Train: step:    860, time: 0.193, loss: 3220.901367\n",
      "Train: step:    870, time: 0.209, loss: 2013.557373\n",
      "Train: step:    880, time: 0.193, loss: 261.328522\n",
      "Train: step:    890, time: 0.189, loss: 2750.971924\n",
      "Train: step:    900, time: 0.239, loss: 1855.838013\n",
      "Train: step:    910, time: 0.194, loss: 2625.729004\n",
      "Train: step:    920, time: 0.191, loss: 1594.467529\n",
      "Train: step:    930, time: 0.254, loss: 2483.961182\n",
      "Train: step:    940, time: 0.195, loss: 3334.870117\n",
      "Train: step:    950, time: 0.185, loss: 366.773407\n",
      "Train: step:    960, time: 0.223, loss: 1249.650391\n",
      "Train: step:    970, time: 0.191, loss: 1780.232422\n",
      "Train: step:    980, time: 0.201, loss: 1483.613892\n",
      "Train: step:    990, time: 0.230, loss: 1243.203247\n",
      "Train: step:   1000, time: 0.189, loss: 2364.834229\n",
      "Train: step:   1010, time: 0.226, loss: 2449.833740\n",
      "Train: step:   1020, time: 0.194, loss: 2774.135986\n",
      "Train: step:   1030, time: 0.204, loss: 1253.120117\n",
      "Train: step:   1040, time: 0.215, loss: 1841.345093\n",
      "Train: step:   1050, time: 0.198, loss: 2823.462158\n",
      "Train: step:   1060, time: 0.222, loss: 1995.580811\n",
      "Train: step:   1070, time: 0.208, loss: 1074.594360\n",
      "Train: step:   1080, time: 0.192, loss: 3087.650635\n",
      "Train: step:   1090, time: 0.217, loss: 3353.179199\n",
      "Train: step:   1100, time: 0.219, loss: 919.390991\n",
      "Train: step:   1110, time: 0.196, loss: 817.728455\n",
      "Train: step:   1120, time: 0.193, loss: 327.462250\n",
      "Train: step:   1130, time: 0.217, loss: 489.384216\n",
      "Train: step:   1140, time: 0.198, loss: 1955.802612\n",
      "Train: step:   1150, time: 0.195, loss: 1694.032227\n",
      "Train: step:   1160, time: 0.203, loss: 1164.582275\n",
      "Train: step:   1170, time: 0.225, loss: 1549.304932\n",
      "Train: step:   1180, time: 0.194, loss: 969.414124\n",
      "Train: step:   1190, time: 0.202, loss: 1051.635620\n",
      "Train: step:   1200, time: 0.223, loss: 1172.634155\n",
      "Train: step:   1210, time: 0.246, loss: 3939.636475\n",
      "Train: step:   1220, time: 0.185, loss: 1569.649048\n",
      "Train: step:   1230, time: 0.221, loss: 968.708618\n",
      "Train: step:   1240, time: 0.192, loss: 1577.605103\n",
      "Train: step:   1250, time: 0.216, loss: 975.745056\n",
      "Train: step:   1260, time: 0.244, loss: 906.398621\n",
      "Train: step:   1270, time: 0.218, loss: 791.315247\n",
      "Train: step:   1280, time: 0.189, loss: 2632.863281\n",
      "Train: step:   1290, time: 0.237, loss: 1112.651367\n",
      "Train: step:   1300, time: 0.252, loss: 1456.129395\n",
      "Train: step:   1310, time: 0.196, loss: 2293.795166\n",
      "Train: step:   1320, time: 0.217, loss: 2697.220459\n",
      "Train: step:   1330, time: 0.201, loss: 2026.020630\n",
      "Train: step:   1340, time: 0.209, loss: 4262.151367\n",
      "Train: step:   1350, time: 0.222, loss: 1977.334351\n",
      "Train: step:   1360, time: 0.206, loss: 1876.554565\n",
      "Train: step:   1370, time: 0.195, loss: 1154.825317\n",
      "Train: step:   1380, time: 0.200, loss: 2004.037109\n",
      "Train: step:   1390, time: 0.196, loss: 701.504333\n",
      "Train: step:   1400, time: 0.195, loss: 1676.413452\n",
      "Train: step:   1410, time: 0.230, loss: 2189.381592\n",
      "Train: step:   1420, time: 0.218, loss: 1751.600830\n",
      "Train: step:   1430, time: 0.196, loss: 1715.781738\n",
      "Train: step:   1440, time: 0.213, loss: 306.925598\n",
      "Train: step:   1450, time: 0.200, loss: 413.647552\n",
      "Train: step:   1460, time: 0.216, loss: 1141.993652\n",
      "Train: step:   1470, time: 0.230, loss: 2586.094482\n",
      "Train: step:   1480, time: 0.220, loss: 2379.571777\n",
      "Train: step:   1490, time: 0.242, loss: 804.807007\n",
      "Train: step:   1500, time: 0.226, loss: 584.831482\n",
      "Train: step:   1510, time: 0.224, loss: 1756.215698\n",
      "Train: step:   1520, time: 0.221, loss: 1005.960938\n",
      "Train: step:   1530, time: 0.206, loss: 964.630310\n",
      "Train: step:   1540, time: 0.208, loss: 1161.187500\n",
      "Train: step:   1550, time: 0.217, loss: 2706.911377\n",
      "Train: step:   1560, time: 0.207, loss: 2186.127930\n",
      "Train: step:   1570, time: 0.217, loss: 1981.168457\n",
      "Train: step:   1580, time: 0.221, loss: 1381.227417\n",
      "Train: step:   1590, time: 0.215, loss: 2832.823486\n",
      "Train: step:   1600, time: 0.209, loss: 224.868561\n",
      "Train: step:   1610, time: 0.206, loss: 1304.110962\n",
      "Train: step:   1620, time: 0.217, loss: 3135.482910\n",
      "Train: step:   1630, time: 0.200, loss: 4424.636230\n",
      "Train: step:   1640, time: 0.220, loss: 948.186768\n",
      "Train: step:   1650, time: 0.209, loss: 3159.361572\n",
      "Train: step:   1660, time: 0.228, loss: 1058.771973\n",
      "Train: step:   1670, time: 0.218, loss: 1645.767822\n",
      "Train: step:   1680, time: 0.237, loss: 755.499634\n",
      "Train: step:   1690, time: 0.214, loss: 1265.273315\n",
      "Train: step:   1700, time: 0.194, loss: 4192.877930\n",
      "Train: step:   1710, time: 0.219, loss: 2202.443359\n",
      "Train: step:   1720, time: 0.194, loss: 2488.435059\n",
      "Train: step:   1730, time: 0.194, loss: 2936.063721\n",
      "Train: step:   1740, time: 0.197, loss: 3219.792480\n",
      "Train: step:   1750, time: 0.192, loss: 1490.048584\n",
      "Train: step:   1760, time: 0.186, loss: 2380.734375\n",
      "Train: step:   1770, time: 0.228, loss: 1368.194214\n",
      "Train: step:   1780, time: 0.190, loss: 1361.769165\n",
      "Train: step:   1790, time: 0.234, loss: 2671.470459\n",
      "Train: step:   1800, time: 0.194, loss: 1888.554565\n",
      "Train: step:   1810, time: 0.192, loss: 910.000488\n",
      "Train: step:   1820, time: 0.187, loss: 345.949280\n",
      "Train: step:   1830, time: 0.196, loss: 2513.853027\n",
      "Train: step:   1840, time: 0.202, loss: 1743.278931\n",
      "Train: step:   1850, time: 0.197, loss: 1007.626282\n",
      "Train: step:   1860, time: 0.227, loss: 1637.042480\n",
      "Train: step:   1870, time: 0.191, loss: 1991.124512\n",
      "Train: step:   1880, time: 0.201, loss: 2157.533691\n",
      "Train: step:   1890, time: 0.210, loss: 303.573517\n",
      "Train: step:   1900, time: 0.220, loss: 581.026428\n",
      "Train: step:   1910, time: 0.191, loss: 1511.275879\n",
      "Train: step:   1920, time: 0.188, loss: 2956.585205\n",
      "Train: step:   1930, time: 0.193, loss: 2303.556885\n",
      "Train: step:   1940, time: 0.216, loss: 1684.769897\n",
      "Train: step:   1950, time: 0.203, loss: 1215.418945\n",
      "Train: step:   1960, time: 0.191, loss: 2151.175293\n",
      "Train: step:   1970, time: 0.202, loss: 1827.015991\n",
      "Train: step:   1980, time: 0.228, loss: 1627.544067\n",
      "Train: step:   1990, time: 0.204, loss: 2724.242432\n",
      "Train: step:   2000, time: 0.218, loss: 2602.162109\n",
      "Train: step:   2010, time: 0.246, loss: 2062.643066\n",
      "Train: step:   2020, time: 0.199, loss: 1885.366943\n",
      "Train: step:   2030, time: 0.193, loss: 425.506439\n",
      "Train: step:   2040, time: 0.233, loss: 1286.377197\n",
      "Train: step:   2050, time: 0.187, loss: 378.085968\n",
      "Train: step:   2060, time: 0.197, loss: 1599.101440\n",
      "Train: step:   2070, time: 0.203, loss: 987.946777\n",
      "Train: step:   2080, time: 0.205, loss: 1352.739014\n",
      "Train: step:   2090, time: 0.206, loss: 2309.684082\n",
      "Train: step:   2100, time: 0.205, loss: 1282.880005\n",
      "Train: step:   2110, time: 0.198, loss: 2445.350830\n",
      "Train: step:   2120, time: 0.194, loss: 147.785080\n",
      "Train: step:   2130, time: 0.208, loss: 1343.388794\n",
      "Train: step:   2140, time: 0.192, loss: 1556.315063\n",
      "Train: step:   2150, time: 0.190, loss: 1877.103149\n",
      "Train: step:   2160, time: 0.208, loss: 1924.016113\n",
      "Train: step:   2170, time: 0.190, loss: 2145.804688\n",
      "Train: step:   2180, time: 0.189, loss: 1636.946045\n",
      "Train: step:   2190, time: 0.220, loss: 2642.630371\n",
      "Train: step:   2200, time: 0.217, loss: 1515.505371\n",
      "Train: step:   2210, time: 0.200, loss: 2672.098389\n",
      "Train: step:   2220, time: 0.196, loss: 3884.287598\n",
      "Train: step:   2230, time: 0.191, loss: 1021.265625\n",
      "Train: step:   2240, time: 0.230, loss: 3123.570557\n",
      "Train: step:   2250, time: 0.198, loss: 2136.619385\n",
      "Train: step:   2260, time: 0.193, loss: 1684.513672\n",
      "Train: step:   2270, time: 0.211, loss: 874.043335\n",
      "Train: step:   2280, time: 0.217, loss: 2274.651611\n",
      "Train: step:   2290, time: 0.218, loss: 2063.489990\n",
      "Train: step:   2300, time: 0.199, loss: 1624.248779\n",
      "Train: step:   2310, time: 0.191, loss: 578.647583\n",
      "Train: step:   2320, time: 0.219, loss: 1767.630249\n",
      "Train: step:   2330, time: 0.240, loss: 2226.780762\n",
      "Train: step:   2340, time: 0.209, loss: 2729.630371\n",
      "Train: step:   2350, time: 0.219, loss: 2290.443604\n",
      "Train: step:   2360, time: 0.196, loss: 2164.233398\n",
      "Train: step:   2370, time: 0.193, loss: 2651.777100\n",
      "Train: step:   2380, time: 0.203, loss: 4184.130859\n",
      "Train: step:   2390, time: 0.190, loss: 3999.635986\n",
      "Train: step:   2400, time: 0.232, loss: 3493.347900\n",
      "Train: step:   2410, time: 0.216, loss: 2162.696533\n",
      "Train: step:   2420, time: 0.197, loss: 2746.634033\n",
      "Train: step:   2430, time: 0.192, loss: 1675.094849\n",
      "Train: step:   2440, time: 0.221, loss: 1760.342529\n",
      "Train: step:   2450, time: 0.202, loss: 1205.939697\n",
      "Train: step:   2460, time: 0.204, loss: 778.461731\n",
      "Train: step:   2470, time: 0.188, loss: 1941.491821\n",
      "Train: step:   2480, time: 0.210, loss: 3267.314941\n",
      "Train: step:   2490, time: 0.219, loss: 3196.527100\n",
      "Train: step:   2500, time: 0.234, loss: 1479.744019\n",
      "Train: step:   2510, time: 0.217, loss: 3423.950439\n",
      "Train: step:   2520, time: 0.193, loss: 3433.678711\n",
      "Train: step:   2530, time: 0.218, loss: 4094.781250\n",
      "Train: step:   2540, time: 0.182, loss: 2323.446289\n",
      "Train: step:   2550, time: 0.187, loss: 1684.675781\n",
      "Train: step:   2560, time: 0.225, loss: 2435.560059\n",
      "Train: step:   2570, time: 0.211, loss: 2760.534424\n",
      "Train: step:   2580, time: 0.215, loss: 3275.932129\n",
      "Train: step:   2590, time: 0.211, loss: 2750.889893\n",
      "Train: step:   2600, time: 0.203, loss: 1850.536377\n",
      "Train: step:   2610, time: 0.210, loss: 2569.389893\n",
      "Train: step:   2620, time: 0.208, loss: 2766.285645\n",
      "Train: step:   2630, time: 0.220, loss: 1227.809082\n",
      "Train: step:   2640, time: 0.213, loss: 1687.348022\n",
      "Train: step:   2650, time: 0.207, loss: 1249.279907\n",
      "Train: step:   2660, time: 0.208, loss: 2701.677246\n",
      "Train: step:   2670, time: 0.229, loss: 1111.369629\n",
      "Train: step:   2680, time: 0.208, loss: 1375.266235\n",
      "Train: step:   2690, time: 0.212, loss: 1768.349487\n",
      "Train: step:   2700, time: 0.213, loss: 2983.471680\n",
      "Train: step:   2710, time: 0.205, loss: 4025.197754\n",
      "Train: step:   2720, time: 0.210, loss: 2141.336670\n",
      "Train: step:   2730, time: 0.220, loss: 2965.473145\n",
      "Train: step:   2740, time: 0.219, loss: 970.219788\n",
      "Train: step:   2750, time: 0.221, loss: 3908.232910\n",
      "Train: step:   2760, time: 0.193, loss: 3132.176270\n",
      "Train: step:   2770, time: 0.195, loss: 2842.269287\n",
      "Train: step:   2780, time: 0.196, loss: 5106.933594\n",
      "Train: step:   2790, time: 0.199, loss: 1268.687012\n",
      "Train: step:   2800, time: 0.201, loss: 1035.472900\n",
      "Train: step:   2810, time: 0.206, loss: 1874.040649\n",
      "Train: step:   2820, time: 0.192, loss: 1568.755737\n",
      "Train: step:   2830, time: 0.192, loss: 1428.907837\n",
      "Train: step:   2840, time: 0.228, loss: 881.395996\n",
      "Train: step:   2850, time: 0.190, loss: 485.593842\n",
      "Train: step:   2860, time: 0.228, loss: 985.511353\n",
      "Train: step:   2870, time: 0.186, loss: 1878.779907\n",
      "Train: step:   2880, time: 0.202, loss: 3615.888428\n",
      "Train: step:   2890, time: 0.191, loss: 1806.607300\n",
      "Train: step:   2900, time: 0.192, loss: 1571.418457\n",
      "Train: step:   2910, time: 0.202, loss: 2122.874268\n",
      "Train: step:   2920, time: 0.200, loss: 2226.072510\n",
      "Train: step:   2930, time: 0.199, loss: 2232.950195\n",
      "Train: step:   2940, time: 0.193, loss: 2275.866455\n",
      "Train: step:   2950, time: 0.203, loss: 3257.578125\n",
      "Train: step:   2960, time: 0.189, loss: 2522.555908\n",
      "Train: step:   2970, time: 0.198, loss: 1484.465576\n",
      "Train: step:   2980, time: 0.204, loss: 1702.984131\n",
      "Train: step:   2990, time: 0.225, loss: 982.441406\n",
      "Train: step:   3000, time: 0.199, loss: 768.708923\n",
      "Train: step:   3010, time: 0.194, loss: 311.406586\n",
      "Train: step:   3020, time: 0.219, loss: 1019.579346\n",
      "Train: step:   3030, time: 0.188, loss: 3216.716553\n",
      "Train: step:   3040, time: 0.195, loss: 1193.118042\n",
      "Train: step:   3050, time: 0.188, loss: 2019.155884\n",
      "Train: step:   3060, time: 0.205, loss: 2473.467041\n",
      "Train: step:   3070, time: 0.194, loss: 2278.527344\n",
      "Train: step:   3080, time: 0.189, loss: 788.857971\n",
      "Train: step:   3090, time: 0.194, loss: 675.179443\n",
      "Train: step:   3100, time: 0.188, loss: 1351.792236\n",
      "Train: step:   3110, time: 0.245, loss: 1854.089966\n",
      "Train: step:   3120, time: 0.214, loss: 1943.692383\n",
      "Train: step:   3130, time: 0.237, loss: 2334.236084\n",
      "Train: step:   3140, time: 0.197, loss: 3615.677490\n",
      "Train: step:   3150, time: 0.203, loss: 1278.774658\n",
      "Train: step:   3160, time: 0.241, loss: 1665.045166\n",
      "Train: step:   3170, time: 0.254, loss: 1913.503052\n",
      "Train: step:   3180, time: 0.215, loss: 2700.088623\n",
      "Train: step:   3190, time: 0.222, loss: 1769.629517\n",
      "Train: step:   3200, time: 0.225, loss: 650.620422\n",
      "Train: step:   3210, time: 0.214, loss: 3007.760254\n",
      "Train: step:   3220, time: 0.238, loss: 2559.739502\n",
      "Train: step:   3230, time: 0.228, loss: 225.817032\n",
      "Train: step:   3240, time: 0.204, loss: 1887.347046\n",
      "Train: step:   3250, time: 0.190, loss: 2096.448242\n",
      "Train: step:   3260, time: 0.194, loss: 1823.520264\n",
      "Train: step:   3270, time: 0.195, loss: 2140.474121\n",
      "Train: step:   3280, time: 0.196, loss: 2018.499512\n",
      "Train: step:   3290, time: 0.281, loss: 1239.226562\n",
      "Train: step:   3300, time: 0.229, loss: 1189.922729\n",
      "Train: step:   3310, time: 0.192, loss: 2660.188232\n",
      "Train: step:   3320, time: 0.217, loss: 4604.436523\n",
      "Train: step:   3330, time: 0.195, loss: 2361.197998\n",
      "Train: step:   3340, time: 0.217, loss: 2645.612305\n",
      "Train: step:   3350, time: 0.198, loss: 2604.578125\n",
      "Train: step:   3360, time: 0.190, loss: 855.699036\n",
      "Train: step:   3370, time: 0.218, loss: 2053.072510\n",
      "Train: step:   3380, time: 0.262, loss: 739.881897\n",
      "Train: step:   3390, time: 0.225, loss: 1813.981934\n",
      "Train: step:   3400, time: 0.224, loss: 1128.681030\n",
      "Train: step:   3410, time: 0.191, loss: 1526.102173\n",
      "Train: step:   3420, time: 0.194, loss: 2339.787354\n",
      "Train: step:   3430, time: 0.234, loss: 1095.050171\n",
      "Train: step:   3440, time: 0.225, loss: 865.378235\n",
      "Train: step:   3450, time: 0.187, loss: 2667.488037\n",
      "Train: step:   3460, time: 0.233, loss: 2829.940674\n",
      "Train: step:   3470, time: 0.232, loss: 1518.652222\n",
      "Train: step:   3480, time: 0.196, loss: 2021.650024\n",
      "Train: step:   3490, time: 0.222, loss: 1326.867676\n",
      "Train: step:   3500, time: 0.230, loss: 1015.833252\n",
      "Train: step:   3510, time: 0.216, loss: 1602.308105\n",
      "Train: step:   3520, time: 0.227, loss: 647.663452\n",
      "Train: step:   3530, time: 0.195, loss: 449.832489\n",
      "Train: step:   3540, time: 0.221, loss: 2656.027100\n",
      "Train: step:   3550, time: 0.217, loss: 1315.060547\n",
      "Train: step:   3560, time: 0.219, loss: 1976.443726\n",
      "Train: step:   3570, time: 0.188, loss: 1965.576782\n",
      "Train: step:   3580, time: 0.225, loss: 2078.366211\n",
      "Train: step:   3590, time: 0.195, loss: 2138.306885\n",
      "Train: step:   3600, time: 0.238, loss: 3159.454102\n",
      "Train: step:   3610, time: 0.214, loss: 2609.335938\n",
      "Train: step:   3620, time: 0.238, loss: 3875.434326\n",
      "Train: step:   3630, time: 0.215, loss: 312.377808\n",
      "Train: step:   3640, time: 0.217, loss: 2389.153564\n",
      "Train: step:   3650, time: 0.219, loss: 1889.316406\n",
      "Train: step:   3660, time: 0.210, loss: 2791.981934\n",
      "Train: step:   3670, time: 0.223, loss: 1206.368408\n",
      "Train: step:   3680, time: 0.214, loss: 1860.332642\n",
      "Train: step:   3690, time: 0.196, loss: 1922.036133\n",
      "Train: step:   3700, time: 0.207, loss: 2380.908691\n",
      "Train: step:   3710, time: 0.239, loss: 2359.077637\n",
      "Train: step:   3720, time: 0.197, loss: 449.171906\n",
      "Train: step:   3730, time: 0.213, loss: 3002.658203\n",
      "Train: step:   3740, time: 0.219, loss: 694.921326\n",
      "Train: step:   3750, time: 0.205, loss: 2342.354980\n",
      "Train: step:   3760, time: 0.198, loss: 1897.374634\n",
      "Train: step:   3770, time: 0.187, loss: 1113.722534\n",
      "Train: step:   3780, time: 0.190, loss: 1813.322754\n",
      "Train: step:   3790, time: 0.218, loss: 1531.784668\n",
      "Train: step:   3800, time: 0.229, loss: 1534.802246\n",
      "Train: step:   3810, time: 0.226, loss: 1183.401001\n",
      "Train: step:   3820, time: 0.215, loss: 578.019653\n",
      "Train: step:   3830, time: 0.203, loss: 681.549683\n",
      "Train: step:   3840, time: 0.206, loss: 1096.968994\n",
      "Train: step:   3850, time: 0.199, loss: 2729.579346\n",
      "Train: step:   3860, time: 0.233, loss: 938.190063\n",
      "Train: step:   3870, time: 0.206, loss: 1631.223755\n",
      "Train: step:   3880, time: 0.223, loss: 2638.127197\n",
      "Train: step:   3890, time: 0.214, loss: 1465.978149\n",
      "Train: step:   3900, time: 0.189, loss: 311.037933\n",
      "Train: step:   3910, time: 0.194, loss: 542.492920\n",
      "Train: step:   3920, time: 0.197, loss: 2901.544678\n",
      "Train: step:   3930, time: 0.232, loss: 4448.205566\n",
      "Train: step:   3940, time: 0.189, loss: 565.349731\n",
      "Train: step:   3950, time: 0.192, loss: 2490.119141\n",
      "Train: step:   3960, time: 0.214, loss: 807.703430\n",
      "Train: step:   3970, time: 0.196, loss: 1129.145142\n",
      "Train: step:   3980, time: 0.196, loss: 518.612183\n",
      "Train: step:   3990, time: 0.209, loss: 2268.410889\n",
      "Train: step:   4000, time: 0.205, loss: 2229.153320\n",
      "Train: step:   4010, time: 0.199, loss: 967.071777\n",
      "Train: step:   4020, time: 0.235, loss: 2054.255127\n",
      "Train: step:   4030, time: 0.200, loss: 1028.908081\n",
      "Train: step:   4040, time: 0.226, loss: 2910.502930\n",
      "Train: step:   4050, time: 0.222, loss: 2068.019287\n",
      "Train: step:   4060, time: 0.195, loss: 876.729797\n",
      "Train: step:   4070, time: 0.209, loss: 2086.856201\n",
      "Train: step:   4080, time: 0.237, loss: 3663.194092\n",
      "Train: step:   4090, time: 0.196, loss: 2725.793457\n",
      "Train: step:   4100, time: 0.200, loss: 2798.427490\n",
      "Train: step:   4110, time: 0.219, loss: 2914.962646\n",
      "Train: step:   4120, time: 0.228, loss: 1656.560425\n",
      "Train: step:   4130, time: 0.192, loss: 777.474487\n",
      "Train: step:   4140, time: 0.202, loss: 731.755676\n",
      "Train: step:   4150, time: 0.196, loss: 5271.286621\n",
      "Train: step:   4160, time: 0.213, loss: 1360.963501\n",
      "Train: step:   4170, time: 0.214, loss: 2897.885742\n",
      "Train: step:   4180, time: 0.199, loss: 1477.492310\n",
      "Train: step:   4190, time: 0.231, loss: 442.658844\n",
      "Train: step:   4200, time: 0.196, loss: 490.492981\n",
      "Train: step:   4210, time: 0.217, loss: 2064.854980\n",
      "Train: step:   4220, time: 0.218, loss: 711.698059\n",
      "Train: step:   4230, time: 0.198, loss: 850.577576\n",
      "Train: step:   4240, time: 0.235, loss: 1165.649292\n",
      "Train: step:   4250, time: 0.232, loss: 1523.756592\n",
      "Train: step:   4260, time: 0.198, loss: 2821.094238\n",
      "Train: step:   4270, time: 0.201, loss: 2882.532471\n",
      "Train: step:   4280, time: 0.202, loss: 3548.663818\n",
      "Train: step:   4290, time: 0.218, loss: 1933.172729\n",
      "Train: step:   4300, time: 0.204, loss: 1558.496826\n",
      "Train: step:   4310, time: 0.196, loss: 1996.921631\n",
      "Train: step:   4320, time: 0.199, loss: 2384.790527\n",
      "Train: step:   4330, time: 0.229, loss: 548.416016\n",
      "Train: step:   4340, time: 0.197, loss: 3299.990723\n",
      "Train: step:   4350, time: 0.205, loss: 1207.858765\n",
      "Train: step:   4360, time: 0.198, loss: 1174.075806\n",
      "Train: step:   4370, time: 0.222, loss: 842.340210\n",
      "Train: step:   4380, time: 0.216, loss: 1084.920166\n",
      "Train: step:   4390, time: 0.230, loss: 3247.890137\n",
      "Train: step:   4400, time: 0.205, loss: 1435.402832\n",
      "Train: step:   4410, time: 0.208, loss: 2922.213379\n",
      "Train: step:   4420, time: 0.197, loss: 2530.081543\n",
      "Train: step:   4430, time: 0.203, loss: 3011.453125\n",
      "Train: step:   4440, time: 0.194, loss: 1852.511230\n",
      "Train: step:   4450, time: 0.201, loss: 2606.804443\n",
      "Train: step:   4460, time: 0.201, loss: 1035.427002\n",
      "Train: step:   4470, time: 0.227, loss: 2469.031738\n",
      "Train: step:   4480, time: 0.216, loss: 303.286438\n",
      "Train: step:   4490, time: 0.215, loss: 3360.709473\n",
      "Train: step:   4500, time: 0.228, loss: 1298.782471\n",
      "Train: step:   4510, time: 0.216, loss: 1037.245239\n",
      "Train: step:   4520, time: 0.236, loss: 1534.589233\n",
      "Train: step:   4530, time: 0.202, loss: 243.437607\n",
      "Train: step:   4540, time: 0.212, loss: 1141.566040\n",
      "Train: step:   4550, time: 0.215, loss: 679.305542\n",
      "Train: step:   4560, time: 0.220, loss: 1641.287109\n",
      "Train: step:   4570, time: 0.211, loss: 1813.410522\n",
      "Train: step:   4580, time: 0.200, loss: 1381.095337\n",
      "Train: step:   4590, time: 0.208, loss: 1387.607056\n",
      "Train: step:   4600, time: 0.241, loss: 1097.521729\n",
      "Train: step:   4610, time: 0.197, loss: 1201.443359\n",
      "Train: step:   4620, time: 0.215, loss: 2281.979736\n",
      "Train: step:   4630, time: 0.203, loss: 3303.060059\n",
      "Train: step:   4640, time: 0.194, loss: 1657.445557\n",
      "Train: step:   4650, time: 0.225, loss: 2361.557373\n",
      "Train: step:   4660, time: 0.228, loss: 2235.021484\n",
      "Train: step:   4670, time: 0.246, loss: 3087.142090\n",
      "Train: step:   4680, time: 0.192, loss: 1233.496338\n",
      "Train: step:   4690, time: 0.198, loss: 1977.488525\n",
      "Train: step:   4700, time: 0.204, loss: 2028.401123\n",
      "Train: step:   4710, time: 0.203, loss: 1561.551270\n",
      "Train: step:   4720, time: 0.201, loss: 4452.010254\n",
      "Train: step:   4730, time: 0.218, loss: 1965.413696\n",
      "Train: step:   4740, time: 0.214, loss: 535.091858\n",
      "Train: step:   4750, time: 0.191, loss: 2283.573975\n",
      "Train: step:   4760, time: 0.195, loss: 457.434662\n",
      "Train: step:   4770, time: 0.197, loss: 4120.926270\n",
      "Train: step:   4780, time: 0.206, loss: 2701.652100\n",
      "Train: step:   4790, time: 0.211, loss: 3733.354004\n",
      "Train: step:   4800, time: 0.189, loss: 1470.869263\n",
      "Train: step:   4810, time: 0.262, loss: 3466.084229\n",
      "Train: step:   4820, time: 0.227, loss: 609.197083\n",
      "Train: step:   4830, time: 0.191, loss: 509.257050\n",
      "Train: step:   4840, time: 0.196, loss: 1039.855347\n",
      "Train: step:   4850, time: 0.203, loss: 1889.984009\n",
      "Train: step:   4860, time: 0.226, loss: 1306.623413\n",
      "Train: step:   4870, time: 0.188, loss: 1122.964844\n",
      "Train: step:   4880, time: 0.195, loss: 1288.101074\n",
      "Train: step:   4890, time: 0.196, loss: 663.847168\n",
      "Train: step:   4900, time: 0.202, loss: 2436.599854\n",
      "Train: step:   4910, time: 0.193, loss: 1196.540527\n",
      "Train: step:   4920, time: 0.195, loss: 1128.939087\n",
      "Train: step:   4930, time: 0.241, loss: 1330.759521\n",
      "Train: step:   4940, time: 0.234, loss: 1731.351074\n",
      "Train: step:   4950, time: 0.220, loss: 1127.323975\n",
      "Train: step:   4960, time: 0.217, loss: 2025.031494\n",
      "Train: step:   4970, time: 0.189, loss: 1955.449341\n",
      "Train: step:   4980, time: 0.204, loss: 1278.744629\n",
      "Train: step:   4990, time: 0.186, loss: 1064.040039\n",
      "Train: step:   5000, time: 0.216, loss: 1281.251343\n",
      "Train: step:   5010, time: 0.217, loss: 998.494751\n",
      "Train: step:   5020, time: 0.220, loss: 667.446228\n",
      "Train: step:   5030, time: 0.204, loss: 1486.739014\n",
      "Train: step:   5040, time: 0.195, loss: 482.974915\n",
      "Train: step:   5050, time: 0.202, loss: 2045.179077\n",
      "Train: step:   5060, time: 0.187, loss: 1152.633667\n",
      "Train: step:   5070, time: 0.191, loss: 739.588867\n",
      "Train: step:   5080, time: 0.186, loss: 523.974792\n",
      "Train: step:   5090, time: 0.187, loss: 1749.243042\n",
      "Train: step:   5100, time: 0.217, loss: 2721.387451\n",
      "Train: step:   5110, time: 0.193, loss: 2385.008301\n",
      "Train: step:   5120, time: 0.188, loss: 1567.793579\n",
      "Train: step:   5130, time: 0.217, loss: 2135.222168\n",
      "Train: step:   5140, time: 0.185, loss: 974.092590\n",
      "Train: step:   5150, time: 0.193, loss: 1716.831421\n",
      "Train: step:   5160, time: 0.193, loss: 453.036926\n",
      "Train: step:   5170, time: 0.195, loss: 532.600281\n",
      "Train: step:   5180, time: 0.199, loss: 2774.088867\n",
      "Train: step:   5190, time: 0.199, loss: 974.275146\n",
      "Train: step:   5200, time: 0.197, loss: 2476.998779\n",
      "Train: step:   5210, time: 0.217, loss: 1341.656494\n",
      "Train: step:   5220, time: 0.190, loss: 2260.588867\n",
      "Train: step:   5230, time: 0.190, loss: 383.937500\n",
      "Train: step:   5240, time: 0.191, loss: 2046.957153\n",
      "Train: step:   5250, time: 0.196, loss: 1448.291870\n",
      "Train: step:   5260, time: 0.191, loss: 970.571777\n",
      "Train: step:   5270, time: 0.191, loss: 2269.102295\n",
      "Train: step:   5280, time: 0.192, loss: 2211.684814\n",
      "Train: step:   5290, time: 0.218, loss: 406.066742\n",
      "Train: step:   5300, time: 0.224, loss: 1656.531982\n",
      "Train: step:   5310, time: 0.215, loss: 1285.015869\n",
      "Train: step:   5320, time: 0.225, loss: 1767.614624\n",
      "Train: step:   5330, time: 0.211, loss: 1007.717773\n",
      "Train: step:   5340, time: 0.184, loss: 909.676880\n",
      "Train: step:   5350, time: 0.195, loss: 1566.352905\n",
      "Train: step:   5360, time: 0.199, loss: 1461.208130\n",
      "Train: step:   5370, time: 0.196, loss: 2211.750244\n",
      "Train: step:   5380, time: 0.186, loss: 1443.785156\n",
      "Train: step:   5390, time: 0.195, loss: 1077.406494\n",
      "Train: step:   5400, time: 0.194, loss: 1388.771484\n",
      "Train: step:   5410, time: 0.190, loss: 1794.596924\n",
      "Train: step:   5420, time: 0.245, loss: 1030.832153\n",
      "Train: step:   5430, time: 0.199, loss: 1713.617554\n",
      "Train: step:   5440, time: 0.190, loss: 2908.948730\n",
      "Train: step:   5450, time: 0.217, loss: 2357.935547\n",
      "Train: step:   5460, time: 0.189, loss: 2504.340820\n",
      "Train: step:   5470, time: 0.218, loss: 2720.960449\n",
      "Train: step:   5480, time: 0.208, loss: 342.485901\n",
      "Train: step:   5490, time: 0.231, loss: 2835.895264\n",
      "Train: step:   5500, time: 0.198, loss: 1438.944824\n",
      "Train: step:   5510, time: 0.192, loss: 919.916687\n",
      "Train: step:   5520, time: 0.214, loss: 919.167603\n",
      "Train: step:   5530, time: 0.201, loss: 2356.717285\n",
      "Train: step:   5540, time: 0.203, loss: 890.460693\n",
      "Train: step:   5550, time: 0.214, loss: 2622.904297\n",
      "Train: step:   5560, time: 0.191, loss: 1349.634644\n",
      "Train: step:   5570, time: 0.225, loss: 1682.135620\n",
      "Train: step:   5580, time: 0.192, loss: 2136.284180\n",
      "Train: step:   5590, time: 0.190, loss: 3133.554688\n",
      "Train: step:   5600, time: 0.190, loss: 1901.422363\n",
      "Train: step:   5610, time: 0.191, loss: 2302.335693\n",
      "Train: step:   5620, time: 0.190, loss: 2334.484375\n",
      "Train: step:   5630, time: 0.192, loss: 2400.510742\n",
      "Train: step:   5640, time: 0.196, loss: 1758.014404\n",
      "Train: step:   5650, time: 0.189, loss: 2182.829102\n",
      "Train: step:   5660, time: 0.200, loss: 3734.854004\n",
      "Train: step:   5670, time: 0.198, loss: 1040.544678\n",
      "Train: step:   5680, time: 0.217, loss: 511.983765\n",
      "Train: step:   5690, time: 0.195, loss: 2582.449707\n",
      "Train: step:   5700, time: 0.237, loss: 754.444580\n",
      "Train: step:   5710, time: 0.217, loss: 615.455566\n",
      "Train: step:   5720, time: 0.203, loss: 684.660400\n",
      "Train: step:   5730, time: 0.187, loss: 612.269897\n",
      "Train: step:   5740, time: 0.186, loss: 2128.278809\n",
      "Train: step:   5750, time: 0.216, loss: 2916.837402\n",
      "Train: step:   5760, time: 0.187, loss: 2422.106934\n",
      "Train: step:   5770, time: 0.184, loss: 1264.239624\n",
      "Train: step:   5780, time: 0.199, loss: 372.865204\n",
      "Train: step:   5790, time: 0.215, loss: 2375.012207\n",
      "Train: step:   5800, time: 0.189, loss: 1292.336304\n",
      "Train: step:   5810, time: 0.192, loss: 1636.063965\n",
      "Train: step:   5820, time: 0.195, loss: 3502.802979\n",
      "Train: step:   5830, time: 0.197, loss: 672.066956\n",
      "Train: step:   5840, time: 0.194, loss: 1452.276489\n",
      "Train: step:   5850, time: 0.192, loss: 1183.746216\n",
      "Train: step:   5860, time: 0.216, loss: 1386.114746\n",
      "Train: step:   5870, time: 0.194, loss: 2039.548096\n",
      "Train: step:   5880, time: 0.190, loss: 2166.435303\n",
      "Train: step:   5890, time: 0.242, loss: 2133.086914\n",
      "Train: step:   5900, time: 0.218, loss: 1974.479980\n",
      "Train: step:   5910, time: 0.219, loss: 2023.404663\n",
      "Train: step:   5920, time: 0.202, loss: 2399.098633\n",
      "Train: step:   5930, time: 0.195, loss: 4191.310059\n",
      "Train: step:   5940, time: 0.212, loss: 1720.948853\n",
      "Train: step:   5950, time: 0.199, loss: 838.776794\n",
      "Train: step:   5960, time: 0.196, loss: 1548.845459\n",
      "Train: step:   5970, time: 0.192, loss: 1233.470215\n",
      "Train: step:   5980, time: 0.189, loss: 2365.417725\n",
      "Train: step:   5990, time: 0.254, loss: 1586.904907\n",
      "Train: step:   6000, time: 0.231, loss: 1964.351318\n",
      "Train: step:   6010, time: 0.198, loss: 831.103210\n",
      "Train: step:   6020, time: 0.188, loss: 1102.626587\n",
      "Train: step:   6030, time: 0.218, loss: 1230.605591\n",
      "Train: step:   6040, time: 0.197, loss: 2062.863770\n",
      "Train: step:   6050, time: 0.204, loss: 2350.548584\n",
      "Train: step:   6060, time: 0.199, loss: 2144.056396\n",
      "Train: step:   6070, time: 0.192, loss: 3314.934570\n",
      "Train: step:   6080, time: 0.230, loss: 1917.925659\n",
      "Train: step:   6090, time: 0.192, loss: 1381.222046\n",
      "Train: step:   6100, time: 0.190, loss: 1417.648560\n",
      "Train: step:   6110, time: 0.185, loss: 3842.906982\n",
      "Train: step:   6120, time: 0.248, loss: 3052.581543\n",
      "Train: step:   6130, time: 0.210, loss: 1416.902588\n",
      "Train: step:   6140, time: 0.195, loss: 619.499634\n",
      "Train: step:   6150, time: 0.188, loss: 3419.677979\n",
      "Train: step:   6160, time: 0.237, loss: 3603.976074\n",
      "Train: step:   6170, time: 0.206, loss: 669.272278\n",
      "Train: step:   6180, time: 0.188, loss: 3010.166260\n",
      "Train: step:   6190, time: 0.220, loss: 200.630417\n",
      "Train: step:   6200, time: 0.235, loss: 3617.518066\n",
      "Train: step:   6210, time: 0.186, loss: 3510.766602\n",
      "Train: step:   6220, time: 0.184, loss: 552.049377\n",
      "Train: step:   6230, time: 0.215, loss: 3900.346191\n",
      "Train: step:   6240, time: 0.190, loss: 1243.230469\n",
      "Train: step:   6250, time: 0.192, loss: 1645.431763\n",
      "Train: step:   6260, time: 0.216, loss: 1612.767700\n",
      "Train: step:   6270, time: 0.203, loss: 2591.666504\n",
      "Train: step:   6280, time: 0.199, loss: 2400.306396\n",
      "Train: step:   6290, time: 0.186, loss: 1159.418701\n",
      "Train: step:   6300, time: 0.231, loss: 3016.002441\n",
      "Train: step:   6310, time: 0.190, loss: 1962.015747\n",
      "Train: step:   6320, time: 0.205, loss: 1295.075684\n",
      "Train: step:   6330, time: 0.186, loss: 1783.066895\n",
      "Train: step:   6340, time: 0.195, loss: 1690.427368\n",
      "Train: step:   6350, time: 0.192, loss: 1848.539429\n",
      "Train: step:   6360, time: 0.194, loss: 1205.730713\n",
      "Train: step:   6370, time: 0.223, loss: 3114.987549\n",
      "Train: step:   6380, time: 0.193, loss: 884.261169\n",
      "Train: step:   6390, time: 0.189, loss: 376.039246\n",
      "Train: step:   6400, time: 0.229, loss: 1648.362427\n",
      "Train: step:   6410, time: 0.203, loss: 1934.855957\n",
      "Train: step:   6420, time: 0.187, loss: 434.139191\n",
      "Train: step:   6430, time: 0.188, loss: 2261.907227\n",
      "Train: step:   6440, time: 0.193, loss: 994.034119\n",
      "Train: step:   6450, time: 0.189, loss: 1423.367554\n",
      "Train: step:   6460, time: 0.186, loss: 1376.182739\n",
      "Train: step:   6470, time: 0.191, loss: 1092.311646\n",
      "Train: step:   6480, time: 0.196, loss: 1743.770508\n",
      "Train: step:   6490, time: 0.187, loss: 1435.963989\n",
      "Train: step:   6500, time: 0.196, loss: 2361.286621\n",
      "Train: step:   6510, time: 0.191, loss: 1044.484253\n",
      "Train: step:   6520, time: 0.188, loss: 2374.857666\n",
      "Train: step:   6530, time: 0.217, loss: 1342.692505\n",
      "Train: step:   6540, time: 0.184, loss: 2327.874512\n",
      "Train: step:   6550, time: 0.195, loss: 1476.188477\n",
      "Train: step:   6560, time: 0.229, loss: 2823.673096\n",
      "Train: step:   6570, time: 0.192, loss: 2983.063477\n",
      "Train: step:   6580, time: 0.191, loss: 2206.153320\n",
      "Train: step:   6590, time: 0.191, loss: 3077.530273\n",
      "Train: step:   6600, time: 0.229, loss: 578.107727\n",
      "Train: step:   6610, time: 0.237, loss: 2466.221191\n",
      "Train: step:   6620, time: 0.196, loss: 1963.958374\n",
      "Train: step:   6630, time: 0.188, loss: 4065.858154\n",
      "Train: step:   6640, time: 0.186, loss: 764.269287\n",
      "Train: step:   6650, time: 0.187, loss: 3619.810791\n",
      "Train: step:   6660, time: 0.230, loss: 2033.822754\n",
      "Train: step:   6670, time: 0.187, loss: 1145.416138\n",
      "Train: step:   6680, time: 0.236, loss: 1450.937256\n",
      "Train: step:   6690, time: 0.189, loss: 3741.358643\n",
      "Train: step:   6700, time: 0.189, loss: 2261.268555\n",
      "Train: step:   6710, time: 0.190, loss: 1063.315918\n",
      "Train: step:   6720, time: 0.245, loss: 2497.749268\n",
      "Train: step:   6730, time: 0.248, loss: 1123.592407\n",
      "Train: step:   6740, time: 0.261, loss: 2226.063721\n",
      "Train: step:   6750, time: 0.184, loss: 604.490479\n",
      "Train: step:   6760, time: 0.233, loss: 1659.612061\n",
      "Train: step:   6770, time: 0.195, loss: 1534.050293\n",
      "Train: step:   6780, time: 0.198, loss: 2361.501709\n",
      "Train: step:   6790, time: 0.236, loss: 1828.739014\n",
      "Train: step:   6800, time: 0.238, loss: 1086.768677\n",
      "Train: step:   6810, time: 0.185, loss: 4848.120117\n",
      "Train: step:   6820, time: 0.188, loss: 2478.603027\n",
      "Train: step:   6830, time: 0.188, loss: 3258.270020\n",
      "Train: step:   6840, time: 0.242, loss: 2751.644287\n",
      "Train: step:   6850, time: 0.185, loss: 2045.422485\n",
      "Train: step:   6860, time: 0.233, loss: 3146.834961\n",
      "Train: step:   6870, time: 0.188, loss: 726.139282\n",
      "Train: step:   6880, time: 0.269, loss: 1075.677490\n",
      "Train: step:   6890, time: 0.236, loss: 2550.826660\n",
      "Train: step:   6900, time: 0.220, loss: 809.724976\n",
      "Train: step:   6910, time: 0.209, loss: 634.138306\n",
      "Train: step:   6920, time: 0.187, loss: 2016.014771\n",
      "Train: step:   6930, time: 0.227, loss: 1205.258179\n",
      "Train: step:   6940, time: 0.218, loss: 2020.269287\n",
      "Train: step:   6950, time: 0.187, loss: 2954.004150\n",
      "Train: step:   6960, time: 0.198, loss: 3154.363281\n",
      "Train: step:   6970, time: 0.259, loss: 872.101074\n",
      "Train: step:   6980, time: 0.220, loss: 2646.334961\n",
      "Train: step:   6990, time: 0.219, loss: 1124.353882\n",
      "Train: step:   7000, time: 0.196, loss: 1912.286255\n",
      "Train: step:   7010, time: 0.187, loss: 1044.094360\n",
      "Train: step:   7020, time: 0.198, loss: 3306.865723\n",
      "Train: step:   7030, time: 0.196, loss: 1670.364502\n",
      "Train: step:   7040, time: 0.193, loss: 546.606506\n",
      "Train: step:   7050, time: 0.188, loss: 1761.555786\n",
      "Train: step:   7060, time: 0.259, loss: 1222.808105\n",
      "Train: step:   7070, time: 0.248, loss: 1347.084229\n",
      "Train: step:   7080, time: 0.195, loss: 3859.055908\n",
      "Train: step:   7090, time: 0.213, loss: 3195.557129\n",
      "Train: step:   7100, time: 0.193, loss: 2067.729492\n",
      "Train: step:   7110, time: 0.194, loss: 1035.779785\n",
      "Train: step:   7120, time: 0.192, loss: 1155.730835\n",
      "Train: step:   7130, time: 0.219, loss: 296.697266\n",
      "Train: step:   7140, time: 0.231, loss: 1596.806519\n",
      "Train: step:   7150, time: 0.218, loss: 1467.515747\n",
      "Train: step:   7160, time: 0.215, loss: 2646.577881\n",
      "Train: step:   7170, time: 0.228, loss: 1742.684814\n",
      "Train: step:   7180, time: 0.206, loss: 3830.674561\n",
      "Train: step:   7190, time: 0.213, loss: 2172.076904\n",
      "Train: step:   7200, time: 0.243, loss: 1254.598389\n",
      "Train: step:   7210, time: 0.209, loss: 910.755066\n",
      "Train: step:   7220, time: 0.187, loss: 3912.992432\n",
      "Train: step:   7230, time: 0.228, loss: 995.978821\n",
      "Train: step:   7240, time: 0.191, loss: 2049.803955\n",
      "Train: step:   7250, time: 0.234, loss: 1134.872314\n",
      "Train: step:   7260, time: 0.191, loss: 1366.827759\n",
      "Train: step:   7270, time: 0.221, loss: 2495.453125\n",
      "Train: step:   7280, time: 0.191, loss: 4055.436035\n",
      "Train: step:   7290, time: 0.194, loss: 2085.327881\n",
      "Train: step:   7300, time: 0.188, loss: 2772.316162\n",
      "Train: step:   7310, time: 0.204, loss: 2620.745361\n",
      "Train: step:   7320, time: 0.193, loss: 1083.779419\n",
      "Train: step:   7330, time: 0.242, loss: 2656.258789\n",
      "Train: step:   7340, time: 0.192, loss: 1577.710938\n",
      "Train: step:   7350, time: 0.184, loss: 933.043823\n",
      "Train: step:   7360, time: 0.192, loss: 2062.032227\n",
      "Train: step:   7370, time: 0.231, loss: 1066.786499\n",
      "Train: step:   7380, time: 0.218, loss: 947.152527\n",
      "Train: step:   7390, time: 0.189, loss: 1439.055176\n",
      "Train: step:   7400, time: 0.199, loss: 3292.872070\n",
      "Train: step:   7410, time: 0.199, loss: 2235.084229\n",
      "Train: step:   7420, time: 0.187, loss: 1275.629028\n",
      "Train: step:   7430, time: 0.186, loss: 2668.965576\n",
      "Train: step:   7440, time: 0.198, loss: 1610.269653\n",
      "Train: step:   7450, time: 0.190, loss: 1116.539185\n",
      "Train: step:   7460, time: 0.200, loss: 955.669373\n",
      "Train: step:   7470, time: 0.221, loss: 2500.544922\n",
      "Train: step:   7480, time: 0.192, loss: 2744.190430\n",
      "Train: step:   7490, time: 0.217, loss: 2570.190430\n",
      "Train: step:   7500, time: 0.217, loss: 2069.790283\n",
      "Train: step:   7510, time: 0.200, loss: 1526.022461\n",
      "Train: step:   7520, time: 0.214, loss: 679.512756\n",
      "Train: step:   7530, time: 0.238, loss: 944.495178\n",
      "Train: step:   7540, time: 0.196, loss: 2463.560303\n",
      "Train: step:   7550, time: 0.186, loss: 1611.597534\n",
      "Train: step:   7560, time: 0.194, loss: 2712.194336\n",
      "Train: step:   7570, time: 0.190, loss: 2130.667480\n",
      "Train: step:   7580, time: 0.211, loss: 1401.436157\n",
      "Train: step:   7590, time: 0.216, loss: 1330.187256\n",
      "Train: step:   7600, time: 0.220, loss: 3292.096924\n",
      "Train: step:   7610, time: 0.240, loss: 3298.033936\n",
      "Train: step:   7620, time: 0.192, loss: 3006.933105\n",
      "Train: step:   7630, time: 0.217, loss: 857.860718\n",
      "Train: step:   7640, time: 0.194, loss: 4621.064941\n",
      "Train: step:   7650, time: 0.193, loss: 2276.766602\n",
      "Train: step:   7660, time: 0.196, loss: 2001.284668\n",
      "Train: step:   7670, time: 0.191, loss: 3188.866699\n",
      "Train: step:   7680, time: 0.189, loss: 614.114868\n",
      "Train: step:   7690, time: 0.188, loss: 1663.014526\n",
      "Train: step:   7700, time: 0.186, loss: 1703.914429\n",
      "Train: step:   7710, time: 0.196, loss: 2528.587646\n",
      "Train: step:   7720, time: 0.183, loss: 3423.624756\n",
      "Train: step:   7730, time: 0.218, loss: 1965.019165\n",
      "Train: step:   7740, time: 0.198, loss: 1307.524780\n",
      "Train: step:   7750, time: 0.190, loss: 1780.067139\n",
      "Train: step:   7760, time: 0.195, loss: 310.616486\n",
      "Train: step:   7770, time: 0.217, loss: 2127.314453\n",
      "Train: step:   7780, time: 0.226, loss: 2555.252686\n",
      "Train: step:   7790, time: 0.185, loss: 1376.483154\n",
      "Train: step:   7800, time: 0.228, loss: 1169.864502\n",
      "Train: step:   7810, time: 0.213, loss: 2705.104248\n",
      "Train: step:   7820, time: 0.216, loss: 2453.188965\n",
      "Train: step:   7830, time: 0.228, loss: 1374.917725\n",
      "Train: step:   7840, time: 0.218, loss: 1345.974976\n",
      "Train: step:   7850, time: 0.197, loss: 1533.119263\n",
      "Train: step:   7860, time: 0.216, loss: 2352.327637\n",
      "Train: step:   7870, time: 0.190, loss: 2011.248047\n",
      "Train: step:   7880, time: 0.193, loss: 3021.695801\n",
      "Train: step:   7890, time: 0.232, loss: 1505.532715\n",
      "Train: step:   7900, time: 0.206, loss: 1377.949585\n",
      "Train: step:   7910, time: 0.203, loss: 367.049133\n",
      "Train: step:   7920, time: 0.201, loss: 3185.046875\n",
      "Train: step:   7930, time: 0.225, loss: 2460.605469\n",
      "Train: step:   7940, time: 0.242, loss: 2269.502930\n",
      "Train: step:   7950, time: 0.177, loss: 2327.541992\n",
      "Train: step:   7960, time: 0.218, loss: 1112.114990\n",
      "Train: step:   7970, time: 0.198, loss: 1740.590820\n",
      "Train: step:   7980, time: 0.258, loss: 550.955383\n",
      "Train: step:   7990, time: 0.186, loss: 2869.345703\n",
      "Train: step:   8000, time: 0.226, loss: 2114.675049\n",
      "Train: step:   8010, time: 0.244, loss: 1895.763550\n",
      "Train: step:   8020, time: 0.190, loss: 1533.992188\n",
      "Train: step:   8030, time: 0.189, loss: 247.456329\n",
      "Train: step:   8040, time: 0.230, loss: 1677.107300\n",
      "Train: step:   8050, time: 0.218, loss: 867.052856\n",
      "Train: step:   8060, time: 0.192, loss: 1520.463257\n",
      "Train: step:   8070, time: 0.251, loss: 488.008026\n",
      "Train: step:   8080, time: 0.254, loss: 4347.753906\n",
      "Train: step:   8090, time: 0.228, loss: 3644.757324\n",
      "Train: step:   8100, time: 0.215, loss: 3060.863281\n",
      "Train: step:   8110, time: 0.229, loss: 2962.173828\n",
      "Train: step:   8120, time: 0.194, loss: 3506.121338\n",
      "Train: step:   8130, time: 0.192, loss: 1253.043091\n",
      "Train: step:   8140, time: 0.189, loss: 2385.494141\n",
      "Train: step:   8150, time: 0.237, loss: 1257.735840\n",
      "Train: step:   8160, time: 0.198, loss: 1547.508911\n",
      "Train: step:   8170, time: 0.228, loss: 981.726746\n",
      "Train: step:   8180, time: 0.227, loss: 1833.844604\n",
      "Train: step:   8190, time: 0.197, loss: 231.595413\n",
      "Train: step:   8200, time: 0.215, loss: 927.155579\n",
      "Train: step:   8210, time: 0.190, loss: 240.964172\n",
      "Train: step:   8220, time: 0.194, loss: 2955.386475\n",
      "Train: step:   8230, time: 0.216, loss: 997.507568\n",
      "Train: step:   8240, time: 0.224, loss: 3049.190186\n",
      "Train: step:   8250, time: 0.194, loss: 974.495300\n",
      "Train: step:   8260, time: 0.193, loss: 1808.569702\n",
      "Train: step:   8270, time: 0.199, loss: 2320.961426\n",
      "Train: step:   8280, time: 0.216, loss: 259.902344\n",
      "Train: step:   8290, time: 0.226, loss: 2680.940918\n",
      "Train: step:   8300, time: 0.226, loss: 3536.957520\n",
      "Train: step:   8310, time: 0.184, loss: 1197.121460\n",
      "Train: step:   8320, time: 0.218, loss: 1865.830811\n",
      "Train: step:   8330, time: 0.188, loss: 1137.972900\n",
      "Train: step:   8340, time: 0.195, loss: 1174.985352\n",
      "Train: step:   8350, time: 0.193, loss: 2160.359863\n",
      "Train: step:   8360, time: 0.194, loss: 3199.607178\n",
      "Train: step:   8370, time: 0.190, loss: 1704.312134\n",
      "Train: step:   8380, time: 0.222, loss: 2066.411377\n",
      "Train: step:   8390, time: 0.198, loss: 420.080963\n",
      "Train: step:   8400, time: 0.233, loss: 1929.868530\n",
      "Train: step:   8410, time: 0.189, loss: 2994.292236\n",
      "Train: step:   8420, time: 0.218, loss: 486.581451\n",
      "Train: step:   8430, time: 0.248, loss: 2211.337158\n",
      "Train: step:   8440, time: 0.245, loss: 3083.298584\n",
      "Train: step:   8450, time: 0.217, loss: 1963.606567\n",
      "Train: step:   8460, time: 0.237, loss: 421.783722\n",
      "Train: step:   8470, time: 0.194, loss: 951.398987\n",
      "Train: step:   8480, time: 0.285, loss: 1890.074585\n",
      "Train: step:   8490, time: 0.249, loss: 1529.382568\n",
      "Train: step:   8500, time: 0.224, loss: 1449.279541\n",
      "Train: step:   8510, time: 0.194, loss: 735.421875\n",
      "Train: step:   8520, time: 0.230, loss: 918.650879\n",
      "Train: step:   8530, time: 0.194, loss: 215.860535\n",
      "Train: step:   8540, time: 0.217, loss: 3197.096924\n",
      "Train: step:   8550, time: 0.192, loss: 3896.864746\n",
      "Train: step:   8560, time: 0.197, loss: 2450.731201\n",
      "Train: step:   8570, time: 0.192, loss: 544.681274\n",
      "Train: step:   8580, time: 0.195, loss: 2291.566650\n",
      "Train: step:   8590, time: 0.212, loss: 415.620789\n",
      "Train: step:   8600, time: 0.194, loss: 1783.756592\n",
      "Train: step:   8610, time: 0.192, loss: 2313.264648\n",
      "Train: step:   8620, time: 0.231, loss: 3491.205811\n",
      "Train: step:   8630, time: 0.199, loss: 1207.683594\n",
      "Train: step:   8640, time: 0.220, loss: 1635.492554\n",
      "Train: step:   8650, time: 0.195, loss: 512.419495\n",
      "Train: step:   8660, time: 0.191, loss: 2595.089600\n",
      "Train: step:   8670, time: 0.189, loss: 2371.259277\n",
      "Train: step:   8680, time: 0.193, loss: 556.887817\n",
      "Train: step:   8690, time: 0.213, loss: 331.649658\n",
      "Train: step:   8700, time: 0.193, loss: 1783.110107\n",
      "Train: step:   8710, time: 0.219, loss: 1470.184082\n",
      "Train: step:   8720, time: 0.216, loss: 1289.754272\n",
      "Train: step:   8730, time: 0.194, loss: 990.828796\n",
      "Train: step:   8740, time: 0.214, loss: 552.551758\n",
      "Train: step:   8750, time: 0.191, loss: 892.823608\n",
      "Train: step:   8760, time: 0.218, loss: 1495.876099\n",
      "Train: step:   8770, time: 0.227, loss: 3466.724854\n",
      "Train: step:   8780, time: 0.199, loss: 2456.995361\n",
      "Train: step:   8790, time: 0.194, loss: 743.538818\n",
      "Train: step:   8800, time: 0.186, loss: 1856.000244\n",
      "Train: step:   8810, time: 0.216, loss: 1923.958740\n",
      "Train: step:   8820, time: 0.229, loss: 1915.875610\n",
      "Train: step:   8830, time: 0.218, loss: 1745.485962\n",
      "Train: step:   8840, time: 0.234, loss: 1124.417603\n",
      "Train: step:   8850, time: 0.186, loss: 1065.174683\n",
      "Train: step:   8860, time: 0.196, loss: 1547.909302\n",
      "Train: step:   8870, time: 0.231, loss: 1902.851440\n",
      "Train: step:   8880, time: 0.189, loss: 2100.150635\n",
      "Train: step:   8890, time: 0.188, loss: 1000.305115\n",
      "Train: step:   8900, time: 0.192, loss: 1310.332520\n",
      "Train: step:   8910, time: 0.230, loss: 3570.933838\n",
      "Train: step:   8920, time: 0.196, loss: 153.018723\n",
      "Train: step:   8930, time: 0.188, loss: 1907.414673\n",
      "Train: step:   8940, time: 0.229, loss: 2111.369385\n",
      "Train: step:   8950, time: 0.189, loss: 150.044601\n",
      "Train: step:   8960, time: 0.186, loss: 1613.812866\n",
      "Train: step:   8970, time: 0.218, loss: 2418.645508\n",
      "Train: step:   8980, time: 0.220, loss: 2378.790039\n",
      "Train: step:   8990, time: 0.238, loss: 2637.722168\n",
      "Train: step:   9000, time: 0.193, loss: 567.533569\n",
      "Train: step:   9010, time: 0.197, loss: 405.545135\n",
      "Train: step:   9020, time: 0.218, loss: 1075.476562\n",
      "Train: step:   9030, time: 0.195, loss: 990.700012\n",
      "Train: step:   9040, time: 0.198, loss: 765.256042\n",
      "Train: step:   9050, time: 0.231, loss: 661.217407\n",
      "Train: step:   9060, time: 0.191, loss: 1936.335205\n",
      "Train: step:   9070, time: 0.190, loss: 2163.334717\n",
      "Train: step:   9080, time: 0.191, loss: 1634.190796\n",
      "Train: step:   9090, time: 0.190, loss: 2848.862061\n",
      "Train: step:   9100, time: 0.223, loss: 2796.061035\n",
      "Train: step:   9110, time: 0.192, loss: 1027.339355\n",
      "Train: step:   9120, time: 0.217, loss: 2650.885010\n",
      "Train: step:   9130, time: 0.228, loss: 3614.906250\n",
      "Train: step:   9140, time: 0.230, loss: 1491.923462\n",
      "Train: step:   9150, time: 0.216, loss: 936.320312\n",
      "Train: step:   9160, time: 0.220, loss: 1712.286621\n",
      "Train: step:   9170, time: 0.218, loss: 1390.059570\n",
      "Train: step:   9180, time: 0.186, loss: 956.094116\n",
      "Train: step:   9190, time: 0.195, loss: 2273.611084\n",
      "Train: step:   9200, time: 0.189, loss: 3618.224365\n",
      "Train: step:   9210, time: 0.250, loss: 1564.683960\n",
      "Train: step:   9220, time: 0.185, loss: 1901.753418\n",
      "Train: step:   9230, time: 0.216, loss: 2676.434082\n",
      "Train: step:   9240, time: 0.193, loss: 2483.260498\n",
      "Train: step:   9250, time: 0.195, loss: 1733.267334\n",
      "Train: step:   9260, time: 0.228, loss: 1187.741943\n",
      "Train: step:   9270, time: 0.241, loss: 826.670471\n",
      "Train: step:   9280, time: 0.191, loss: 1133.254028\n",
      "Train: step:   9290, time: 0.193, loss: 1267.514893\n",
      "Train: step:   9300, time: 0.236, loss: 2050.553711\n",
      "Train: step:   9310, time: 0.192, loss: 1294.738281\n",
      "Train: step:   9320, time: 0.240, loss: 1945.445068\n",
      "Train: step:   9330, time: 0.193, loss: 796.214050\n",
      "Train: step:   9340, time: 0.192, loss: 2490.435303\n",
      "Train: step:   9350, time: 0.222, loss: 5559.565430\n",
      "Train: step:   9360, time: 0.193, loss: 2672.930420\n",
      "Train: step:   9370, time: 0.190, loss: 978.428772\n",
      "Train: step:   9380, time: 0.188, loss: 356.490936\n",
      "Train: step:   9390, time: 0.197, loss: 2718.968750\n",
      "Train: step:   9400, time: 0.188, loss: 2348.537842\n",
      "Train: step:   9410, time: 0.206, loss: 1071.948364\n",
      "Train: step:   9420, time: 0.186, loss: 583.240845\n",
      "Train: step:   9430, time: 0.194, loss: 2376.312744\n",
      "Train: step:   9440, time: 0.218, loss: 388.617126\n",
      "Train: step:   9450, time: 0.219, loss: 907.866394\n",
      "Train: step:   9460, time: 0.217, loss: 925.401428\n",
      "Train: step:   9470, time: 0.199, loss: 2386.947510\n",
      "Train: step:   9480, time: 0.200, loss: 2700.436035\n",
      "Train: step:   9490, time: 0.190, loss: 2472.327881\n",
      "Train: step:   9500, time: 0.189, loss: 2099.239746\n",
      "Train: step:   9510, time: 0.193, loss: 2296.257812\n",
      "Train: step:   9520, time: 0.187, loss: 1816.303223\n",
      "Train: step:   9530, time: 0.226, loss: 2230.063965\n",
      "Train: step:   9540, time: 0.260, loss: 2256.169678\n",
      "Train: step:   9550, time: 0.190, loss: 2618.763428\n",
      "Train: step:   9560, time: 0.186, loss: 2126.743164\n",
      "Train: step:   9570, time: 0.192, loss: 2393.325684\n",
      "Train: step:   9580, time: 0.217, loss: 1484.790405\n",
      "Train: step:   9590, time: 0.193, loss: 2481.669434\n",
      "Train: step:   9600, time: 0.196, loss: 1255.236572\n",
      "Train: step:   9610, time: 0.196, loss: 1711.560181\n",
      "Train: step:   9620, time: 0.192, loss: 1848.409302\n",
      "Train: step:   9630, time: 0.217, loss: 2331.847412\n",
      "Train: step:   9640, time: 0.216, loss: 2930.513184\n",
      "Train: step:   9650, time: 0.194, loss: 528.803894\n",
      "Train: step:   9660, time: 0.194, loss: 1489.300659\n",
      "Train: step:   9670, time: 0.186, loss: 2990.783203\n",
      "Train: step:   9680, time: 0.191, loss: 352.059631\n",
      "Train: step:   9690, time: 0.184, loss: 1206.228760\n",
      "Train: step:   9700, time: 0.196, loss: 1425.222534\n",
      "Train: step:   9710, time: 0.193, loss: 2488.262939\n",
      "Train: step:   9720, time: 0.225, loss: 2741.372803\n",
      "Train: step:   9730, time: 0.228, loss: 783.570557\n",
      "Train: step:   9740, time: 0.192, loss: 1558.781494\n",
      "Train: step:   9750, time: 0.187, loss: 1693.721558\n",
      "Train: step:   9760, time: 0.189, loss: 1725.669067\n",
      "Train: step:   9770, time: 0.190, loss: 1715.158936\n",
      "Train: step:   9780, time: 0.220, loss: 1894.536621\n",
      "Train: step:   9790, time: 0.227, loss: 2084.489258\n",
      "Train: step:   9800, time: 0.193, loss: 1004.555908\n",
      "Train: step:   9810, time: 0.230, loss: 697.945068\n",
      "Train: step:   9820, time: 0.192, loss: 1562.118042\n",
      "Train: step:   9830, time: 0.201, loss: 3155.693359\n",
      "Train: step:   9840, time: 0.215, loss: 1588.456177\n",
      "Train: step:   9850, time: 0.190, loss: 1701.921753\n",
      "Train: step:   9860, time: 0.220, loss: 670.000793\n",
      "Train: step:   9870, time: 0.189, loss: 2787.158203\n",
      "Train: step:   9880, time: 0.196, loss: 890.844849\n",
      "Train: step:   9890, time: 0.198, loss: 1665.535645\n",
      "Train: step:   9900, time: 0.188, loss: 803.854065\n",
      "Train: step:   9910, time: 0.195, loss: 1615.150757\n",
      "Train: step:   9920, time: 0.190, loss: 3265.523682\n",
      "Train: step:   9930, time: 0.196, loss: 422.463684\n",
      "Train: step:   9940, time: 0.229, loss: 2229.766602\n",
      "Train: step:   9950, time: 0.225, loss: 1037.283203\n",
      "Train: step:   9960, time: 0.202, loss: 2809.806396\n",
      "Train: step:   9970, time: 0.188, loss: 2781.742188\n",
      "Train: step:   9980, time: 0.184, loss: 1787.744385\n",
      "Train: step:   9990, time: 0.217, loss: 1672.088013\n",
      "Train: step:  10000, time: 0.223, loss: 870.083801\n",
      "Train: step:  10010, time: 0.193, loss: 1439.734741\n",
      "Train: step:  10020, time: 0.230, loss: 1653.356567\n",
      "Train: step:  10030, time: 0.223, loss: 1641.703247\n",
      "Train: step:  10040, time: 0.230, loss: 2266.923340\n",
      "Train: step:  10050, time: 0.192, loss: 1791.975342\n",
      "Train: step:  10060, time: 0.209, loss: 608.217896\n",
      "Train: step:  10070, time: 0.184, loss: 832.929260\n",
      "Train: step:  10080, time: 0.197, loss: 382.284698\n",
      "Train: step:  10090, time: 0.195, loss: 2264.535645\n",
      "Train: step:  10100, time: 0.190, loss: 3773.222656\n",
      "Train: step:  10110, time: 0.227, loss: 1318.178467\n",
      "Train: step:  10120, time: 0.184, loss: 339.901947\n",
      "Train: step:  10130, time: 0.190, loss: 1930.116699\n",
      "Train: step:  10140, time: 0.192, loss: 712.586304\n",
      "Train: step:  10150, time: 0.190, loss: 1955.923706\n",
      "Train: step:  10160, time: 0.223, loss: 705.894897\n",
      "Train: step:  10170, time: 0.190, loss: 568.143494\n",
      "Train: step:  10180, time: 0.188, loss: 1359.822754\n",
      "Train: step:  10190, time: 0.190, loss: 3553.154785\n",
      "Train: step:  10200, time: 0.216, loss: 3527.401123\n",
      "Train: step:  10210, time: 0.192, loss: 652.252563\n",
      "Train: step:  10220, time: 0.191, loss: 3437.375000\n",
      "Train: step:  10230, time: 0.227, loss: 750.450684\n",
      "Train: step:  10240, time: 0.261, loss: 470.180267\n",
      "Train: step:  10250, time: 0.189, loss: 763.642700\n",
      "Train: step:  10260, time: 0.188, loss: 1708.159546\n",
      "Train: step:  10270, time: 0.217, loss: 487.669067\n",
      "Train: step:  10280, time: 0.197, loss: 4687.191895\n",
      "Train: step:  10290, time: 0.185, loss: 935.411682\n",
      "Train: step:  10300, time: 0.187, loss: 3433.931396\n",
      "Train: step:  10310, time: 0.192, loss: 1920.947266\n",
      "Train: step:  10320, time: 0.190, loss: 480.239075\n",
      "Train: step:  10330, time: 0.217, loss: 1113.396118\n",
      "Train: step:  10340, time: 0.194, loss: 1881.172119\n",
      "Train: step:  10350, time: 0.196, loss: 257.179230\n",
      "Train: step:  10360, time: 0.199, loss: 1187.308228\n",
      "Train: step:  10370, time: 0.194, loss: 2283.775879\n",
      "Train: step:  10380, time: 0.206, loss: 870.935669\n",
      "Train: step:  10390, time: 0.186, loss: 942.983032\n",
      "Train: step:  10400, time: 0.184, loss: 221.912094\n",
      "Train: step:  10410, time: 0.225, loss: 2242.133545\n",
      "Train: step:  10420, time: 0.215, loss: 387.658325\n",
      "Train: step:  10430, time: 0.186, loss: 1763.726318\n",
      "Train: step:  10440, time: 0.234, loss: 1398.966309\n",
      "Train: step:  10450, time: 0.239, loss: 1773.900269\n",
      "Train: step:  10460, time: 0.194, loss: 1263.985352\n",
      "Train: step:  10470, time: 0.194, loss: 2931.609619\n",
      "Train: step:  10480, time: 0.193, loss: 540.922729\n",
      "Train: step:  10490, time: 0.198, loss: 2419.009766\n",
      "Train: step:  10500, time: 0.239, loss: 3106.022705\n",
      "Train: step:  10510, time: 0.227, loss: 979.828003\n",
      "Train: step:  10520, time: 0.218, loss: 3318.683350\n",
      "Train: step:  10530, time: 0.191, loss: 3149.075928\n",
      "Train: step:  10540, time: 0.190, loss: 575.188416\n",
      "Train: step:  10550, time: 0.209, loss: 3608.562012\n",
      "Train: step:  10560, time: 0.252, loss: 860.924255\n",
      "Train: step:  10570, time: 0.238, loss: 1477.764771\n",
      "Train: step:  10580, time: 0.217, loss: 2001.293091\n",
      "Train: step:  10590, time: 0.213, loss: 941.140930\n",
      "Train: step:  10600, time: 0.195, loss: 2137.751221\n",
      "Train: step:  10610, time: 0.195, loss: 1415.060913\n",
      "Train: step:  10620, time: 0.227, loss: 1799.054688\n",
      "Train: step:  10630, time: 0.191, loss: 2625.568115\n",
      "Train: step:  10640, time: 0.192, loss: 1487.913452\n",
      "Train: step:  10650, time: 0.226, loss: 3246.449219\n",
      "Train: step:  10660, time: 0.198, loss: 1068.933472\n",
      "Train: step:  10670, time: 0.197, loss: 1836.729004\n",
      "Train: step:  10680, time: 0.196, loss: 1144.745850\n",
      "Train: step:  10690, time: 0.203, loss: 4311.810547\n",
      "Train: step:  10700, time: 0.193, loss: 1913.921631\n",
      "Train: step:  10710, time: 0.189, loss: 517.138428\n",
      "Train: step:  10720, time: 0.194, loss: 2414.715576\n",
      "Train: step:  10730, time: 0.194, loss: 2178.060791\n",
      "Train: step:  10740, time: 0.195, loss: 2541.928467\n",
      "Train: step:  10750, time: 0.199, loss: 1463.560425\n",
      "Train: step:  10760, time: 0.184, loss: 1551.383423\n",
      "Train: step:  10770, time: 0.191, loss: 1870.904785\n",
      "Train: step:  10780, time: 0.198, loss: 1545.424683\n",
      "Train: step:  10790, time: 0.188, loss: 1002.949585\n",
      "Train: step:  10800, time: 0.190, loss: 2236.620850\n",
      "Train: step:  10810, time: 0.194, loss: 416.460114\n",
      "Train: step:  10820, time: 0.248, loss: 1286.467407\n",
      "Train: step:  10830, time: 0.247, loss: 1608.529419\n",
      "Train: step:  10840, time: 0.228, loss: 3620.071289\n",
      "Train: step:  10850, time: 0.217, loss: 4654.250977\n",
      "Train: step:  10860, time: 0.200, loss: 2454.704834\n",
      "Train: step:  10870, time: 0.201, loss: 2576.015381\n",
      "Train: step:  10880, time: 0.199, loss: 2138.938232\n",
      "Train: step:  10890, time: 0.228, loss: 1234.440186\n",
      "Train: step:  10900, time: 0.207, loss: 652.847290\n",
      "Train: step:  10910, time: 0.207, loss: 2801.264160\n",
      "Train: step:  10920, time: 0.206, loss: 2834.523193\n",
      "Train: step:  10930, time: 0.228, loss: 1571.992798\n",
      "Train: step:  10940, time: 0.202, loss: 1428.322632\n",
      "Train: step:  10950, time: 0.203, loss: 1486.292969\n",
      "Train: step:  10960, time: 0.201, loss: 2163.629639\n",
      "Train: step:  10970, time: 0.202, loss: 1694.738770\n",
      "Train: step:  10980, time: 0.198, loss: 752.321411\n",
      "Train: step:  10990, time: 0.246, loss: 3696.754395\n",
      "Train: step:  11000, time: 0.205, loss: 1553.920044\n",
      "Train: step:  11010, time: 0.231, loss: 1385.817993\n",
      "Train: step:  11020, time: 0.238, loss: 2176.621826\n",
      "Train: step:  11030, time: 0.186, loss: 1669.450928\n",
      "Train: step:  11040, time: 0.225, loss: 2061.792236\n",
      "Train: step:  11050, time: 0.242, loss: 2437.018799\n",
      "Train: step:  11060, time: 0.227, loss: 2955.322998\n",
      "Train: step:  11070, time: 0.202, loss: 2584.712891\n",
      "Train: step:  11080, time: 0.236, loss: 353.142883\n",
      "Train: step:  11090, time: 0.192, loss: 1369.735596\n",
      "Train: step:  11100, time: 0.260, loss: 2611.166992\n",
      "Train: step:  11110, time: 0.224, loss: 1587.934326\n",
      "Train: step:  11120, time: 0.218, loss: 1178.530518\n",
      "Train: step:  11130, time: 0.196, loss: 3433.404785\n",
      "Train: step:  11140, time: 0.195, loss: 1409.118530\n",
      "Train: step:  11150, time: 0.226, loss: 681.181519\n",
      "Train: step:  11160, time: 0.219, loss: 3365.247803\n",
      "Train: step:  11170, time: 0.192, loss: 1064.872192\n",
      "Train: step:  11180, time: 0.195, loss: 2266.494873\n",
      "Train: step:  11190, time: 0.192, loss: 1945.759766\n",
      "Train: step:  11200, time: 0.196, loss: 789.503357\n",
      "Train: step:  11210, time: 0.191, loss: 313.891205\n",
      "Train: step:  11220, time: 0.218, loss: 332.012329\n",
      "Train: step:  11230, time: 0.196, loss: 2257.550293\n",
      "Train: step:  11240, time: 0.192, loss: 519.462708\n",
      "Train: step:  11250, time: 0.218, loss: 1625.119263\n",
      "Train: step:  11260, time: 0.219, loss: 1370.816772\n",
      "Train: step:  11270, time: 0.233, loss: 2068.062744\n",
      "Train: step:  11280, time: 0.226, loss: 3585.790527\n",
      "Train: step:  11290, time: 0.184, loss: 1385.636841\n",
      "Train: step:  11300, time: 0.191, loss: 1499.325195\n",
      "Train: step:  11310, time: 0.194, loss: 2584.341797\n",
      "Train: step:  11320, time: 0.200, loss: 2171.108398\n",
      "Train: step:  11330, time: 0.192, loss: 3003.979736\n",
      "Train: step:  11340, time: 0.189, loss: 930.921265\n",
      "Train: step:  11350, time: 0.212, loss: 2353.769287\n",
      "Train: step:  11360, time: 0.194, loss: 1220.083618\n",
      "Train: step:  11370, time: 0.198, loss: 826.180420\n",
      "Train: step:  11380, time: 0.206, loss: 1685.324707\n",
      "Train: step:  11390, time: 0.231, loss: 852.404663\n",
      "Train: step:  11400, time: 0.220, loss: 1562.272461\n",
      "Train: step:  11410, time: 0.188, loss: 510.634277\n",
      "Train: step:  11420, time: 0.220, loss: 3835.660645\n",
      "Train: step:  11430, time: 0.228, loss: 252.328140\n",
      "Train: step:  11440, time: 0.216, loss: 2688.759277\n",
      "Train: step:  11450, time: 0.220, loss: 1910.939331\n",
      "Train: step:  11460, time: 0.186, loss: 1389.854614\n",
      "Train: step:  11470, time: 0.238, loss: 426.659637\n",
      "Train: step:  11480, time: 0.218, loss: 2533.837646\n",
      "Train: step:  11490, time: 0.219, loss: 1588.017700\n",
      "Train: step:  11500, time: 0.217, loss: 2243.676025\n",
      "Train: step:  11510, time: 0.220, loss: 1105.317261\n",
      "Train: step:  11520, time: 0.246, loss: 2639.150391\n",
      "Train: step:  11530, time: 0.214, loss: 1662.266357\n",
      "Train: step:  11540, time: 0.198, loss: 1494.672119\n",
      "Train: step:  11550, time: 0.236, loss: 2110.653809\n",
      "Train: step:  11560, time: 0.197, loss: 1771.319458\n",
      "Train: step:  11570, time: 0.186, loss: 345.202759\n",
      "Train: step:  11580, time: 0.204, loss: 459.294525\n",
      "Train: step:  11590, time: 0.189, loss: 835.791809\n",
      "Train: step:  11600, time: 0.190, loss: 1148.654785\n",
      "Train: step:  11610, time: 0.194, loss: 1829.972290\n",
      "Train: step:  11620, time: 0.189, loss: 2671.651611\n",
      "Train: step:  11630, time: 0.216, loss: 2638.793213\n",
      "Train: step:  11640, time: 0.191, loss: 2316.129639\n",
      "Train: step:  11650, time: 0.202, loss: 2612.316162\n",
      "Train: step:  11660, time: 0.254, loss: 2560.716309\n",
      "Train: step:  11670, time: 0.233, loss: 2067.780029\n",
      "Train: step:  11680, time: 0.219, loss: 378.266602\n",
      "Train: step:  11690, time: 0.183, loss: 2302.220215\n",
      "Train: step:  11700, time: 0.217, loss: 819.327332\n",
      "Train: step:  11710, time: 0.196, loss: 2405.906006\n",
      "Train: step:  11720, time: 0.218, loss: 1822.219727\n",
      "Train: step:  11730, time: 0.191, loss: 2541.347656\n",
      "Train: step:  11740, time: 0.219, loss: 1008.637329\n",
      "Train: step:  11750, time: 0.189, loss: 1351.718872\n",
      "Train: step:  11760, time: 0.189, loss: 1900.222900\n",
      "Train: step:  11770, time: 0.199, loss: 687.657837\n",
      "Train: step:  11780, time: 0.229, loss: 585.168457\n",
      "Train: step:  11790, time: 0.216, loss: 701.128906\n",
      "Train: step:  11800, time: 0.186, loss: 2357.202637\n",
      "Train: step:  11810, time: 0.217, loss: 1801.024048\n",
      "Train: step:  11820, time: 0.216, loss: 2526.709473\n",
      "Train: step:  11830, time: 0.190, loss: 1986.729370\n",
      "Train: step:  11840, time: 0.195, loss: 2517.192383\n",
      "Train: step:  11850, time: 0.232, loss: 1393.474731\n",
      "Train: step:  11860, time: 0.218, loss: 1424.197632\n",
      "Train: step:  11870, time: 0.217, loss: 1573.923950\n",
      "Train: step:  11880, time: 0.188, loss: 2394.700928\n",
      "Train: step:  11890, time: 0.189, loss: 2862.989502\n",
      "Train: step:  11900, time: 0.190, loss: 1612.947388\n",
      "Train: step:  11910, time: 0.192, loss: 2441.309082\n",
      "Train: step:  11920, time: 0.185, loss: 1762.593262\n",
      "Train: step:  11930, time: 0.188, loss: 969.753784\n",
      "Train: step:  11940, time: 0.206, loss: 1675.059082\n",
      "Train: step:  11950, time: 0.217, loss: 234.816010\n",
      "Train: step:  11960, time: 0.188, loss: 1711.271484\n",
      "Train: step:  11970, time: 0.217, loss: 2551.100342\n",
      "Train: step:  11980, time: 0.187, loss: 3979.555664\n",
      "Train: step:  11990, time: 0.189, loss: 2908.265137\n",
      "Train: step:  12000, time: 0.218, loss: 1594.657104\n",
      "Train: step:  12010, time: 0.202, loss: 653.522217\n",
      "Train: step:  12020, time: 0.226, loss: 1745.190796\n",
      "Train: step:  12030, time: 0.184, loss: 3116.973633\n",
      "Train: step:  12040, time: 0.193, loss: 2653.233398\n",
      "Train: step:  12050, time: 0.192, loss: 1726.032349\n",
      "Train: step:  12060, time: 0.192, loss: 1686.018677\n",
      "Train: step:  12070, time: 0.199, loss: 2091.905029\n",
      "Train: step:  12080, time: 0.189, loss: 2990.951904\n",
      "Train: step:  12090, time: 0.215, loss: 755.586487\n",
      "Train: step:  12100, time: 0.188, loss: 841.417664\n",
      "Train: step:  12110, time: 0.198, loss: 2289.485840\n",
      "Train: step:  12120, time: 0.196, loss: 2208.567139\n",
      "Train: step:  12130, time: 0.189, loss: 1613.849487\n",
      "Train: step:  12140, time: 0.187, loss: 941.552429\n",
      "Train: step:  12150, time: 0.215, loss: 658.559631\n",
      "Train: step:  12160, time: 0.194, loss: 1597.028687\n",
      "Train: step:  12170, time: 0.189, loss: 2220.633789\n",
      "Train: step:  12180, time: 0.190, loss: 3254.254395\n",
      "Train: step:  12190, time: 0.222, loss: 1799.610596\n",
      "Train: step:  12200, time: 0.195, loss: 2424.979248\n",
      "Train: step:  12210, time: 0.192, loss: 260.801758\n",
      "Train: step:  12220, time: 0.189, loss: 344.597290\n",
      "Train: step:  12230, time: 0.212, loss: 2074.813232\n",
      "Train: step:  12240, time: 0.187, loss: 526.137207\n",
      "Train: step:  12250, time: 0.193, loss: 266.708710\n",
      "Train: step:  12260, time: 0.196, loss: 1671.891846\n",
      "Train: step:  12270, time: 0.191, loss: 2159.156250\n",
      "Train: step:  12280, time: 0.193, loss: 1751.603516\n",
      "Train: step:  12290, time: 0.204, loss: 3599.731201\n",
      "Train: step:  12300, time: 0.188, loss: 943.027039\n",
      "Train: step:  12310, time: 0.201, loss: 1682.686279\n",
      "Train: step:  12320, time: 0.184, loss: 2959.110107\n",
      "Train: step:  12330, time: 0.189, loss: 3698.558594\n",
      "Train: step:  12340, time: 0.193, loss: 1919.672119\n",
      "Train: step:  12350, time: 0.206, loss: 3308.677979\n",
      "Train: step:  12360, time: 0.222, loss: 1132.836792\n",
      "Train: step:  12370, time: 0.187, loss: 1718.253174\n",
      "Train: step:  12380, time: 0.217, loss: 704.681274\n",
      "Train: step:  12390, time: 0.196, loss: 956.366455\n",
      "Train: step:  12400, time: 0.206, loss: 2209.440430\n",
      "Train: step:  12410, time: 0.216, loss: 2400.204590\n",
      "Train: step:  12420, time: 0.189, loss: 3167.395508\n",
      "Train: step:  12430, time: 0.210, loss: 332.516846\n",
      "Train: step:  12440, time: 0.217, loss: 2764.648193\n",
      "Train: step:  12450, time: 0.195, loss: 1653.624512\n",
      "Train: step:  12460, time: 0.192, loss: 2707.596191\n",
      "Train: step:  12470, time: 0.192, loss: 1597.282837\n",
      "Train: step:  12480, time: 0.202, loss: 1081.669189\n",
      "Train: step:  12490, time: 0.194, loss: 2047.118652\n",
      "Train: step:  12500, time: 0.235, loss: 2407.142334\n",
      "Train: step:  12510, time: 0.218, loss: 486.132294\n",
      "Train: step:  12520, time: 0.188, loss: 2770.635498\n",
      "Train: step:  12530, time: 0.214, loss: 237.098587\n",
      "Train: step:  12540, time: 0.191, loss: 2046.408325\n",
      "Train: step:  12550, time: 0.192, loss: 974.335449\n",
      "Train: step:  12560, time: 0.198, loss: 2147.099365\n",
      "Train: step:  12570, time: 0.183, loss: 1688.058105\n",
      "Train: step:  12580, time: 0.193, loss: 714.326050\n",
      "Train: step:  12590, time: 0.190, loss: 723.623169\n",
      "Train: step:  12600, time: 0.204, loss: 1641.577759\n",
      "Train: step:  12610, time: 0.196, loss: 1616.083252\n",
      "Train: step:  12620, time: 0.229, loss: 652.116211\n",
      "Train: step:  12630, time: 0.191, loss: 1374.769653\n",
      "Train: step:  12640, time: 0.194, loss: 2765.013672\n",
      "Train: step:  12650, time: 0.193, loss: 2044.834106\n",
      "Train: step:  12660, time: 0.209, loss: 276.085205\n",
      "Train: step:  12670, time: 0.199, loss: 1760.297974\n",
      "Train: step:  12680, time: 0.189, loss: 1996.162964\n",
      "Train: step:  12690, time: 0.186, loss: 1678.994263\n",
      "Train: step:  12700, time: 0.217, loss: 4283.954590\n",
      "Train: step:  12710, time: 0.184, loss: 2534.826172\n",
      "Train: step:  12720, time: 0.222, loss: 586.128418\n",
      "Train: step:  12730, time: 0.188, loss: 797.783813\n",
      "Train: step:  12740, time: 0.233, loss: 610.099548\n",
      "Train: step:  12750, time: 0.190, loss: 2156.344238\n",
      "Train: step:  12760, time: 0.192, loss: 1955.469604\n",
      "Train: step:  12770, time: 0.196, loss: 1316.996094\n",
      "Train: step:  12780, time: 0.190, loss: 2151.397949\n",
      "Train: step:  12790, time: 0.194, loss: 2263.364014\n",
      "Train: step:  12800, time: 0.190, loss: 2577.414307\n",
      "Train: step:  12810, time: 0.192, loss: 2439.206299\n",
      "Train: step:  12820, time: 0.220, loss: 1520.933838\n",
      "Train: step:  12830, time: 0.192, loss: 3447.042480\n",
      "Train: step:  12840, time: 0.219, loss: 2071.736572\n",
      "Train: step:  12850, time: 0.214, loss: 1238.726196\n",
      "Train: step:  12860, time: 0.201, loss: 2024.170288\n",
      "Train: step:  12870, time: 0.194, loss: 1064.509766\n",
      "Train: step:  12880, time: 0.190, loss: 1958.281738\n",
      "Train: step:  12890, time: 0.191, loss: 849.702759\n",
      "Train: step:  12900, time: 0.228, loss: 1335.363525\n",
      "Train: step:  12910, time: 0.190, loss: 1443.518921\n",
      "Train: step:  12920, time: 0.247, loss: 4520.368164\n",
      "Train: step:  12930, time: 0.218, loss: 1490.039429\n",
      "Train: step:  12940, time: 0.216, loss: 2596.062988\n",
      "Train: step:  12950, time: 0.190, loss: 2298.958984\n",
      "Train: step:  12960, time: 0.187, loss: 1252.879028\n",
      "Train: step:  12970, time: 0.190, loss: 1455.707886\n",
      "Train: step:  12980, time: 0.200, loss: 2556.649658\n",
      "Train: step:  12990, time: 0.220, loss: 1636.042236\n",
      "Train: step:  13000, time: 0.187, loss: 2409.514648\n",
      "Train: step:  13010, time: 0.183, loss: 710.728149\n",
      "Train: step:  13020, time: 0.181, loss: 436.604950\n",
      "Train: step:  13030, time: 0.195, loss: 2415.128174\n",
      "Train: step:  13040, time: 0.191, loss: 1432.472412\n",
      "Train: step:  13050, time: 0.195, loss: 2252.487061\n",
      "Train: step:  13060, time: 0.216, loss: 1330.588623\n",
      "Train: step:  13070, time: 0.229, loss: 2195.580078\n",
      "Train: step:  13080, time: 0.191, loss: 3179.641113\n",
      "Train: step:  13090, time: 0.189, loss: 1040.355713\n",
      "Train: step:  13100, time: 0.195, loss: 2658.658203\n",
      "Train: step:  13110, time: 0.202, loss: 2037.846802\n",
      "Train: step:  13120, time: 0.237, loss: 2428.306152\n",
      "Train: step:  13130, time: 0.238, loss: 1103.782837\n",
      "Train: step:  13140, time: 0.192, loss: 1678.561279\n",
      "Train: step:  13150, time: 0.195, loss: 1498.412109\n",
      "Train: step:  13160, time: 0.189, loss: 1021.999695\n",
      "Train: step:  13170, time: 0.189, loss: 778.329834\n",
      "Train: step:  13180, time: 0.194, loss: 2253.303711\n",
      "Train: step:  13190, time: 0.202, loss: 1186.186523\n",
      "Train: step:  13200, time: 0.190, loss: 1489.206299\n",
      "Train: step:  13210, time: 0.237, loss: 2035.064209\n",
      "Train: step:  13220, time: 0.186, loss: 2004.458008\n",
      "Train: step:  13230, time: 0.194, loss: 2358.718262\n",
      "Train: step:  13240, time: 0.193, loss: 1490.102173\n",
      "Train: step:  13250, time: 0.192, loss: 390.870239\n",
      "Train: step:  13260, time: 0.230, loss: 1751.908813\n",
      "Train: step:  13270, time: 0.186, loss: 1892.543579\n",
      "Train: step:  13280, time: 0.191, loss: 1579.599487\n",
      "Train: step:  13290, time: 0.186, loss: 1801.613770\n",
      "Train: step:  13300, time: 0.192, loss: 1020.698364\n",
      "Train: step:  13310, time: 0.197, loss: 3218.979980\n",
      "Train: step:  13320, time: 0.207, loss: 1286.307007\n",
      "Train: step:  13330, time: 0.188, loss: 1752.471680\n",
      "Train: step:  13340, time: 0.188, loss: 783.618835\n",
      "Train: step:  13350, time: 0.221, loss: 1059.997192\n",
      "Train: step:  13360, time: 0.192, loss: 602.766235\n",
      "Train: step:  13370, time: 0.193, loss: 536.660767\n",
      "Train: step:  13380, time: 0.200, loss: 2615.768311\n",
      "Train: step:  13390, time: 0.193, loss: 3130.913330\n",
      "Train: step:  13400, time: 0.200, loss: 687.601929\n",
      "Train: step:  13410, time: 0.199, loss: 1719.569702\n",
      "Train: step:  13420, time: 0.195, loss: 2078.207764\n",
      "Train: step:  13430, time: 0.235, loss: 1578.788330\n",
      "Train: step:  13440, time: 0.191, loss: 2225.312256\n",
      "Train: step:  13450, time: 0.191, loss: 1780.558105\n",
      "Train: step:  13460, time: 0.183, loss: 1689.699219\n",
      "Train: step:  13470, time: 0.190, loss: 2751.939453\n",
      "Train: step:  13480, time: 0.192, loss: 2226.732910\n",
      "Train: step:  13490, time: 0.190, loss: 316.795654\n",
      "Train: step:  13500, time: 0.187, loss: 1587.029663\n",
      "Train: step:  13510, time: 0.181, loss: 1628.357056\n",
      "Train: step:  13520, time: 0.207, loss: 287.658936\n",
      "Train: step:  13530, time: 0.194, loss: 3960.434814\n",
      "Train: step:  13540, time: 0.182, loss: 373.236664\n",
      "Train: step:  13550, time: 0.230, loss: 1622.360718\n",
      "Train: step:  13560, time: 0.191, loss: 783.155090\n",
      "Train: step:  13570, time: 0.197, loss: 3027.653564\n",
      "Train: step:  13580, time: 0.213, loss: 1557.431519\n",
      "Train: step:  13590, time: 0.200, loss: 805.981323\n",
      "Train: step:  13600, time: 0.193, loss: 375.499359\n",
      "Train: step:  13610, time: 0.186, loss: 1852.242798\n",
      "Train: step:  13620, time: 0.198, loss: 2797.464600\n",
      "Train: step:  13630, time: 0.229, loss: 1324.965942\n",
      "Train: step:  13640, time: 0.187, loss: 2032.209839\n",
      "Train: step:  13650, time: 0.189, loss: 3219.569092\n",
      "Train: step:  13660, time: 0.195, loss: 845.507141\n",
      "Train: step:  13670, time: 0.226, loss: 1194.333374\n",
      "Train: step:  13680, time: 0.188, loss: 3360.339600\n",
      "Train: step:  13690, time: 0.225, loss: 1538.003174\n",
      "Train: step:  13700, time: 0.189, loss: 1487.166504\n",
      "Train: step:  13710, time: 0.237, loss: 892.874084\n",
      "Train: step:  13720, time: 0.218, loss: 921.176453\n",
      "Train: step:  13730, time: 0.195, loss: 515.705688\n",
      "Train: step:  13740, time: 0.196, loss: 1949.116455\n",
      "Train: step:  13750, time: 0.193, loss: 1804.618652\n",
      "Train: step:  13760, time: 0.218, loss: 1883.531494\n",
      "Train: step:  13770, time: 0.193, loss: 2522.980713\n",
      "Train: step:  13780, time: 0.231, loss: 1096.804199\n",
      "Train: step:  13790, time: 0.218, loss: 910.489441\n",
      "Train: step:  13800, time: 0.189, loss: 2176.269287\n",
      "Train: step:  13810, time: 0.194, loss: 1137.107544\n",
      "Train: step:  13820, time: 0.222, loss: 393.394714\n",
      "Train: step:  13830, time: 0.190, loss: 2196.159180\n",
      "Train: step:  13840, time: 0.231, loss: 1305.571289\n",
      "Train: step:  13850, time: 0.204, loss: 3270.191895\n",
      "Train: step:  13860, time: 0.187, loss: 2538.447754\n",
      "Train: step:  13870, time: 0.192, loss: 2801.689453\n",
      "Train: step:  13880, time: 0.249, loss: 1385.551025\n",
      "Train: step:  13890, time: 0.235, loss: 3151.042480\n",
      "Train: step:  13900, time: 0.198, loss: 491.524933\n",
      "Train: step:  13910, time: 0.221, loss: 2689.186035\n",
      "Train: step:  13920, time: 0.201, loss: 2155.309570\n",
      "Train: step:  13930, time: 0.188, loss: 1923.305664\n",
      "Train: step:  13940, time: 0.196, loss: 4220.580078\n",
      "Train: step:  13950, time: 0.184, loss: 2976.095703\n",
      "Train: step:  13960, time: 0.196, loss: 2525.740967\n",
      "Train: step:  13970, time: 0.214, loss: 1132.605713\n",
      "Train: step:  13980, time: 0.195, loss: 591.548767\n",
      "Train: step:  13990, time: 0.237, loss: 1050.027222\n",
      "Train: step:  14000, time: 0.240, loss: 4494.965820\n",
      "Train: step:  14010, time: 0.250, loss: 2822.626465\n",
      "Train: step:  14020, time: 0.190, loss: 1618.245728\n",
      "Train: step:  14030, time: 0.190, loss: 1219.565674\n",
      "Train: step:  14040, time: 0.197, loss: 1203.207031\n",
      "Train: step:  14050, time: 0.196, loss: 3024.467285\n",
      "Train: step:  14060, time: 0.193, loss: 1138.901489\n",
      "Train: step:  14070, time: 0.221, loss: 2831.635010\n",
      "Train: step:  14080, time: 0.194, loss: 1088.504150\n",
      "Train: step:  14090, time: 0.190, loss: 3522.311279\n",
      "Train: step:  14100, time: 0.187, loss: 1127.504272\n",
      "Train: step:  14110, time: 0.185, loss: 774.112122\n",
      "Train: step:  14120, time: 0.193, loss: 2753.205811\n",
      "Train: step:  14130, time: 0.196, loss: 1724.912109\n",
      "Train: step:  14140, time: 0.198, loss: 2263.160156\n",
      "Train: step:  14150, time: 0.204, loss: 2816.211426\n",
      "Train: step:  14160, time: 0.231, loss: 2684.640869\n",
      "Train: step:  14170, time: 0.195, loss: 870.303528\n",
      "Train: step:  14180, time: 0.248, loss: 2147.630615\n",
      "Train: step:  14190, time: 0.194, loss: 1516.887817\n",
      "Train: step:  14200, time: 0.218, loss: 2288.456787\n",
      "Train: step:  14210, time: 0.238, loss: 751.604797\n",
      "Train: step:  14220, time: 0.200, loss: 618.912903\n",
      "Train: step:  14230, time: 0.231, loss: 2277.446777\n",
      "Train: step:  14240, time: 0.190, loss: 1110.159424\n",
      "Train: step:  14250, time: 0.192, loss: 1048.738403\n",
      "Train: step:  14260, time: 0.201, loss: 547.962341\n",
      "Train: step:  14270, time: 0.195, loss: 2307.176514\n",
      "Train: step:  14280, time: 0.195, loss: 1615.526367\n",
      "Train: step:  14290, time: 0.193, loss: 472.246368\n",
      "Train: step:  14300, time: 0.192, loss: 1553.139648\n",
      "Train: step:  14310, time: 0.192, loss: 1173.994995\n",
      "Train: step:  14320, time: 0.206, loss: 2478.891357\n",
      "Train: step:  14330, time: 0.194, loss: 736.584839\n",
      "Train: step:  14340, time: 0.199, loss: 2772.588379\n",
      "Train: step:  14350, time: 0.189, loss: 3058.008789\n",
      "Train: step:  14360, time: 0.187, loss: 2544.290039\n",
      "Train: step:  14370, time: 0.216, loss: 1418.214111\n",
      "Train: step:  14380, time: 0.187, loss: 1871.402954\n",
      "Train: step:  14390, time: 0.190, loss: 918.912415\n",
      "Train: step:  14400, time: 0.216, loss: 2139.840332\n",
      "Train: step:  14410, time: 0.197, loss: 2825.147949\n",
      "Train: step:  14420, time: 0.196, loss: 1636.354126\n",
      "Train: step:  14430, time: 0.250, loss: 1546.706055\n",
      "Train: step:  14440, time: 0.195, loss: 2329.651855\n",
      "Train: step:  14450, time: 0.191, loss: 645.108215\n",
      "Train: step:  14460, time: 0.192, loss: 818.134766\n",
      "Train: step:  14470, time: 0.194, loss: 1133.089111\n",
      "Train: step:  14480, time: 0.219, loss: 2432.712402\n",
      "Train: step:  14490, time: 0.222, loss: 1688.595093\n",
      "Train: step:  14500, time: 0.230, loss: 623.344788\n",
      "Train: step:  14510, time: 0.213, loss: 1756.215210\n",
      "Train: step:  14520, time: 0.217, loss: 2282.248535\n",
      "Train: step:  14530, time: 0.216, loss: 955.856201\n",
      "Train: step:  14540, time: 0.223, loss: 2762.207031\n",
      "Train: step:  14550, time: 0.192, loss: 3104.700439\n",
      "Train: step:  14560, time: 0.191, loss: 2350.429688\n",
      "Train: step:  14570, time: 0.197, loss: 2414.353760\n",
      "Train: step:  14580, time: 0.186, loss: 2871.144775\n",
      "Train: step:  14590, time: 0.202, loss: 2162.770264\n",
      "Train: step:  14600, time: 0.199, loss: 2237.424805\n",
      "Train: step:  14610, time: 0.232, loss: 706.800110\n",
      "Train: step:  14620, time: 0.210, loss: 611.460632\n",
      "Train: step:  14630, time: 0.194, loss: 246.490402\n",
      "Train: step:  14640, time: 0.196, loss: 549.102844\n",
      "Train: step:  14650, time: 0.190, loss: 615.426392\n",
      "Train: step:  14660, time: 0.194, loss: 3534.427979\n",
      "Train: step:  14670, time: 0.192, loss: 1653.456177\n",
      "Train: step:  14680, time: 0.194, loss: 2155.482666\n",
      "Train: step:  14690, time: 0.199, loss: 2328.708252\n",
      "Train: step:  14700, time: 0.200, loss: 1792.152710\n",
      "Train: step:  14710, time: 0.186, loss: 3729.052246\n",
      "Train: step:  14720, time: 0.199, loss: 3222.087646\n",
      "Train: step:  14730, time: 0.221, loss: 396.415802\n",
      "Train: step:  14740, time: 0.193, loss: 2831.362305\n",
      "Train: step:  14750, time: 0.208, loss: 1456.198853\n",
      "Train: step:  14760, time: 0.228, loss: 1745.579346\n",
      "Train: step:  14770, time: 0.229, loss: 2741.509766\n",
      "Train: step:  14780, time: 0.180, loss: 634.966064\n",
      "Train: step:  14790, time: 0.220, loss: 2305.603516\n",
      "Train: step:  14800, time: 0.200, loss: 1510.370728\n",
      "Train: step:  14810, time: 0.235, loss: 1759.384888\n",
      "Train: step:  14820, time: 0.199, loss: 1984.984619\n",
      "Train: step:  14830, time: 0.190, loss: 909.269653\n",
      "Train: step:  14840, time: 0.195, loss: 2580.586670\n",
      "Train: step:  14850, time: 0.197, loss: 2478.531494\n",
      "Train: step:  14860, time: 0.194, loss: 1267.906860\n",
      "Train: step:  14870, time: 0.194, loss: 421.410492\n",
      "Train: step:  14880, time: 0.184, loss: 1114.066162\n",
      "Train: step:  14890, time: 0.186, loss: 2332.250000\n",
      "Train: step:  14900, time: 0.218, loss: 2678.388916\n",
      "Train: step:  14910, time: 0.237, loss: 3008.806152\n",
      "Train: step:  14920, time: 0.190, loss: 1322.632324\n",
      "Train: step:  14930, time: 0.201, loss: 1550.096924\n",
      "Train: step:  14940, time: 0.212, loss: 2546.257568\n",
      "Train: step:  14950, time: 0.227, loss: 873.802917\n",
      "Train: step:  14960, time: 0.222, loss: 643.108337\n",
      "Train: step:  14970, time: 0.187, loss: 1954.107666\n",
      "Train: step:  14980, time: 0.216, loss: 669.175720\n",
      "Train: step:  14990, time: 0.191, loss: 744.523438\n",
      "Train: step:  15000, time: 0.188, loss: 807.338135\n",
      "Train: step:  15010, time: 0.226, loss: 3703.788330\n",
      "Train: step:  15020, time: 0.218, loss: 2557.456055\n",
      "Train: step:  15030, time: 0.189, loss: 1465.237915\n",
      "Train: step:  15040, time: 0.189, loss: 6513.924316\n",
      "Train: step:  15050, time: 0.191, loss: 2566.821045\n",
      "Train: step:  15060, time: 0.183, loss: 3233.950195\n",
      "Train: step:  15070, time: 0.195, loss: 2319.002441\n",
      "Train: step:  15080, time: 0.181, loss: 1570.664062\n",
      "Train: step:  15090, time: 0.188, loss: 898.622070\n",
      "Train: step:  15100, time: 0.193, loss: 1964.157227\n",
      "Train: step:  15110, time: 0.189, loss: 2293.763672\n",
      "Train: step:  15120, time: 0.222, loss: 1476.810547\n",
      "Train: step:  15130, time: 0.193, loss: 2501.509766\n",
      "Train: step:  15140, time: 0.218, loss: 1531.731567\n",
      "Train: step:  15150, time: 0.188, loss: 1733.318848\n",
      "Train: step:  15160, time: 0.192, loss: 1935.083862\n",
      "Train: step:  15170, time: 0.188, loss: 2291.011475\n",
      "Train: step:  15180, time: 0.194, loss: 2638.109131\n",
      "Train: step:  15190, time: 0.225, loss: 2616.357666\n",
      "Train: step:  15200, time: 0.186, loss: 2328.212646\n",
      "Train: step:  15210, time: 0.185, loss: 1646.265503\n",
      "Train: step:  15220, time: 0.199, loss: 1904.015869\n",
      "Train: step:  15230, time: 0.185, loss: 1355.332764\n",
      "Train: step:  15240, time: 0.185, loss: 1544.234985\n",
      "Train: step:  15250, time: 0.195, loss: 2193.414795\n",
      "Train: step:  15260, time: 0.214, loss: 2141.930664\n",
      "Train: step:  15270, time: 0.190, loss: 1309.738037\n",
      "Train: step:  15280, time: 0.216, loss: 2866.795410\n",
      "Train: step:  15290, time: 0.193, loss: 833.373596\n",
      "Train: step:  15300, time: 0.199, loss: 1030.667847\n",
      "Train: step:  15310, time: 0.228, loss: 1516.616943\n",
      "Train: step:  15320, time: 0.188, loss: 2225.882324\n",
      "Train: step:  15330, time: 0.187, loss: 171.029236\n",
      "Train: step:  15340, time: 0.192, loss: 1392.142456\n",
      "Train: step:  15350, time: 0.211, loss: 1778.554688\n",
      "Train: step:  15360, time: 0.189, loss: 1614.701294\n",
      "Train: step:  15370, time: 0.203, loss: 1089.453979\n",
      "Train: step:  15380, time: 0.237, loss: 1311.845093\n",
      "Train: step:  15390, time: 0.191, loss: 1159.898193\n",
      "Train: step:  15400, time: 0.218, loss: 965.047119\n",
      "Train: step:  15410, time: 0.199, loss: 1154.526611\n",
      "Train: step:  15420, time: 0.230, loss: 3864.207031\n",
      "Train: step:  15430, time: 0.218, loss: 2786.118896\n",
      "Train: step:  15440, time: 0.217, loss: 2867.377441\n",
      "Train: step:  15450, time: 0.191, loss: 1699.332642\n",
      "Train: step:  15460, time: 0.192, loss: 560.783691\n",
      "Train: step:  15470, time: 0.189, loss: 1420.764160\n",
      "Train: step:  15480, time: 0.217, loss: 2197.798096\n",
      "Train: step:  15490, time: 0.186, loss: 1651.333374\n",
      "Train: step:  15500, time: 0.195, loss: 1819.828125\n",
      "Train: step:  15510, time: 0.192, loss: 1827.488647\n",
      "Train: step:  15520, time: 0.195, loss: 1707.074219\n",
      "Train: step:  15530, time: 0.209, loss: 686.900208\n",
      "Train: step:  15540, time: 0.198, loss: 820.931030\n",
      "Train: step:  15550, time: 0.226, loss: 658.198120\n",
      "Train: step:  15560, time: 0.185, loss: 1093.884888\n",
      "Train: step:  15570, time: 0.199, loss: 2289.502441\n",
      "Train: step:  15580, time: 0.188, loss: 1566.757568\n",
      "Train: step:  15590, time: 0.217, loss: 4096.932617\n",
      "Train: step:  15600, time: 0.205, loss: 367.338379\n",
      "Train: step:  15610, time: 0.216, loss: 889.300476\n",
      "Train: step:  15620, time: 0.191, loss: 3624.996094\n",
      "Train: step:  15630, time: 0.194, loss: 1817.804932\n",
      "Train: step:  15640, time: 0.185, loss: 3325.518555\n",
      "Train: step:  15650, time: 0.189, loss: 1166.472900\n",
      "Train: step:  15660, time: 0.191, loss: 1318.644531\n",
      "Train: step:  15670, time: 0.188, loss: 2053.501221\n",
      "Train: step:  15680, time: 0.187, loss: 1807.975464\n",
      "Train: step:  15690, time: 0.194, loss: 677.830200\n",
      "Train: step:  15700, time: 0.186, loss: 2422.625977\n",
      "Train: step:  15710, time: 0.191, loss: 391.029449\n",
      "Train: step:  15720, time: 0.234, loss: 439.245117\n",
      "Train: step:  15730, time: 0.215, loss: 2537.182373\n",
      "Train: step:  15740, time: 0.189, loss: 556.627869\n",
      "Train: step:  15750, time: 0.186, loss: 1060.811157\n",
      "Train: step:  15760, time: 0.230, loss: 1798.888672\n",
      "Train: step:  15770, time: 0.204, loss: 1218.281738\n",
      "Train: step:  15780, time: 0.189, loss: 3113.447754\n",
      "Train: step:  15790, time: 0.188, loss: 2010.512817\n",
      "Train: step:  15800, time: 0.220, loss: 1269.177979\n",
      "Train: step:  15810, time: 0.192, loss: 1485.040405\n",
      "Train: step:  15820, time: 0.198, loss: 2378.608398\n",
      "Train: step:  15830, time: 0.194, loss: 1997.006836\n",
      "Train: step:  15840, time: 0.187, loss: 1619.710938\n",
      "Train: step:  15850, time: 0.191, loss: 1582.340820\n",
      "Train: step:  15860, time: 0.190, loss: 1686.190796\n",
      "Train: step:  15870, time: 0.195, loss: 2819.059570\n",
      "Train: step:  15880, time: 0.190, loss: 1307.490479\n",
      "Train: step:  15890, time: 0.214, loss: 1909.551636\n",
      "Train: step:  15900, time: 0.226, loss: 2589.255615\n",
      "Train: step:  15910, time: 0.215, loss: 2748.037109\n",
      "Train: step:  15920, time: 0.235, loss: 1317.662598\n",
      "Train: step:  15930, time: 0.186, loss: 1513.387207\n",
      "Train: step:  15940, time: 0.186, loss: 2160.659180\n",
      "Train: step:  15950, time: 0.221, loss: 2493.336670\n",
      "Train: step:  15960, time: 0.192, loss: 1578.425781\n",
      "Train: step:  15970, time: 0.193, loss: 2070.794678\n",
      "Train: step:  15980, time: 0.201, loss: 282.896606\n",
      "Train: step:  15990, time: 0.185, loss: 382.014435\n",
      "Train: step:  16000, time: 0.187, loss: 3700.175537\n",
      "Train: step:  16010, time: 0.189, loss: 1453.813232\n",
      "Train: step:  16020, time: 0.219, loss: 2628.343506\n",
      "Train: step:  16030, time: 0.188, loss: 1405.486328\n",
      "Train: step:  16040, time: 0.195, loss: 2967.482422\n",
      "Train: step:  16050, time: 0.186, loss: 779.615662\n",
      "Train: step:  16060, time: 0.193, loss: 985.226074\n",
      "Train: step:  16070, time: 0.231, loss: 2241.961426\n",
      "Train: step:  16080, time: 0.219, loss: 2500.533936\n",
      "Train: step:  16090, time: 0.198, loss: 1588.623169\n",
      "Train: step:  16100, time: 0.188, loss: 471.238220\n",
      "Train: step:  16110, time: 0.185, loss: 1434.961914\n",
      "Train: step:  16120, time: 0.183, loss: 3773.021484\n",
      "Train: step:  16130, time: 0.185, loss: 1032.143066\n",
      "Train: step:  16140, time: 0.191, loss: 1767.240479\n",
      "Train: step:  16150, time: 0.229, loss: 884.152283\n",
      "Train: step:  16160, time: 0.185, loss: 1598.512939\n",
      "Train: step:  16170, time: 0.190, loss: 1274.949951\n",
      "Train: step:  16180, time: 0.191, loss: 2399.068848\n",
      "Train: step:  16190, time: 0.232, loss: 2088.202148\n",
      "Train: step:  16200, time: 0.200, loss: 1773.775024\n",
      "Train: step:  16210, time: 0.240, loss: 1070.398804\n",
      "Train: step:  16220, time: 0.205, loss: 3003.542480\n",
      "Train: step:  16230, time: 0.195, loss: 1566.570190\n",
      "Train: step:  16240, time: 0.185, loss: 2549.890137\n",
      "Train: step:  16250, time: 0.186, loss: 701.583435\n",
      "Train: step:  16260, time: 0.185, loss: 2818.260254\n",
      "Train: step:  16270, time: 0.232, loss: 2996.494385\n",
      "Train: step:  16280, time: 0.191, loss: 632.340210\n",
      "Train: step:  16290, time: 0.190, loss: 2794.464111\n",
      "Train: step:  16300, time: 0.218, loss: 1125.800537\n",
      "Train: step:  16310, time: 0.219, loss: 1084.567139\n",
      "Train: step:  16320, time: 0.195, loss: 1815.090576\n",
      "Train: step:  16330, time: 0.217, loss: 1084.142700\n",
      "Train: step:  16340, time: 0.192, loss: 1663.289673\n",
      "Train: step:  16350, time: 0.235, loss: 3114.664551\n",
      "Train: step:  16360, time: 0.196, loss: 1875.115845\n",
      "Train: step:  16370, time: 0.189, loss: 1708.163208\n",
      "Train: step:  16380, time: 0.193, loss: 1281.148071\n",
      "Train: step:  16390, time: 0.219, loss: 1038.302979\n",
      "Train: step:  16400, time: 0.233, loss: 2610.951172\n",
      "Train: step:  16410, time: 0.193, loss: 2415.915771\n",
      "Train: step:  16420, time: 0.195, loss: 889.699951\n",
      "Train: step:  16430, time: 0.215, loss: 1722.200073\n",
      "Train: step:  16440, time: 0.197, loss: 2794.568604\n",
      "Train: step:  16450, time: 0.191, loss: 1203.299194\n",
      "Train: step:  16460, time: 0.186, loss: 2657.565430\n",
      "Train: step:  16470, time: 0.194, loss: 1192.306274\n",
      "Train: step:  16480, time: 0.196, loss: 1571.444214\n",
      "Train: step:  16490, time: 0.190, loss: 2857.345215\n",
      "Train: step:  16500, time: 0.190, loss: 1211.568970\n",
      "Train: step:  16510, time: 0.198, loss: 1864.318848\n",
      "Train: step:  16520, time: 0.203, loss: 1093.450684\n",
      "Train: step:  16530, time: 0.188, loss: 1323.110352\n",
      "Train: step:  16540, time: 0.188, loss: 2130.806641\n",
      "Train: step:  16550, time: 0.193, loss: 1905.077515\n",
      "Train: step:  16560, time: 0.187, loss: 2216.625488\n",
      "Train: step:  16570, time: 0.221, loss: 1170.902466\n",
      "Train: step:  16580, time: 0.195, loss: 1113.529053\n",
      "Train: step:  16590, time: 0.193, loss: 898.084961\n",
      "Train: step:  16600, time: 0.191, loss: 2350.757812\n",
      "Train: step:  16610, time: 0.216, loss: 389.971283\n",
      "Train: step:  16620, time: 0.185, loss: 2000.724976\n",
      "Train: step:  16630, time: 0.219, loss: 766.381531\n",
      "Train: step:  16640, time: 0.195, loss: 1482.737671\n",
      "Train: step:  16650, time: 0.217, loss: 768.497375\n",
      "Train: step:  16660, time: 0.193, loss: 1984.695679\n",
      "Train: step:  16670, time: 0.218, loss: 1532.629517\n",
      "Train: step:  16680, time: 0.217, loss: 2355.585205\n",
      "Train: step:  16690, time: 0.189, loss: 547.908813\n",
      "Train: step:  16700, time: 0.217, loss: 2125.058838\n",
      "Train: step:  16710, time: 0.220, loss: 333.875458\n",
      "Train: step:  16720, time: 0.218, loss: 721.370117\n",
      "Train: step:  16730, time: 0.196, loss: 2704.799072\n",
      "Train: step:  16740, time: 0.235, loss: 2946.261230\n",
      "Train: step:  16750, time: 0.230, loss: 6459.298340\n",
      "Train: step:  16760, time: 0.194, loss: 2882.838623\n",
      "Train: step:  16770, time: 0.191, loss: 678.227722\n",
      "Train: step:  16780, time: 0.214, loss: 1291.199463\n",
      "Train: step:  16790, time: 0.228, loss: 2468.659912\n",
      "Train: step:  16800, time: 0.192, loss: 2048.397461\n",
      "Train: step:  16810, time: 0.189, loss: 3115.422363\n",
      "Train: step:  16820, time: 0.194, loss: 2356.610596\n",
      "Train: step:  16830, time: 0.188, loss: 3758.828125\n",
      "Train: step:  16840, time: 0.255, loss: 3868.867432\n",
      "Train: step:  16850, time: 0.195, loss: 532.068237\n",
      "Train: step:  16860, time: 0.189, loss: 189.328186\n",
      "Train: step:  16870, time: 0.189, loss: 1139.933472\n",
      "Train: step:  16880, time: 0.194, loss: 6842.001465\n",
      "Train: step:  16890, time: 0.238, loss: 2178.036621\n",
      "Train: step:  16900, time: 0.238, loss: 2263.713135\n",
      "Train: step:  16910, time: 0.192, loss: 415.238556\n",
      "Train: step:  16920, time: 0.228, loss: 2282.583252\n",
      "Train: step:  16930, time: 0.228, loss: 1334.610962\n",
      "Train: step:  16940, time: 0.196, loss: 801.736084\n",
      "Train: step:  16950, time: 0.197, loss: 634.139587\n",
      "Train: step:  16960, time: 0.191, loss: 1655.040161\n",
      "Train: step:  16970, time: 0.305, loss: 2367.937500\n",
      "Train: step:  16980, time: 0.190, loss: 345.286041\n",
      "Train: step:  16990, time: 0.219, loss: 3422.559326\n",
      "Train: step:  17000, time: 0.191, loss: 661.249512\n",
      "Train: step:  17010, time: 0.190, loss: 2218.037109\n",
      "Train: step:  17020, time: 0.229, loss: 1182.553223\n",
      "Train: step:  17030, time: 0.223, loss: 2565.526367\n",
      "Train: step:  17040, time: 0.190, loss: 3606.879639\n",
      "Train: step:  17050, time: 0.217, loss: 1033.511108\n",
      "Train: step:  17060, time: 0.186, loss: 3001.687500\n",
      "Train: step:  17070, time: 0.191, loss: 2606.085205\n",
      "Train: step:  17080, time: 0.194, loss: 2883.877686\n",
      "Train: step:  17090, time: 0.217, loss: 2889.134766\n",
      "Train: step:  17100, time: 0.210, loss: 1325.622803\n",
      "Train: step:  17110, time: 0.198, loss: 1149.722534\n",
      "Train: step:  17120, time: 0.192, loss: 633.810608\n",
      "Train: step:  17130, time: 0.188, loss: 1458.204102\n",
      "Train: step:  17140, time: 0.197, loss: 1956.036377\n",
      "Train: step:  17150, time: 0.240, loss: 580.160278\n",
      "Train: step:  17160, time: 0.184, loss: 590.075928\n",
      "Train: step:  17170, time: 0.224, loss: 1494.386353\n",
      "Train: step:  17180, time: 0.192, loss: 1934.263550\n",
      "Train: step:  17190, time: 0.209, loss: 2601.654541\n",
      "Train: step:  17200, time: 0.195, loss: 235.210648\n",
      "Train: step:  17210, time: 0.250, loss: 2416.105713\n",
      "Train: step:  17220, time: 0.186, loss: 1822.674927\n",
      "Train: step:  17230, time: 0.222, loss: 2208.405029\n",
      "Train: step:  17240, time: 0.191, loss: 558.524414\n",
      "Train: step:  17250, time: 0.228, loss: 504.518005\n",
      "Train: step:  17260, time: 0.192, loss: 706.867065\n",
      "Train: step:  17270, time: 0.194, loss: 2538.413818\n",
      "Train: step:  17280, time: 0.216, loss: 1091.340820\n",
      "Train: step:  17290, time: 0.190, loss: 4068.147461\n",
      "Train: step:  17300, time: 0.195, loss: 2464.742920\n",
      "Train: step:  17310, time: 0.198, loss: 831.043823\n",
      "Train: step:  17320, time: 0.198, loss: 3005.744141\n",
      "Train: step:  17330, time: 0.221, loss: 3391.541748\n",
      "Train: step:  17340, time: 0.190, loss: 729.935120\n",
      "Train: step:  17350, time: 0.217, loss: 440.434235\n",
      "Train: step:  17360, time: 0.187, loss: 490.916290\n",
      "Train: step:  17370, time: 0.191, loss: 2732.976318\n",
      "Train: step:  17380, time: 0.187, loss: 2304.629150\n",
      "Train: step:  17390, time: 0.187, loss: 3059.077637\n",
      "Train: step:  17400, time: 0.220, loss: 1140.126343\n",
      "Train: step:  17410, time: 0.190, loss: 2260.052246\n",
      "Train: step:  17420, time: 0.239, loss: 972.899048\n",
      "Train: step:  17430, time: 0.196, loss: 1271.664185\n",
      "Train: step:  17440, time: 0.201, loss: 2626.948730\n",
      "Train: step:  17450, time: 0.196, loss: 2021.833496\n",
      "Train: step:  17460, time: 0.244, loss: 2146.831055\n",
      "Train: step:  17470, time: 0.194, loss: 2655.936523\n",
      "Train: step:  17480, time: 0.198, loss: 1306.051636\n",
      "Train: step:  17490, time: 0.199, loss: 1000.104675\n",
      "Train: step:  17500, time: 0.196, loss: 1396.252686\n",
      "Train: step:  17510, time: 0.190, loss: 1418.322876\n",
      "Train: step:  17520, time: 0.196, loss: 1818.771240\n",
      "Train: step:  17530, time: 0.198, loss: 452.122375\n",
      "Train: step:  17540, time: 0.186, loss: 1382.348022\n",
      "Train: step:  17550, time: 0.190, loss: 902.546936\n",
      "Train: step:  17560, time: 0.190, loss: 1297.140015\n",
      "Train: step:  17570, time: 0.189, loss: 1784.774414\n",
      "Train: step:  17580, time: 0.184, loss: 3463.368652\n",
      "Train: step:  17590, time: 0.218, loss: 2989.689941\n",
      "Train: step:  17600, time: 0.192, loss: 3103.278320\n",
      "Train: step:  17610, time: 0.190, loss: 2539.041504\n",
      "Train: step:  17620, time: 0.199, loss: 2995.096924\n",
      "Train: step:  17630, time: 0.195, loss: 2235.708008\n",
      "Train: step:  17640, time: 0.278, loss: 568.213989\n",
      "Train: step:  17650, time: 0.200, loss: 532.507507\n",
      "Train: step:  17660, time: 0.193, loss: 2892.130371\n",
      "Train: step:  17670, time: 0.220, loss: 1417.289062\n",
      "Train: step:  17680, time: 0.224, loss: 1473.647949\n",
      "Train: step:  17690, time: 0.220, loss: 1334.422119\n",
      "Train: step:  17700, time: 0.232, loss: 1521.671021\n",
      "Train: step:  17710, time: 0.196, loss: 2735.226318\n",
      "Train: step:  17720, time: 0.221, loss: 1742.579468\n",
      "Train: step:  17730, time: 0.226, loss: 405.267273\n",
      "Train: step:  17740, time: 0.200, loss: 371.808228\n",
      "Train: step:  17750, time: 0.190, loss: 1211.980957\n",
      "Train: step:  17760, time: 0.189, loss: 1937.772949\n",
      "Train: step:  17770, time: 0.234, loss: 1293.865112\n",
      "Train: step:  17780, time: 0.251, loss: 1824.116821\n",
      "Train: step:  17790, time: 0.189, loss: 1904.490479\n",
      "Train: step:  17800, time: 0.191, loss: 1268.783813\n",
      "Train: step:  17810, time: 0.220, loss: 2322.141113\n",
      "Train: step:  17820, time: 0.200, loss: 2085.546143\n",
      "Train: step:  17830, time: 0.220, loss: 2279.934326\n",
      "Train: step:  17840, time: 0.195, loss: 1253.287354\n",
      "Train: step:  17850, time: 0.233, loss: 2272.297607\n",
      "Train: step:  17860, time: 0.198, loss: 2203.259033\n",
      "Train: step:  17870, time: 0.200, loss: 1575.050049\n",
      "Train: step:  17880, time: 0.223, loss: 1647.153564\n",
      "Train: step:  17890, time: 0.252, loss: 1954.644531\n",
      "Train: step:  17900, time: 0.189, loss: 671.874023\n",
      "Train: step:  17910, time: 0.208, loss: 3608.971680\n",
      "Train: step:  17920, time: 0.209, loss: 3544.052246\n",
      "Train: step:  17930, time: 0.191, loss: 4606.712402\n",
      "Train: step:  17940, time: 0.218, loss: 1472.899902\n",
      "Train: step:  17950, time: 0.218, loss: 1962.552490\n",
      "Train: step:  17960, time: 0.189, loss: 2458.987793\n",
      "Train: step:  17970, time: 0.187, loss: 2413.589600\n",
      "Train: step:  17980, time: 0.190, loss: 1873.234863\n",
      "Train: step:  17990, time: 0.198, loss: 564.102661\n",
      "Train: step:  18000, time: 0.192, loss: 2273.229492\n",
      "Train: step:  18010, time: 0.188, loss: 2561.742188\n",
      "Train: step:  18020, time: 0.197, loss: 2580.826172\n",
      "Train: step:  18030, time: 0.253, loss: 4594.702637\n",
      "Train: step:  18040, time: 0.188, loss: 922.387878\n",
      "Train: step:  18050, time: 0.215, loss: 2242.542725\n",
      "Train: step:  18060, time: 0.190, loss: 981.202942\n",
      "Train: step:  18070, time: 0.196, loss: 975.876953\n",
      "Train: step:  18080, time: 0.189, loss: 713.389038\n",
      "Train: step:  18090, time: 0.193, loss: 1988.721069\n",
      "Train: step:  18100, time: 0.199, loss: 4669.075684\n",
      "Train: step:  18110, time: 0.203, loss: 2168.561279\n",
      "Train: step:  18120, time: 0.189, loss: 922.838623\n",
      "Train: step:  18130, time: 0.188, loss: 674.261047\n",
      "Train: step:  18140, time: 0.192, loss: 1723.722412\n",
      "Train: step:  18150, time: 0.228, loss: 2777.841064\n",
      "Train: step:  18160, time: 0.218, loss: 2823.791748\n",
      "Train: step:  18170, time: 0.217, loss: 3231.688477\n",
      "Train: step:  18180, time: 0.237, loss: 1039.246826\n",
      "Train: step:  18190, time: 0.230, loss: 917.325867\n",
      "Train: step:  18200, time: 0.192, loss: 2026.162598\n",
      "Train: step:  18210, time: 0.190, loss: 2749.125488\n",
      "Train: step:  18220, time: 0.186, loss: 1598.314697\n",
      "Train: step:  18230, time: 0.229, loss: 1533.101196\n",
      "Train: step:  18240, time: 0.184, loss: 1195.608521\n",
      "Train: step:  18250, time: 0.185, loss: 1445.192627\n",
      "Train: step:  18260, time: 0.191, loss: 1434.591553\n",
      "Train: step:  18270, time: 0.190, loss: 844.230469\n",
      "Train: step:  18280, time: 0.257, loss: 675.281067\n",
      "Train: step:  18290, time: 0.225, loss: 3353.660645\n",
      "Train: step:  18300, time: 0.196, loss: 1944.750366\n",
      "Train: step:  18310, time: 0.210, loss: 4492.587402\n",
      "Train: step:  18320, time: 0.191, loss: 2220.600342\n",
      "Train: step:  18330, time: 0.188, loss: 1123.935547\n",
      "Train: step:  18340, time: 0.229, loss: 2061.362793\n",
      "Train: step:  18350, time: 0.191, loss: 1797.893677\n",
      "Train: step:  18360, time: 0.198, loss: 252.907379\n",
      "Train: step:  18370, time: 0.216, loss: 2649.168945\n",
      "Train: step:  18380, time: 0.190, loss: 2215.466064\n",
      "Train: step:  18390, time: 0.231, loss: 689.392151\n",
      "Train: step:  18400, time: 0.190, loss: 771.625854\n",
      "Train: step:  18410, time: 0.214, loss: 2020.805786\n",
      "Train: step:  18420, time: 0.220, loss: 575.280457\n",
      "Train: step:  18430, time: 0.191, loss: 1644.615723\n",
      "Train: step:  18440, time: 0.221, loss: 2635.672607\n",
      "Train: step:  18450, time: 0.218, loss: 1534.224365\n",
      "Train: step:  18460, time: 0.199, loss: 3502.282715\n",
      "Train: step:  18470, time: 0.219, loss: 2267.423340\n",
      "Train: step:  18480, time: 0.186, loss: 1857.645020\n",
      "Train: step:  18490, time: 0.247, loss: 3166.894775\n",
      "Train: step:  18500, time: 0.230, loss: 1363.520508\n",
      "Train: step:  18510, time: 0.235, loss: 2982.246338\n",
      "Train: step:  18520, time: 0.263, loss: 1497.364136\n",
      "Train: step:  18530, time: 0.188, loss: 2776.405273\n",
      "Train: step:  18540, time: 0.195, loss: 1817.242676\n",
      "Train: step:  18550, time: 0.239, loss: 1230.365723\n",
      "Train: step:  18560, time: 0.190, loss: 2367.902832\n",
      "Train: step:  18570, time: 0.191, loss: 2705.599609\n",
      "Train: step:  18580, time: 0.190, loss: 2068.646484\n",
      "Train: step:  18590, time: 0.188, loss: 768.590637\n",
      "Train: step:  18600, time: 0.193, loss: 2078.596436\n",
      "Train: step:  18610, time: 0.200, loss: 1672.797607\n",
      "Train: step:  18620, time: 0.197, loss: 1438.993774\n",
      "Train: step:  18630, time: 0.202, loss: 2732.322510\n",
      "Train: step:  18640, time: 0.183, loss: 1752.629639\n",
      "Train: step:  18650, time: 0.189, loss: 2669.374023\n",
      "Train: step:  18660, time: 0.194, loss: 659.068665\n",
      "Train: step:  18670, time: 0.187, loss: 1290.239746\n",
      "Train: step:  18680, time: 0.191, loss: 1717.875610\n",
      "Train: step:  18690, time: 0.202, loss: 2158.207031\n",
      "Train: step:  18700, time: 0.194, loss: 1300.076172\n",
      "Train: step:  18710, time: 0.226, loss: 1946.098022\n",
      "Train: step:  18720, time: 0.217, loss: 2507.706299\n",
      "Train: step:  18730, time: 0.190, loss: 2271.381836\n",
      "Train: step:  18740, time: 0.229, loss: 1564.135864\n",
      "Train: step:  18750, time: 0.186, loss: 563.498047\n",
      "Train: step:  18760, time: 0.186, loss: 1727.650757\n",
      "Train: step:  18770, time: 0.188, loss: 939.960083\n",
      "Train: step:  18780, time: 0.187, loss: 1708.244019\n",
      "Train: step:  18790, time: 0.192, loss: 654.243652\n",
      "Train: step:  18800, time: 0.191, loss: 1884.587158\n",
      "Train: step:  18810, time: 0.189, loss: 3407.874756\n",
      "Train: step:  18820, time: 0.213, loss: 1514.317139\n",
      "Train: step:  18830, time: 0.197, loss: 2087.466797\n",
      "Train: step:  18840, time: 0.196, loss: 3180.674316\n",
      "Train: step:  18850, time: 0.221, loss: 1566.980957\n",
      "Train: step:  18860, time: 0.192, loss: 1233.009521\n",
      "Train: step:  18870, time: 0.196, loss: 1875.849854\n",
      "Train: step:  18880, time: 0.194, loss: 1642.254639\n",
      "Train: step:  18890, time: 0.191, loss: 3441.222168\n",
      "Train: step:  18900, time: 0.185, loss: 3159.590088\n",
      "Train: step:  18910, time: 0.192, loss: 868.296692\n",
      "Train: step:  18920, time: 0.195, loss: 323.136047\n",
      "Train: step:  18930, time: 0.187, loss: 1292.795654\n",
      "Train: step:  18940, time: 0.216, loss: 2018.478271\n",
      "Train: step:  18950, time: 0.190, loss: 1239.858521\n",
      "Train: step:  18960, time: 0.195, loss: 1202.864868\n",
      "Train: step:  18970, time: 0.197, loss: 2481.007080\n",
      "Train: step:  18980, time: 0.192, loss: 289.689758\n",
      "Train: step:  18990, time: 0.189, loss: 400.048065\n",
      "Train: step:  19000, time: 0.191, loss: 432.288361\n",
      "Train: step:  19010, time: 0.217, loss: 622.597595\n",
      "Train: step:  19020, time: 0.186, loss: 1772.109375\n",
      "Train: step:  19030, time: 0.191, loss: 2233.242188\n",
      "Train: step:  19040, time: 0.192, loss: 1954.320068\n",
      "Train: step:  19050, time: 0.185, loss: 3222.126953\n",
      "Train: step:  19060, time: 0.199, loss: 1527.508301\n",
      "Train: step:  19070, time: 0.196, loss: 809.249939\n",
      "Train: step:  19080, time: 0.220, loss: 2099.489258\n",
      "Train: step:  19090, time: 0.192, loss: 1520.502441\n",
      "Train: step:  19100, time: 0.199, loss: 1462.200195\n",
      "Train: step:  19110, time: 0.199, loss: 1921.885132\n",
      "Train: step:  19120, time: 0.197, loss: 867.773193\n",
      "Train: step:  19130, time: 0.253, loss: 1451.441284\n",
      "Train: step:  19140, time: 0.197, loss: 989.991089\n",
      "Train: step:  19150, time: 0.191, loss: 450.703827\n",
      "Train: step:  19160, time: 0.194, loss: 580.653748\n",
      "Train: step:  19170, time: 0.222, loss: 2390.665771\n",
      "Train: step:  19180, time: 0.198, loss: 2498.672852\n",
      "Train: step:  19190, time: 0.219, loss: 1541.953857\n",
      "Train: step:  19200, time: 0.218, loss: 1238.368408\n",
      "Train: step:  19210, time: 0.212, loss: 2124.257812\n",
      "Train: step:  19220, time: 0.203, loss: 911.999451\n",
      "Train: step:  19230, time: 0.187, loss: 567.060364\n",
      "Train: step:  19240, time: 0.217, loss: 1703.710327\n",
      "Train: step:  19250, time: 0.251, loss: 696.591431\n",
      "Train: step:  19260, time: 0.226, loss: 1424.857788\n",
      "Train: step:  19270, time: 0.185, loss: 1435.364868\n",
      "Train: step:  19280, time: 0.217, loss: 2233.055664\n",
      "Train: step:  19290, time: 0.194, loss: 3407.911133\n",
      "Train: step:  19300, time: 0.188, loss: 252.876556\n",
      "Train: step:  19310, time: 0.220, loss: 2059.485352\n",
      "Train: step:  19320, time: 0.192, loss: 896.779114\n",
      "Train: step:  19330, time: 0.193, loss: 2352.877930\n",
      "Train: step:  19340, time: 0.187, loss: 1946.163086\n",
      "Train: step:  19350, time: 0.196, loss: 1698.159912\n",
      "Train: step:  19360, time: 0.188, loss: 484.989990\n",
      "Train: step:  19370, time: 0.190, loss: 1520.302979\n",
      "Train: step:  19380, time: 0.241, loss: 2147.712158\n",
      "Train: step:  19390, time: 0.198, loss: 2863.326904\n",
      "Train: step:  19400, time: 0.185, loss: 2488.016357\n",
      "Train: step:  19410, time: 0.187, loss: 1317.989746\n",
      "Train: step:  19420, time: 0.217, loss: 1885.600098\n",
      "Train: step:  19430, time: 0.240, loss: 2964.982178\n",
      "Train: step:  19440, time: 0.191, loss: 1757.354858\n",
      "Train: step:  19450, time: 0.236, loss: 3550.940186\n",
      "Train: step:  19460, time: 0.190, loss: 1290.193604\n",
      "Train: step:  19470, time: 0.221, loss: 2056.968994\n",
      "Train: step:  19480, time: 0.188, loss: 2237.178711\n",
      "Train: step:  19490, time: 0.189, loss: 1570.314575\n",
      "Train: step:  19500, time: 0.217, loss: 1676.410767\n",
      "Train: step:  19510, time: 0.192, loss: 1226.353271\n",
      "Train: step:  19520, time: 0.190, loss: 1565.956665\n",
      "Train: step:  19530, time: 0.190, loss: 373.745758\n",
      "Train: step:  19540, time: 0.227, loss: 3463.947510\n",
      "Train: step:  19550, time: 0.201, loss: 2014.213623\n",
      "Train: step:  19560, time: 0.192, loss: 2513.780762\n",
      "Train: step:  19570, time: 0.230, loss: 789.290222\n",
      "Train: step:  19580, time: 0.195, loss: 3474.647949\n",
      "Train: step:  19590, time: 0.192, loss: 2523.483154\n",
      "Train: step:  19600, time: 0.189, loss: 2152.271484\n",
      "Train: step:  19610, time: 0.236, loss: 1731.715820\n",
      "Train: step:  19620, time: 0.186, loss: 3259.866455\n",
      "Train: step:  19630, time: 0.200, loss: 2509.813477\n",
      "Train: step:  19640, time: 0.194, loss: 3495.472168\n",
      "Train: step:  19650, time: 0.192, loss: 2085.018799\n",
      "Train: step:  19660, time: 0.194, loss: 3182.948730\n",
      "Train: step:  19670, time: 0.219, loss: 2056.425293\n",
      "Train: step:  19680, time: 0.219, loss: 1128.948486\n",
      "Train: step:  19690, time: 0.191, loss: 1829.005493\n",
      "Train: step:  19700, time: 0.190, loss: 1542.794312\n",
      "Train: step:  19710, time: 0.194, loss: 1265.298340\n",
      "Train: step:  19720, time: 0.187, loss: 2245.900391\n",
      "Train: step:  19730, time: 0.230, loss: 600.795044\n",
      "Train: step:  19740, time: 0.189, loss: 333.824738\n",
      "Train: step:  19750, time: 0.193, loss: 3436.853516\n",
      "Train: step:  19760, time: 0.193, loss: 2474.862061\n",
      "Train: step:  19770, time: 0.187, loss: 1454.615723\n",
      "Train: step:  19780, time: 0.185, loss: 407.200409\n",
      "Train: step:  19790, time: 0.189, loss: 1629.619019\n",
      "Train: step:  19800, time: 0.215, loss: 1883.763428\n",
      "Train: step:  19810, time: 0.189, loss: 2778.621826\n",
      "Train: step:  19820, time: 0.190, loss: 2072.489014\n",
      "Train: step:  19830, time: 0.236, loss: 2237.777344\n",
      "Train: step:  19840, time: 0.204, loss: 2072.598633\n",
      "Train: step:  19850, time: 0.185, loss: 1112.534790\n",
      "Train: step:  19860, time: 0.197, loss: 4932.753418\n",
      "Train: step:  19870, time: 0.195, loss: 5201.748047\n",
      "Train: step:  19880, time: 0.195, loss: 799.694092\n",
      "Train: step:  19890, time: 0.229, loss: 2447.717041\n",
      "Train: step:  19900, time: 0.185, loss: 4052.224365\n",
      "Train: step:  19910, time: 0.194, loss: 2124.084229\n",
      "Train: step:  19920, time: 0.233, loss: 2232.059326\n",
      "Train: step:  19930, time: 0.210, loss: 1360.116699\n",
      "Train: step:  19940, time: 0.186, loss: 1253.759277\n",
      "Train: step:  19950, time: 0.188, loss: 2185.650635\n",
      "Train: step:  19960, time: 0.191, loss: 1479.905029\n",
      "Train: step:  19970, time: 0.188, loss: 1958.761353\n",
      "Train: step:  19980, time: 0.189, loss: 806.228455\n",
      "Train: step:  19990, time: 0.217, loss: 1286.977173\n",
      "Train: step:  20000, time: 0.196, loss: 2465.651611\n",
      "Train: step:  20010, time: 0.192, loss: 1467.480469\n",
      "Train: step:  20020, time: 0.218, loss: 315.637756\n",
      "Train: step:  20030, time: 0.227, loss: 1972.836060\n",
      "Train: step:  20040, time: 0.192, loss: 1480.676514\n",
      "Train: step:  20050, time: 0.186, loss: 801.007690\n",
      "Train: step:  20060, time: 0.190, loss: 3994.579102\n",
      "Train: step:  20070, time: 0.193, loss: 3194.527100\n",
      "Train: step:  20080, time: 0.213, loss: 2746.402344\n",
      "Train: step:  20090, time: 0.191, loss: 1784.089722\n",
      "Train: step:  20100, time: 0.194, loss: 1623.355835\n",
      "Train: step:  20110, time: 0.186, loss: 1621.417358\n",
      "Train: step:  20120, time: 0.191, loss: 1618.996826\n",
      "Train: step:  20130, time: 0.197, loss: 817.627625\n",
      "Train: step:  20140, time: 0.224, loss: 1973.760498\n",
      "Train: step:  20150, time: 0.185, loss: 2840.978271\n",
      "Train: step:  20160, time: 0.227, loss: 1618.487183\n",
      "Train: step:  20170, time: 0.186, loss: 2637.904785\n",
      "Train: step:  20180, time: 0.223, loss: 2208.899414\n",
      "Train: step:  20190, time: 0.216, loss: 1593.783447\n",
      "Train: step:  20200, time: 0.195, loss: 357.160309\n",
      "Train: step:  20210, time: 0.195, loss: 1120.837646\n",
      "Train: step:  20220, time: 0.191, loss: 568.587402\n",
      "Train: step:  20230, time: 0.188, loss: 291.390778\n",
      "Train: step:  20240, time: 0.188, loss: 2047.840332\n",
      "Train: step:  20250, time: 0.189, loss: 2067.810791\n",
      "Train: step:  20260, time: 0.191, loss: 1053.322998\n",
      "Train: step:  20270, time: 0.228, loss: 968.204956\n",
      "Train: step:  20280, time: 0.217, loss: 3117.009277\n",
      "Train: step:  20290, time: 0.184, loss: 2056.814209\n",
      "Train: step:  20300, time: 0.237, loss: 372.128906\n",
      "Train: step:  20310, time: 0.216, loss: 1576.028564\n",
      "Train: step:  20320, time: 0.192, loss: 1781.986206\n",
      "Train: step:  20330, time: 0.192, loss: 484.203125\n",
      "Train: step:  20340, time: 0.189, loss: 2048.708252\n",
      "Train: step:  20350, time: 0.190, loss: 384.048615\n",
      "Train: step:  20360, time: 0.192, loss: 4082.369629\n",
      "Train: step:  20370, time: 0.193, loss: 1769.462769\n",
      "Train: step:  20380, time: 0.187, loss: 2056.324463\n",
      "Train: step:  20390, time: 0.185, loss: 922.588867\n",
      "Train: step:  20400, time: 0.197, loss: 1945.856445\n",
      "Train: step:  20410, time: 0.186, loss: 831.105103\n",
      "Train: step:  20420, time: 0.186, loss: 1327.007324\n",
      "Train: step:  20430, time: 0.185, loss: 1180.803955\n",
      "Train: step:  20440, time: 0.189, loss: 1044.611084\n",
      "Train: step:  20450, time: 0.194, loss: 2540.297363\n",
      "Train: step:  20460, time: 0.217, loss: 1432.315552\n",
      "Train: step:  20470, time: 0.219, loss: 2625.152344\n",
      "Train: step:  20480, time: 0.193, loss: 858.983582\n",
      "Train: step:  20490, time: 0.190, loss: 670.091125\n",
      "Train: step:  20500, time: 0.183, loss: 1694.561890\n",
      "Train: step:  20510, time: 0.193, loss: 1116.438232\n",
      "Train: step:  20520, time: 0.219, loss: 3886.768799\n",
      "Train: step:  20530, time: 0.184, loss: 1689.618652\n",
      "Train: step:  20540, time: 0.242, loss: 3930.002930\n",
      "Train: step:  20550, time: 0.192, loss: 2909.022217\n",
      "Train: step:  20560, time: 0.218, loss: 2203.169922\n",
      "Train: step:  20570, time: 0.183, loss: 1698.973145\n",
      "Train: step:  20580, time: 0.215, loss: 2162.807373\n",
      "Train: step:  20590, time: 0.194, loss: 894.473816\n",
      "Train: step:  20600, time: 0.194, loss: 1865.595947\n",
      "Train: step:  20610, time: 0.211, loss: 483.408112\n",
      "Train: step:  20620, time: 0.194, loss: 1446.875244\n",
      "Train: step:  20630, time: 0.191, loss: 1052.245972\n",
      "Train: step:  20640, time: 0.269, loss: 2042.895752\n",
      "Train: step:  20650, time: 0.219, loss: 1228.161621\n",
      "Train: step:  20660, time: 0.188, loss: 1070.504150\n",
      "Train: step:  20670, time: 0.187, loss: 3448.857178\n",
      "Train: step:  20680, time: 0.198, loss: 770.996460\n",
      "Train: step:  20690, time: 0.189, loss: 697.508118\n",
      "Train: step:  20700, time: 0.191, loss: 2293.863037\n",
      "Train: step:  20710, time: 0.188, loss: 708.780151\n",
      "Train: step:  20720, time: 0.181, loss: 1675.907593\n",
      "Train: step:  20730, time: 0.196, loss: 1973.994873\n",
      "Train: step:  20740, time: 0.206, loss: 1598.434326\n",
      "Train: step:  20750, time: 0.190, loss: 1118.827637\n",
      "Train: step:  20760, time: 0.195, loss: 2292.267090\n",
      "Train: step:  20770, time: 0.197, loss: 896.010193\n",
      "Train: step:  20780, time: 0.226, loss: 496.597626\n",
      "Train: step:  20790, time: 0.192, loss: 1880.538574\n",
      "Train: step:  20800, time: 0.206, loss: 2685.790527\n",
      "Train: step:  20810, time: 0.189, loss: 1456.317993\n",
      "Train: step:  20820, time: 0.195, loss: 2478.776855\n",
      "Train: step:  20830, time: 0.203, loss: 1258.099854\n",
      "Train: step:  20840, time: 0.188, loss: 1784.266357\n",
      "Train: step:  20850, time: 0.189, loss: 1294.421143\n",
      "Train: step:  20860, time: 0.196, loss: 2925.803955\n",
      "Train: step:  20870, time: 0.200, loss: 1732.753174\n",
      "Train: step:  20880, time: 0.191, loss: 1643.733276\n",
      "Train: step:  20890, time: 0.191, loss: 1239.709839\n",
      "Train: step:  20900, time: 0.188, loss: 1859.194702\n",
      "Train: step:  20910, time: 0.193, loss: 640.846558\n",
      "Train: step:  20920, time: 0.227, loss: 2098.879639\n",
      "Train: step:  20930, time: 0.195, loss: 669.540649\n",
      "Train: step:  20940, time: 0.220, loss: 1158.942993\n",
      "Train: step:  20950, time: 0.248, loss: 4540.865234\n",
      "Train: step:  20960, time: 0.190, loss: 1029.989258\n",
      "Train: step:  20970, time: 0.194, loss: 691.166748\n",
      "Train: step:  20980, time: 0.229, loss: 825.315125\n",
      "Train: step:  20990, time: 0.194, loss: 1741.255005\n",
      "Train: step:  21000, time: 0.202, loss: 594.762695\n",
      "Train: step:  21010, time: 0.233, loss: 1972.293091\n",
      "Train: step:  21020, time: 0.190, loss: 2725.583496\n",
      "Train: step:  21030, time: 0.195, loss: 3216.092041\n",
      "Train: step:  21040, time: 0.186, loss: 1343.027710\n",
      "Train: step:  21050, time: 0.193, loss: 1741.240479\n",
      "Train: step:  21060, time: 0.232, loss: 2666.934082\n",
      "Train: step:  21070, time: 0.191, loss: 2428.595459\n",
      "Train: step:  21080, time: 0.198, loss: 1624.380981\n",
      "Train: step:  21090, time: 0.209, loss: 1745.241333\n",
      "Train: step:  21100, time: 0.189, loss: 2039.022583\n",
      "Train: step:  21110, time: 0.216, loss: 2775.374512\n",
      "Train: step:  21120, time: 0.187, loss: 314.533417\n",
      "Train: step:  21130, time: 0.195, loss: 2328.568848\n",
      "Train: step:  21140, time: 0.188, loss: 2404.111084\n",
      "Train: step:  21150, time: 0.198, loss: 3753.131592\n",
      "Train: step:  21160, time: 0.193, loss: 306.496429\n",
      "Train: step:  21170, time: 0.197, loss: 2779.918701\n",
      "Train: step:  21180, time: 0.221, loss: 1394.661987\n",
      "Train: step:  21190, time: 0.233, loss: 2629.856445\n",
      "Train: step:  21200, time: 0.241, loss: 1675.167480\n",
      "Train: step:  21210, time: 0.194, loss: 851.330139\n",
      "Train: step:  21220, time: 0.193, loss: 1218.132568\n",
      "Train: step:  21230, time: 0.214, loss: 1717.899658\n",
      "Train: step:  21240, time: 0.239, loss: 327.847717\n",
      "Train: step:  21250, time: 0.185, loss: 2077.540039\n",
      "Train: step:  21260, time: 0.188, loss: 1346.187378\n",
      "Train: step:  21270, time: 0.192, loss: 1293.386230\n",
      "Train: step:  21280, time: 0.219, loss: 1335.318726\n",
      "Train: step:  21290, time: 0.245, loss: 1769.330322\n",
      "Train: step:  21300, time: 0.227, loss: 2210.913330\n",
      "Train: step:  21310, time: 0.202, loss: 3339.068848\n",
      "Train: step:  21320, time: 0.190, loss: 831.475342\n",
      "Train: step:  21330, time: 0.217, loss: 629.184326\n",
      "Train: step:  21340, time: 0.218, loss: 285.784943\n",
      "Train: step:  21350, time: 0.228, loss: 1993.156372\n",
      "Train: step:  21360, time: 0.191, loss: 1910.077515\n",
      "Train: step:  21370, time: 0.227, loss: 777.823242\n",
      "Train: step:  21380, time: 0.239, loss: 2722.794922\n",
      "Train: step:  21390, time: 0.189, loss: 844.990479\n",
      "Train: step:  21400, time: 0.190, loss: 1807.966309\n",
      "Train: step:  21410, time: 0.219, loss: 4112.104004\n",
      "Train: step:  21420, time: 0.198, loss: 1276.027100\n",
      "Train: step:  21430, time: 0.196, loss: 2732.559082\n",
      "Train: step:  21440, time: 0.198, loss: 1073.276001\n",
      "Train: step:  21450, time: 0.208, loss: 1157.341187\n",
      "Train: step:  21460, time: 0.193, loss: 1938.411499\n",
      "Train: step:  21470, time: 0.189, loss: 1707.744141\n",
      "Train: step:  21480, time: 0.218, loss: 983.051758\n",
      "Train: step:  21490, time: 0.189, loss: 1235.222290\n",
      "Train: step:  21500, time: 0.191, loss: 663.231323\n",
      "Train: step:  21510, time: 0.223, loss: 1365.529541\n",
      "Train: step:  21520, time: 0.219, loss: 1783.273071\n",
      "Train: step:  21530, time: 0.201, loss: 1492.369385\n",
      "Train: step:  21540, time: 0.233, loss: 1601.382446\n",
      "Train: step:  21550, time: 0.202, loss: 2971.870850\n",
      "Train: step:  21560, time: 0.214, loss: 371.994781\n",
      "Train: step:  21570, time: 0.193, loss: 1679.952393\n",
      "Train: step:  21580, time: 0.195, loss: 2081.508301\n",
      "Train: step:  21590, time: 0.190, loss: 1447.227173\n",
      "Train: step:  21600, time: 0.196, loss: 1486.617554\n",
      "Train: step:  21610, time: 0.194, loss: 1459.384155\n",
      "Train: step:  21620, time: 0.230, loss: 1373.613770\n",
      "Train: step:  21630, time: 0.194, loss: 1253.799683\n",
      "Train: step:  21640, time: 0.190, loss: 2089.422363\n",
      "Train: step:  21650, time: 0.191, loss: 1050.454346\n",
      "Train: step:  21660, time: 0.193, loss: 1206.430054\n",
      "Train: step:  21670, time: 0.189, loss: 2316.087646\n",
      "Train: step:  21680, time: 0.246, loss: 1229.733154\n",
      "Train: step:  21690, time: 0.193, loss: 2050.684570\n",
      "Train: step:  21700, time: 0.232, loss: 585.816284\n",
      "Train: step:  21710, time: 0.241, loss: 724.686584\n",
      "Train: step:  21720, time: 0.194, loss: 1858.826416\n",
      "Train: step:  21730, time: 0.195, loss: 3379.446777\n",
      "Train: step:  21740, time: 0.198, loss: 2661.403076\n",
      "Train: step:  21750, time: 0.193, loss: 3270.422852\n",
      "Train: step:  21760, time: 0.190, loss: 652.793457\n",
      "Train: step:  21770, time: 0.185, loss: 1996.413330\n",
      "Train: step:  21780, time: 0.229, loss: 1530.903564\n",
      "Train: step:  21790, time: 0.190, loss: 1439.828247\n",
      "Train: step:  21800, time: 0.188, loss: 1055.665527\n",
      "Train: step:  21810, time: 0.206, loss: 2722.489502\n",
      "Train: step:  21820, time: 0.187, loss: 1215.702881\n",
      "Train: step:  21830, time: 0.189, loss: 1707.179199\n",
      "Train: step:  21840, time: 0.237, loss: 2512.329834\n",
      "Train: step:  21850, time: 0.184, loss: 1404.603882\n",
      "Train: step:  21860, time: 0.188, loss: 2654.230713\n",
      "Train: step:  21870, time: 0.224, loss: 2019.556152\n",
      "Train: step:  21880, time: 0.195, loss: 1026.520874\n",
      "Train: step:  21890, time: 0.186, loss: 1767.574585\n",
      "Train: step:  21900, time: 0.239, loss: 1962.382935\n",
      "Train: step:  21910, time: 0.217, loss: 2108.608398\n",
      "Train: step:  21920, time: 0.244, loss: 2631.808105\n",
      "Train: step:  21930, time: 0.188, loss: 1760.748657\n",
      "Train: step:  21940, time: 0.193, loss: 3230.528076\n",
      "Train: step:  21950, time: 0.192, loss: 1561.947021\n",
      "Train: step:  21960, time: 0.227, loss: 849.061890\n",
      "Train: step:  21970, time: 0.245, loss: 1149.537720\n",
      "Train: step:  21980, time: 0.193, loss: 783.901306\n",
      "Train: step:  21990, time: 0.257, loss: 845.350159\n",
      "Train: step:  22000, time: 0.217, loss: 2868.797119\n",
      "Train: step:  22010, time: 0.217, loss: 1619.206543\n",
      "Train: step:  22020, time: 0.190, loss: 2330.190430\n",
      "Train: step:  22030, time: 0.195, loss: 2410.376953\n",
      "Train: step:  22040, time: 0.187, loss: 4607.782715\n",
      "Train: step:  22050, time: 0.222, loss: 2424.887451\n",
      "Train: step:  22060, time: 0.285, loss: 779.022461\n",
      "Train: step:  22070, time: 0.190, loss: 2757.202148\n",
      "Train: step:  22080, time: 0.187, loss: 3092.752197\n",
      "Train: step:  22090, time: 0.191, loss: 3816.771240\n",
      "Train: step:  22100, time: 0.198, loss: 1686.919678\n",
      "Train: step:  22110, time: 0.222, loss: 1997.542725\n",
      "Train: step:  22120, time: 0.187, loss: 2766.269043\n",
      "Train: step:  22130, time: 0.191, loss: 3958.346924\n",
      "Train: step:  22140, time: 0.217, loss: 2360.101318\n",
      "Train: step:  22150, time: 0.217, loss: 353.663635\n",
      "Train: step:  22160, time: 0.190, loss: 1989.370605\n",
      "Train: step:  22170, time: 0.187, loss: 3859.530029\n",
      "Train: step:  22180, time: 0.210, loss: 1134.911621\n",
      "Train: step:  22190, time: 0.199, loss: 2231.984375\n",
      "Train: step:  22200, time: 0.225, loss: 782.538269\n",
      "Train: step:  22210, time: 0.218, loss: 3601.753174\n",
      "Train: step:  22220, time: 0.187, loss: 2957.161865\n",
      "Train: step:  22230, time: 0.188, loss: 1701.401611\n",
      "Train: step:  22240, time: 0.196, loss: 668.103455\n",
      "Train: step:  22250, time: 0.200, loss: 1525.063599\n",
      "Train: step:  22260, time: 0.190, loss: 2704.280273\n",
      "Train: step:  22270, time: 0.216, loss: 858.785461\n",
      "Train: step:  22280, time: 0.196, loss: 1850.014160\n",
      "Train: step:  22290, time: 0.231, loss: 3038.375488\n",
      "Train: step:  22300, time: 0.203, loss: 295.655457\n",
      "Train: step:  22310, time: 0.222, loss: 3128.897217\n",
      "Train: step:  22320, time: 0.216, loss: 2127.476807\n",
      "Train: step:  22330, time: 0.195, loss: 2972.728027\n",
      "Train: step:  22340, time: 0.190, loss: 294.562439\n",
      "Train: step:  22350, time: 0.195, loss: 1380.177490\n",
      "Train: step:  22360, time: 0.189, loss: 2803.141846\n",
      "Train: step:  22370, time: 0.188, loss: 1643.294678\n",
      "Train: step:  22380, time: 0.193, loss: 1340.004272\n",
      "Train: step:  22390, time: 0.220, loss: 4275.267578\n",
      "Train: step:  22400, time: 0.188, loss: 1911.925781\n",
      "Train: step:  22410, time: 0.189, loss: 2179.143066\n",
      "Train: step:  22420, time: 0.191, loss: 3849.365234\n",
      "Train: step:  22430, time: 0.193, loss: 3179.399414\n",
      "Train: step:  22440, time: 0.188, loss: 1432.375000\n",
      "Train: step:  22450, time: 0.220, loss: 2332.000977\n",
      "Train: step:  22460, time: 0.192, loss: 2373.834961\n",
      "Train: step:  22470, time: 0.203, loss: 1316.979492\n",
      "Train: step:  22480, time: 0.196, loss: 648.784973\n",
      "Train: step:  22490, time: 0.218, loss: 1455.035278\n",
      "Train: step:  22500, time: 0.192, loss: 1049.560547\n",
      "Train: step:  22510, time: 0.192, loss: 2448.239014\n",
      "Train: step:  22520, time: 0.233, loss: 1381.293335\n",
      "Train: step:  22530, time: 0.193, loss: 2150.981201\n",
      "Train: step:  22540, time: 0.187, loss: 1078.666870\n",
      "Train: step:  22550, time: 0.230, loss: 446.554077\n",
      "Train: step:  22560, time: 0.198, loss: 2359.162598\n",
      "Train: step:  22570, time: 0.228, loss: 742.593079\n",
      "Train: step:  22580, time: 0.195, loss: 978.226074\n",
      "Train: step:  22590, time: 0.191, loss: 976.118652\n",
      "Train: step:  22600, time: 0.192, loss: 752.226318\n",
      "Train: step:  22610, time: 0.190, loss: 4720.446289\n",
      "Train: step:  22620, time: 0.208, loss: 2091.623779\n",
      "Train: step:  22630, time: 0.237, loss: 1903.356079\n",
      "Train: step:  22640, time: 0.197, loss: 767.518188\n",
      "Train: step:  22650, time: 0.239, loss: 538.257019\n",
      "Train: step:  22660, time: 0.223, loss: 2738.824219\n",
      "Train: step:  22670, time: 0.188, loss: 633.995300\n",
      "Train: step:  22680, time: 0.221, loss: 878.983459\n",
      "Train: step:  22690, time: 0.194, loss: 1304.789062\n",
      "Train: step:  22700, time: 0.203, loss: 2572.826660\n",
      "Train: step:  22710, time: 0.189, loss: 672.668091\n",
      "Train: step:  22720, time: 0.189, loss: 2072.745117\n",
      "Train: step:  22730, time: 0.227, loss: 662.215759\n",
      "Train: step:  22740, time: 0.193, loss: 500.415619\n",
      "Train: step:  22750, time: 0.186, loss: 1699.939331\n",
      "Train: step:  22760, time: 0.193, loss: 1653.281738\n",
      "Train: step:  22770, time: 0.199, loss: 875.595398\n",
      "Train: step:  22780, time: 0.184, loss: 2429.260254\n",
      "Train: step:  22790, time: 0.250, loss: 853.586853\n",
      "Train: step:  22800, time: 0.196, loss: 1565.014893\n",
      "Train: step:  22810, time: 0.216, loss: 2042.395508\n",
      "Train: step:  22820, time: 0.219, loss: 1837.933838\n",
      "Train: step:  22830, time: 0.219, loss: 1337.756348\n",
      "Train: step:  22840, time: 0.233, loss: 1374.050293\n",
      "Train: step:  22850, time: 0.219, loss: 3330.186279\n",
      "Train: step:  22860, time: 0.184, loss: 1741.389038\n",
      "Train: step:  22870, time: 0.198, loss: 1901.116455\n",
      "Train: step:  22880, time: 0.219, loss: 2776.365234\n",
      "Train: step:  22890, time: 0.196, loss: 3771.585449\n",
      "Train: step:  22900, time: 0.193, loss: 1112.249268\n",
      "Train: step:  22910, time: 0.193, loss: 1667.802124\n",
      "Train: step:  22920, time: 0.255, loss: 3201.695557\n",
      "Train: step:  22930, time: 0.194, loss: 1952.764160\n",
      "Train: step:  22940, time: 0.227, loss: 2445.163574\n",
      "Train: step:  22950, time: 0.187, loss: 2772.000732\n",
      "Train: step:  22960, time: 0.217, loss: 1435.166138\n",
      "Train: step:  22970, time: 0.195, loss: 1101.467407\n",
      "Train: step:  22980, time: 0.200, loss: 3132.886719\n",
      "Train: step:  22990, time: 0.208, loss: 1293.269409\n",
      "Train: step:  23000, time: 0.238, loss: 337.041077\n",
      "Train: step:  23010, time: 0.187, loss: 622.960205\n",
      "Train: step:  23020, time: 0.209, loss: 2077.687012\n",
      "Train: step:  23030, time: 0.227, loss: 782.489014\n",
      "Train: step:  23040, time: 0.199, loss: 2848.925293\n",
      "Train: step:  23050, time: 0.215, loss: 2235.607666\n",
      "Train: step:  23060, time: 0.214, loss: 439.657349\n",
      "Train: step:  23070, time: 0.218, loss: 2251.328369\n",
      "Train: step:  23080, time: 0.228, loss: 1695.519897\n",
      "Train: step:  23090, time: 0.189, loss: 1663.956055\n",
      "Train: step:  23100, time: 0.219, loss: 2166.599121\n",
      "Train: step:  23110, time: 0.192, loss: 2929.715820\n",
      "Train: step:  23120, time: 0.197, loss: 1814.622070\n",
      "Train: step:  23130, time: 0.259, loss: 1316.419678\n",
      "Train: step:  23140, time: 0.196, loss: 2493.996826\n",
      "Train: step:  23150, time: 0.243, loss: 3221.150879\n",
      "Train: step:  23160, time: 0.194, loss: 3759.554443\n",
      "Train: step:  23170, time: 0.193, loss: 2657.765381\n",
      "Train: step:  23180, time: 0.220, loss: 984.611145\n",
      "Train: step:  23190, time: 0.192, loss: 1958.246948\n",
      "Train: step:  23200, time: 0.187, loss: 1851.759277\n",
      "Train: step:  23210, time: 0.202, loss: 2143.513672\n",
      "Train: step:  23220, time: 0.225, loss: 3923.616455\n",
      "Train: step:  23230, time: 0.192, loss: 920.011230\n",
      "Train: step:  23240, time: 0.217, loss: 1546.292847\n",
      "Train: step:  23250, time: 0.202, loss: 1519.884888\n",
      "Train: step:  23260, time: 0.193, loss: 2513.264404\n",
      "Train: step:  23270, time: 0.196, loss: 3321.020020\n",
      "Train: step:  23280, time: 0.191, loss: 1068.011108\n",
      "Train: step:  23290, time: 0.200, loss: 2558.681396\n",
      "Train: step:  23300, time: 0.186, loss: 1288.196167\n",
      "Train: step:  23310, time: 0.189, loss: 2400.907227\n",
      "Train: step:  23320, time: 0.195, loss: 2376.662598\n",
      "Train: step:  23330, time: 0.190, loss: 594.484802\n",
      "Train: step:  23340, time: 0.197, loss: 2342.097412\n",
      "Train: step:  23350, time: 0.194, loss: 1828.756348\n",
      "Train: step:  23360, time: 0.197, loss: 1539.683228\n",
      "Train: step:  23370, time: 0.184, loss: 2214.548340\n",
      "Train: step:  23380, time: 0.199, loss: 1675.033813\n",
      "Train: step:  23390, time: 0.208, loss: 3076.884277\n",
      "Train: step:  23400, time: 0.188, loss: 1228.089722\n",
      "Train: step:  23410, time: 0.192, loss: 528.974121\n",
      "Train: step:  23420, time: 0.191, loss: 2120.761719\n",
      "Train: step:  23430, time: 0.198, loss: 2434.502686\n",
      "Train: step:  23440, time: 0.190, loss: 903.450989\n",
      "Train: step:  23450, time: 0.236, loss: 3124.296631\n",
      "Train: step:  23460, time: 0.191, loss: 2406.633545\n",
      "Train: step:  23470, time: 0.219, loss: 3178.140381\n",
      "Train: step:  23480, time: 0.192, loss: 1439.246338\n",
      "Train: step:  23490, time: 0.192, loss: 1682.302368\n",
      "Train: step:  23500, time: 0.193, loss: 771.015259\n",
      "Train: step:  23510, time: 0.191, loss: 1327.364136\n",
      "Train: step:  23520, time: 0.191, loss: 1212.669678\n",
      "Train: step:  23530, time: 0.199, loss: 1128.071899\n",
      "Train: step:  23540, time: 0.192, loss: 2099.915527\n",
      "Train: step:  23550, time: 0.227, loss: 3819.213867\n",
      "Train: step:  23560, time: 0.190, loss: 2386.082764\n",
      "Train: step:  23570, time: 0.190, loss: 4638.971680\n",
      "Train: step:  23580, time: 0.184, loss: 3017.324219\n",
      "Train: step:  23590, time: 0.200, loss: 2936.589111\n",
      "Train: step:  23600, time: 0.231, loss: 3199.016602\n",
      "Train: step:  23610, time: 0.196, loss: 2194.645020\n",
      "Train: step:  23620, time: 0.233, loss: 1655.494507\n",
      "Train: step:  23630, time: 0.193, loss: 3292.508789\n",
      "Train: step:  23640, time: 0.193, loss: 1037.901367\n",
      "Train: step:  23650, time: 0.194, loss: 647.228149\n",
      "Train: step:  23660, time: 0.232, loss: 6281.520996\n",
      "Train: step:  23670, time: 0.191, loss: 1953.316162\n",
      "Train: step:  23680, time: 0.197, loss: 755.596924\n",
      "Train: step:  23690, time: 0.188, loss: 2146.602295\n",
      "Train: step:  23700, time: 0.194, loss: 4198.040039\n",
      "Train: step:  23710, time: 0.216, loss: 781.571655\n",
      "Train: step:  23720, time: 0.218, loss: 233.506470\n",
      "Train: step:  23730, time: 0.195, loss: 1975.654175\n",
      "Train: step:  23740, time: 0.233, loss: 1854.445190\n",
      "Train: step:  23750, time: 0.185, loss: 2043.469849\n",
      "Train: step:  23760, time: 0.197, loss: 2046.229980\n",
      "Train: step:  23770, time: 0.186, loss: 2183.060547\n",
      "Train: step:  23780, time: 0.248, loss: 3188.099121\n",
      "Train: step:  23790, time: 0.222, loss: 1577.208496\n",
      "Train: step:  23800, time: 0.188, loss: 1668.994629\n",
      "Train: step:  23810, time: 0.184, loss: 976.449036\n",
      "Train: step:  23820, time: 0.204, loss: 3315.530762\n",
      "Train: step:  23830, time: 0.197, loss: 2251.746094\n",
      "Train: step:  23840, time: 0.194, loss: 3629.687012\n",
      "Train: step:  23850, time: 0.190, loss: 1209.815308\n",
      "Train: step:  23860, time: 0.199, loss: 638.784729\n",
      "Train: step:  23870, time: 0.195, loss: 2055.429443\n",
      "Train: step:  23880, time: 0.228, loss: 1908.576782\n",
      "Train: step:  23890, time: 0.245, loss: 1524.337280\n",
      "Train: step:  23900, time: 0.191, loss: 1210.211182\n",
      "Train: step:  23910, time: 0.206, loss: 5209.292480\n",
      "Train: step:  23920, time: 0.214, loss: 3637.281250\n",
      "Train: step:  23930, time: 0.198, loss: 571.416260\n",
      "Train: step:  23940, time: 0.196, loss: 3942.071533\n",
      "Train: step:  23950, time: 0.220, loss: 1756.262207\n",
      "Train: step:  23960, time: 0.197, loss: 1927.729370\n",
      "Train: step:  23970, time: 0.217, loss: 3234.118164\n",
      "Train: step:  23980, time: 0.190, loss: 2221.551758\n",
      "Train: step:  23990, time: 0.187, loss: 2404.101562\n",
      "Train: step:  24000, time: 0.196, loss: 1763.709351\n",
      "Train: step:  24010, time: 0.244, loss: 2650.229736\n",
      "Train: step:  24020, time: 0.227, loss: 985.782227\n",
      "Train: step:  24030, time: 0.192, loss: 2369.499512\n",
      "Train: step:  24040, time: 0.190, loss: 1012.391296\n",
      "Train: step:  24050, time: 0.194, loss: 920.956116\n",
      "Train: step:  24060, time: 0.189, loss: 1440.948853\n",
      "Train: step:  24070, time: 0.190, loss: 2554.376221\n",
      "Train: step:  24080, time: 0.188, loss: 3277.072021\n",
      "Train: step:  24090, time: 0.205, loss: 1091.352539\n",
      "Train: step:  24100, time: 0.184, loss: 763.037231\n",
      "Train: step:  24110, time: 0.189, loss: 419.377991\n",
      "Train: step:  24120, time: 0.182, loss: 2541.662354\n",
      "Train: step:  24130, time: 0.193, loss: 1516.799438\n",
      "Train: step:  24140, time: 0.197, loss: 717.996033\n",
      "Train: step:  24150, time: 0.241, loss: 389.776672\n",
      "Train: step:  24160, time: 0.186, loss: 3171.329346\n",
      "Train: step:  24170, time: 0.217, loss: 902.854126\n",
      "Train: step:  24180, time: 0.191, loss: 1853.523438\n",
      "Train: step:  24190, time: 0.270, loss: 1161.433594\n",
      "Train: step:  24200, time: 0.194, loss: 3469.587402\n",
      "Train: step:  24210, time: 0.189, loss: 1895.620239\n",
      "Train: step:  24220, time: 0.189, loss: 951.743164\n",
      "Train: step:  24230, time: 0.193, loss: 866.940979\n",
      "Train: step:  24240, time: 0.198, loss: 1876.976685\n",
      "Train: step:  24250, time: 0.192, loss: 1245.916504\n",
      "Train: step:  24260, time: 0.194, loss: 1460.076538\n",
      "Train: step:  24270, time: 0.190, loss: 2743.739502\n",
      "Train: step:  24280, time: 0.187, loss: 1948.088623\n",
      "Train: step:  24290, time: 0.222, loss: 1157.672974\n",
      "Train: step:  24300, time: 0.188, loss: 2303.375000\n",
      "Train: step:  24310, time: 0.231, loss: 1299.785278\n",
      "Train: step:  24320, time: 0.195, loss: 3262.980957\n",
      "Train: step:  24330, time: 0.229, loss: 2986.855957\n",
      "Train: step:  24340, time: 0.236, loss: 2342.279053\n",
      "Train: step:  24350, time: 0.227, loss: 1344.155762\n",
      "Train: step:  24360, time: 0.216, loss: 2009.797852\n",
      "Train: step:  24370, time: 0.186, loss: 2626.249023\n",
      "Train: step:  24380, time: 0.219, loss: 1587.056763\n",
      "Train: step:  24390, time: 0.202, loss: 5115.894531\n",
      "Train: step:  24400, time: 0.194, loss: 221.791321\n",
      "Train: step:  24410, time: 0.190, loss: 321.513184\n",
      "Train: step:  24420, time: 0.217, loss: 1980.389038\n",
      "Train: step:  24430, time: 0.193, loss: 2762.315918\n",
      "Train: step:  24440, time: 0.186, loss: 2115.750244\n",
      "Train: step:  24450, time: 0.198, loss: 3720.153809\n",
      "Train: step:  24460, time: 0.217, loss: 978.656799\n",
      "Train: step:  24470, time: 0.202, loss: 754.270874\n",
      "Train: step:  24480, time: 0.194, loss: 518.298157\n",
      "Train: step:  24490, time: 0.195, loss: 2793.723877\n",
      "Train: step:  24500, time: 0.217, loss: 400.783417\n",
      "Train: step:  24510, time: 0.195, loss: 994.445496\n",
      "Train: step:  24520, time: 0.225, loss: 1574.013184\n",
      "Train: step:  24530, time: 0.187, loss: 583.741028\n",
      "Train: step:  24540, time: 0.191, loss: 394.357758\n",
      "Train: step:  24550, time: 0.234, loss: 1747.037109\n",
      "Train: step:  24560, time: 0.238, loss: 1041.005859\n",
      "Train: step:  24570, time: 0.189, loss: 2592.101562\n",
      "Train: step:  24580, time: 0.219, loss: 2215.640625\n",
      "Train: step:  24590, time: 0.194, loss: 754.492859\n",
      "Train: step:  24600, time: 0.191, loss: 868.545227\n",
      "Train: step:  24610, time: 0.216, loss: 3011.050781\n",
      "Train: step:  24620, time: 0.185, loss: 309.568298\n",
      "Train: step:  24630, time: 0.190, loss: 1551.363770\n",
      "Train: step:  24640, time: 0.228, loss: 974.244141\n",
      "Train: step:  24650, time: 0.198, loss: 1789.070679\n",
      "Train: step:  24660, time: 0.249, loss: 2231.301758\n",
      "Train: step:  24670, time: 0.227, loss: 324.862244\n",
      "Train: step:  24680, time: 0.191, loss: 1922.829712\n",
      "Train: step:  24690, time: 0.195, loss: 239.756866\n",
      "Train: step:  24700, time: 0.184, loss: 1278.984131\n",
      "Train: step:  24710, time: 0.192, loss: 2143.097656\n",
      "Train: step:  24720, time: 0.207, loss: 409.170776\n",
      "Train: step:  24730, time: 0.226, loss: 1608.843384\n",
      "Train: step:  24740, time: 0.216, loss: 2423.997314\n",
      "Train: step:  24750, time: 0.195, loss: 1422.589111\n",
      "Train: step:  24760, time: 0.195, loss: 1787.474121\n",
      "Train: step:  24770, time: 0.228, loss: 660.529358\n",
      "Train: step:  24780, time: 0.206, loss: 3354.118652\n",
      "Train: step:  24790, time: 0.192, loss: 2422.478760\n",
      "Train: step:  24800, time: 0.214, loss: 539.230835\n",
      "Train: step:  24810, time: 0.197, loss: 2367.840576\n",
      "Train: step:  24820, time: 0.220, loss: 3134.658691\n",
      "Train: step:  24830, time: 0.224, loss: 767.737854\n",
      "Train: step:  24840, time: 0.193, loss: 2906.614990\n",
      "Train: step:  24850, time: 0.200, loss: 747.003967\n",
      "Train: step:  24860, time: 0.195, loss: 1544.326294\n",
      "Train: step:  24870, time: 0.192, loss: 1046.942749\n",
      "Train: step:  24880, time: 0.209, loss: 989.781433\n",
      "Train: step:  24890, time: 0.221, loss: 1498.126343\n",
      "Train: step:  24900, time: 0.201, loss: 1121.463501\n",
      "Train: step:  24910, time: 0.255, loss: 3122.918457\n",
      "Train: step:  24920, time: 0.252, loss: 2385.482422\n",
      "Train: step:  24930, time: 0.190, loss: 611.966492\n",
      "Train: step:  24940, time: 0.197, loss: 1081.128784\n",
      "Train: step:  24950, time: 0.191, loss: 3975.241699\n",
      "Train: step:  24960, time: 0.193, loss: 2398.404541\n",
      "Train: step:  24970, time: 0.230, loss: 2658.700439\n",
      "Train: step:  24980, time: 0.208, loss: 1313.287842\n",
      "Train: step:  24990, time: 0.225, loss: 1029.035767\n",
      "Train: step:  25000, time: 0.220, loss: 1609.345093\n",
      "Train: step:  25010, time: 0.194, loss: 2448.494141\n",
      "Train: step:  25020, time: 0.243, loss: 1965.173462\n",
      "Train: step:  25030, time: 0.200, loss: 1325.901978\n",
      "Train: step:  25040, time: 0.202, loss: 2076.277588\n",
      "Train: step:  25050, time: 0.196, loss: 775.886230\n",
      "Train: step:  25060, time: 0.194, loss: 2360.662598\n",
      "Train: step:  25070, time: 0.197, loss: 1349.855225\n",
      "Train: step:  25080, time: 0.201, loss: 1779.006470\n",
      "Train: step:  25090, time: 0.227, loss: 1852.904175\n",
      "Train: step:  25100, time: 0.220, loss: 1651.001953\n",
      "Train: step:  25110, time: 0.236, loss: 879.146179\n",
      "Train: step:  25120, time: 0.192, loss: 1078.056396\n",
      "Train: step:  25130, time: 0.200, loss: 616.621887\n",
      "Train: step:  25140, time: 0.197, loss: 942.185974\n",
      "Train: step:  25150, time: 0.202, loss: 1426.258545\n",
      "Train: step:  25160, time: 0.206, loss: 3424.988525\n",
      "Train: step:  25170, time: 0.186, loss: 2718.587891\n",
      "Train: step:  25180, time: 0.190, loss: 1220.427246\n",
      "Train: step:  25190, time: 0.191, loss: 3078.679199\n",
      "Train: step:  25200, time: 0.191, loss: 2791.694336\n",
      "Train: step:  25210, time: 0.192, loss: 1421.907104\n",
      "Train: step:  25220, time: 0.218, loss: 521.968628\n",
      "Train: step:  25230, time: 0.235, loss: 2484.383301\n",
      "Train: step:  25240, time: 0.184, loss: 2431.805176\n",
      "Train: step:  25250, time: 0.250, loss: 1068.276611\n",
      "Train: step:  25260, time: 0.214, loss: 2049.823486\n",
      "Train: step:  25270, time: 0.224, loss: 2971.463379\n",
      "Train: step:  25280, time: 0.198, loss: 1276.352295\n",
      "Train: step:  25290, time: 0.233, loss: 456.905548\n",
      "Train: step:  25300, time: 0.218, loss: 1966.728638\n",
      "Train: step:  25310, time: 0.193, loss: 2055.850586\n",
      "Train: step:  25320, time: 0.192, loss: 1158.319336\n",
      "Train: step:  25330, time: 0.194, loss: 1197.089600\n",
      "Train: step:  25340, time: 0.261, loss: 3202.051758\n",
      "Train: step:  25350, time: 0.186, loss: 2383.463623\n",
      "Train: step:  25360, time: 0.193, loss: 3083.075195\n",
      "Train: step:  25370, time: 0.184, loss: 1777.866455\n",
      "Train: step:  25380, time: 0.213, loss: 764.276306\n",
      "Train: step:  25390, time: 0.227, loss: 2519.561768\n",
      "Train: step:  25400, time: 0.192, loss: 886.746704\n",
      "Train: step:  25410, time: 0.189, loss: 1923.983521\n",
      "Train: step:  25420, time: 0.210, loss: 2221.991211\n",
      "Train: step:  25430, time: 0.217, loss: 1623.952271\n",
      "Train: step:  25440, time: 0.192, loss: 910.650696\n",
      "Train: step:  25450, time: 0.196, loss: 1190.035645\n",
      "Train: step:  25460, time: 0.239, loss: 229.265686\n",
      "Train: step:  25470, time: 0.229, loss: 877.710632\n",
      "Train: step:  25480, time: 0.194, loss: 3237.982666\n",
      "Train: step:  25490, time: 0.191, loss: 1275.572510\n",
      "Train: step:  25500, time: 0.198, loss: 916.043579\n",
      "Train: step:  25510, time: 0.188, loss: 1825.466064\n",
      "Train: step:  25520, time: 0.225, loss: 2982.522949\n",
      "Train: step:  25530, time: 0.193, loss: 2331.133545\n",
      "Train: step:  25540, time: 0.199, loss: 2129.048096\n",
      "Train: step:  25550, time: 0.190, loss: 4238.189941\n",
      "Train: step:  25560, time: 0.189, loss: 2263.660645\n",
      "Train: step:  25570, time: 0.201, loss: 3332.144043\n",
      "Train: step:  25580, time: 0.188, loss: 1648.867798\n",
      "Train: step:  25590, time: 0.217, loss: 1871.728882\n",
      "Train: step:  25600, time: 0.229, loss: 896.064087\n",
      "Train: step:  25610, time: 0.193, loss: 2921.845215\n",
      "Train: step:  25620, time: 0.239, loss: 1859.770020\n",
      "Train: step:  25630, time: 0.195, loss: 1391.807251\n",
      "Train: step:  25640, time: 0.193, loss: 1360.405884\n",
      "Train: step:  25650, time: 0.193, loss: 3902.000732\n",
      "Train: step:  25660, time: 0.235, loss: 1365.236816\n",
      "Train: step:  25670, time: 0.198, loss: 1987.554810\n",
      "Train: step:  25680, time: 0.199, loss: 3235.548584\n",
      "Train: step:  25690, time: 0.225, loss: 1406.637695\n",
      "Train: step:  25700, time: 0.196, loss: 2078.977295\n",
      "Train: step:  25710, time: 0.219, loss: 1464.708252\n",
      "Train: step:  25720, time: 0.246, loss: 1473.861816\n",
      "Train: step:  25730, time: 0.190, loss: 2688.698486\n",
      "Train: step:  25740, time: 0.189, loss: 396.807343\n",
      "Train: step:  25750, time: 0.207, loss: 703.143005\n",
      "Train: step:  25760, time: 0.199, loss: 443.964417\n",
      "Train: step:  25770, time: 0.194, loss: 2147.547607\n",
      "Train: step:  25780, time: 0.189, loss: 1610.254272\n",
      "Train: step:  25790, time: 0.198, loss: 2901.058105\n",
      "Train: step:  25800, time: 0.198, loss: 184.670288\n",
      "Train: step:  25810, time: 0.215, loss: 1399.843872\n",
      "Train: step:  25820, time: 0.189, loss: 1770.186157\n",
      "Train: step:  25830, time: 0.199, loss: 1326.935547\n",
      "Train: step:  25840, time: 0.188, loss: 4052.616455\n",
      "Train: step:  25850, time: 0.223, loss: 2319.305908\n",
      "Train: step:  25860, time: 0.196, loss: 3557.927490\n",
      "Train: step:  25870, time: 0.196, loss: 1462.359131\n",
      "Train: step:  25880, time: 0.187, loss: 2630.749512\n",
      "Train: step:  25890, time: 0.193, loss: 1175.150879\n",
      "Train: step:  25900, time: 0.196, loss: 2401.139404\n",
      "Train: step:  25910, time: 0.217, loss: 1384.130127\n",
      "Train: step:  25920, time: 0.197, loss: 2000.567993\n",
      "Train: step:  25930, time: 0.197, loss: 2537.000244\n",
      "Train: step:  25940, time: 0.190, loss: 2217.295898\n",
      "Train: step:  25950, time: 0.203, loss: 1385.285522\n",
      "Train: step:  25960, time: 0.187, loss: 1036.238281\n",
      "Train: step:  25970, time: 0.184, loss: 1386.393677\n",
      "Train: step:  25980, time: 0.195, loss: 1763.145874\n",
      "Train: step:  25990, time: 0.185, loss: 804.931763\n",
      "Train: step:  26000, time: 0.235, loss: 315.956177\n",
      "Train: step:  26010, time: 0.185, loss: 2020.416748\n",
      "Train: step:  26020, time: 0.191, loss: 1833.522705\n",
      "Train: step:  26030, time: 0.196, loss: 1287.295898\n",
      "Train: step:  26040, time: 0.190, loss: 444.609833\n",
      "Train: step:  26050, time: 0.196, loss: 2846.938232\n",
      "Train: step:  26060, time: 0.230, loss: 2790.677002\n",
      "Train: step:  26070, time: 0.209, loss: 1001.623108\n",
      "Train: step:  26080, time: 0.226, loss: 1414.985352\n",
      "Train: step:  26090, time: 0.223, loss: 1035.624146\n",
      "Train: step:  26100, time: 0.243, loss: 2751.509521\n",
      "Train: step:  26110, time: 0.186, loss: 2089.245850\n",
      "Train: step:  26120, time: 0.229, loss: 2427.214111\n",
      "Train: step:  26130, time: 0.188, loss: 1784.845825\n",
      "Train: step:  26140, time: 0.194, loss: 2717.235352\n",
      "Train: step:  26150, time: 0.191, loss: 2794.150146\n",
      "Train: step:  26160, time: 0.201, loss: 1647.298096\n",
      "Train: step:  26170, time: 0.190, loss: 1313.855957\n",
      "Train: step:  26180, time: 0.217, loss: 2228.218262\n",
      "Train: step:  26190, time: 0.228, loss: 1482.132202\n",
      "Train: step:  26200, time: 0.234, loss: 660.868652\n",
      "Train: step:  26210, time: 0.184, loss: 3239.189453\n",
      "Train: step:  26220, time: 0.191, loss: 1727.859741\n",
      "Train: step:  26230, time: 0.230, loss: 3066.746826\n",
      "Train: step:  26240, time: 0.267, loss: 869.330566\n",
      "Train: step:  26250, time: 0.222, loss: 1148.313721\n",
      "Train: step:  26260, time: 0.228, loss: 1993.736572\n",
      "Train: step:  26270, time: 0.235, loss: 1286.865601\n",
      "Train: step:  26280, time: 0.187, loss: 534.378174\n",
      "Train: step:  26290, time: 0.193, loss: 2469.198242\n",
      "Train: step:  26300, time: 0.219, loss: 1920.291382\n",
      "Train: step:  26310, time: 0.190, loss: 920.185669\n",
      "Train: step:  26320, time: 0.195, loss: 580.080261\n",
      "Train: step:  26330, time: 0.196, loss: 1141.817993\n",
      "Train: step:  26340, time: 0.197, loss: 1876.336304\n",
      "Train: step:  26350, time: 0.188, loss: 2469.250977\n",
      "Train: step:  26360, time: 0.197, loss: 1516.394775\n",
      "Train: step:  26370, time: 0.203, loss: 1775.696777\n",
      "Train: step:  26380, time: 0.192, loss: 2977.141113\n",
      "Train: step:  26390, time: 0.190, loss: 310.725861\n",
      "Train: step:  26400, time: 0.192, loss: 2154.231445\n",
      "Train: step:  26410, time: 0.198, loss: 2284.168213\n",
      "Train: step:  26420, time: 0.195, loss: 2997.205078\n",
      "Train: step:  26430, time: 0.192, loss: 4242.096191\n",
      "Train: step:  26440, time: 0.189, loss: 1108.139038\n",
      "Train: step:  26450, time: 0.185, loss: 3519.981934\n",
      "Train: step:  26460, time: 0.189, loss: 1422.512939\n",
      "Train: step:  26470, time: 0.188, loss: 1856.801758\n",
      "Train: step:  26480, time: 0.218, loss: 1720.682129\n",
      "Train: step:  26490, time: 0.186, loss: 2796.596924\n",
      "Train: step:  26500, time: 0.195, loss: 930.521423\n",
      "Train: step:  26510, time: 0.192, loss: 2853.948486\n",
      "Train: step:  26520, time: 0.195, loss: 1684.443237\n",
      "Train: step:  26530, time: 0.198, loss: 1931.480469\n",
      "Train: step:  26540, time: 0.191, loss: 2681.534424\n",
      "Train: step:  26550, time: 0.195, loss: 1162.178345\n",
      "Train: step:  26560, time: 0.186, loss: 2635.218506\n",
      "Train: step:  26570, time: 0.187, loss: 2853.127686\n",
      "Train: step:  26580, time: 0.212, loss: 1948.087646\n",
      "Train: step:  26590, time: 0.199, loss: 1985.737061\n",
      "Train: step:  26600, time: 0.261, loss: 3399.146729\n",
      "Train: step:  26610, time: 0.247, loss: 1751.539185\n",
      "Train: step:  26620, time: 0.228, loss: 2278.202148\n",
      "Train: step:  26630, time: 0.184, loss: 1066.999268\n",
      "Train: step:  26640, time: 0.195, loss: 2832.728027\n",
      "Train: step:  26650, time: 0.213, loss: 832.300110\n",
      "Train: step:  26660, time: 0.194, loss: 1045.529175\n",
      "Train: step:  26670, time: 0.191, loss: 1003.163879\n",
      "Train: step:  26680, time: 0.221, loss: 861.285950\n",
      "Train: step:  26690, time: 0.192, loss: 3711.811768\n",
      "Train: step:  26700, time: 0.186, loss: 2186.552002\n",
      "Train: step:  26710, time: 0.231, loss: 1533.511963\n",
      "Train: step:  26720, time: 0.239, loss: 2830.950928\n",
      "Train: step:  26730, time: 0.234, loss: 3910.837158\n",
      "Train: step:  26740, time: 0.187, loss: 821.179382\n",
      "Train: step:  26750, time: 0.192, loss: 2216.109375\n",
      "Train: step:  26760, time: 0.213, loss: 495.006775\n",
      "Train: step:  26770, time: 0.197, loss: 1740.353760\n",
      "Train: step:  26780, time: 0.202, loss: 1795.929321\n",
      "Train: step:  26790, time: 0.193, loss: 817.399902\n",
      "Train: step:  26800, time: 0.192, loss: 664.748657\n",
      "Train: step:  26810, time: 0.194, loss: 2098.248291\n",
      "Train: step:  26820, time: 0.200, loss: 1342.796265\n",
      "Train: step:  26830, time: 0.247, loss: 578.454529\n",
      "Train: step:  26840, time: 0.184, loss: 3016.904053\n",
      "Train: step:  26850, time: 0.192, loss: 824.937805\n",
      "Train: step:  26860, time: 0.187, loss: 705.327148\n",
      "Train: step:  26870, time: 0.192, loss: 766.860962\n",
      "Train: step:  26880, time: 0.225, loss: 2293.792725\n",
      "Train: step:  26890, time: 0.221, loss: 2532.960449\n",
      "Train: step:  26900, time: 0.186, loss: 2434.927490\n",
      "Train: step:  26910, time: 0.242, loss: 2414.070068\n",
      "Train: step:  26920, time: 0.191, loss: 618.406677\n",
      "Train: step:  26930, time: 0.189, loss: 1630.778564\n",
      "Train: step:  26940, time: 0.220, loss: 2723.133789\n",
      "Train: step:  26950, time: 0.221, loss: 1746.294067\n",
      "Train: step:  26960, time: 0.194, loss: 2976.993408\n",
      "Train: step:  26970, time: 0.214, loss: 1175.296509\n",
      "Train: step:  26980, time: 0.186, loss: 2400.492920\n",
      "Train: step:  26990, time: 0.194, loss: 2132.905029\n",
      "Train: step:  27000, time: 0.192, loss: 1836.516602\n",
      "Train: step:  27010, time: 0.218, loss: 2763.605225\n",
      "Train: step:  27020, time: 0.190, loss: 419.316254\n",
      "Train: step:  27030, time: 0.207, loss: 3941.572754\n",
      "Train: step:  27040, time: 0.240, loss: 2419.990723\n",
      "Train: step:  27050, time: 0.191, loss: 268.823242\n",
      "Train: step:  27060, time: 0.192, loss: 917.139099\n",
      "Train: step:  27070, time: 0.227, loss: 1036.181396\n",
      "Train: step:  27080, time: 0.253, loss: 2498.444580\n",
      "Train: step:  27090, time: 0.233, loss: 1268.951782\n",
      "Train: step:  27100, time: 0.191, loss: 1299.273438\n",
      "Train: step:  27110, time: 0.189, loss: 2200.152100\n",
      "Train: step:  27120, time: 0.217, loss: 2157.609375\n",
      "Train: step:  27130, time: 0.193, loss: 1122.987793\n",
      "Train: step:  27140, time: 0.188, loss: 455.955719\n",
      "Train: step:  27150, time: 0.199, loss: 2858.420654\n",
      "Train: step:  27160, time: 0.226, loss: 3229.353027\n",
      "Train: step:  27170, time: 0.194, loss: 2334.512695\n",
      "Train: step:  27180, time: 0.217, loss: 814.469360\n",
      "Train: step:  27190, time: 0.196, loss: 2684.018311\n",
      "Train: step:  27200, time: 0.218, loss: 2274.760986\n",
      "Train: step:  27210, time: 0.189, loss: 2474.569580\n",
      "Train: step:  27220, time: 0.189, loss: 2552.345703\n",
      "Train: step:  27230, time: 0.220, loss: 660.477295\n",
      "Train: step:  27240, time: 0.191, loss: 1427.902344\n",
      "Train: step:  27250, time: 0.185, loss: 342.576630\n",
      "Train: step:  27260, time: 0.189, loss: 1234.384033\n",
      "Train: step:  27270, time: 0.226, loss: 2534.932129\n",
      "Train: step:  27280, time: 0.195, loss: 1314.239990\n",
      "Train: step:  27290, time: 0.189, loss: 2583.197998\n",
      "Train: step:  27300, time: 0.218, loss: 1552.487061\n",
      "Train: step:  27310, time: 0.207, loss: 4559.592773\n",
      "Train: step:  27320, time: 0.199, loss: 2885.326660\n",
      "Train: step:  27330, time: 0.196, loss: 2078.198730\n",
      "Train: step:  27340, time: 0.193, loss: 4698.689941\n",
      "Train: step:  27350, time: 0.209, loss: 3034.451660\n",
      "Train: step:  27360, time: 0.189, loss: 4668.914551\n",
      "Train: step:  27370, time: 0.193, loss: 1260.892334\n",
      "Train: step:  27380, time: 0.201, loss: 1364.916870\n",
      "Train: step:  27390, time: 0.185, loss: 1781.234985\n",
      "Train: step:  27400, time: 0.187, loss: 1778.383179\n",
      "Train: step:  27410, time: 0.216, loss: 1658.757812\n",
      "Train: step:  27420, time: 0.194, loss: 1548.730469\n",
      "Train: step:  27430, time: 0.236, loss: 871.331909\n",
      "Train: step:  27440, time: 0.191, loss: 479.800079\n",
      "Train: step:  27450, time: 0.251, loss: 1891.440186\n",
      "Train: step:  27460, time: 0.193, loss: 238.487076\n",
      "Train: step:  27470, time: 0.190, loss: 2373.074219\n",
      "Train: step:  27480, time: 0.194, loss: 2579.961670\n",
      "Train: step:  27490, time: 0.191, loss: 3040.448242\n",
      "Train: step:  27500, time: 0.227, loss: 1560.396118\n",
      "Train: step:  27510, time: 0.189, loss: 1041.030762\n",
      "Train: step:  27520, time: 0.194, loss: 571.457031\n",
      "Train: step:  27530, time: 0.200, loss: 4244.216797\n",
      "Train: step:  27540, time: 0.231, loss: 182.854538\n",
      "Train: step:  27550, time: 0.195, loss: 934.533936\n",
      "Train: step:  27560, time: 0.216, loss: 3208.204590\n",
      "Train: step:  27570, time: 0.191, loss: 905.963257\n",
      "Train: step:  27580, time: 0.216, loss: 1092.283936\n",
      "Train: step:  27590, time: 0.233, loss: 2744.673828\n",
      "Train: step:  27600, time: 0.190, loss: 1517.371460\n",
      "Train: step:  27610, time: 0.228, loss: 2189.961182\n",
      "Train: step:  27620, time: 0.236, loss: 1641.730469\n",
      "Train: step:  27630, time: 0.193, loss: 1704.381958\n",
      "Train: step:  27640, time: 0.228, loss: 3288.793945\n",
      "Train: step:  27650, time: 0.229, loss: 328.489777\n",
      "Train: step:  27660, time: 0.189, loss: 1345.750000\n",
      "Train: step:  27670, time: 0.187, loss: 1823.543701\n",
      "Train: step:  27680, time: 0.190, loss: 684.618896\n",
      "Train: step:  27690, time: 0.228, loss: 2132.014893\n",
      "Train: step:  27700, time: 0.203, loss: 1499.670288\n",
      "Train: step:  27710, time: 0.200, loss: 1097.635254\n",
      "Train: step:  27720, time: 0.187, loss: 2004.142578\n",
      "Train: step:  27730, time: 0.197, loss: 1793.475830\n",
      "Train: step:  27740, time: 0.194, loss: 1094.520386\n",
      "Train: step:  27750, time: 0.276, loss: 3265.123535\n",
      "Train: step:  27760, time: 0.186, loss: 2485.414795\n",
      "Train: step:  27770, time: 0.216, loss: 1336.217163\n",
      "Train: step:  27780, time: 0.219, loss: 1150.798706\n",
      "Train: step:  27790, time: 0.252, loss: 1124.798706\n",
      "Train: step:  27800, time: 0.229, loss: 1877.998047\n",
      "Train: step:  27810, time: 0.204, loss: 3155.535400\n",
      "Train: step:  27820, time: 0.190, loss: 2302.102539\n",
      "Train: step:  27830, time: 0.194, loss: 1504.235352\n",
      "Train: step:  27840, time: 0.197, loss: 336.325195\n",
      "Train: step:  27850, time: 0.200, loss: 1367.610596\n",
      "Train: step:  27860, time: 0.218, loss: 2442.675049\n",
      "Train: step:  27870, time: 0.187, loss: 2133.770996\n",
      "Train: step:  27880, time: 0.211, loss: 2434.036865\n",
      "Train: step:  27890, time: 0.193, loss: 2931.012939\n",
      "Train: step:  27900, time: 0.190, loss: 779.330505\n",
      "Train: step:  27910, time: 0.201, loss: 2589.380859\n",
      "Train: step:  27920, time: 0.219, loss: 2049.659424\n",
      "Train: step:  27930, time: 0.188, loss: 1128.236084\n",
      "Train: step:  27940, time: 0.189, loss: 1409.378296\n",
      "Train: step:  27950, time: 0.192, loss: 2503.893799\n",
      "Train: step:  27960, time: 0.220, loss: 2714.140381\n",
      "Train: step:  27970, time: 0.250, loss: 4027.551270\n",
      "Train: step:  27980, time: 0.230, loss: 1380.849731\n",
      "Train: step:  27990, time: 0.207, loss: 2910.070801\n",
      "Train: step:  28000, time: 0.191, loss: 2735.165771\n",
      "Train: step:  28010, time: 0.194, loss: 692.351257\n",
      "Train: step:  28020, time: 0.281, loss: 1047.638184\n",
      "Train: step:  28030, time: 0.182, loss: 683.617371\n",
      "Train: step:  28040, time: 0.200, loss: 382.481140\n",
      "Train: step:  28050, time: 0.222, loss: 2405.708252\n",
      "Train: step:  28060, time: 0.230, loss: 867.938904\n",
      "Train: step:  28070, time: 0.224, loss: 645.723083\n",
      "Train: step:  28080, time: 0.191, loss: 1276.446655\n",
      "Train: step:  28090, time: 0.192, loss: 1699.064209\n",
      "Train: step:  28100, time: 0.217, loss: 3191.351074\n",
      "Train: step:  28110, time: 0.216, loss: 1504.649902\n",
      "Train: step:  28120, time: 0.233, loss: 1631.587158\n",
      "Train: step:  28130, time: 0.200, loss: 619.786438\n",
      "Train: step:  28140, time: 0.189, loss: 748.277039\n",
      "Train: step:  28150, time: 0.192, loss: 2229.116211\n",
      "Train: step:  28160, time: 0.243, loss: 1519.439453\n",
      "Train: step:  28170, time: 0.188, loss: 2989.104980\n",
      "Train: step:  28180, time: 0.197, loss: 3700.501221\n",
      "Train: step:  28190, time: 0.199, loss: 597.804443\n",
      "Train: step:  28200, time: 0.189, loss: 156.832855\n",
      "Train: step:  28210, time: 0.191, loss: 2108.424316\n",
      "Train: step:  28220, time: 0.203, loss: 1753.854736\n",
      "Train: step:  28230, time: 0.199, loss: 692.503418\n",
      "Train: step:  28240, time: 0.190, loss: 423.683716\n",
      "Train: step:  28250, time: 0.196, loss: 1019.851196\n",
      "Train: step:  28260, time: 0.238, loss: 726.019165\n",
      "Train: step:  28270, time: 0.194, loss: 1715.839966\n",
      "Train: step:  28280, time: 0.189, loss: 2045.092773\n",
      "Train: step:  28290, time: 0.216, loss: 1381.732910\n",
      "Train: step:  28300, time: 0.183, loss: 2286.557129\n",
      "Train: step:  28310, time: 0.186, loss: 802.649841\n",
      "Train: step:  28320, time: 0.198, loss: 3440.466064\n",
      "Train: step:  28330, time: 0.216, loss: 3054.138184\n",
      "Train: step:  28340, time: 0.192, loss: 906.547058\n",
      "Train: step:  28350, time: 0.187, loss: 1263.926147\n",
      "Train: step:  28360, time: 0.189, loss: 953.967224\n",
      "Train: step:  28370, time: 0.187, loss: 1444.716431\n",
      "Train: step:  28380, time: 0.194, loss: 843.548157\n",
      "Train: step:  28390, time: 0.199, loss: 5135.934570\n",
      "Train: step:  28400, time: 0.227, loss: 2268.557861\n",
      "Train: step:  28410, time: 0.193, loss: 1016.210999\n",
      "Train: step:  28420, time: 0.217, loss: 3060.527832\n",
      "Train: step:  28430, time: 0.211, loss: 2085.120117\n",
      "Train: step:  28440, time: 0.193, loss: 1445.383545\n",
      "Train: step:  28450, time: 0.188, loss: 931.997864\n",
      "Train: step:  28460, time: 0.190, loss: 2373.786865\n",
      "Train: step:  28470, time: 0.187, loss: 3578.611328\n",
      "Train: step:  28480, time: 0.217, loss: 1566.162354\n",
      "Train: step:  28490, time: 0.238, loss: 1447.031006\n",
      "Train: step:  28500, time: 0.188, loss: 1190.885742\n",
      "Train: step:  28510, time: 0.231, loss: 3679.350830\n",
      "Train: step:  28520, time: 0.198, loss: 2751.744873\n",
      "Train: step:  28530, time: 0.189, loss: 1767.490112\n",
      "Train: step:  28540, time: 0.197, loss: 1796.684692\n",
      "Train: step:  28550, time: 0.216, loss: 814.401367\n",
      "Train: step:  28560, time: 0.187, loss: 1243.414917\n",
      "Train: step:  28570, time: 0.235, loss: 1809.832031\n",
      "Train: step:  28580, time: 0.199, loss: 312.062103\n",
      "Train: step:  28590, time: 0.204, loss: 952.642334\n",
      "Train: step:  28600, time: 0.204, loss: 3046.290283\n",
      "Train: step:  28610, time: 0.207, loss: 958.835449\n",
      "Train: step:  28620, time: 0.229, loss: 1634.907837\n",
      "Train: step:  28630, time: 0.191, loss: 1468.305420\n",
      "Train: step:  28640, time: 0.271, loss: 3347.045898\n",
      "Train: step:  28650, time: 0.187, loss: 4713.409668\n",
      "Train: step:  28660, time: 0.230, loss: 237.943024\n",
      "Train: step:  28670, time: 0.227, loss: 492.106689\n",
      "Train: step:  28680, time: 0.218, loss: 947.619873\n",
      "Train: step:  28690, time: 0.246, loss: 641.818176\n",
      "Train: step:  28700, time: 0.229, loss: 934.934143\n",
      "Train: step:  28710, time: 0.239, loss: 1462.011353\n",
      "Train: step:  28720, time: 0.204, loss: 1405.544922\n",
      "Train: step:  28730, time: 0.197, loss: 1030.564575\n",
      "Train: step:  28740, time: 0.188, loss: 1192.062256\n",
      "Train: step:  28750, time: 0.204, loss: 2326.714600\n",
      "Train: step:  28760, time: 0.194, loss: 904.567627\n",
      "Train: step:  28770, time: 0.188, loss: 2692.046631\n",
      "Train: step:  28780, time: 0.197, loss: 1810.448242\n",
      "Train: step:  28790, time: 0.228, loss: 3300.774414\n",
      "Train: step:  28800, time: 0.193, loss: 1188.064331\n",
      "Train: step:  28810, time: 0.186, loss: 2715.908447\n",
      "Train: step:  28820, time: 0.216, loss: 2910.315674\n",
      "Train: step:  28830, time: 0.192, loss: 4297.100586\n",
      "Train: step:  28840, time: 0.199, loss: 2570.583740\n",
      "Train: step:  28850, time: 0.198, loss: 1718.368408\n",
      "Train: step:  28860, time: 0.195, loss: 787.535828\n",
      "Train: step:  28870, time: 0.221, loss: 1415.144775\n",
      "Train: step:  28880, time: 0.191, loss: 2375.914307\n",
      "Train: step:  28890, time: 0.198, loss: 1580.703979\n",
      "Train: step:  28900, time: 0.190, loss: 1874.943359\n",
      "Train: step:  28910, time: 0.191, loss: 785.591309\n",
      "Train: step:  28920, time: 0.192, loss: 713.899048\n",
      "Train: step:  28930, time: 0.220, loss: 2288.970215\n",
      "Train: step:  28940, time: 0.192, loss: 2270.830811\n",
      "Train: step:  28950, time: 0.196, loss: 2438.118164\n",
      "Train: step:  28960, time: 0.188, loss: 1785.780884\n",
      "Train: step:  28970, time: 0.220, loss: 1267.739014\n",
      "Train: step:  28980, time: 0.191, loss: 2752.696045\n",
      "Train: step:  28990, time: 0.195, loss: 1521.063232\n",
      "Train: step:  29000, time: 0.196, loss: 2783.053467\n",
      "Train: step:  29010, time: 0.193, loss: 1126.247681\n",
      "Train: step:  29020, time: 0.217, loss: 556.058044\n",
      "Train: step:  29030, time: 0.196, loss: 1271.916870\n",
      "Train: step:  29040, time: 0.196, loss: 2706.528809\n",
      "Train: step:  29050, time: 0.217, loss: 635.647766\n",
      "Train: step:  29060, time: 0.194, loss: 1648.184937\n",
      "Train: step:  29070, time: 0.189, loss: 1316.216309\n",
      "Train: step:  29080, time: 0.193, loss: 1734.003418\n",
      "Train: step:  29090, time: 0.194, loss: 1738.197021\n",
      "Train: step:  29100, time: 0.227, loss: 2329.253174\n",
      "Train: step:  29110, time: 0.191, loss: 2460.675293\n",
      "Train: step:  29120, time: 0.186, loss: 3038.737793\n",
      "Train: step:  29130, time: 0.186, loss: 2333.081787\n",
      "Train: step:  29140, time: 0.195, loss: 3022.928711\n",
      "Train: step:  29150, time: 0.229, loss: 2595.663086\n",
      "Train: step:  29160, time: 0.188, loss: 2493.937744\n",
      "Train: step:  29170, time: 0.196, loss: 1391.574585\n",
      "Train: step:  29180, time: 0.198, loss: 971.659180\n",
      "Train: step:  29190, time: 0.209, loss: 868.899902\n",
      "Train: step:  29200, time: 0.199, loss: 2227.520264\n",
      "Train: step:  29210, time: 0.190, loss: 3217.145020\n",
      "Train: step:  29220, time: 0.198, loss: 823.089172\n",
      "Train: step:  29230, time: 0.193, loss: 1470.106323\n",
      "Train: step:  29240, time: 0.213, loss: 756.298523\n",
      "Train: step:  29250, time: 0.230, loss: 1998.577393\n",
      "Train: step:  29260, time: 0.190, loss: 1341.055176\n",
      "Train: step:  29270, time: 0.196, loss: 1796.107056\n",
      "Train: step:  29280, time: 0.196, loss: 1457.969360\n",
      "Train: step:  29290, time: 0.219, loss: 808.295776\n",
      "Train: step:  29300, time: 0.192, loss: 1744.121704\n",
      "Train: step:  29310, time: 0.197, loss: 1254.599609\n",
      "Train: step:  29320, time: 0.200, loss: 2111.331787\n",
      "Train: step:  29330, time: 0.201, loss: 1085.549683\n",
      "Train: step:  29340, time: 0.187, loss: 1810.550049\n",
      "Train: step:  29350, time: 0.237, loss: 245.485016\n",
      "Train: step:  29360, time: 0.216, loss: 3053.882080\n",
      "Train: step:  29370, time: 0.197, loss: 4026.584961\n",
      "Train: step:  29380, time: 0.233, loss: 568.212708\n",
      "Train: step:  29390, time: 0.196, loss: 3864.363525\n",
      "Train: step:  29400, time: 0.189, loss: 1916.604858\n",
      "Train: step:  29410, time: 0.187, loss: 414.838470\n",
      "Train: step:  29420, time: 0.189, loss: 2717.293457\n",
      "Train: step:  29430, time: 0.189, loss: 1318.948486\n",
      "Train: step:  29440, time: 0.190, loss: 934.808289\n",
      "Train: step:  29450, time: 0.229, loss: 2003.628418\n",
      "Train: step:  29460, time: 0.185, loss: 1733.586426\n",
      "Train: step:  29470, time: 0.237, loss: 1068.482300\n",
      "Train: step:  29480, time: 0.188, loss: 875.088013\n",
      "Train: step:  29490, time: 0.189, loss: 1809.389648\n",
      "Train: step:  29500, time: 0.191, loss: 805.566833\n",
      "Train: step:  29510, time: 0.228, loss: 2155.156982\n",
      "Train: step:  29520, time: 0.191, loss: 867.622009\n",
      "Train: step:  29530, time: 0.194, loss: 3029.629150\n",
      "Train: step:  29540, time: 0.191, loss: 607.950500\n",
      "Train: step:  29550, time: 0.195, loss: 2238.832031\n",
      "Train: step:  29560, time: 0.217, loss: 1000.201111\n",
      "Train: step:  29570, time: 0.189, loss: 733.858887\n",
      "Train: step:  29580, time: 0.200, loss: 2688.825684\n",
      "Train: step:  29590, time: 0.194, loss: 1019.653992\n",
      "Train: step:  29600, time: 0.184, loss: 2703.835693\n",
      "Train: step:  29610, time: 0.221, loss: 1578.731934\n",
      "Train: step:  29620, time: 0.195, loss: 2640.316895\n",
      "Train: step:  29630, time: 0.210, loss: 2729.213135\n",
      "Train: step:  29640, time: 0.197, loss: 876.251953\n",
      "Train: step:  29650, time: 0.189, loss: 2472.060791\n",
      "Train: step:  29660, time: 0.194, loss: 2489.418701\n",
      "Train: step:  29670, time: 0.210, loss: 2284.834229\n",
      "Train: step:  29680, time: 0.186, loss: 849.165588\n",
      "Train: step:  29690, time: 0.188, loss: 1093.836060\n",
      "Train: step:  29700, time: 0.214, loss: 441.470520\n",
      "Train: step:  29710, time: 0.195, loss: 1272.399536\n",
      "Train: step:  29720, time: 0.188, loss: 1804.830688\n",
      "Train: step:  29730, time: 0.196, loss: 1361.847168\n",
      "Train: step:  29740, time: 0.197, loss: 466.904663\n",
      "Train: step:  29750, time: 0.230, loss: 2862.580566\n",
      "Train: step:  29760, time: 0.197, loss: 863.710754\n",
      "Train: step:  29770, time: 0.191, loss: 2651.497314\n",
      "Train: step:  29780, time: 0.198, loss: 353.609283\n",
      "Train: step:  29790, time: 0.317, loss: 1935.477173\n",
      "Train: step:  29800, time: 0.198, loss: 3713.171143\n",
      "Train: step:  29810, time: 0.201, loss: 1468.347534\n",
      "Train: step:  29820, time: 0.205, loss: 1577.593750\n",
      "Train: step:  29830, time: 0.209, loss: 2090.601562\n",
      "Train: step:  29840, time: 0.231, loss: 3170.549316\n",
      "Train: step:  29850, time: 0.183, loss: 1245.802124\n",
      "Train: step:  29860, time: 0.204, loss: 1336.736572\n",
      "Train: step:  29870, time: 0.197, loss: 1143.973022\n",
      "Train: step:  29880, time: 0.203, loss: 2715.126221\n",
      "Train: step:  29890, time: 0.192, loss: 1476.078735\n",
      "Train: step:  29900, time: 0.196, loss: 2578.200928\n",
      "Train: step:  29910, time: 0.192, loss: 1678.791504\n",
      "Train: step:  29920, time: 0.203, loss: 692.399353\n",
      "Train: step:  29930, time: 0.209, loss: 3484.263672\n",
      "Train: step:  29940, time: 0.193, loss: 1494.817383\n",
      "Train: step:  29950, time: 0.233, loss: 1430.001343\n",
      "Train: step:  29960, time: 0.190, loss: 2371.367432\n",
      "Train: step:  29970, time: 0.192, loss: 729.681641\n",
      "Train: step:  29980, time: 0.193, loss: 2436.798340\n",
      "Train: step:  29990, time: 0.197, loss: 2683.100098\n",
      "Train: step:  30000, time: 0.238, loss: 1695.714722\n",
      "Train: step:  30010, time: 0.185, loss: 433.723877\n",
      "Train: step:  30020, time: 0.188, loss: 1912.141235\n",
      "Train: step:  30030, time: 0.191, loss: 687.390076\n",
      "Train: step:  30040, time: 0.190, loss: 1731.117798\n",
      "Train: step:  30050, time: 0.191, loss: 1168.101929\n",
      "Train: step:  30060, time: 0.194, loss: 3636.789062\n",
      "Train: step:  30070, time: 0.203, loss: 1047.191528\n",
      "Train: step:  30080, time: 0.187, loss: 2578.722900\n",
      "Train: step:  30090, time: 0.190, loss: 1528.929810\n",
      "Train: step:  30100, time: 0.220, loss: 1825.982300\n",
      "Train: step:  30110, time: 0.219, loss: 2621.007812\n",
      "Train: step:  30120, time: 0.235, loss: 1743.599121\n",
      "Train: step:  30130, time: 0.219, loss: 1589.619507\n",
      "Train: step:  30140, time: 0.192, loss: 3026.296143\n",
      "Train: step:  30150, time: 0.202, loss: 2236.557861\n",
      "Train: step:  30160, time: 0.212, loss: 733.751770\n",
      "Train: step:  30170, time: 0.198, loss: 2656.645508\n",
      "Train: step:  30180, time: 0.193, loss: 2293.405273\n",
      "Train: step:  30190, time: 0.196, loss: 1479.164795\n",
      "Train: step:  30200, time: 0.190, loss: 2235.810303\n",
      "Train: step:  30210, time: 0.193, loss: 701.642029\n",
      "Train: step:  30220, time: 0.191, loss: 2135.404297\n",
      "Train: step:  30230, time: 0.195, loss: 302.510742\n",
      "Train: step:  30240, time: 0.197, loss: 2887.211670\n",
      "Train: step:  30250, time: 0.202, loss: 707.158264\n",
      "Train: step:  30260, time: 0.190, loss: 1652.495117\n",
      "Train: step:  30270, time: 0.211, loss: 1495.458618\n",
      "Train: step:  30280, time: 0.220, loss: 1702.806641\n",
      "Train: step:  30290, time: 0.195, loss: 307.338745\n",
      "Train: step:  30300, time: 0.194, loss: 2990.323975\n",
      "Train: step:  30310, time: 0.194, loss: 2037.742798\n",
      "Train: step:  30320, time: 0.191, loss: 1940.135620\n",
      "Train: step:  30330, time: 0.203, loss: 1057.157227\n",
      "Train: step:  30340, time: 0.191, loss: 6572.954590\n",
      "Train: step:  30350, time: 0.192, loss: 3146.917236\n",
      "Train: step:  30360, time: 0.195, loss: 2896.760254\n",
      "Train: step:  30370, time: 0.194, loss: 1248.108276\n",
      "Train: step:  30380, time: 0.234, loss: 315.753845\n",
      "Train: step:  30390, time: 0.190, loss: 2301.170898\n",
      "Train: step:  30400, time: 0.194, loss: 1999.950928\n",
      "Train: step:  30410, time: 0.219, loss: 3499.000488\n",
      "Train: step:  30420, time: 0.190, loss: 2939.677979\n",
      "Train: step:  30430, time: 0.185, loss: 1659.142578\n",
      "Train: step:  30440, time: 0.186, loss: 2627.439209\n",
      "Train: step:  30450, time: 0.185, loss: 2140.112793\n",
      "Train: step:  30460, time: 0.240, loss: 2291.023193\n",
      "Train: step:  30470, time: 0.193, loss: 3420.007568\n",
      "Train: step:  30480, time: 0.189, loss: 1746.847412\n",
      "Train: step:  30490, time: 0.221, loss: 877.622620\n",
      "Train: step:  30500, time: 0.193, loss: 1660.578247\n",
      "Train: step:  30510, time: 0.186, loss: 398.026337\n",
      "Train: step:  30520, time: 0.197, loss: 1417.983398\n",
      "Train: step:  30530, time: 0.188, loss: 2168.708496\n",
      "Train: step:  30540, time: 0.191, loss: 3773.091797\n",
      "Train: step:  30550, time: 0.236, loss: 639.164124\n",
      "Train: step:  30560, time: 0.192, loss: 401.680237\n",
      "Train: step:  30570, time: 0.191, loss: 2903.691162\n",
      "Train: step:  30580, time: 0.195, loss: 2757.303955\n",
      "Train: step:  30590, time: 0.188, loss: 1588.984375\n",
      "Train: step:  30600, time: 0.186, loss: 969.581909\n",
      "Train: step:  30610, time: 0.224, loss: 1843.898804\n",
      "Train: step:  30620, time: 0.193, loss: 1450.228027\n",
      "Train: step:  30630, time: 0.187, loss: 1258.505615\n",
      "Train: step:  30640, time: 0.224, loss: 1764.304077\n",
      "Train: step:  30650, time: 0.194, loss: 2625.678955\n",
      "Train: step:  30660, time: 0.195, loss: 633.870972\n",
      "Train: step:  30670, time: 0.190, loss: 1575.050415\n",
      "Train: step:  30680, time: 0.194, loss: 3429.725098\n",
      "Train: step:  30690, time: 0.194, loss: 586.447998\n",
      "Train: step:  30700, time: 0.197, loss: 2001.530029\n",
      "Train: step:  30710, time: 0.187, loss: 1426.475098\n",
      "Train: step:  30720, time: 0.187, loss: 1459.572510\n",
      "Train: step:  30730, time: 0.197, loss: 2681.923340\n",
      "Train: step:  30740, time: 0.192, loss: 1042.186279\n",
      "Train: step:  30750, time: 0.193, loss: 470.401917\n",
      "Train: step:  30760, time: 0.196, loss: 1791.121338\n",
      "Train: step:  30770, time: 0.194, loss: 249.652832\n",
      "Train: step:  30780, time: 0.188, loss: 2677.391357\n",
      "Train: step:  30790, time: 0.189, loss: 1512.951050\n",
      "Train: step:  30800, time: 0.231, loss: 1271.876709\n",
      "Train: step:  30810, time: 0.192, loss: 936.755188\n",
      "Train: step:  30820, time: 0.217, loss: 3537.493408\n",
      "Train: step:  30830, time: 0.199, loss: 809.040527\n",
      "Train: step:  30840, time: 0.192, loss: 1380.641846\n",
      "Train: step:  30850, time: 0.191, loss: 2327.020020\n",
      "Train: step:  30860, time: 0.202, loss: 1384.306763\n",
      "Train: step:  30870, time: 0.218, loss: 2840.605713\n",
      "Train: step:  30880, time: 0.221, loss: 2270.129395\n",
      "Train: step:  30890, time: 0.200, loss: 1410.814941\n",
      "Train: step:  30900, time: 0.201, loss: 3444.709473\n",
      "Train: step:  30910, time: 0.202, loss: 2381.142334\n",
      "Train: step:  30920, time: 0.222, loss: 3720.504639\n",
      "Train: step:  30930, time: 0.193, loss: 821.987549\n",
      "Train: step:  30940, time: 0.191, loss: 893.327454\n",
      "Train: step:  30950, time: 0.193, loss: 2000.401367\n",
      "Train: step:  30960, time: 0.221, loss: 2867.803955\n",
      "Train: step:  30970, time: 0.188, loss: 756.833374\n",
      "Train: step:  30980, time: 0.194, loss: 1774.612793\n",
      "Train: step:  30990, time: 0.192, loss: 2245.608154\n",
      "Train: step:  31000, time: 0.227, loss: 835.328186\n",
      "Train: step:  31010, time: 0.195, loss: 3775.897705\n",
      "Train: step:  31020, time: 0.195, loss: 643.368408\n",
      "Train: step:  31030, time: 0.188, loss: 2980.292236\n",
      "Train: step:  31040, time: 0.189, loss: 1125.030640\n",
      "Train: step:  31050, time: 0.196, loss: 4139.902344\n",
      "Train: step:  31060, time: 0.223, loss: 1497.259155\n",
      "Train: step:  31070, time: 0.218, loss: 990.980713\n",
      "Train: step:  31080, time: 0.192, loss: 1744.616455\n",
      "Train: step:  31090, time: 0.197, loss: 437.592255\n",
      "Train: step:  31100, time: 0.189, loss: 3823.891846\n",
      "Train: step:  31110, time: 0.186, loss: 878.852173\n",
      "Train: step:  31120, time: 0.189, loss: 1707.278564\n",
      "Train: step:  31130, time: 0.223, loss: 1125.016724\n",
      "Train: step:  31140, time: 0.219, loss: 773.326782\n",
      "Train: step:  31150, time: 0.216, loss: 1660.655884\n",
      "Train: step:  31160, time: 0.192, loss: 1764.250854\n",
      "Train: step:  31170, time: 0.192, loss: 2647.618896\n",
      "Train: step:  31180, time: 0.195, loss: 1447.308228\n",
      "Train: step:  31190, time: 0.216, loss: 1675.530884\n",
      "Train: step:  31200, time: 0.192, loss: 1357.442261\n",
      "Train: step:  31210, time: 0.193, loss: 2118.658447\n",
      "Train: step:  31220, time: 0.192, loss: 1682.285889\n",
      "Train: step:  31230, time: 0.219, loss: 1598.532959\n",
      "Train: step:  31240, time: 0.186, loss: 1053.962036\n",
      "Train: step:  31250, time: 0.190, loss: 1212.246948\n",
      "Train: step:  31260, time: 0.195, loss: 1153.440186\n",
      "Train: step:  31270, time: 0.193, loss: 997.127319\n",
      "Train: step:  31280, time: 0.192, loss: 2292.264404\n",
      "Train: step:  31290, time: 0.192, loss: 3710.852539\n",
      "Train: step:  31300, time: 0.194, loss: 1534.692993\n",
      "Train: step:  31310, time: 0.281, loss: 2889.116455\n",
      "Train: step:  31320, time: 0.221, loss: 943.510864\n",
      "Train: step:  31330, time: 0.189, loss: 6151.008301\n",
      "Train: step:  31340, time: 0.188, loss: 556.894775\n",
      "Train: step:  31350, time: 0.203, loss: 2663.152344\n",
      "Train: step:  31360, time: 0.218, loss: 1889.842285\n",
      "Train: step:  31370, time: 0.216, loss: 2256.137207\n",
      "Train: step:  31380, time: 0.243, loss: 3357.181641\n",
      "Train: step:  31390, time: 0.204, loss: 2164.159668\n",
      "Train: step:  31400, time: 0.194, loss: 1497.160278\n",
      "Train: step:  31410, time: 0.187, loss: 1107.321899\n",
      "Train: step:  31420, time: 0.199, loss: 1707.515625\n",
      "Train: step:  31430, time: 0.199, loss: 1108.906738\n",
      "Train: step:  31440, time: 0.205, loss: 970.659302\n",
      "Train: step:  31450, time: 0.226, loss: 2626.222900\n",
      "Train: step:  31460, time: 0.194, loss: 971.846191\n",
      "Train: step:  31470, time: 0.195, loss: 397.654694\n",
      "Train: step:  31480, time: 0.192, loss: 825.120483\n",
      "Train: step:  31490, time: 0.223, loss: 2205.027344\n",
      "Train: step:  31500, time: 0.194, loss: 4182.214355\n",
      "Train: step:  31510, time: 0.193, loss: 2295.718994\n",
      "Train: step:  31520, time: 0.199, loss: 827.964905\n",
      "Train: step:  31530, time: 0.199, loss: 1355.579834\n",
      "Train: step:  31540, time: 0.197, loss: 1537.976196\n",
      "Train: step:  31550, time: 0.195, loss: 1589.166626\n",
      "Train: step:  31560, time: 0.220, loss: 2169.491943\n",
      "Train: step:  31570, time: 0.202, loss: 2352.522705\n",
      "Train: step:  31580, time: 0.194, loss: 963.580811\n",
      "Train: step:  31590, time: 0.195, loss: 3100.273926\n",
      "Train: step:  31600, time: 0.208, loss: 3130.075439\n",
      "Train: step:  31610, time: 0.260, loss: 2899.159180\n",
      "Train: step:  31620, time: 0.220, loss: 743.174133\n",
      "Train: step:  31630, time: 0.194, loss: 3522.108887\n",
      "Train: step:  31640, time: 0.196, loss: 625.335999\n",
      "Train: step:  31650, time: 0.193, loss: 3310.005371\n",
      "Train: step:  31660, time: 0.200, loss: 1045.850586\n",
      "Train: step:  31670, time: 0.202, loss: 1186.789795\n",
      "Train: step:  31680, time: 0.204, loss: 3108.210205\n",
      "Train: step:  31690, time: 0.194, loss: 663.532288\n",
      "Train: step:  31700, time: 0.197, loss: 4052.335205\n",
      "Train: step:  31710, time: 0.191, loss: 2610.191162\n",
      "Train: step:  31720, time: 0.192, loss: 1428.543945\n",
      "Train: step:  31730, time: 0.196, loss: 1717.696411\n",
      "Train: step:  31740, time: 0.206, loss: 873.765198\n",
      "Train: step:  31750, time: 0.221, loss: 1166.538696\n",
      "Train: step:  31760, time: 0.224, loss: 1456.412964\n",
      "Train: step:  31770, time: 0.206, loss: 884.853638\n",
      "Train: step:  31780, time: 0.192, loss: 2718.042480\n",
      "Train: step:  31790, time: 0.219, loss: 1439.562744\n",
      "Train: step:  31800, time: 0.197, loss: 2308.655273\n",
      "Train: step:  31810, time: 0.198, loss: 1544.750000\n",
      "Train: step:  31820, time: 0.191, loss: 2047.849609\n",
      "Train: step:  31830, time: 0.196, loss: 2616.862305\n",
      "Train: step:  31840, time: 0.200, loss: 901.930908\n",
      "Train: step:  31850, time: 0.225, loss: 1406.322998\n",
      "Train: step:  31860, time: 0.241, loss: 1143.578247\n",
      "Train: step:  31870, time: 0.209, loss: 2843.898193\n",
      "Train: step:  31880, time: 0.235, loss: 1915.209717\n",
      "Train: step:  31890, time: 0.201, loss: 3056.091797\n",
      "Train: step:  31900, time: 0.196, loss: 1718.827515\n",
      "Train: step:  31910, time: 0.196, loss: 1026.402466\n",
      "Train: step:  31920, time: 0.238, loss: 2396.608398\n",
      "Train: step:  31930, time: 0.194, loss: 1295.623291\n",
      "Train: step:  31940, time: 0.190, loss: 513.796204\n",
      "Train: step:  31950, time: 0.198, loss: 641.621521\n",
      "Train: step:  31960, time: 0.202, loss: 2070.029785\n",
      "Train: step:  31970, time: 0.203, loss: 3257.731934\n",
      "Train: step:  31980, time: 0.201, loss: 1962.869385\n",
      "Train: step:  31990, time: 0.222, loss: 2626.795410\n",
      "Train: step:  32000, time: 0.204, loss: 2237.183594\n",
      "Train: step:  32010, time: 0.248, loss: 544.047241\n",
      "Train: step:  32020, time: 0.225, loss: 1578.727783\n",
      "Train: step:  32030, time: 0.221, loss: 1964.776123\n",
      "Train: step:  32040, time: 0.194, loss: 1995.059937\n",
      "Train: step:  32050, time: 0.202, loss: 2365.409912\n",
      "Train: step:  32060, time: 0.193, loss: 1102.364136\n",
      "Train: step:  32070, time: 0.197, loss: 1186.343506\n",
      "Train: step:  32080, time: 0.197, loss: 360.538818\n",
      "Train: step:  32090, time: 0.192, loss: 559.641357\n",
      "Train: step:  32100, time: 0.190, loss: 1415.239014\n",
      "Train: step:  32110, time: 0.192, loss: 2286.819092\n",
      "Train: step:  32120, time: 0.195, loss: 886.995422\n",
      "Train: step:  32130, time: 0.199, loss: 1386.968506\n",
      "Train: step:  32140, time: 0.198, loss: 686.797668\n",
      "Train: step:  32150, time: 0.200, loss: 2451.344727\n",
      "Train: step:  32160, time: 0.220, loss: 815.293579\n",
      "Train: step:  32170, time: 0.214, loss: 1063.192993\n",
      "Train: step:  32180, time: 0.205, loss: 1821.227173\n",
      "Train: step:  32190, time: 0.200, loss: 3475.287354\n",
      "Train: step:  32200, time: 0.232, loss: 5574.148438\n",
      "Train: step:  32210, time: 0.192, loss: 2434.231689\n",
      "Train: step:  32220, time: 0.189, loss: 1642.614990\n",
      "Train: step:  32230, time: 0.202, loss: 2912.856445\n",
      "Train: step:  32240, time: 0.231, loss: 949.733948\n",
      "Train: step:  32250, time: 0.231, loss: 1894.397095\n",
      "Train: step:  32260, time: 0.190, loss: 1466.062134\n",
      "Train: step:  32270, time: 0.236, loss: 1782.487549\n",
      "Train: step:  32280, time: 0.195, loss: 1561.189331\n",
      "Train: step:  32290, time: 0.224, loss: 2858.865723\n",
      "Train: step:  32300, time: 0.225, loss: 2209.950684\n",
      "Train: step:  32310, time: 0.205, loss: 1221.893188\n",
      "Train: step:  32320, time: 0.198, loss: 1185.178467\n",
      "Train: step:  32330, time: 0.225, loss: 2194.045654\n",
      "Train: step:  32340, time: 0.201, loss: 1008.518127\n",
      "Train: step:  32350, time: 0.225, loss: 2089.432129\n",
      "Train: step:  32360, time: 0.210, loss: 1769.098755\n",
      "Train: step:  32370, time: 0.210, loss: 357.686859\n",
      "Train: step:  32380, time: 0.220, loss: 1014.174072\n",
      "Train: step:  32390, time: 0.262, loss: 2494.839600\n",
      "Train: step:  32400, time: 0.196, loss: 3476.120850\n",
      "Train: step:  32410, time: 0.200, loss: 452.621246\n",
      "Train: step:  32420, time: 0.199, loss: 3332.225586\n",
      "Train: step:  32430, time: 0.197, loss: 194.816116\n",
      "Train: step:  32440, time: 0.242, loss: 1663.253052\n",
      "Train: step:  32450, time: 0.200, loss: 2361.924072\n",
      "Train: step:  32460, time: 0.196, loss: 899.143616\n",
      "Train: step:  32470, time: 0.201, loss: 2015.744019\n",
      "Train: step:  32480, time: 0.201, loss: 780.176880\n",
      "Train: step:  32490, time: 0.206, loss: 1608.180298\n",
      "Train: step:  32500, time: 0.217, loss: 204.444763\n",
      "Train: step:  32510, time: 0.203, loss: 225.389694\n",
      "Train: step:  32520, time: 0.225, loss: 2680.742920\n",
      "Train: step:  32530, time: 0.205, loss: 466.594604\n",
      "Train: step:  32540, time: 0.192, loss: 1508.013306\n",
      "Train: step:  32550, time: 0.198, loss: 3670.457275\n",
      "Train: step:  32560, time: 0.203, loss: 2158.721680\n",
      "Train: step:  32570, time: 0.195, loss: 1328.385498\n",
      "Train: step:  32580, time: 0.220, loss: 3248.008301\n",
      "Train: step:  32590, time: 0.197, loss: 2533.933594\n",
      "Train: step:  32600, time: 0.212, loss: 3709.978760\n",
      "Train: step:  32610, time: 0.193, loss: 1795.078491\n",
      "Train: step:  32620, time: 0.235, loss: 781.249207\n",
      "Train: step:  32630, time: 0.192, loss: 2312.494141\n",
      "Train: step:  32640, time: 0.202, loss: 431.329163\n",
      "Train: step:  32650, time: 0.200, loss: 2408.924316\n",
      "Train: step:  32660, time: 0.214, loss: 1976.824829\n",
      "Train: step:  32670, time: 0.197, loss: 1858.853882\n",
      "Train: step:  32680, time: 0.198, loss: 1675.608398\n",
      "Train: step:  32690, time: 0.198, loss: 1948.003418\n",
      "Train: step:  32700, time: 0.194, loss: 2504.953857\n",
      "Train: step:  32710, time: 0.238, loss: 1405.310303\n",
      "Train: step:  32720, time: 0.197, loss: 1439.899902\n",
      "Train: step:  32730, time: 0.198, loss: 3105.594971\n",
      "Train: step:  32740, time: 0.194, loss: 1091.119873\n",
      "Train: step:  32750, time: 0.192, loss: 1086.583374\n",
      "Train: step:  32760, time: 0.231, loss: 1055.184204\n",
      "Train: step:  32770, time: 0.220, loss: 1839.777954\n",
      "Train: step:  32780, time: 0.223, loss: 3137.100342\n",
      "Train: step:  32790, time: 0.198, loss: 3506.063721\n",
      "Train: step:  32800, time: 0.198, loss: 779.400269\n",
      "Train: step:  32810, time: 0.225, loss: 316.816681\n",
      "Train: step:  32820, time: 0.202, loss: 1456.139893\n",
      "Train: step:  32830, time: 0.195, loss: 1034.413696\n",
      "Train: step:  32840, time: 0.198, loss: 1960.574341\n",
      "Train: step:  32850, time: 0.198, loss: 673.487244\n",
      "Train: step:  32860, time: 0.211, loss: 2786.730225\n",
      "Train: step:  32870, time: 0.220, loss: 2376.227295\n",
      "Train: step:  32880, time: 0.198, loss: 1462.609253\n",
      "Train: step:  32890, time: 0.203, loss: 2295.554199\n",
      "Train: step:  32900, time: 0.208, loss: 3256.588379\n",
      "Train: step:  32910, time: 0.194, loss: 1567.617188\n",
      "Train: step:  32920, time: 0.190, loss: 1362.641968\n",
      "Train: step:  32930, time: 0.209, loss: 1975.776855\n",
      "Train: step:  32940, time: 0.196, loss: 1404.038452\n",
      "Train: step:  32950, time: 0.225, loss: 3478.895996\n",
      "Train: step:  32960, time: 0.197, loss: 2660.782471\n",
      "Train: step:  32970, time: 0.190, loss: 2913.198975\n",
      "Train: step:  32980, time: 0.223, loss: 1006.669739\n",
      "Train: step:  32990, time: 0.194, loss: 1077.018188\n",
      "Train: step:  33000, time: 0.193, loss: 608.421936\n",
      "Train: step:  33010, time: 0.225, loss: 4425.093750\n",
      "Train: step:  33020, time: 0.195, loss: 3158.661377\n",
      "Train: step:  33030, time: 0.192, loss: 656.398071\n",
      "Train: step:  33040, time: 0.200, loss: 2476.144287\n",
      "Train: step:  33050, time: 0.193, loss: 969.611023\n",
      "Train: step:  33060, time: 0.209, loss: 3165.337646\n",
      "Train: step:  33070, time: 0.228, loss: 1640.918091\n",
      "Train: step:  33080, time: 0.195, loss: 2385.119141\n",
      "Train: step:  33090, time: 0.223, loss: 1598.708740\n",
      "Train: step:  33100, time: 0.192, loss: 1495.717163\n",
      "Train: step:  33110, time: 0.190, loss: 1870.116821\n",
      "Train: step:  33120, time: 0.213, loss: 3742.591797\n",
      "Train: step:  33130, time: 0.191, loss: 1154.105835\n",
      "Train: step:  33140, time: 0.201, loss: 1051.304077\n",
      "Train: step:  33150, time: 0.202, loss: 1468.883667\n",
      "Train: step:  33160, time: 0.195, loss: 2139.352051\n",
      "Train: step:  33170, time: 0.200, loss: 736.406067\n",
      "Train: step:  33180, time: 0.195, loss: 781.128784\n",
      "Train: step:  33190, time: 0.195, loss: 1498.449585\n",
      "Train: step:  33200, time: 0.200, loss: 1063.369385\n",
      "Train: step:  33210, time: 0.185, loss: 3056.142334\n",
      "Train: step:  33220, time: 0.200, loss: 2648.522461\n",
      "Train: step:  33230, time: 0.205, loss: 3543.616699\n",
      "Train: step:  33240, time: 0.197, loss: 1217.746704\n",
      "Train: step:  33250, time: 0.191, loss: 3225.312012\n",
      "Train: step:  33260, time: 0.194, loss: 2265.475830\n",
      "Train: step:  33270, time: 0.224, loss: 1300.173096\n",
      "Train: step:  33280, time: 0.237, loss: 804.414856\n",
      "Train: step:  33290, time: 0.239, loss: 2109.558350\n",
      "Train: step:  33300, time: 0.224, loss: 2714.212402\n",
      "Train: step:  33310, time: 0.225, loss: 1403.080933\n",
      "Train: step:  33320, time: 0.217, loss: 215.362946\n",
      "Train: step:  33330, time: 0.196, loss: 1545.886230\n",
      "Train: step:  33340, time: 0.191, loss: 658.439270\n",
      "Train: step:  33350, time: 0.198, loss: 2259.927490\n",
      "Train: step:  33360, time: 0.230, loss: 1555.326416\n",
      "Train: step:  33370, time: 0.242, loss: 3093.647461\n",
      "Train: step:  33380, time: 0.224, loss: 3274.494873\n",
      "Train: step:  33390, time: 0.200, loss: 398.683868\n",
      "Train: step:  33400, time: 0.220, loss: 3768.700684\n",
      "Train: step:  33410, time: 0.200, loss: 1961.107666\n",
      "Train: step:  33420, time: 0.184, loss: 2672.352051\n",
      "Train: step:  33430, time: 0.194, loss: 1729.478271\n",
      "Train: step:  33440, time: 0.192, loss: 929.849182\n",
      "Train: step:  33450, time: 0.239, loss: 1442.737549\n",
      "Train: step:  33460, time: 0.192, loss: 2073.240479\n",
      "Train: step:  33470, time: 0.192, loss: 2518.689453\n",
      "Train: step:  33480, time: 0.195, loss: 1661.078003\n",
      "Train: step:  33490, time: 0.201, loss: 2740.196289\n",
      "Train: step:  33500, time: 0.229, loss: 2228.149414\n",
      "Train: step:  33510, time: 0.231, loss: 3094.333008\n",
      "Train: step:  33520, time: 0.199, loss: 2141.633301\n",
      "Train: step:  33530, time: 0.197, loss: 2501.228516\n",
      "Train: step:  33540, time: 0.194, loss: 813.992676\n",
      "Train: step:  33550, time: 0.190, loss: 3300.304688\n",
      "Train: step:  33560, time: 0.221, loss: 3402.024170\n",
      "Train: step:  33570, time: 0.219, loss: 1093.893677\n",
      "Train: step:  33580, time: 0.196, loss: 2488.662109\n",
      "Train: step:  33590, time: 0.193, loss: 1848.855957\n",
      "Train: step:  33600, time: 0.213, loss: 1385.873291\n",
      "Train: step:  33610, time: 0.202, loss: 2303.448730\n",
      "Train: step:  33620, time: 0.228, loss: 1582.070190\n",
      "Train: step:  33630, time: 0.200, loss: 3165.828613\n",
      "Train: step:  33640, time: 0.213, loss: 2405.475342\n",
      "Train: step:  33650, time: 0.238, loss: 1835.637329\n",
      "Train: step:  33660, time: 0.204, loss: 2474.077637\n",
      "Train: step:  33670, time: 0.236, loss: 945.412354\n",
      "Train: step:  33680, time: 0.224, loss: 2082.527832\n",
      "Train: step:  33690, time: 0.201, loss: 1218.469116\n",
      "Train: step:  33700, time: 0.211, loss: 3190.945068\n",
      "Train: step:  33710, time: 0.223, loss: 1163.760376\n",
      "Train: step:  33720, time: 0.183, loss: 1339.700562\n",
      "Train: step:  33730, time: 0.241, loss: 2348.864258\n",
      "Train: step:  33740, time: 0.223, loss: 4573.275391\n",
      "Train: step:  33750, time: 0.233, loss: 2772.037598\n",
      "Train: step:  33760, time: 0.194, loss: 2538.958008\n",
      "Train: step:  33770, time: 0.192, loss: 1424.853638\n",
      "Train: step:  33780, time: 0.227, loss: 982.002258\n",
      "Train: step:  33790, time: 0.218, loss: 1896.358521\n",
      "Train: step:  33800, time: 0.195, loss: 1920.097778\n",
      "Train: step:  33810, time: 0.193, loss: 1508.344604\n",
      "Train: step:  33820, time: 0.197, loss: 1003.224976\n",
      "Train: step:  33830, time: 0.190, loss: 1176.408569\n",
      "Train: step:  33840, time: 0.215, loss: 2295.156738\n",
      "Train: step:  33850, time: 0.190, loss: 3269.519531\n",
      "Train: step:  33860, time: 0.190, loss: 2391.872559\n",
      "Train: step:  33870, time: 0.221, loss: 1460.144287\n",
      "Train: step:  33880, time: 0.188, loss: 1295.396484\n",
      "Train: step:  33890, time: 0.229, loss: 3713.614746\n",
      "Train: step:  33900, time: 0.195, loss: 1687.193359\n",
      "Train: step:  33910, time: 0.217, loss: 1617.220947\n",
      "Train: step:  33920, time: 0.217, loss: 3403.247803\n",
      "Train: step:  33930, time: 0.189, loss: 1459.808594\n",
      "Train: step:  33940, time: 0.199, loss: 4130.647949\n",
      "Train: step:  33950, time: 0.187, loss: 2251.010254\n",
      "Train: step:  33960, time: 0.198, loss: 1827.985962\n",
      "Train: step:  33970, time: 0.195, loss: 987.187622\n",
      "Train: step:  33980, time: 0.207, loss: 930.408691\n",
      "Train: step:  33990, time: 0.193, loss: 2374.305664\n",
      "Train: step:  34000, time: 0.185, loss: 1341.619629\n",
      "Train: step:  34010, time: 0.223, loss: 2930.124756\n",
      "Train: step:  34020, time: 0.197, loss: 2639.465820\n",
      "Train: step:  34030, time: 0.207, loss: 2058.846680\n",
      "Train: step:  34040, time: 0.197, loss: 1643.638184\n",
      "Train: step:  34050, time: 0.217, loss: 2381.772217\n",
      "Train: step:  34060, time: 0.187, loss: 1802.256104\n",
      "Train: step:  34070, time: 0.192, loss: 786.913574\n",
      "Train: step:  34080, time: 0.218, loss: 2470.706055\n",
      "Train: step:  34090, time: 0.196, loss: 1039.592285\n",
      "Train: step:  34100, time: 0.194, loss: 769.679504\n",
      "Train: step:  34110, time: 0.191, loss: 864.900452\n",
      "Train: step:  34120, time: 0.218, loss: 3165.044189\n",
      "Train: step:  34130, time: 0.191, loss: 512.430847\n",
      "Train: step:  34140, time: 0.220, loss: 1488.027588\n",
      "Train: step:  34150, time: 0.185, loss: 966.442017\n",
      "Train: step:  34160, time: 0.191, loss: 529.110840\n",
      "Train: step:  34170, time: 0.190, loss: 1830.828003\n",
      "Train: step:  34180, time: 0.202, loss: 940.737183\n",
      "Train: step:  34190, time: 0.186, loss: 2984.224365\n",
      "Train: step:  34200, time: 0.185, loss: 1905.900269\n",
      "Train: step:  34210, time: 0.191, loss: 957.706970\n",
      "Train: step:  34220, time: 0.222, loss: 1678.602051\n",
      "Train: step:  34230, time: 0.184, loss: 1189.777832\n",
      "Train: step:  34240, time: 0.193, loss: 3314.053955\n",
      "Train: step:  34250, time: 0.235, loss: 1706.879150\n",
      "Train: step:  34260, time: 0.203, loss: 1618.998901\n",
      "Train: step:  34270, time: 0.189, loss: 2547.146973\n",
      "Train: step:  34280, time: 0.197, loss: 1631.647461\n",
      "Train: step:  34290, time: 0.192, loss: 1896.000244\n",
      "Train: step:  34300, time: 0.194, loss: 1572.681274\n",
      "Train: step:  34310, time: 0.187, loss: 1707.035034\n",
      "Train: step:  34320, time: 0.191, loss: 872.989319\n",
      "Train: step:  34330, time: 0.184, loss: 2035.715088\n",
      "Train: step:  34340, time: 0.184, loss: 3160.656738\n",
      "Train: step:  34350, time: 0.199, loss: 2471.260742\n",
      "Train: step:  34360, time: 0.212, loss: 1723.948975\n",
      "Train: step:  34370, time: 0.194, loss: 347.635468\n",
      "Train: step:  34380, time: 0.194, loss: 957.130249\n",
      "Train: step:  34390, time: 0.198, loss: 2349.392578\n",
      "Train: step:  34400, time: 0.225, loss: 1075.853271\n",
      "Train: step:  34410, time: 0.185, loss: 326.810547\n",
      "Train: step:  34420, time: 0.187, loss: 1575.772095\n",
      "Train: step:  34430, time: 0.190, loss: 946.650146\n",
      "Train: step:  34440, time: 0.186, loss: 688.634766\n",
      "Train: step:  34450, time: 0.190, loss: 1685.682251\n",
      "Train: step:  34460, time: 0.188, loss: 658.917786\n",
      "Train: step:  34470, time: 0.189, loss: 1153.351929\n",
      "Train: step:  34480, time: 0.189, loss: 2052.675537\n",
      "Train: step:  34490, time: 0.191, loss: 492.228577\n",
      "Train: step:  34500, time: 0.183, loss: 1613.885986\n",
      "Train: step:  34510, time: 0.237, loss: 1622.092163\n",
      "Train: step:  34520, time: 0.191, loss: 410.301819\n",
      "Train: step:  34530, time: 0.209, loss: 1148.664551\n",
      "Train: step:  34540, time: 0.193, loss: 1911.514160\n",
      "Train: step:  34550, time: 0.192, loss: 2087.003906\n",
      "Train: step:  34560, time: 0.189, loss: 2484.658936\n",
      "Train: step:  34570, time: 0.221, loss: 1343.747192\n",
      "Train: step:  34580, time: 0.227, loss: 1814.463867\n",
      "Train: step:  34590, time: 0.186, loss: 640.901001\n",
      "Train: step:  34600, time: 0.186, loss: 1509.095825\n",
      "Train: step:  34610, time: 0.192, loss: 2199.562012\n",
      "Train: step:  34620, time: 0.248, loss: 1785.445190\n",
      "Train: step:  34630, time: 0.221, loss: 1800.364990\n",
      "Train: step:  34640, time: 0.185, loss: 1948.306030\n",
      "Train: step:  34650, time: 0.235, loss: 1618.523315\n",
      "Train: step:  34660, time: 0.217, loss: 1699.529541\n",
      "Train: step:  34670, time: 0.194, loss: 2080.047363\n",
      "Train: step:  34680, time: 0.217, loss: 2279.824951\n",
      "Train: step:  34690, time: 0.236, loss: 1212.727173\n",
      "Train: step:  34700, time: 0.216, loss: 294.010681\n",
      "Train: step:  34710, time: 0.210, loss: 587.006409\n",
      "Train: step:  34720, time: 0.190, loss: 2763.533447\n",
      "Train: step:  34730, time: 0.192, loss: 2448.135742\n",
      "Train: step:  34740, time: 0.191, loss: 2403.765625\n",
      "Train: step:  34750, time: 0.216, loss: 423.746674\n",
      "Train: step:  34760, time: 0.193, loss: 836.911072\n",
      "Train: step:  34770, time: 0.191, loss: 3834.761475\n",
      "Train: step:  34780, time: 0.197, loss: 2874.042236\n",
      "Train: step:  34790, time: 0.192, loss: 2534.591797\n",
      "Train: step:  34800, time: 0.193, loss: 3399.897217\n",
      "Train: step:  34810, time: 0.196, loss: 559.937378\n",
      "Train: step:  34820, time: 0.194, loss: 1183.848022\n",
      "Train: step:  34830, time: 0.195, loss: 895.732178\n",
      "Train: step:  34840, time: 0.195, loss: 1522.633911\n",
      "Train: step:  34850, time: 0.199, loss: 1711.527954\n",
      "Train: step:  34860, time: 0.217, loss: 438.423553\n",
      "Train: step:  34870, time: 0.258, loss: 1273.810791\n",
      "Train: step:  34880, time: 0.227, loss: 2460.015869\n",
      "Train: step:  34890, time: 0.217, loss: 324.564240\n",
      "Train: step:  34900, time: 0.188, loss: 2459.328613\n",
      "Train: step:  34910, time: 0.192, loss: 1605.054810\n",
      "Train: step:  34920, time: 0.190, loss: 2384.399902\n",
      "Train: step:  34930, time: 0.189, loss: 1461.332031\n",
      "Train: step:  34940, time: 0.188, loss: 812.648682\n",
      "Train: step:  34950, time: 0.193, loss: 1955.615601\n",
      "Train: step:  34960, time: 0.190, loss: 1291.111084\n",
      "Train: step:  34970, time: 0.192, loss: 1805.663940\n",
      "Train: step:  34980, time: 0.194, loss: 1840.908691\n",
      "Train: step:  34990, time: 0.197, loss: 835.865356\n",
      "Train: step:  35000, time: 0.190, loss: 1009.285034\n",
      "Train: step:  35010, time: 0.222, loss: 3491.755371\n",
      "Train: step:  35020, time: 0.204, loss: 1526.085571\n",
      "Train: step:  35030, time: 0.223, loss: 718.697083\n",
      "Train: step:  35040, time: 0.194, loss: 244.434433\n",
      "Train: step:  35050, time: 0.198, loss: 3301.149414\n",
      "Train: step:  35060, time: 0.216, loss: 4264.899414\n",
      "Train: step:  35070, time: 0.190, loss: 1215.881226\n",
      "Train: step:  35080, time: 0.193, loss: 1615.230103\n",
      "Train: step:  35090, time: 0.199, loss: 2193.705566\n",
      "Train: step:  35100, time: 0.229, loss: 3275.405029\n",
      "Train: step:  35110, time: 0.197, loss: 953.671814\n",
      "Train: step:  35120, time: 0.194, loss: 676.495239\n",
      "Train: step:  35130, time: 0.188, loss: 1862.846069\n",
      "Train: step:  35140, time: 0.209, loss: 3411.981445\n",
      "Train: step:  35150, time: 0.189, loss: 940.550232\n",
      "Train: step:  35160, time: 0.187, loss: 1978.243164\n",
      "Train: step:  35170, time: 0.185, loss: 3119.928955\n",
      "Train: step:  35180, time: 0.190, loss: 1657.259888\n",
      "Train: step:  35190, time: 0.262, loss: 1393.489990\n",
      "Train: step:  35200, time: 0.230, loss: 1403.060059\n",
      "Train: step:  35210, time: 0.236, loss: 1852.501221\n",
      "Train: step:  35220, time: 0.209, loss: 1872.946167\n",
      "Train: step:  35230, time: 0.194, loss: 2937.886230\n",
      "Train: step:  35240, time: 0.202, loss: 3118.371582\n",
      "Train: step:  35250, time: 0.214, loss: 1338.086426\n",
      "Train: step:  35260, time: 0.201, loss: 3327.881592\n",
      "Train: step:  35270, time: 0.193, loss: 1954.593994\n",
      "Train: step:  35280, time: 0.213, loss: 1817.114502\n",
      "Train: step:  35290, time: 0.195, loss: 1922.324219\n",
      "Train: step:  35300, time: 0.192, loss: 1524.551270\n",
      "Train: step:  35310, time: 0.193, loss: 2832.657959\n",
      "Train: step:  35320, time: 0.197, loss: 864.868835\n",
      "Train: step:  35330, time: 0.225, loss: 172.310486\n",
      "Train: step:  35340, time: 0.185, loss: 515.706421\n",
      "Train: step:  35350, time: 0.188, loss: 1249.744385\n",
      "Train: step:  35360, time: 0.189, loss: 2906.085449\n",
      "Train: step:  35370, time: 0.197, loss: 4607.863281\n",
      "Train: step:  35380, time: 0.224, loss: 1921.508789\n",
      "Train: step:  35390, time: 0.190, loss: 987.951050\n",
      "Train: step:  35400, time: 0.190, loss: 1428.275146\n",
      "Train: step:  35410, time: 0.195, loss: 1235.345947\n",
      "Train: step:  35420, time: 0.191, loss: 892.844299\n",
      "Train: step:  35430, time: 0.190, loss: 1112.200073\n",
      "Train: step:  35440, time: 0.186, loss: 271.011932\n",
      "Train: step:  35450, time: 0.186, loss: 1194.768188\n",
      "Train: step:  35460, time: 0.184, loss: 3027.589600\n",
      "Train: step:  35470, time: 0.190, loss: 2060.523438\n",
      "Train: step:  35480, time: 0.188, loss: 1499.220947\n",
      "Train: step:  35490, time: 0.228, loss: 1732.966187\n",
      "Train: step:  35500, time: 0.229, loss: 3548.768799\n",
      "Train: step:  35510, time: 0.192, loss: 3658.293701\n",
      "Train: step:  35520, time: 0.191, loss: 131.062668\n",
      "Train: step:  35530, time: 0.187, loss: 1142.432495\n",
      "Train: step:  35540, time: 0.186, loss: 1438.397095\n",
      "Train: step:  35550, time: 0.220, loss: 1669.906494\n",
      "Train: step:  35560, time: 0.193, loss: 3279.554199\n",
      "Train: step:  35570, time: 0.189, loss: 1884.075073\n",
      "Train: step:  35580, time: 0.185, loss: 995.880249\n",
      "Train: step:  35590, time: 0.234, loss: 1512.586548\n",
      "Train: step:  35600, time: 0.190, loss: 1709.284302\n",
      "Train: step:  35610, time: 0.234, loss: 1955.257446\n",
      "Train: step:  35620, time: 0.200, loss: 2356.347900\n",
      "Train: step:  35630, time: 0.185, loss: 348.061096\n",
      "Train: step:  35640, time: 0.231, loss: 3118.095947\n",
      "Train: step:  35650, time: 0.190, loss: 1501.122559\n",
      "Train: step:  35660, time: 0.188, loss: 375.986053\n",
      "Train: step:  35670, time: 0.191, loss: 520.254089\n",
      "Train: step:  35680, time: 0.187, loss: 2029.096313\n",
      "Train: step:  35690, time: 0.202, loss: 1595.348389\n",
      "Train: step:  35700, time: 0.187, loss: 2620.551270\n",
      "Train: step:  35710, time: 0.195, loss: 734.779175\n",
      "Train: step:  35720, time: 0.182, loss: 652.444763\n",
      "Train: step:  35730, time: 0.206, loss: 2901.874023\n",
      "Train: step:  35740, time: 0.190, loss: 1665.847290\n",
      "Train: step:  35750, time: 0.215, loss: 1855.116333\n",
      "Train: step:  35760, time: 0.193, loss: 2043.547119\n",
      "Train: step:  35770, time: 0.207, loss: 2117.721191\n",
      "Train: step:  35780, time: 0.189, loss: 1218.119995\n",
      "Train: step:  35790, time: 0.232, loss: 1830.863770\n",
      "Train: step:  35800, time: 0.189, loss: 1701.761719\n",
      "Train: step:  35810, time: 0.194, loss: 544.970459\n",
      "Train: step:  35820, time: 0.191, loss: 2332.375244\n",
      "Train: step:  35830, time: 0.220, loss: 1066.040894\n",
      "Train: step:  35840, time: 0.190, loss: 1452.355591\n",
      "Train: step:  35850, time: 0.187, loss: 1649.828125\n",
      "Train: step:  35860, time: 0.187, loss: 2908.882324\n",
      "Train: step:  35870, time: 0.192, loss: 968.975159\n",
      "Train: step:  35880, time: 0.182, loss: 399.998688\n",
      "Train: step:  35890, time: 0.232, loss: 1214.029419\n",
      "Train: step:  35900, time: 0.215, loss: 1236.379517\n",
      "Train: step:  35910, time: 0.196, loss: 2759.843262\n",
      "Train: step:  35920, time: 0.191, loss: 1179.615967\n",
      "Train: step:  35930, time: 0.192, loss: 1140.943970\n",
      "Train: step:  35940, time: 0.200, loss: 2565.398193\n",
      "Train: step:  35950, time: 0.186, loss: 706.588257\n",
      "Train: step:  35960, time: 0.194, loss: 1211.723022\n",
      "Train: step:  35970, time: 0.192, loss: 2111.591797\n",
      "Train: step:  35980, time: 0.184, loss: 3477.044189\n",
      "Train: step:  35990, time: 0.186, loss: 1675.585205\n",
      "Train: step:  36000, time: 0.236, loss: 2461.807129\n",
      "Train: step:  36010, time: 0.184, loss: 2339.195801\n",
      "Train: step:  36020, time: 0.194, loss: 1731.086060\n",
      "Train: step:  36030, time: 0.189, loss: 2681.585205\n",
      "Train: step:  36040, time: 0.197, loss: 2943.143555\n",
      "Train: step:  36050, time: 0.197, loss: 1079.954346\n",
      "Train: step:  36060, time: 0.199, loss: 2393.712402\n",
      "Train: step:  36070, time: 0.202, loss: 1043.712769\n",
      "Train: step:  36080, time: 0.245, loss: 2962.932373\n",
      "Train: step:  36090, time: 0.185, loss: 2370.232422\n",
      "Train: step:  36100, time: 0.198, loss: 3667.849365\n",
      "Train: step:  36110, time: 0.193, loss: 1706.769165\n",
      "Train: step:  36120, time: 0.236, loss: 427.234009\n",
      "Train: step:  36130, time: 0.217, loss: 2207.687744\n",
      "Train: step:  36140, time: 0.187, loss: 853.675476\n",
      "Train: step:  36150, time: 0.183, loss: 2499.766357\n",
      "Train: step:  36160, time: 0.188, loss: 2945.909180\n",
      "Train: step:  36170, time: 0.182, loss: 3225.580078\n",
      "Train: step:  36180, time: 0.189, loss: 851.296387\n",
      "Train: step:  36190, time: 0.183, loss: 2042.678589\n",
      "Train: step:  36200, time: 0.192, loss: 2129.944092\n",
      "Train: step:  36210, time: 0.225, loss: 1832.768677\n",
      "Train: step:  36220, time: 0.190, loss: 3324.587158\n",
      "Train: step:  36230, time: 0.191, loss: 3175.602051\n",
      "Train: step:  36240, time: 0.185, loss: 2507.857178\n",
      "Train: step:  36250, time: 0.202, loss: 4259.322754\n",
      "Train: step:  36260, time: 0.247, loss: 1824.393066\n",
      "Train: step:  36270, time: 0.199, loss: 1831.744507\n",
      "Train: step:  36280, time: 0.191, loss: 945.031555\n",
      "Train: step:  36290, time: 0.246, loss: 813.539307\n",
      "Train: step:  36300, time: 0.187, loss: 1279.079468\n",
      "Train: step:  36310, time: 0.221, loss: 1597.343140\n",
      "Train: step:  36320, time: 0.192, loss: 2589.192627\n",
      "Train: step:  36330, time: 0.246, loss: 2595.159180\n",
      "Train: step:  36340, time: 0.193, loss: 2162.601318\n",
      "Train: step:  36350, time: 0.213, loss: 1592.206299\n",
      "Train: step:  36360, time: 0.195, loss: 2977.080566\n",
      "Train: step:  36370, time: 0.218, loss: 3837.426514\n",
      "Train: step:  36380, time: 0.219, loss: 3001.769775\n",
      "Train: step:  36390, time: 0.192, loss: 2082.816650\n",
      "Train: step:  36400, time: 0.188, loss: 1007.434875\n",
      "Train: step:  36410, time: 0.191, loss: 2666.261230\n",
      "Train: step:  36420, time: 0.200, loss: 1867.203857\n",
      "Train: step:  36430, time: 0.191, loss: 961.559204\n",
      "Train: step:  36440, time: 0.219, loss: 2408.732178\n",
      "Train: step:  36450, time: 0.187, loss: 1979.836670\n",
      "Train: step:  36460, time: 0.219, loss: 2817.295898\n",
      "Train: step:  36470, time: 0.192, loss: 1503.665771\n",
      "Train: step:  36480, time: 0.197, loss: 1024.068848\n",
      "Train: step:  36490, time: 0.196, loss: 368.617798\n",
      "Train: step:  36500, time: 0.191, loss: 2022.352051\n",
      "Train: step:  36510, time: 0.190, loss: 384.978699\n",
      "Train: step:  36520, time: 0.193, loss: 2512.734131\n",
      "Train: step:  36530, time: 0.187, loss: 1948.179565\n",
      "Train: step:  36540, time: 0.191, loss: 2867.252197\n",
      "Train: step:  36550, time: 0.231, loss: 1225.428589\n",
      "Train: step:  36560, time: 0.195, loss: 1038.565674\n",
      "Train: step:  36570, time: 0.195, loss: 1590.893799\n",
      "Train: step:  36580, time: 0.218, loss: 1201.111572\n",
      "Train: step:  36590, time: 0.188, loss: 930.221436\n",
      "Train: step:  36600, time: 0.188, loss: 736.547485\n",
      "Train: step:  36610, time: 0.192, loss: 1129.415771\n",
      "Train: step:  36620, time: 0.193, loss: 2031.766113\n",
      "Train: step:  36630, time: 0.189, loss: 1782.445190\n",
      "Train: step:  36640, time: 0.202, loss: 2129.759033\n",
      "Train: step:  36650, time: 0.199, loss: 1581.567505\n",
      "Train: step:  36660, time: 0.208, loss: 1611.473145\n",
      "Train: step:  36670, time: 0.229, loss: 1078.626099\n",
      "Train: step:  36680, time: 0.207, loss: 2748.988037\n",
      "Train: step:  36690, time: 0.196, loss: 841.843811\n",
      "Train: step:  36700, time: 0.189, loss: 2706.193359\n",
      "Train: step:  36710, time: 0.230, loss: 1824.434570\n",
      "Train: step:  36720, time: 0.221, loss: 1898.269897\n",
      "Train: step:  36730, time: 0.233, loss: 825.532776\n",
      "Train: step:  36740, time: 0.197, loss: 680.230896\n",
      "Train: step:  36750, time: 0.197, loss: 1421.890503\n",
      "Train: step:  36760, time: 0.195, loss: 581.213440\n",
      "Train: step:  36770, time: 0.191, loss: 1668.130493\n",
      "Train: step:  36780, time: 0.194, loss: 1256.961548\n",
      "Train: step:  36790, time: 0.193, loss: 3592.605469\n",
      "Train: step:  36800, time: 0.181, loss: 2592.435547\n",
      "Train: step:  36810, time: 0.239, loss: 1885.304443\n",
      "Train: step:  36820, time: 0.219, loss: 1085.105591\n",
      "Train: step:  36830, time: 0.188, loss: 1865.124634\n",
      "Train: step:  36840, time: 0.186, loss: 2530.352295\n",
      "Train: step:  36850, time: 0.217, loss: 771.570374\n",
      "Train: step:  36860, time: 0.230, loss: 2254.472656\n",
      "Train: step:  36870, time: 0.186, loss: 2850.701172\n",
      "Train: step:  36880, time: 0.186, loss: 2688.459717\n",
      "Train: step:  36890, time: 0.209, loss: 1563.181274\n",
      "Train: step:  36900, time: 0.185, loss: 805.309692\n",
      "Train: step:  36910, time: 0.189, loss: 2234.026123\n",
      "Train: step:  36920, time: 0.183, loss: 1831.907959\n",
      "Train: step:  36930, time: 0.185, loss: 2125.468506\n",
      "Train: step:  36940, time: 0.188, loss: 1166.316284\n",
      "Train: step:  36950, time: 0.198, loss: 3112.781250\n",
      "Train: step:  36960, time: 0.193, loss: 670.506409\n",
      "Train: step:  36970, time: 0.190, loss: 2846.586670\n",
      "Train: step:  36980, time: 0.203, loss: 686.555176\n",
      "Train: step:  36990, time: 0.183, loss: 3119.694336\n",
      "Train: step:  37000, time: 0.207, loss: 2844.171875\n",
      "Train: step:  37010, time: 0.187, loss: 637.791504\n",
      "Train: step:  37020, time: 0.195, loss: 1561.650513\n",
      "Train: step:  37030, time: 0.203, loss: 617.099548\n",
      "Train: step:  37040, time: 0.195, loss: 2065.927246\n",
      "Train: step:  37050, time: 0.229, loss: 1375.445435\n",
      "Train: step:  37060, time: 0.193, loss: 2349.845703\n",
      "Train: step:  37070, time: 0.191, loss: 1906.451416\n",
      "Train: step:  37080, time: 0.193, loss: 1062.981934\n",
      "Train: step:  37090, time: 0.188, loss: 1678.360962\n",
      "Train: step:  37100, time: 0.217, loss: 1642.657959\n",
      "Train: step:  37110, time: 0.188, loss: 233.491898\n",
      "Train: step:  37120, time: 0.190, loss: 2166.252686\n",
      "Train: step:  37130, time: 0.183, loss: 635.851807\n",
      "Train: step:  37140, time: 0.187, loss: 468.597260\n",
      "Train: step:  37150, time: 0.185, loss: 2045.137085\n",
      "Train: step:  37160, time: 0.187, loss: 2543.407959\n",
      "Train: step:  37170, time: 0.197, loss: 1952.020630\n",
      "Train: step:  37180, time: 0.191, loss: 226.704590\n",
      "Train: step:  37190, time: 0.235, loss: 2710.330322\n",
      "Train: step:  37200, time: 0.240, loss: 824.497986\n",
      "Train: step:  37210, time: 0.191, loss: 2330.543457\n",
      "Train: step:  37220, time: 0.184, loss: 3014.539307\n",
      "Train: step:  37230, time: 0.192, loss: 903.368408\n",
      "Train: step:  37240, time: 0.198, loss: 3297.467041\n",
      "Train: step:  37250, time: 0.196, loss: 624.174316\n",
      "Train: step:  37260, time: 0.187, loss: 4290.760254\n",
      "Train: step:  37270, time: 0.192, loss: 1641.604370\n",
      "Train: step:  37280, time: 0.190, loss: 1135.272705\n",
      "Train: step:  37290, time: 0.218, loss: 2113.207520\n",
      "Train: step:  37300, time: 0.187, loss: 1877.142334\n",
      "Train: step:  37310, time: 0.194, loss: 2066.989746\n",
      "Train: step:  37320, time: 0.243, loss: 3859.153809\n",
      "Train: step:  37330, time: 0.199, loss: 2449.699463\n",
      "Train: step:  37340, time: 0.215, loss: 911.112671\n",
      "Train: step:  37350, time: 0.220, loss: 1864.403198\n",
      "Train: step:  37360, time: 0.193, loss: 694.280212\n",
      "Train: step:  37370, time: 0.229, loss: 1649.944580\n",
      "Train: step:  37380, time: 0.206, loss: 1206.000977\n",
      "Train: step:  37390, time: 0.188, loss: 259.677856\n",
      "Train: step:  37400, time: 0.209, loss: 1045.340454\n",
      "Train: step:  37410, time: 0.197, loss: 409.476166\n",
      "Train: step:  37420, time: 0.200, loss: 2980.735352\n",
      "Train: step:  37430, time: 0.189, loss: 1352.627563\n",
      "Train: step:  37440, time: 0.189, loss: 2395.221436\n",
      "Train: step:  37450, time: 0.207, loss: 1140.537720\n",
      "Train: step:  37460, time: 0.188, loss: 2192.452881\n",
      "Train: step:  37470, time: 0.185, loss: 2552.722412\n",
      "Train: step:  37480, time: 0.190, loss: 1152.916626\n",
      "Train: step:  37490, time: 0.188, loss: 2428.068604\n",
      "Train: step:  37500, time: 0.203, loss: 2647.814697\n",
      "Train: step:  37510, time: 0.219, loss: 1543.075928\n",
      "Train: step:  37520, time: 0.188, loss: 2252.774658\n",
      "Train: step:  37530, time: 0.189, loss: 3183.710449\n",
      "Train: step:  37540, time: 0.190, loss: 2695.963379\n",
      "Train: step:  37550, time: 0.194, loss: 3399.602051\n",
      "Train: step:  37560, time: 0.183, loss: 407.694366\n",
      "Train: step:  37570, time: 0.184, loss: 2201.149414\n",
      "Train: step:  37580, time: 0.218, loss: 3491.545166\n",
      "Train: step:  37590, time: 0.216, loss: 513.347229\n",
      "Train: step:  37600, time: 0.190, loss: 3152.714600\n",
      "Train: step:  37610, time: 0.228, loss: 3432.545898\n",
      "Train: step:  37620, time: 0.195, loss: 2574.019775\n",
      "Train: step:  37630, time: 0.204, loss: 1492.507202\n",
      "Train: step:  37640, time: 0.236, loss: 4190.338379\n",
      "Train: step:  37650, time: 0.197, loss: 1577.978760\n",
      "Train: step:  37660, time: 0.188, loss: 2225.179688\n",
      "Train: step:  37670, time: 0.185, loss: 2347.324951\n",
      "Train: step:  37680, time: 0.189, loss: 2985.670654\n",
      "Train: step:  37690, time: 0.190, loss: 2233.777344\n",
      "Train: step:  37700, time: 0.193, loss: 4304.846680\n",
      "Train: step:  37710, time: 0.194, loss: 1147.323242\n",
      "Train: step:  37720, time: 0.187, loss: 2251.734375\n",
      "Train: step:  37730, time: 0.213, loss: 2333.352539\n",
      "Train: step:  37740, time: 0.186, loss: 4007.022217\n",
      "Train: step:  37750, time: 0.216, loss: 1277.539917\n",
      "Train: step:  37760, time: 0.218, loss: 1161.425293\n",
      "Train: step:  37770, time: 0.188, loss: 1013.750732\n",
      "Train: step:  37780, time: 0.214, loss: 767.215210\n",
      "Train: step:  37790, time: 0.209, loss: 2081.447754\n",
      "Train: step:  37800, time: 0.199, loss: 2471.270508\n",
      "Train: step:  37810, time: 0.210, loss: 1053.964355\n",
      "Train: step:  37820, time: 0.218, loss: 1613.708130\n",
      "Train: step:  37830, time: 0.246, loss: 2411.630371\n",
      "Train: step:  37840, time: 0.200, loss: 1451.027588\n",
      "Train: step:  37850, time: 0.187, loss: 1529.314209\n",
      "Train: step:  37860, time: 0.247, loss: 1187.038452\n",
      "Train: step:  37870, time: 0.195, loss: 1166.952148\n",
      "Train: step:  37880, time: 0.193, loss: 1254.665527\n",
      "Train: step:  37890, time: 0.249, loss: 1910.843994\n",
      "Train: step:  37900, time: 0.184, loss: 1264.879517\n",
      "Train: step:  37910, time: 0.217, loss: 2944.657715\n",
      "Train: step:  37920, time: 0.196, loss: 4243.890137\n",
      "Train: step:  37930, time: 0.226, loss: 705.546631\n",
      "Train: step:  37940, time: 0.194, loss: 1092.518799\n",
      "Train: step:  37950, time: 0.188, loss: 3413.315674\n",
      "Train: step:  37960, time: 0.192, loss: 1114.796875\n",
      "Train: step:  37970, time: 0.197, loss: 1106.351440\n",
      "Train: step:  37980, time: 0.217, loss: 4411.521973\n",
      "Train: step:  37990, time: 0.300, loss: 1136.848145\n",
      "Train: step:  38000, time: 0.190, loss: 1435.204102\n",
      "Train: step:  38010, time: 0.195, loss: 851.098145\n",
      "Train: step:  38020, time: 0.221, loss: 909.358215\n",
      "Train: step:  38030, time: 0.200, loss: 1346.094238\n",
      "Train: step:  38040, time: 0.228, loss: 1136.005615\n",
      "Train: step:  38050, time: 0.190, loss: 1737.815063\n",
      "Train: step:  38060, time: 0.251, loss: 1178.539307\n",
      "Train: step:  38070, time: 0.225, loss: 2369.896240\n",
      "Train: step:  38080, time: 0.186, loss: 540.603821\n",
      "Train: step:  38090, time: 0.208, loss: 3099.849609\n",
      "Train: step:  38100, time: 0.190, loss: 2608.720947\n",
      "Train: step:  38110, time: 0.219, loss: 659.034058\n",
      "Train: step:  38120, time: 0.216, loss: 558.343018\n",
      "Train: step:  38130, time: 0.236, loss: 3965.178467\n",
      "Train: step:  38140, time: 0.188, loss: 1131.258423\n",
      "Train: step:  38150, time: 0.193, loss: 3500.013428\n",
      "Train: step:  38160, time: 0.193, loss: 3611.679932\n",
      "Train: step:  38170, time: 0.217, loss: 1201.902222\n",
      "Train: step:  38180, time: 0.219, loss: 1234.659546\n",
      "Train: step:  38190, time: 0.217, loss: 1959.419678\n",
      "Train: step:  38200, time: 0.193, loss: 1683.560303\n",
      "Train: step:  38210, time: 0.192, loss: 889.636108\n",
      "Train: step:  38220, time: 0.189, loss: 584.266785\n",
      "Train: step:  38230, time: 0.213, loss: 2818.450684\n",
      "Train: step:  38240, time: 0.203, loss: 888.671448\n",
      "Train: step:  38250, time: 0.220, loss: 469.437714\n",
      "Train: step:  38260, time: 0.187, loss: 294.306885\n",
      "Train: step:  38270, time: 0.229, loss: 1054.047729\n",
      "Train: step:  38280, time: 0.202, loss: 1672.700684\n",
      "Train: step:  38290, time: 0.185, loss: 257.681488\n",
      "Train: step:  38300, time: 0.203, loss: 1251.033081\n",
      "Train: step:  38310, time: 0.190, loss: 4360.412598\n",
      "Train: step:  38320, time: 0.187, loss: 404.277344\n",
      "Train: step:  38330, time: 0.263, loss: 2673.577637\n",
      "Train: step:  38340, time: 0.218, loss: 890.983521\n",
      "Train: step:  38350, time: 0.195, loss: 2702.925293\n",
      "Train: step:  38360, time: 0.233, loss: 3798.508057\n",
      "Train: step:  38370, time: 0.186, loss: 904.372559\n",
      "Train: step:  38380, time: 0.194, loss: 2931.422607\n",
      "Train: step:  38390, time: 0.187, loss: 2442.552246\n",
      "Train: step:  38400, time: 0.188, loss: 3417.161865\n",
      "Train: step:  38410, time: 0.216, loss: 2307.005371\n",
      "Train: step:  38420, time: 0.190, loss: 2464.273682\n",
      "Train: step:  38430, time: 0.192, loss: 366.568481\n",
      "Train: step:  38440, time: 0.215, loss: 2467.329346\n",
      "Train: step:  38450, time: 0.191, loss: 1677.378418\n",
      "Train: step:  38460, time: 0.189, loss: 1960.009644\n",
      "Train: step:  38470, time: 0.220, loss: 1741.220825\n",
      "Train: step:  38480, time: 0.192, loss: 4076.822021\n",
      "Train: step:  38490, time: 0.229, loss: 2807.602295\n",
      "Train: step:  38500, time: 0.194, loss: 2299.600098\n",
      "Train: step:  38510, time: 0.186, loss: 2120.920898\n",
      "Train: step:  38520, time: 0.229, loss: 2450.998291\n",
      "Train: step:  38530, time: 0.214, loss: 646.597046\n",
      "Train: step:  38540, time: 0.228, loss: 985.645203\n",
      "Train: step:  38550, time: 0.191, loss: 880.824036\n",
      "Train: step:  38560, time: 0.188, loss: 1902.408203\n",
      "Train: step:  38570, time: 0.224, loss: 1833.180664\n",
      "Train: step:  38580, time: 0.194, loss: 1986.594482\n",
      "Train: step:  38590, time: 0.188, loss: 493.423767\n",
      "Train: step:  38600, time: 0.188, loss: 2766.619385\n",
      "Train: step:  38610, time: 0.219, loss: 1991.973755\n",
      "Train: step:  38620, time: 0.217, loss: 951.261536\n",
      "Train: step:  38630, time: 0.236, loss: 911.778687\n",
      "Train: step:  38640, time: 0.195, loss: 3092.090576\n",
      "Train: step:  38650, time: 0.189, loss: 1618.468628\n",
      "Train: step:  38660, time: 0.188, loss: 1357.609985\n",
      "Train: step:  38670, time: 0.196, loss: 382.475739\n",
      "Train: step:  38680, time: 0.190, loss: 875.580811\n",
      "Train: step:  38690, time: 0.200, loss: 2101.826416\n",
      "Train: step:  38700, time: 0.192, loss: 4822.213867\n",
      "Train: step:  38710, time: 0.191, loss: 3075.468506\n",
      "Train: step:  38720, time: 0.192, loss: 2102.302002\n",
      "Train: step:  38730, time: 0.188, loss: 2604.766602\n",
      "Train: step:  38740, time: 0.195, loss: 1527.488770\n",
      "Train: step:  38750, time: 0.223, loss: 1316.747314\n",
      "Train: step:  38760, time: 0.188, loss: 1890.800659\n",
      "Train: step:  38770, time: 0.189, loss: 3463.697266\n",
      "Train: step:  38780, time: 0.230, loss: 455.286835\n",
      "Train: step:  38790, time: 0.192, loss: 2637.191650\n",
      "Train: step:  38800, time: 0.189, loss: 3534.983887\n",
      "Train: step:  38810, time: 0.227, loss: 2315.497559\n",
      "Train: step:  38820, time: 0.193, loss: 1440.882080\n",
      "Train: step:  38830, time: 0.194, loss: 2653.372803\n",
      "Train: step:  38840, time: 0.193, loss: 1158.567749\n",
      "Train: step:  38850, time: 0.190, loss: 483.259857\n",
      "Train: step:  38860, time: 0.189, loss: 2814.318359\n",
      "Train: step:  38870, time: 0.227, loss: 1675.383301\n",
      "Train: step:  38880, time: 0.193, loss: 1939.232666\n",
      "Train: step:  38890, time: 0.185, loss: 3126.470947\n",
      "Train: step:  38900, time: 0.190, loss: 297.945160\n",
      "Train: step:  38910, time: 0.186, loss: 1696.470581\n",
      "Train: step:  38920, time: 0.185, loss: 1497.317261\n",
      "Train: step:  38930, time: 0.221, loss: 1681.599976\n",
      "Train: step:  38940, time: 0.197, loss: 1334.059082\n",
      "Train: step:  38950, time: 0.186, loss: 3979.757324\n",
      "Train: step:  38960, time: 0.222, loss: 884.035217\n",
      "Train: step:  38970, time: 0.233, loss: 1067.602905\n",
      "Train: step:  38980, time: 0.199, loss: 1308.213501\n",
      "Train: step:  38990, time: 0.227, loss: 535.595764\n",
      "Train: step:  39000, time: 0.227, loss: 2875.868408\n",
      "Train: step:  39010, time: 0.185, loss: 3426.013428\n",
      "Train: step:  39020, time: 0.188, loss: 2089.215088\n",
      "Train: step:  39030, time: 0.197, loss: 2190.433350\n",
      "Train: step:  39040, time: 0.190, loss: 1724.647949\n",
      "Train: step:  39050, time: 0.185, loss: 511.417511\n",
      "Train: step:  39060, time: 0.190, loss: 1918.114136\n",
      "Train: step:  39070, time: 0.190, loss: 2504.387451\n",
      "Train: step:  39080, time: 0.197, loss: 1450.223633\n",
      "Train: step:  39090, time: 0.187, loss: 2604.630859\n",
      "Train: step:  39100, time: 0.218, loss: 2322.999268\n",
      "Train: step:  39110, time: 0.197, loss: 2335.601318\n",
      "Train: step:  39120, time: 0.190, loss: 2419.757812\n",
      "Train: step:  39130, time: 0.195, loss: 2684.799072\n",
      "Train: step:  39140, time: 0.227, loss: 2288.008545\n",
      "Train: step:  39150, time: 0.185, loss: 2355.766602\n",
      "Train: step:  39160, time: 0.230, loss: 1886.388916\n",
      "Train: step:  39170, time: 0.192, loss: 1702.386841\n",
      "Train: step:  39180, time: 0.190, loss: 2614.654297\n",
      "Train: step:  39190, time: 0.189, loss: 1840.691040\n",
      "Train: step:  39200, time: 0.191, loss: 2129.537598\n",
      "Train: step:  39210, time: 0.194, loss: 642.597107\n",
      "Train: step:  39220, time: 0.185, loss: 2720.132080\n",
      "Train: step:  39230, time: 0.206, loss: 1193.890381\n",
      "Train: step:  39240, time: 0.191, loss: 2932.266846\n",
      "Train: step:  39250, time: 0.190, loss: 1596.881226\n",
      "Train: step:  39260, time: 0.235, loss: 617.867432\n",
      "Train: step:  39270, time: 0.193, loss: 2011.416626\n",
      "Train: step:  39280, time: 0.187, loss: 1125.427979\n",
      "Train: step:  39290, time: 0.185, loss: 2209.177490\n",
      "Train: step:  39300, time: 0.189, loss: 2427.704102\n",
      "Train: step:  39310, time: 0.198, loss: 1781.088379\n",
      "Train: step:  39320, time: 0.217, loss: 1105.260254\n",
      "Train: step:  39330, time: 0.232, loss: 2352.593506\n",
      "Train: step:  39340, time: 0.225, loss: 3007.957764\n",
      "Train: step:  39350, time: 0.242, loss: 2455.775146\n",
      "Train: step:  39360, time: 0.189, loss: 2687.115234\n",
      "Train: step:  39370, time: 0.230, loss: 717.613892\n",
      "Train: step:  39380, time: 0.223, loss: 2835.441162\n",
      "Train: step:  39390, time: 0.216, loss: 2492.710693\n",
      "Train: step:  39400, time: 0.192, loss: 2645.719482\n",
      "Train: step:  39410, time: 0.187, loss: 1852.236328\n",
      "Train: step:  39420, time: 0.217, loss: 997.796326\n",
      "Train: step:  39430, time: 0.195, loss: 913.076538\n",
      "Train: step:  39440, time: 0.193, loss: 1296.782349\n",
      "Train: step:  39450, time: 0.217, loss: 1170.474243\n",
      "Train: step:  39460, time: 0.191, loss: 1212.271851\n",
      "Train: step:  39470, time: 0.189, loss: 1785.725830\n",
      "Train: step:  39480, time: 0.221, loss: 2579.540283\n",
      "Train: step:  39490, time: 0.240, loss: 1735.283447\n",
      "Train: step:  39500, time: 0.201, loss: 2362.722412\n",
      "Train: step:  39510, time: 0.192, loss: 374.725861\n",
      "Train: step:  39520, time: 0.223, loss: 737.976379\n",
      "Train: step:  39530, time: 0.231, loss: 887.683167\n",
      "Train: step:  39540, time: 0.237, loss: 3207.249268\n",
      "Train: step:  39550, time: 0.218, loss: 4132.308594\n",
      "Train: step:  39560, time: 0.211, loss: 1261.888550\n",
      "Train: step:  39570, time: 0.193, loss: 1576.570312\n",
      "Train: step:  39580, time: 0.190, loss: 2230.374756\n",
      "Train: step:  39590, time: 0.231, loss: 1269.578979\n",
      "Train: step:  39600, time: 0.217, loss: 546.819946\n",
      "Train: step:  39610, time: 0.219, loss: 2209.093750\n",
      "Train: step:  39620, time: 0.195, loss: 914.344788\n",
      "Train: step:  39630, time: 0.191, loss: 2191.075439\n",
      "Train: step:  39640, time: 0.194, loss: 1070.287231\n",
      "Train: step:  39650, time: 0.191, loss: 1681.433960\n",
      "Train: step:  39660, time: 0.232, loss: 1786.643799\n",
      "Train: step:  39670, time: 0.195, loss: 2673.636719\n",
      "Train: step:  39680, time: 0.180, loss: 1960.084595\n",
      "Train: step:  39690, time: 0.225, loss: 478.636353\n",
      "Train: step:  39700, time: 0.222, loss: 1086.822388\n",
      "Train: step:  39710, time: 0.190, loss: 2037.642456\n",
      "Train: step:  39720, time: 0.199, loss: 2646.888428\n",
      "Train: step:  39730, time: 0.193, loss: 3198.556396\n",
      "Train: step:  39740, time: 0.220, loss: 1551.804199\n",
      "Train: step:  39750, time: 0.191, loss: 1107.157227\n",
      "Train: step:  39760, time: 0.227, loss: 562.809082\n",
      "Train: step:  39770, time: 0.202, loss: 4397.856934\n",
      "Train: step:  39780, time: 0.197, loss: 1157.015625\n",
      "Train: step:  39790, time: 0.193, loss: 2216.553711\n",
      "Train: step:  39800, time: 0.188, loss: 1472.451050\n",
      "Train: step:  39810, time: 0.194, loss: 1089.185913\n",
      "Train: step:  39820, time: 0.196, loss: 911.339417\n",
      "Train: step:  39830, time: 0.192, loss: 2172.868652\n",
      "Train: step:  39840, time: 0.196, loss: 1183.641968\n",
      "Train: step:  39850, time: 0.245, loss: 1136.153809\n",
      "Train: step:  39860, time: 0.194, loss: 1648.606323\n",
      "Train: step:  39870, time: 0.191, loss: 1623.924683\n",
      "Train: step:  39880, time: 0.190, loss: 2459.877441\n",
      "Train: step:  39890, time: 0.196, loss: 1320.239502\n",
      "Train: step:  39900, time: 0.196, loss: 993.698486\n",
      "Train: step:  39910, time: 0.193, loss: 1328.219116\n",
      "Train: step:  39920, time: 0.194, loss: 1777.305420\n",
      "Train: step:  39930, time: 0.190, loss: 3193.135010\n",
      "Train: step:  39940, time: 0.189, loss: 1793.873169\n",
      "Train: step:  39950, time: 0.192, loss: 2897.111572\n",
      "Train: step:  39960, time: 0.209, loss: 760.696960\n",
      "Train: step:  39970, time: 0.192, loss: 1273.146484\n",
      "Train: step:  39980, time: 0.184, loss: 2639.166260\n",
      "Train: step:  39990, time: 0.249, loss: 1135.917236\n",
      "Train: step:  40000, time: 0.221, loss: 2288.968750\n",
      "Train: step:  40010, time: 0.232, loss: 3426.063477\n",
      "Train: step:  40020, time: 0.231, loss: 3014.960693\n",
      "Train: step:  40030, time: 0.195, loss: 3170.719971\n",
      "Train: step:  40040, time: 0.187, loss: 2392.635254\n",
      "Train: step:  40050, time: 0.238, loss: 3042.716797\n",
      "Train: step:  40060, time: 0.231, loss: 4446.040527\n",
      "Train: step:  40070, time: 0.196, loss: 2201.861328\n",
      "Train: step:  40080, time: 0.218, loss: 1550.451172\n",
      "Train: step:  40090, time: 0.251, loss: 3010.514160\n",
      "Train: step:  40100, time: 0.201, loss: 2272.715576\n",
      "Train: step:  40110, time: 0.188, loss: 2139.523438\n",
      "Train: step:  40120, time: 0.205, loss: 2352.906494\n",
      "Train: step:  40130, time: 0.228, loss: 2462.755859\n",
      "Train: step:  40140, time: 0.187, loss: 1495.166382\n",
      "Train: step:  40150, time: 0.189, loss: 2115.931396\n",
      "Train: step:  40160, time: 0.203, loss: 2230.157715\n",
      "Train: step:  40170, time: 0.190, loss: 2034.026611\n",
      "Train: step:  40180, time: 0.219, loss: 815.957825\n",
      "Train: step:  40190, time: 0.195, loss: 2575.895508\n",
      "Train: step:  40200, time: 0.190, loss: 2688.961914\n",
      "Train: step:  40210, time: 0.231, loss: 1950.596313\n",
      "Train: step:  40220, time: 0.218, loss: 2060.479980\n",
      "Train: step:  40230, time: 0.184, loss: 438.100433\n",
      "Train: step:  40240, time: 0.186, loss: 1565.996582\n",
      "Train: step:  40250, time: 0.188, loss: 4129.545410\n",
      "Train: step:  40260, time: 0.184, loss: 4273.850586\n",
      "Train: step:  40270, time: 0.216, loss: 1926.740601\n",
      "Train: step:  40280, time: 0.197, loss: 1071.933716\n",
      "Train: step:  40290, time: 0.193, loss: 2768.077637\n",
      "Train: step:  40300, time: 0.198, loss: 1729.494507\n",
      "Train: step:  40310, time: 0.189, loss: 815.641602\n",
      "Train: step:  40320, time: 0.189, loss: 2041.982056\n",
      "Train: step:  40330, time: 0.193, loss: 1988.470581\n",
      "Train: step:  40340, time: 0.218, loss: 1348.866577\n",
      "Train: step:  40350, time: 0.220, loss: 2614.639893\n",
      "Train: step:  40360, time: 0.203, loss: 1036.728760\n",
      "Train: step:  40370, time: 0.197, loss: 3214.077393\n",
      "Train: step:  40380, time: 0.244, loss: 1531.289673\n",
      "Train: step:  40390, time: 0.213, loss: 761.920959\n",
      "Train: step:  40400, time: 0.201, loss: 1128.681519\n",
      "Train: step:  40410, time: 0.207, loss: 970.235168\n",
      "Train: step:  40420, time: 0.188, loss: 3057.174561\n",
      "Train: step:  40430, time: 0.192, loss: 822.066040\n",
      "Train: step:  40440, time: 0.205, loss: 2227.465332\n",
      "Train: step:  40450, time: 0.186, loss: 834.496521\n",
      "Train: step:  40460, time: 0.215, loss: 2152.206543\n",
      "Train: step:  40470, time: 0.200, loss: 2333.067627\n",
      "Train: step:  40480, time: 0.243, loss: 1422.512451\n",
      "Train: step:  40490, time: 0.241, loss: 2947.162354\n",
      "Train: step:  40500, time: 0.219, loss: 2731.646240\n",
      "Train: step:  40510, time: 0.219, loss: 1578.776489\n",
      "Train: step:  40520, time: 0.189, loss: 3470.181641\n",
      "Train: step:  40530, time: 0.191, loss: 1677.342285\n",
      "Train: step:  40540, time: 0.218, loss: 768.300842\n",
      "Train: step:  40550, time: 0.189, loss: 914.105530\n",
      "Train: step:  40560, time: 0.218, loss: 956.500183\n",
      "Train: step:  40570, time: 0.189, loss: 2494.477783\n",
      "Train: step:  40580, time: 0.194, loss: 1894.891235\n",
      "Train: step:  40590, time: 0.196, loss: 751.849304\n",
      "Train: step:  40600, time: 0.192, loss: 1775.965698\n",
      "Train: step:  40610, time: 0.188, loss: 1092.625366\n",
      "Train: step:  40620, time: 0.196, loss: 2241.049316\n",
      "Train: step:  40630, time: 0.192, loss: 1845.733154\n",
      "Train: step:  40640, time: 0.187, loss: 2794.176270\n",
      "Train: step:  40650, time: 0.194, loss: 3825.083252\n",
      "Train: step:  40660, time: 0.197, loss: 1484.827515\n",
      "Train: step:  40670, time: 0.202, loss: 1927.565063\n",
      "Train: step:  40680, time: 0.199, loss: 538.092773\n",
      "Train: step:  40690, time: 0.195, loss: 3562.393066\n",
      "Train: step:  40700, time: 0.197, loss: 2557.611572\n",
      "Train: step:  40710, time: 0.202, loss: 2347.026367\n",
      "Train: step:  40720, time: 0.205, loss: 372.811676\n",
      "Train: step:  40730, time: 0.192, loss: 989.693176\n",
      "Train: step:  40740, time: 0.191, loss: 3628.210449\n",
      "Train: step:  40750, time: 0.223, loss: 984.852478\n",
      "Train: step:  40760, time: 0.231, loss: 1677.593140\n",
      "Train: step:  40770, time: 0.191, loss: 2613.126221\n",
      "Train: step:  40780, time: 0.192, loss: 4045.953125\n",
      "Train: step:  40790, time: 0.195, loss: 1100.179565\n",
      "Train: step:  40800, time: 0.186, loss: 2487.662109\n",
      "Train: step:  40810, time: 0.196, loss: 1903.489136\n",
      "Train: step:  40820, time: 0.195, loss: 516.613220\n",
      "Train: step:  40830, time: 0.191, loss: 3089.451416\n",
      "Train: step:  40840, time: 0.186, loss: 619.237183\n",
      "Train: step:  40850, time: 0.191, loss: 2912.219971\n",
      "Train: step:  40860, time: 0.203, loss: 1212.687500\n",
      "Train: step:  40870, time: 0.190, loss: 1289.884277\n",
      "Train: step:  40880, time: 0.198, loss: 1892.484619\n",
      "Train: step:  40890, time: 0.190, loss: 1249.877197\n",
      "Train: step:  40900, time: 0.194, loss: 904.750183\n",
      "Train: step:  40910, time: 0.226, loss: 796.676880\n",
      "Train: step:  40920, time: 0.192, loss: 2666.995850\n",
      "Train: step:  40930, time: 0.199, loss: 1799.302124\n",
      "Train: step:  40940, time: 0.191, loss: 803.984680\n",
      "Train: step:  40950, time: 0.186, loss: 2473.326904\n",
      "Train: step:  40960, time: 0.193, loss: 2434.982666\n",
      "Train: step:  40970, time: 0.186, loss: 1697.738770\n",
      "Train: step:  40980, time: 0.191, loss: 1260.510742\n",
      "Train: step:  40990, time: 0.190, loss: 3526.613281\n",
      "Train: step:  41000, time: 0.190, loss: 3919.544678\n",
      "Train: step:  41010, time: 0.189, loss: 470.457153\n",
      "Train: step:  41020, time: 0.264, loss: 1011.435486\n",
      "Train: step:  41030, time: 0.186, loss: 2980.944580\n",
      "Train: step:  41040, time: 0.192, loss: 793.493469\n",
      "Train: step:  41050, time: 0.220, loss: 956.417725\n",
      "Train: step:  41060, time: 0.217, loss: 1671.460938\n",
      "Train: step:  41070, time: 0.192, loss: 2119.513916\n",
      "Train: step:  41080, time: 0.189, loss: 1207.057373\n",
      "Train: step:  41090, time: 0.189, loss: 1237.194580\n",
      "Train: step:  41100, time: 0.193, loss: 2467.390137\n",
      "Train: step:  41110, time: 0.227, loss: 971.793640\n",
      "Train: step:  41120, time: 0.188, loss: 1958.476440\n",
      "Train: step:  41130, time: 0.191, loss: 850.164856\n",
      "Train: step:  41140, time: 0.194, loss: 1043.677734\n",
      "Train: step:  41150, time: 0.196, loss: 4193.835449\n",
      "Train: step:  41160, time: 0.225, loss: 2170.123535\n",
      "Train: step:  41170, time: 0.187, loss: 1228.255493\n",
      "Train: step:  41180, time: 0.192, loss: 359.515900\n",
      "Train: step:  41190, time: 0.195, loss: 884.685303\n",
      "Train: step:  41200, time: 0.185, loss: 2659.921631\n",
      "Train: step:  41210, time: 0.213, loss: 3289.325928\n",
      "Train: step:  41220, time: 0.190, loss: 2181.525146\n",
      "Train: step:  41230, time: 0.192, loss: 2546.584473\n",
      "Train: step:  41240, time: 0.243, loss: 1524.336670\n",
      "Train: step:  41250, time: 0.216, loss: 1383.801514\n",
      "Train: step:  41260, time: 0.185, loss: 1841.876831\n",
      "Train: step:  41270, time: 0.191, loss: 1417.701172\n",
      "Train: step:  41280, time: 0.215, loss: 2178.070801\n",
      "Train: step:  41290, time: 0.191, loss: 506.906525\n",
      "Train: step:  41300, time: 0.214, loss: 1935.588379\n",
      "Train: step:  41310, time: 0.187, loss: 2338.651123\n",
      "Train: step:  41320, time: 0.195, loss: 1285.300781\n",
      "Train: step:  41330, time: 0.194, loss: 3226.671143\n",
      "Train: step:  41340, time: 0.203, loss: 1257.234741\n",
      "Train: step:  41350, time: 0.188, loss: 2749.780029\n",
      "Train: step:  41360, time: 0.249, loss: 1598.427490\n",
      "Train: step:  41370, time: 0.251, loss: 1103.256836\n",
      "Train: step:  41380, time: 0.186, loss: 2157.687988\n",
      "Train: step:  41390, time: 0.190, loss: 1609.929565\n",
      "Train: step:  41400, time: 0.186, loss: 2283.491699\n",
      "Train: step:  41410, time: 0.188, loss: 956.443909\n",
      "Train: step:  41420, time: 0.187, loss: 2157.397461\n",
      "Train: step:  41430, time: 0.198, loss: 1797.369019\n",
      "Train: step:  41440, time: 0.198, loss: 1414.695557\n",
      "Train: step:  41450, time: 0.235, loss: 891.505371\n",
      "Train: step:  41460, time: 0.228, loss: 2791.333496\n",
      "Train: step:  41470, time: 0.188, loss: 2540.458984\n",
      "Train: step:  41480, time: 0.193, loss: 525.407715\n",
      "Train: step:  41490, time: 0.190, loss: 1574.723877\n",
      "Train: step:  41500, time: 0.231, loss: 438.633423\n",
      "Train: step:  41510, time: 0.218, loss: 2478.470947\n",
      "Train: step:  41520, time: 0.202, loss: 3168.828369\n",
      "Train: step:  41530, time: 0.221, loss: 1007.808472\n",
      "Train: step:  41540, time: 0.217, loss: 2667.522705\n",
      "Train: step:  41550, time: 0.196, loss: 2994.664795\n",
      "Train: step:  41560, time: 0.198, loss: 1505.095825\n",
      "Train: step:  41570, time: 0.195, loss: 1369.697388\n",
      "Train: step:  41580, time: 0.194, loss: 3553.047852\n",
      "Train: step:  41590, time: 0.189, loss: 721.160461\n",
      "Train: step:  41600, time: 0.188, loss: 579.827087\n",
      "Train: step:  41610, time: 0.185, loss: 1758.407471\n",
      "Train: step:  41620, time: 0.240, loss: 699.429382\n",
      "Train: step:  41630, time: 0.219, loss: 1929.325439\n",
      "Train: step:  41640, time: 0.229, loss: 2418.782715\n",
      "Train: step:  41650, time: 0.199, loss: 366.944031\n",
      "Train: step:  41660, time: 0.187, loss: 494.754913\n",
      "Train: step:  41670, time: 0.188, loss: 1401.709961\n",
      "Train: step:  41680, time: 0.193, loss: 2486.846680\n",
      "Train: step:  41690, time: 0.236, loss: 3553.878662\n",
      "Train: step:  41700, time: 0.185, loss: 2078.083008\n",
      "Train: step:  41710, time: 0.206, loss: 2364.115234\n",
      "Train: step:  41720, time: 0.189, loss: 987.284058\n",
      "Train: step:  41730, time: 0.212, loss: 3490.165527\n",
      "Train: step:  41740, time: 0.228, loss: 2647.997070\n",
      "Train: step:  41750, time: 0.192, loss: 2720.489502\n",
      "Train: step:  41760, time: 0.190, loss: 2202.864258\n",
      "Train: step:  41770, time: 0.206, loss: 2075.495361\n",
      "Train: step:  41780, time: 0.184, loss: 1654.989868\n",
      "Train: step:  41790, time: 0.188, loss: 3237.389160\n",
      "Train: step:  41800, time: 0.190, loss: 1958.328735\n",
      "Train: step:  41810, time: 0.200, loss: 2433.242188\n",
      "Train: step:  41820, time: 0.188, loss: 3761.449951\n",
      "Train: step:  41830, time: 0.210, loss: 2204.863281\n",
      "Train: step:  41840, time: 0.189, loss: 2223.122559\n",
      "Train: step:  41850, time: 0.198, loss: 967.561401\n",
      "Train: step:  41860, time: 0.202, loss: 529.990906\n",
      "Train: step:  41870, time: 0.194, loss: 2359.011475\n",
      "Train: step:  41880, time: 0.188, loss: 2594.123779\n",
      "Train: step:  41890, time: 0.229, loss: 923.443604\n",
      "Train: step:  41900, time: 0.192, loss: 2281.486328\n",
      "Train: step:  41910, time: 0.192, loss: 1572.331055\n",
      "Train: step:  41920, time: 0.191, loss: 1000.881714\n",
      "Train: step:  41930, time: 0.190, loss: 333.730286\n",
      "Train: step:  41940, time: 0.192, loss: 3505.150391\n",
      "Train: step:  41950, time: 0.191, loss: 1560.632446\n",
      "Train: step:  41960, time: 0.198, loss: 1015.192139\n",
      "Train: step:  41970, time: 0.200, loss: 2365.425049\n",
      "Train: step:  41980, time: 0.189, loss: 1235.231934\n",
      "Train: step:  41990, time: 0.206, loss: 3390.456055\n",
      "Train: step:  42000, time: 0.195, loss: 2997.382080\n",
      "Train: step:  42010, time: 0.237, loss: 3045.933350\n",
      "Train: step:  42020, time: 0.190, loss: 1458.739258\n",
      "Train: step:  42030, time: 0.186, loss: 2928.861816\n",
      "Train: step:  42040, time: 0.245, loss: 3139.763672\n",
      "Train: step:  42050, time: 0.215, loss: 3129.059082\n",
      "Train: step:  42060, time: 0.217, loss: 1035.479980\n",
      "Train: step:  42070, time: 0.216, loss: 2630.617188\n",
      "Train: step:  42080, time: 0.188, loss: 3413.819336\n",
      "Train: step:  42090, time: 0.217, loss: 3768.972412\n",
      "Train: step:  42100, time: 0.196, loss: 554.658997\n",
      "Train: step:  42110, time: 0.221, loss: 213.686264\n",
      "Train: step:  42120, time: 0.195, loss: 484.276672\n",
      "Train: step:  42130, time: 0.188, loss: 3752.876709\n",
      "Train: step:  42140, time: 0.228, loss: 2031.326172\n",
      "Train: step:  42150, time: 0.196, loss: 1374.675537\n",
      "Train: step:  42160, time: 0.217, loss: 2484.243164\n",
      "Train: step:  42170, time: 0.196, loss: 310.685974\n",
      "Train: step:  42180, time: 0.209, loss: 3342.929199\n",
      "Train: step:  42190, time: 0.188, loss: 1080.383179\n",
      "Train: step:  42200, time: 0.196, loss: 934.572876\n",
      "Train: step:  42210, time: 0.193, loss: 1408.413086\n",
      "Train: step:  42220, time: 0.230, loss: 1828.260376\n",
      "Train: step:  42230, time: 0.284, loss: 2560.045898\n",
      "Train: step:  42240, time: 0.183, loss: 1311.742554\n",
      "Train: step:  42250, time: 0.190, loss: 2733.988525\n",
      "Train: step:  42260, time: 0.195, loss: 1080.988892\n",
      "Train: step:  42270, time: 0.191, loss: 4758.211914\n",
      "Train: step:  42280, time: 0.196, loss: 1402.687012\n",
      "Train: step:  42290, time: 0.191, loss: 1844.587158\n",
      "Train: step:  42300, time: 0.187, loss: 3506.540527\n",
      "Train: step:  42310, time: 0.186, loss: 2757.754883\n",
      "Train: step:  42320, time: 0.225, loss: 1143.927002\n",
      "Train: step:  42330, time: 0.191, loss: 3700.409180\n",
      "Train: step:  42340, time: 0.193, loss: 4155.811523\n",
      "Train: step:  42350, time: 0.215, loss: 3727.914307\n",
      "Train: step:  42360, time: 0.185, loss: 873.524048\n",
      "Train: step:  42370, time: 0.194, loss: 3628.474121\n",
      "Train: step:  42380, time: 0.198, loss: 3031.198975\n",
      "Train: step:  42390, time: 0.190, loss: 1104.482910\n",
      "Train: step:  42400, time: 0.218, loss: 738.096313\n",
      "Train: step:  42410, time: 0.186, loss: 1051.741577\n",
      "Train: step:  42420, time: 0.237, loss: 1230.568726\n",
      "Train: step:  42430, time: 0.187, loss: 2437.848877\n",
      "Train: step:  42440, time: 0.192, loss: 2145.542236\n",
      "Train: step:  42450, time: 0.186, loss: 1285.930542\n",
      "Train: step:  42460, time: 0.190, loss: 2833.602539\n",
      "Train: step:  42470, time: 0.193, loss: 3101.300049\n",
      "Train: step:  42480, time: 0.193, loss: 2232.124756\n",
      "Train: step:  42490, time: 0.189, loss: 1804.065674\n",
      "Train: step:  42500, time: 0.184, loss: 433.606750\n",
      "Train: step:  42510, time: 0.192, loss: 2640.180908\n",
      "Train: step:  42520, time: 0.191, loss: 737.429382\n",
      "Train: step:  42530, time: 0.197, loss: 2255.320068\n",
      "Train: step:  42540, time: 0.219, loss: 3064.571289\n",
      "Train: step:  42550, time: 0.193, loss: 971.443115\n",
      "Train: step:  42560, time: 0.184, loss: 769.943420\n",
      "Train: step:  42570, time: 0.212, loss: 2062.457520\n",
      "Train: step:  42580, time: 0.196, loss: 1940.548096\n",
      "Train: step:  42590, time: 0.195, loss: 1570.726318\n",
      "Train: step:  42600, time: 0.190, loss: 1949.298096\n",
      "Train: step:  42610, time: 0.186, loss: 1335.006348\n",
      "Train: step:  42620, time: 0.187, loss: 3293.356934\n",
      "Train: step:  42630, time: 0.188, loss: 209.991074\n",
      "Train: step:  42640, time: 0.201, loss: 1134.908569\n",
      "Train: step:  42650, time: 0.218, loss: 751.080872\n",
      "Train: step:  42660, time: 0.214, loss: 1645.992188\n",
      "Train: step:  42670, time: 0.185, loss: 1086.832764\n",
      "Train: step:  42680, time: 0.187, loss: 1489.779907\n",
      "Train: step:  42690, time: 0.214, loss: 1108.057495\n",
      "Train: step:  42700, time: 0.192, loss: 1211.693970\n",
      "Train: step:  42710, time: 0.186, loss: 1227.906738\n",
      "Train: step:  42720, time: 0.217, loss: 1685.447144\n",
      "Train: step:  42730, time: 0.186, loss: 1769.392456\n",
      "Train: step:  42740, time: 0.187, loss: 2213.377441\n",
      "Train: step:  42750, time: 0.187, loss: 1313.821411\n",
      "Train: step:  42760, time: 0.187, loss: 2322.115723\n",
      "Train: step:  42770, time: 0.195, loss: 1917.051147\n",
      "Train: step:  42780, time: 0.188, loss: 2378.565430\n",
      "Train: step:  42790, time: 0.190, loss: 2530.960449\n",
      "Train: step:  42800, time: 0.218, loss: 2420.635986\n",
      "Train: step:  42810, time: 0.185, loss: 2700.925781\n",
      "Train: step:  42820, time: 0.217, loss: 2655.247314\n",
      "Train: step:  42830, time: 0.193, loss: 1281.117188\n",
      "Train: step:  42840, time: 0.231, loss: 2443.723389\n",
      "Train: step:  42850, time: 0.188, loss: 920.241272\n",
      "Train: step:  42860, time: 0.230, loss: 1386.581421\n",
      "Train: step:  42870, time: 0.196, loss: 3124.753906\n",
      "Train: step:  42880, time: 0.226, loss: 2846.069092\n",
      "Train: step:  42890, time: 0.186, loss: 1129.679565\n",
      "Train: step:  42900, time: 0.184, loss: 1026.132080\n",
      "Train: step:  42910, time: 0.229, loss: 1888.274292\n",
      "Train: step:  42920, time: 0.188, loss: 2919.005615\n",
      "Train: step:  42930, time: 0.187, loss: 2098.329834\n",
      "Train: step:  42940, time: 0.211, loss: 1607.179810\n",
      "Train: step:  42950, time: 0.227, loss: 2923.443604\n",
      "Train: step:  42960, time: 0.227, loss: 1810.943970\n",
      "Train: step:  42970, time: 0.196, loss: 2057.600586\n",
      "Train: step:  42980, time: 0.227, loss: 1376.707520\n",
      "Train: step:  42990, time: 0.190, loss: 3193.556152\n",
      "Train: step:  43000, time: 0.190, loss: 1559.918945\n",
      "Train: step:  43010, time: 0.211, loss: 1941.310059\n",
      "Train: step:  43020, time: 0.256, loss: 2908.217529\n",
      "Train: step:  43030, time: 0.217, loss: 827.033386\n",
      "Train: step:  43040, time: 0.232, loss: 4134.818848\n",
      "Train: step:  43050, time: 0.188, loss: 2466.333008\n",
      "Train: step:  43060, time: 0.190, loss: 4105.977051\n",
      "Train: step:  43070, time: 0.203, loss: 1307.024536\n",
      "Train: step:  43080, time: 0.186, loss: 1918.355347\n",
      "Train: step:  43090, time: 0.189, loss: 1745.567627\n",
      "Train: step:  43100, time: 0.216, loss: 3297.561768\n",
      "Train: step:  43110, time: 0.185, loss: 2228.513184\n",
      "Train: step:  43120, time: 0.227, loss: 1752.513306\n",
      "Train: step:  43130, time: 0.189, loss: 2261.501221\n",
      "Train: step:  43140, time: 0.231, loss: 3512.052002\n",
      "Train: step:  43150, time: 0.233, loss: 2416.386963\n",
      "Train: step:  43160, time: 0.185, loss: 890.902710\n",
      "Train: step:  43170, time: 0.190, loss: 880.142090\n",
      "Train: step:  43180, time: 0.216, loss: 2153.363770\n",
      "Train: step:  43190, time: 0.188, loss: 2348.733643\n",
      "Train: step:  43200, time: 0.193, loss: 784.343384\n",
      "Train: step:  43210, time: 0.217, loss: 2857.849854\n",
      "Train: step:  43220, time: 0.218, loss: 1962.208862\n",
      "Train: step:  43230, time: 0.192, loss: 1225.123047\n",
      "Train: step:  43240, time: 0.230, loss: 629.914490\n",
      "Train: step:  43250, time: 0.192, loss: 2960.175781\n",
      "Train: step:  43260, time: 0.217, loss: 763.084534\n",
      "Train: step:  43270, time: 0.191, loss: 2095.720459\n",
      "Train: step:  43280, time: 0.237, loss: 2145.230957\n",
      "Train: step:  43290, time: 0.186, loss: 1209.850464\n",
      "Train: step:  43300, time: 0.194, loss: 1181.559692\n",
      "Train: step:  43310, time: 0.246, loss: 3162.643311\n",
      "Train: step:  43320, time: 0.227, loss: 2282.187012\n",
      "Train: step:  43330, time: 0.229, loss: 1885.785522\n",
      "Train: step:  43340, time: 0.187, loss: 702.882568\n",
      "Train: step:  43350, time: 0.241, loss: 3586.013184\n",
      "Train: step:  43360, time: 0.201, loss: 1650.480591\n",
      "Train: step:  43370, time: 0.200, loss: 778.974792\n",
      "Train: step:  43380, time: 0.190, loss: 1238.056519\n",
      "Train: step:  43390, time: 0.219, loss: 2321.683105\n",
      "Train: step:  43400, time: 0.200, loss: 3216.210693\n",
      "Train: step:  43410, time: 0.189, loss: 1415.301147\n",
      "Train: step:  43420, time: 0.192, loss: 613.013916\n",
      "Train: step:  43430, time: 0.190, loss: 1093.818237\n",
      "Train: step:  43440, time: 0.196, loss: 2004.869263\n",
      "Train: step:  43450, time: 0.219, loss: 3113.181885\n",
      "Train: step:  43460, time: 0.205, loss: 3276.937012\n",
      "Train: step:  43470, time: 0.192, loss: 2022.578369\n",
      "Train: step:  43480, time: 0.201, loss: 670.748047\n",
      "Train: step:  43490, time: 0.193, loss: 3810.165039\n",
      "Train: step:  43500, time: 0.245, loss: 1467.694214\n",
      "Train: step:  43510, time: 0.217, loss: 538.310303\n",
      "Train: step:  43520, time: 0.216, loss: 2104.296631\n",
      "Train: step:  43530, time: 0.201, loss: 1208.602295\n",
      "Train: step:  43540, time: 0.188, loss: 883.862183\n",
      "Train: step:  43550, time: 0.217, loss: 1351.831299\n",
      "Train: step:  43560, time: 0.194, loss: 3558.064453\n",
      "Train: step:  43570, time: 0.281, loss: 2215.974854\n",
      "Train: step:  43580, time: 0.248, loss: 2984.663818\n",
      "Train: step:  43590, time: 0.217, loss: 2358.568848\n",
      "Train: step:  43600, time: 0.196, loss: 1151.910034\n",
      "Train: step:  43610, time: 0.204, loss: 1514.312744\n",
      "Train: step:  43620, time: 0.193, loss: 1625.147217\n",
      "Train: step:  43630, time: 0.215, loss: 686.763916\n",
      "Train: step:  43640, time: 0.194, loss: 1181.037231\n",
      "Train: step:  43650, time: 0.217, loss: 1416.706909\n",
      "Train: step:  43660, time: 0.186, loss: 2482.580566\n",
      "Train: step:  43670, time: 0.217, loss: 1364.546021\n",
      "Train: step:  43680, time: 0.227, loss: 2528.645996\n",
      "Train: step:  43690, time: 0.218, loss: 2227.513184\n",
      "Train: step:  43700, time: 0.219, loss: 788.083740\n",
      "Train: step:  43710, time: 0.254, loss: 1622.364380\n",
      "Train: step:  43720, time: 0.193, loss: 2863.885498\n",
      "Train: step:  43730, time: 0.227, loss: 1166.557861\n",
      "Train: step:  43740, time: 0.243, loss: 1190.825806\n",
      "Train: step:  43750, time: 0.194, loss: 954.805908\n",
      "Train: step:  43760, time: 0.213, loss: 2445.296875\n",
      "Train: step:  43770, time: 0.216, loss: 1156.596069\n",
      "Train: step:  43780, time: 0.183, loss: 2594.991211\n",
      "Train: step:  43790, time: 0.213, loss: 2579.656982\n",
      "Train: step:  43800, time: 0.184, loss: 1053.591919\n",
      "Train: step:  43810, time: 0.187, loss: 1130.996216\n",
      "Train: step:  43820, time: 0.190, loss: 2244.429688\n",
      "Train: step:  43830, time: 0.190, loss: 1052.965088\n",
      "Train: step:  43840, time: 0.229, loss: 1275.901245\n",
      "Train: step:  43850, time: 0.246, loss: 2509.181396\n",
      "Train: step:  43860, time: 0.184, loss: 1704.340454\n",
      "Train: step:  43870, time: 0.189, loss: 490.045044\n",
      "Train: step:  43880, time: 0.189, loss: 3961.366211\n",
      "Train: step:  43890, time: 0.187, loss: 1514.066772\n",
      "Train: step:  43900, time: 0.190, loss: 1155.283936\n",
      "Train: step:  43910, time: 0.190, loss: 1200.318726\n",
      "Train: step:  43920, time: 0.193, loss: 1656.895874\n",
      "Train: step:  43930, time: 0.211, loss: 2302.960693\n",
      "Train: step:  43940, time: 0.196, loss: 3164.686523\n",
      "Train: step:  43950, time: 0.193, loss: 950.530701\n",
      "Train: step:  43960, time: 0.192, loss: 854.742493\n",
      "Train: step:  43970, time: 0.226, loss: 2670.561035\n",
      "Train: step:  43980, time: 0.188, loss: 2859.252441\n",
      "Train: step:  43990, time: 0.231, loss: 1520.709595\n",
      "Train: step:  44000, time: 0.218, loss: 247.992020\n",
      "Train: step:  44010, time: 0.251, loss: 3056.559326\n",
      "Train: step:  44020, time: 0.229, loss: 3190.138428\n",
      "Train: step:  44030, time: 0.183, loss: 4164.888672\n",
      "Train: step:  44040, time: 0.184, loss: 1788.645752\n",
      "Train: step:  44050, time: 0.190, loss: 1853.334229\n",
      "Train: step:  44060, time: 0.192, loss: 709.774414\n",
      "Train: step:  44070, time: 0.187, loss: 1507.994507\n",
      "Train: step:  44080, time: 0.190, loss: 2306.807373\n",
      "Train: step:  44090, time: 0.184, loss: 2142.601562\n",
      "Train: step:  44100, time: 0.184, loss: 4435.854492\n",
      "Train: step:  44110, time: 0.186, loss: 2214.409912\n",
      "Train: step:  44120, time: 0.186, loss: 1078.648315\n",
      "Train: step:  44130, time: 0.188, loss: 1896.189575\n",
      "Train: step:  44140, time: 0.193, loss: 2618.647461\n",
      "Train: step:  44150, time: 0.214, loss: 1866.657959\n",
      "Train: step:  44160, time: 0.185, loss: 1068.517090\n",
      "Train: step:  44170, time: 0.183, loss: 1234.673462\n",
      "Train: step:  44180, time: 0.187, loss: 1780.665283\n",
      "Train: step:  44190, time: 0.189, loss: 4040.281982\n",
      "Train: step:  44200, time: 0.193, loss: 1369.573975\n",
      "Train: step:  44210, time: 0.194, loss: 1725.610718\n",
      "Train: step:  44220, time: 0.191, loss: 1937.731201\n",
      "Train: step:  44230, time: 0.218, loss: 1976.609009\n",
      "Train: step:  44240, time: 0.218, loss: 3414.423340\n",
      "Train: step:  44250, time: 0.240, loss: 1794.722412\n",
      "Train: step:  44260, time: 0.195, loss: 3211.454346\n",
      "Train: step:  44270, time: 0.231, loss: 2655.538818\n",
      "Train: step:  44280, time: 0.216, loss: 820.608521\n",
      "Train: step:  44290, time: 0.189, loss: 1483.020996\n",
      "Train: step:  44300, time: 0.187, loss: 1246.004272\n",
      "Train: step:  44310, time: 0.197, loss: 709.527954\n",
      "Train: step:  44320, time: 0.180, loss: 1963.955078\n",
      "Train: step:  44330, time: 0.188, loss: 1658.213135\n",
      "Train: step:  44340, time: 0.230, loss: 1826.790649\n",
      "Train: step:  44350, time: 0.187, loss: 1291.585205\n",
      "Train: step:  44360, time: 0.188, loss: 2072.353027\n",
      "Train: step:  44370, time: 0.193, loss: 518.217285\n",
      "Train: step:  44380, time: 0.200, loss: 3830.789551\n",
      "Train: step:  44390, time: 0.187, loss: 1350.569336\n",
      "Train: step:  44400, time: 0.218, loss: 652.402893\n",
      "Train: step:  44410, time: 0.192, loss: 2294.224854\n",
      "Train: step:  44420, time: 0.247, loss: 1587.733765\n",
      "Train: step:  44430, time: 0.226, loss: 701.735779\n",
      "Train: step:  44440, time: 0.223, loss: 470.278534\n",
      "Train: step:  44450, time: 0.217, loss: 1884.724731\n",
      "Train: step:  44460, time: 0.194, loss: 3650.895020\n",
      "Train: step:  44470, time: 0.186, loss: 3453.936279\n",
      "Train: step:  44480, time: 0.194, loss: 2533.908691\n",
      "Train: step:  44490, time: 0.225, loss: 2961.913818\n",
      "Train: step:  44500, time: 0.194, loss: 3229.222900\n",
      "Train: step:  44510, time: 0.217, loss: 521.304321\n",
      "Train: step:  44520, time: 0.230, loss: 1347.715088\n",
      "Train: step:  44530, time: 0.189, loss: 3764.251465\n",
      "Train: step:  44540, time: 0.216, loss: 1974.044067\n",
      "Train: step:  44550, time: 0.191, loss: 2212.286133\n",
      "Train: step:  44560, time: 0.191, loss: 644.574158\n",
      "Train: step:  44570, time: 0.191, loss: 1744.271118\n",
      "Train: step:  44580, time: 0.197, loss: 1989.798950\n",
      "Train: step:  44590, time: 0.219, loss: 2625.592529\n",
      "Train: step:  44600, time: 0.257, loss: 2245.926270\n",
      "Train: step:  44610, time: 0.197, loss: 2591.354248\n",
      "Train: step:  44620, time: 0.202, loss: 3210.952881\n",
      "Train: step:  44630, time: 0.217, loss: 413.278778\n",
      "Train: step:  44640, time: 0.189, loss: 1426.725952\n",
      "Train: step:  44650, time: 0.187, loss: 2539.509033\n",
      "Train: step:  44660, time: 0.196, loss: 1335.491943\n",
      "Train: step:  44670, time: 0.189, loss: 767.568115\n",
      "Train: step:  44680, time: 0.220, loss: 1746.890259\n",
      "Train: step:  44690, time: 0.186, loss: 2606.737061\n",
      "Train: step:  44700, time: 0.186, loss: 2282.781738\n",
      "Train: step:  44710, time: 0.196, loss: 1525.141724\n",
      "Train: step:  44720, time: 0.191, loss: 1514.923096\n",
      "Train: step:  44730, time: 0.226, loss: 3354.468018\n",
      "Train: step:  44740, time: 0.195, loss: 758.474487\n",
      "Train: step:  44750, time: 0.233, loss: 2971.960693\n",
      "Train: step:  44760, time: 0.252, loss: 2015.588989\n",
      "Train: step:  44770, time: 0.187, loss: 1671.437012\n",
      "Train: step:  44780, time: 0.217, loss: 207.887787\n",
      "Train: step:  44790, time: 0.192, loss: 1140.383057\n",
      "Train: step:  44800, time: 0.187, loss: 1839.605713\n",
      "Train: step:  44810, time: 0.218, loss: 2338.353760\n",
      "Train: step:  44820, time: 0.187, loss: 2057.211670\n",
      "Train: step:  44830, time: 0.217, loss: 3053.190430\n",
      "Train: step:  44840, time: 0.228, loss: 4265.000488\n",
      "Train: step:  44850, time: 0.185, loss: 716.413513\n",
      "Train: step:  44860, time: 0.186, loss: 1969.664917\n",
      "Train: step:  44870, time: 0.192, loss: 4334.170898\n",
      "Train: step:  44880, time: 0.198, loss: 1244.274170\n",
      "Train: step:  44890, time: 0.239, loss: 1886.058350\n",
      "Train: step:  44900, time: 0.219, loss: 1615.501831\n",
      "Train: step:  44910, time: 0.191, loss: 634.138977\n",
      "Train: step:  44920, time: 0.215, loss: 576.801208\n",
      "Train: step:  44930, time: 0.188, loss: 397.016113\n",
      "Train: step:  44940, time: 0.188, loss: 1485.547363\n",
      "Train: step:  44950, time: 0.253, loss: 3631.153076\n",
      "Train: step:  44960, time: 0.187, loss: 1378.565308\n",
      "Train: step:  44970, time: 0.191, loss: 1209.796509\n",
      "Train: step:  44980, time: 0.183, loss: 1219.399292\n",
      "Train: step:  44990, time: 0.217, loss: 2365.073975\n",
      "Train: step:  45000, time: 0.189, loss: 2847.776123\n",
      "Train: step:  45010, time: 0.192, loss: 2666.187988\n",
      "Train: step:  45020, time: 0.221, loss: 2166.093750\n",
      "Train: step:  45030, time: 0.195, loss: 447.631226\n",
      "Train: step:  45040, time: 0.188, loss: 1708.498901\n",
      "Train: step:  45050, time: 0.218, loss: 3217.927246\n",
      "Train: step:  45060, time: 0.191, loss: 1346.853882\n",
      "Train: step:  45070, time: 0.191, loss: 1453.040649\n",
      "Train: step:  45080, time: 0.188, loss: 2683.291504\n",
      "Train: step:  45090, time: 0.190, loss: 584.856201\n",
      "Train: step:  45100, time: 0.183, loss: 1727.340942\n",
      "Train: step:  45110, time: 0.240, loss: 2439.979980\n",
      "Train: step:  45120, time: 0.232, loss: 3094.634521\n",
      "Train: step:  45130, time: 0.192, loss: 578.930481\n",
      "Train: step:  45140, time: 0.217, loss: 1712.313232\n",
      "Train: step:  45150, time: 0.199, loss: 2108.373047\n",
      "Train: step:  45160, time: 0.214, loss: 1820.410889\n",
      "Train: step:  45170, time: 0.190, loss: 2962.880127\n",
      "Train: step:  45180, time: 0.198, loss: 1677.664429\n",
      "Train: step:  45190, time: 0.197, loss: 1958.148193\n",
      "Train: step:  45200, time: 0.231, loss: 2202.694336\n",
      "Train: step:  45210, time: 0.191, loss: 1663.716431\n",
      "Train: step:  45220, time: 0.216, loss: 1728.537720\n",
      "Train: step:  45230, time: 0.222, loss: 1458.552490\n",
      "Train: step:  45240, time: 0.190, loss: 1984.309692\n",
      "Train: step:  45250, time: 0.196, loss: 947.624512\n",
      "Train: step:  45260, time: 0.195, loss: 1228.958008\n",
      "Train: step:  45270, time: 0.191, loss: 2069.082275\n",
      "Train: step:  45280, time: 0.252, loss: 3687.965576\n",
      "Train: step:  45290, time: 0.194, loss: 1234.354492\n",
      "Train: step:  45300, time: 0.195, loss: 2119.443115\n",
      "Train: step:  45310, time: 0.217, loss: 2010.095093\n",
      "Train: step:  45320, time: 0.227, loss: 1754.253174\n",
      "Train: step:  45330, time: 0.193, loss: 1567.525024\n",
      "Train: step:  45340, time: 0.194, loss: 278.603333\n",
      "Train: step:  45350, time: 0.185, loss: 584.899841\n",
      "Train: step:  45360, time: 0.219, loss: 1313.323975\n",
      "Train: step:  45370, time: 0.185, loss: 441.033722\n",
      "Train: step:  45380, time: 0.191, loss: 2042.474976\n",
      "Train: step:  45390, time: 0.218, loss: 3191.695557\n",
      "Train: step:  45400, time: 0.244, loss: 875.955139\n",
      "Train: step:  45410, time: 0.201, loss: 1467.604980\n",
      "Train: step:  45420, time: 0.188, loss: 854.862000\n",
      "Train: step:  45430, time: 0.184, loss: 986.479004\n",
      "Train: step:  45440, time: 0.187, loss: 406.623322\n",
      "Train: step:  45450, time: 0.237, loss: 1356.045166\n",
      "Train: step:  45460, time: 0.217, loss: 1340.029785\n",
      "Train: step:  45470, time: 0.251, loss: 902.368958\n",
      "Train: step:  45480, time: 0.193, loss: 1335.875854\n",
      "Train: step:  45490, time: 0.246, loss: 2866.719482\n",
      "Train: step:  45500, time: 0.222, loss: 1165.559814\n",
      "Train: step:  45510, time: 0.186, loss: 2669.124023\n",
      "Train: step:  45520, time: 0.186, loss: 1430.002563\n",
      "Train: step:  45530, time: 0.189, loss: 1758.078613\n",
      "Train: step:  45540, time: 0.192, loss: 2526.148926\n",
      "Train: step:  45550, time: 0.191, loss: 1331.497070\n",
      "Train: step:  45560, time: 0.198, loss: 1673.482056\n",
      "Train: step:  45570, time: 0.194, loss: 543.365295\n",
      "Train: step:  45580, time: 0.187, loss: 1256.633789\n",
      "Train: step:  45590, time: 0.190, loss: 675.240234\n",
      "Train: step:  45600, time: 0.194, loss: 3666.749512\n",
      "Train: step:  45610, time: 0.235, loss: 1255.095825\n",
      "Train: step:  45620, time: 0.201, loss: 1312.693237\n",
      "Train: step:  45630, time: 0.194, loss: 2230.606201\n",
      "Train: step:  45640, time: 0.187, loss: 548.264038\n",
      "Train: step:  45650, time: 0.216, loss: 1791.894531\n",
      "Train: step:  45660, time: 0.196, loss: 2897.561523\n",
      "Train: step:  45670, time: 0.203, loss: 3142.199951\n",
      "Train: step:  45680, time: 0.190, loss: 804.210815\n",
      "Train: step:  45690, time: 0.199, loss: 2562.889648\n",
      "Train: step:  45700, time: 0.189, loss: 2638.468262\n",
      "Train: step:  45710, time: 0.191, loss: 2531.711182\n",
      "Train: step:  45720, time: 0.188, loss: 839.695862\n",
      "Train: step:  45730, time: 0.192, loss: 3135.548828\n",
      "Train: step:  45740, time: 0.220, loss: 484.042816\n",
      "Train: step:  45750, time: 0.230, loss: 1577.026489\n",
      "Train: step:  45760, time: 0.248, loss: 2454.667969\n",
      "Train: step:  45770, time: 0.199, loss: 254.884674\n",
      "Train: step:  45780, time: 0.192, loss: 2016.192627\n",
      "Train: step:  45790, time: 0.219, loss: 1928.499634\n",
      "Train: step:  45800, time: 0.216, loss: 2538.819824\n",
      "Train: step:  45810, time: 0.234, loss: 1252.751831\n",
      "Train: step:  45820, time: 0.186, loss: 1391.449463\n",
      "Train: step:  45830, time: 0.227, loss: 451.677887\n",
      "Train: step:  45840, time: 0.190, loss: 2242.278809\n",
      "Train: step:  45850, time: 0.189, loss: 1863.597778\n",
      "Train: step:  45860, time: 0.185, loss: 3375.471924\n",
      "Train: step:  45870, time: 0.222, loss: 644.255005\n",
      "Train: step:  45880, time: 0.199, loss: 1859.735596\n",
      "Train: step:  45890, time: 0.229, loss: 2398.743408\n",
      "Train: step:  45900, time: 0.201, loss: 1339.366699\n",
      "Train: step:  45910, time: 0.191, loss: 3346.084473\n",
      "Train: step:  45920, time: 0.193, loss: 570.336670\n",
      "Train: step:  45930, time: 0.231, loss: 279.424744\n",
      "Train: step:  45940, time: 0.228, loss: 1542.874512\n",
      "Train: step:  45950, time: 0.217, loss: 3140.059570\n",
      "Train: step:  45960, time: 0.186, loss: 1960.879395\n",
      "Train: step:  45970, time: 0.192, loss: 1686.630981\n",
      "Train: step:  45980, time: 0.227, loss: 1402.794312\n",
      "Train: step:  45990, time: 0.185, loss: 1299.172607\n",
      "Train: step:  46000, time: 0.190, loss: 1748.796509\n",
      "Train: step:  46010, time: 0.187, loss: 1464.571167\n",
      "Train: step:  46020, time: 0.191, loss: 1713.491943\n",
      "Train: step:  46030, time: 0.213, loss: 1864.540894\n",
      "Train: step:  46040, time: 0.186, loss: 837.690552\n",
      "Train: step:  46050, time: 0.190, loss: 404.242432\n",
      "Train: step:  46060, time: 0.190, loss: 1716.782593\n",
      "Train: step:  46070, time: 0.185, loss: 1574.095459\n",
      "Train: step:  46080, time: 0.187, loss: 2478.559814\n",
      "Train: step:  46090, time: 0.196, loss: 2899.024170\n",
      "Train: step:  46100, time: 0.194, loss: 1755.754883\n",
      "Train: step:  46110, time: 0.215, loss: 1107.912476\n",
      "Train: step:  46120, time: 0.190, loss: 1565.913330\n",
      "Train: step:  46130, time: 0.185, loss: 2529.490967\n",
      "Train: step:  46140, time: 0.185, loss: 2244.055420\n",
      "Train: step:  46150, time: 0.188, loss: 1593.634888\n",
      "Train: step:  46160, time: 0.194, loss: 1111.198853\n",
      "Train: step:  46170, time: 0.195, loss: 1987.107300\n",
      "Train: step:  46180, time: 0.187, loss: 1841.102051\n",
      "Train: step:  46190, time: 0.186, loss: 2202.686768\n",
      "Train: step:  46200, time: 0.186, loss: 2244.286133\n",
      "Train: step:  46210, time: 0.217, loss: 1495.213501\n",
      "Train: step:  46220, time: 0.187, loss: 790.300354\n",
      "Train: step:  46230, time: 0.227, loss: 675.312805\n",
      "Train: step:  46240, time: 0.196, loss: 564.414246\n",
      "Train: step:  46250, time: 0.197, loss: 713.031860\n",
      "Train: step:  46260, time: 0.201, loss: 2514.744141\n",
      "Train: step:  46270, time: 0.189, loss: 2146.338135\n",
      "Train: step:  46280, time: 0.184, loss: 2132.426270\n",
      "Train: step:  46290, time: 0.237, loss: 2361.843506\n",
      "Train: step:  46300, time: 0.189, loss: 2455.524902\n",
      "Train: step:  46310, time: 0.192, loss: 278.113464\n",
      "Train: step:  46320, time: 0.188, loss: 1507.780151\n",
      "Train: step:  46330, time: 0.187, loss: 2044.041992\n",
      "Train: step:  46340, time: 0.218, loss: 2120.664795\n",
      "Train: step:  46350, time: 0.185, loss: 3654.210449\n",
      "Train: step:  46360, time: 0.190, loss: 337.985657\n",
      "Train: step:  46370, time: 0.190, loss: 1140.307495\n",
      "Train: step:  46380, time: 0.220, loss: 1413.176392\n",
      "Train: step:  46390, time: 0.193, loss: 1050.450562\n",
      "Train: step:  46400, time: 0.190, loss: 3045.197754\n",
      "Train: step:  46410, time: 0.227, loss: 2043.606934\n",
      "Train: step:  46420, time: 0.193, loss: 729.450073\n",
      "Train: step:  46430, time: 0.193, loss: 2543.682617\n",
      "Train: step:  46440, time: 0.207, loss: 887.061157\n",
      "Train: step:  46450, time: 0.244, loss: 1930.249756\n",
      "Train: step:  46460, time: 0.253, loss: 1433.396729\n",
      "Train: step:  46470, time: 0.189, loss: 1515.233276\n",
      "Train: step:  46480, time: 0.188, loss: 3108.798584\n",
      "Train: step:  46490, time: 0.223, loss: 819.562683\n",
      "Train: step:  46500, time: 0.210, loss: 1893.557861\n",
      "Train: step:  46510, time: 0.184, loss: 388.445068\n",
      "Train: step:  46520, time: 0.194, loss: 3344.038818\n",
      "Train: step:  46530, time: 0.197, loss: 2714.785400\n",
      "Train: step:  46540, time: 0.198, loss: 626.394836\n",
      "Train: step:  46550, time: 0.184, loss: 856.630615\n",
      "Train: step:  46560, time: 0.228, loss: 1586.884521\n",
      "Train: step:  46570, time: 0.189, loss: 705.738220\n",
      "Train: step:  46580, time: 0.187, loss: 1538.700928\n",
      "Train: step:  46590, time: 0.183, loss: 3332.190186\n",
      "Train: step:  46600, time: 0.195, loss: 2293.809570\n",
      "Train: step:  46610, time: 0.187, loss: 1245.007080\n",
      "Train: step:  46620, time: 0.187, loss: 845.504700\n",
      "Train: step:  46630, time: 0.196, loss: 249.976059\n",
      "Train: step:  46640, time: 0.229, loss: 2839.914307\n",
      "Train: step:  46650, time: 0.212, loss: 1206.048218\n",
      "Train: step:  46660, time: 0.233, loss: 233.736511\n",
      "Train: step:  46670, time: 0.228, loss: 530.208130\n",
      "Train: step:  46680, time: 0.216, loss: 1412.089111\n",
      "Train: step:  46690, time: 0.190, loss: 3137.156738\n",
      "Train: step:  46700, time: 0.186, loss: 2134.663330\n",
      "Train: step:  46710, time: 0.218, loss: 1195.200073\n",
      "Train: step:  46720, time: 0.188, loss: 2707.312012\n",
      "Train: step:  46730, time: 0.226, loss: 2880.398926\n",
      "Train: step:  46740, time: 0.223, loss: 3458.783936\n",
      "Train: step:  46750, time: 0.192, loss: 976.992554\n",
      "Train: step:  46760, time: 0.228, loss: 480.832520\n",
      "Train: step:  46770, time: 0.195, loss: 1483.027466\n",
      "Train: step:  46780, time: 0.198, loss: 1121.620972\n",
      "Train: step:  46790, time: 0.227, loss: 3474.767578\n",
      "Train: step:  46800, time: 0.207, loss: 2648.785400\n",
      "Train: step:  46810, time: 0.211, loss: 1403.107300\n",
      "Train: step:  46820, time: 0.185, loss: 1681.811523\n",
      "Train: step:  46830, time: 0.188, loss: 1233.879517\n",
      "Train: step:  46840, time: 0.241, loss: 1036.316895\n",
      "Train: step:  46850, time: 0.185, loss: 1317.424927\n",
      "Train: step:  46860, time: 0.185, loss: 1166.476929\n",
      "Train: step:  46870, time: 0.184, loss: 2560.986328\n",
      "Train: step:  46880, time: 0.213, loss: 521.511719\n",
      "Train: step:  46890, time: 0.212, loss: 2701.474854\n",
      "Train: step:  46900, time: 0.227, loss: 1008.971863\n",
      "Train: step:  46910, time: 0.218, loss: 1459.292603\n",
      "Train: step:  46920, time: 0.188, loss: 1142.126587\n",
      "Train: step:  46930, time: 0.190, loss: 2025.565674\n",
      "Train: step:  46940, time: 0.192, loss: 2132.485840\n",
      "Train: step:  46950, time: 0.215, loss: 1813.892822\n",
      "Train: step:  46960, time: 0.194, loss: 1255.081665\n",
      "Train: step:  46970, time: 0.191, loss: 1427.266968\n",
      "Train: step:  46980, time: 0.186, loss: 3790.733887\n",
      "Train: step:  46990, time: 0.189, loss: 1988.248779\n",
      "Train: step:  47000, time: 0.187, loss: 585.002014\n",
      "Train: step:  47010, time: 0.191, loss: 396.867645\n",
      "Train: step:  47020, time: 0.191, loss: 384.984406\n",
      "Train: step:  47030, time: 0.195, loss: 1322.979004\n",
      "Train: step:  47040, time: 0.186, loss: 1107.557495\n",
      "Train: step:  47050, time: 0.186, loss: 3302.590332\n",
      "Train: step:  47060, time: 0.196, loss: 2099.654297\n",
      "Train: step:  47070, time: 0.217, loss: 3289.690674\n",
      "Train: step:  47080, time: 0.217, loss: 2984.522461\n",
      "Train: step:  47090, time: 0.199, loss: 620.242981\n",
      "Train: step:  47100, time: 0.222, loss: 3014.681152\n",
      "Train: step:  47110, time: 0.191, loss: 2102.888916\n",
      "Train: step:  47120, time: 0.189, loss: 878.920654\n",
      "Train: step:  47130, time: 0.198, loss: 3432.471680\n",
      "Train: step:  47140, time: 0.189, loss: 2037.745361\n",
      "Train: step:  47150, time: 0.187, loss: 1205.604492\n",
      "Train: step:  47160, time: 0.249, loss: 696.064758\n",
      "Train: step:  47170, time: 0.211, loss: 3240.119873\n",
      "Train: step:  47180, time: 0.225, loss: 1178.215088\n",
      "Train: step:  47190, time: 0.185, loss: 380.671906\n",
      "Train: step:  47200, time: 0.195, loss: 1690.808472\n",
      "Train: step:  47210, time: 0.199, loss: 976.077332\n",
      "Train: step:  47220, time: 0.192, loss: 1753.399536\n",
      "Train: step:  47230, time: 0.216, loss: 1868.407715\n",
      "Train: step:  47240, time: 0.193, loss: 2425.186523\n",
      "Train: step:  47250, time: 0.188, loss: 2873.641602\n",
      "Train: step:  47260, time: 0.241, loss: 2203.829102\n",
      "Train: step:  47270, time: 0.185, loss: 2998.121826\n",
      "Train: step:  47280, time: 0.216, loss: 1889.483032\n",
      "Train: step:  47290, time: 0.185, loss: 1396.016357\n",
      "Train: step:  47300, time: 0.222, loss: 2371.770020\n",
      "Train: step:  47310, time: 0.187, loss: 1205.687866\n",
      "Train: step:  47320, time: 0.187, loss: 1486.461304\n",
      "Train: step:  47330, time: 0.227, loss: 2565.262207\n",
      "Train: step:  47340, time: 0.186, loss: 1668.597656\n",
      "Train: step:  47350, time: 0.225, loss: 4292.757324\n",
      "Train: step:  47360, time: 0.188, loss: 3255.166016\n",
      "Train: step:  47370, time: 0.190, loss: 612.532654\n",
      "Train: step:  47380, time: 0.193, loss: 2554.639648\n",
      "Train: step:  47390, time: 0.187, loss: 3230.885254\n",
      "Train: step:  47400, time: 0.185, loss: 442.653687\n",
      "Train: step:  47410, time: 0.236, loss: 1374.126099\n",
      "Train: step:  47420, time: 0.186, loss: 1926.143921\n",
      "Train: step:  47430, time: 0.217, loss: 2161.427490\n",
      "Train: step:  47440, time: 0.240, loss: 2445.786133\n",
      "Train: step:  47450, time: 0.192, loss: 2328.296143\n",
      "Train: step:  47460, time: 0.191, loss: 746.161499\n",
      "Train: step:  47470, time: 0.193, loss: 2356.666748\n",
      "Train: step:  47480, time: 0.181, loss: 3041.524658\n",
      "Train: step:  47490, time: 0.214, loss: 1342.540405\n",
      "Train: step:  47500, time: 0.245, loss: 2487.030762\n",
      "Train: step:  47510, time: 0.195, loss: 4109.026855\n",
      "Train: step:  47520, time: 0.204, loss: 951.305237\n",
      "Train: step:  47530, time: 0.251, loss: 1197.831665\n",
      "Train: step:  47540, time: 0.193, loss: 2324.900391\n",
      "Train: step:  47550, time: 0.190, loss: 1134.292114\n",
      "Train: step:  47560, time: 0.196, loss: 1805.684814\n",
      "Train: step:  47570, time: 0.190, loss: 351.912506\n",
      "Train: step:  47580, time: 0.230, loss: 1425.437988\n",
      "Train: step:  47590, time: 0.193, loss: 893.946899\n",
      "Train: step:  47600, time: 0.189, loss: 2609.060303\n",
      "Train: step:  47610, time: 0.192, loss: 1237.776489\n",
      "Train: step:  47620, time: 0.183, loss: 2167.874512\n",
      "Train: step:  47630, time: 0.217, loss: 6098.485352\n",
      "Train: step:  47640, time: 0.191, loss: 1214.821655\n",
      "Train: step:  47650, time: 0.192, loss: 1383.089844\n",
      "Train: step:  47660, time: 0.217, loss: 1378.643433\n",
      "Train: step:  47670, time: 0.199, loss: 2031.533447\n",
      "Train: step:  47680, time: 0.192, loss: 3328.188721\n",
      "Train: step:  47690, time: 0.214, loss: 1658.265015\n",
      "Train: step:  47700, time: 0.228, loss: 979.653748\n",
      "Train: step:  47710, time: 0.230, loss: 2767.913086\n",
      "Train: step:  47720, time: 0.246, loss: 2335.442139\n",
      "Train: step:  47730, time: 0.190, loss: 2388.830566\n",
      "Train: step:  47740, time: 0.229, loss: 2574.438965\n",
      "Train: step:  47750, time: 0.193, loss: 935.352783\n",
      "Train: step:  47760, time: 0.191, loss: 1704.403442\n",
      "Train: step:  47770, time: 0.201, loss: 2942.748291\n",
      "Train: step:  47780, time: 0.184, loss: 4503.059570\n",
      "Train: step:  47790, time: 0.221, loss: 2824.958984\n",
      "Train: step:  47800, time: 0.203, loss: 1752.313477\n",
      "Train: step:  47810, time: 0.217, loss: 1053.161865\n",
      "Train: step:  47820, time: 0.191, loss: 1336.391235\n",
      "Train: step:  47830, time: 0.184, loss: 205.764999\n",
      "Train: step:  47840, time: 0.232, loss: 1438.597778\n",
      "Train: step:  47850, time: 0.186, loss: 2860.765625\n",
      "Train: step:  47860, time: 0.194, loss: 1381.838745\n",
      "Train: step:  47870, time: 0.191, loss: 2765.862305\n",
      "Train: step:  47880, time: 0.190, loss: 3041.524414\n",
      "Train: step:  47890, time: 0.201, loss: 1081.196289\n",
      "Train: step:  47900, time: 0.192, loss: 1862.452026\n",
      "Train: step:  47910, time: 0.194, loss: 2371.439697\n",
      "Train: step:  47920, time: 0.217, loss: 2917.742432\n",
      "Train: step:  47930, time: 0.227, loss: 1729.367310\n",
      "Train: step:  47940, time: 0.188, loss: 2557.121582\n",
      "Train: step:  47950, time: 0.196, loss: 2282.353760\n",
      "Train: step:  47960, time: 0.187, loss: 1687.812500\n",
      "Train: step:  47970, time: 0.217, loss: 1177.471802\n",
      "Train: step:  47980, time: 0.187, loss: 2421.084717\n",
      "Train: step:  47990, time: 0.192, loss: 1622.031494\n",
      "Train: step:  48000, time: 0.193, loss: 1803.433716\n",
      "Train: step:  48010, time: 0.217, loss: 1381.045776\n",
      "Train: step:  48020, time: 0.186, loss: 2311.393311\n",
      "Train: step:  48030, time: 0.197, loss: 1025.445312\n",
      "Train: step:  48040, time: 0.250, loss: 1651.483887\n",
      "Train: step:  48050, time: 0.191, loss: 2811.008057\n",
      "Train: step:  48060, time: 0.194, loss: 1726.962646\n",
      "Train: step:  48070, time: 0.193, loss: 2612.306885\n",
      "Train: step:  48080, time: 0.187, loss: 3852.822754\n",
      "Train: step:  48090, time: 0.197, loss: 3407.802979\n",
      "Train: step:  48100, time: 0.194, loss: 1132.832642\n",
      "Train: step:  48110, time: 0.185, loss: 2444.336670\n",
      "Train: step:  48120, time: 0.227, loss: 301.689209\n",
      "Train: step:  48130, time: 0.235, loss: 1119.733887\n",
      "Train: step:  48140, time: 0.192, loss: 1276.885620\n",
      "Train: step:  48150, time: 0.184, loss: 3246.696289\n",
      "Train: step:  48160, time: 0.188, loss: 1911.726318\n",
      "Train: step:  48170, time: 0.191, loss: 1371.942871\n",
      "Train: step:  48180, time: 0.184, loss: 2186.629639\n",
      "Train: step:  48190, time: 0.192, loss: 638.185059\n",
      "Train: step:  48200, time: 0.230, loss: 1996.444458\n",
      "Train: step:  48210, time: 0.202, loss: 3568.298340\n",
      "Train: step:  48220, time: 0.187, loss: 409.562866\n",
      "Train: step:  48230, time: 0.191, loss: 1660.766968\n",
      "Train: step:  48240, time: 0.191, loss: 2158.325439\n",
      "Train: step:  48250, time: 0.186, loss: 2632.021484\n",
      "Train: step:  48260, time: 0.190, loss: 554.898499\n",
      "Train: step:  48270, time: 0.197, loss: 1998.843506\n",
      "Train: step:  48280, time: 0.202, loss: 3087.475830\n",
      "Train: step:  48290, time: 0.186, loss: 3347.572998\n",
      "Train: step:  48300, time: 0.193, loss: 1882.669922\n",
      "Train: step:  48310, time: 0.188, loss: 1746.332397\n",
      "Train: step:  48320, time: 0.186, loss: 2424.459473\n",
      "Train: step:  48330, time: 0.197, loss: 1229.113403\n",
      "Train: step:  48340, time: 0.197, loss: 549.018127\n",
      "Train: step:  48350, time: 0.199, loss: 942.749268\n",
      "Train: step:  48360, time: 0.253, loss: 639.772156\n",
      "Train: step:  48370, time: 0.219, loss: 3127.368408\n",
      "Train: step:  48380, time: 0.216, loss: 1871.511963\n",
      "Train: step:  48390, time: 0.194, loss: 2625.101318\n",
      "Train: step:  48400, time: 0.195, loss: 2139.073730\n",
      "Train: step:  48410, time: 0.247, loss: 1364.533936\n",
      "Train: step:  48420, time: 0.235, loss: 2979.074707\n",
      "Train: step:  48430, time: 0.195, loss: 960.415833\n",
      "Train: step:  48440, time: 0.191, loss: 2723.511719\n",
      "Train: step:  48450, time: 0.194, loss: 322.042236\n",
      "Train: step:  48460, time: 0.187, loss: 1763.877197\n",
      "Train: step:  48470, time: 0.253, loss: 2450.772949\n",
      "Train: step:  48480, time: 0.190, loss: 956.291992\n",
      "Train: step:  48490, time: 0.229, loss: 3083.690186\n",
      "Train: step:  48500, time: 0.195, loss: 1724.028320\n",
      "Train: step:  48510, time: 0.202, loss: 3784.104004\n",
      "Train: step:  48520, time: 0.189, loss: 3837.034912\n",
      "Train: step:  48530, time: 0.189, loss: 1239.194214\n",
      "Train: step:  48540, time: 0.188, loss: 742.137817\n",
      "Train: step:  48550, time: 0.190, loss: 925.044617\n",
      "Train: step:  48560, time: 0.189, loss: 3144.852295\n",
      "Train: step:  48570, time: 0.229, loss: 3800.884277\n",
      "Train: step:  48580, time: 0.221, loss: 1068.670410\n",
      "Train: step:  48590, time: 0.186, loss: 1607.312378\n",
      "Train: step:  48600, time: 0.190, loss: 3835.811279\n",
      "Train: step:  48610, time: 0.212, loss: 1869.845459\n",
      "Train: step:  48620, time: 0.229, loss: 2532.152344\n",
      "Train: step:  48630, time: 0.183, loss: 1752.825806\n",
      "Train: step:  48640, time: 0.193, loss: 325.069519\n",
      "Train: step:  48650, time: 0.187, loss: 1586.511108\n",
      "Train: step:  48660, time: 0.188, loss: 764.886169\n",
      "Train: step:  48670, time: 0.216, loss: 902.371338\n",
      "Train: step:  48680, time: 0.217, loss: 1209.263306\n",
      "Train: step:  48690, time: 0.218, loss: 1789.082764\n",
      "Train: step:  48700, time: 0.187, loss: 2825.305420\n",
      "Train: step:  48710, time: 0.247, loss: 936.514465\n",
      "Train: step:  48720, time: 0.216, loss: 2095.769287\n",
      "Train: step:  48730, time: 0.222, loss: 669.384827\n",
      "Train: step:  48740, time: 0.199, loss: 1289.466553\n",
      "Train: step:  48750, time: 0.191, loss: 1825.517944\n",
      "Train: step:  48760, time: 0.185, loss: 3215.240967\n",
      "Train: step:  48770, time: 0.189, loss: 3838.653809\n",
      "Train: step:  48780, time: 0.199, loss: 183.254562\n",
      "Train: step:  48790, time: 0.194, loss: 4067.033203\n",
      "Train: step:  48800, time: 0.192, loss: 1126.386963\n",
      "Train: step:  48810, time: 0.189, loss: 1315.446533\n",
      "Train: step:  48820, time: 0.189, loss: 1426.093506\n",
      "Train: step:  48830, time: 0.187, loss: 2930.087891\n",
      "Train: step:  48840, time: 0.218, loss: 2562.147217\n",
      "Train: step:  48850, time: 0.226, loss: 1138.032715\n",
      "Train: step:  48860, time: 0.195, loss: 2621.810303\n",
      "Train: step:  48870, time: 0.197, loss: 737.132324\n",
      "Train: step:  48880, time: 0.228, loss: 2282.112061\n",
      "Train: step:  48890, time: 0.223, loss: 2089.976562\n",
      "Train: step:  48900, time: 0.221, loss: 2710.711182\n",
      "Train: step:  48910, time: 0.193, loss: 1909.157104\n",
      "Train: step:  48920, time: 0.201, loss: 1772.328491\n",
      "Train: step:  48930, time: 0.195, loss: 1482.105103\n",
      "Train: step:  48940, time: 0.194, loss: 2529.452881\n",
      "Train: step:  48950, time: 0.194, loss: 2791.561768\n",
      "Train: step:  48960, time: 0.181, loss: 2174.066650\n",
      "Train: step:  48970, time: 0.193, loss: 1594.304321\n",
      "Train: step:  48980, time: 0.190, loss: 2431.735352\n",
      "Train: step:  48990, time: 0.236, loss: 3031.105469\n",
      "Train: step:  49000, time: 0.184, loss: 1625.349121\n",
      "Train: step:  49010, time: 0.186, loss: 2263.891113\n",
      "Train: step:  49020, time: 0.186, loss: 3907.837646\n",
      "Train: step:  49030, time: 0.214, loss: 543.434021\n",
      "Train: step:  49040, time: 0.192, loss: 3295.054688\n",
      "Train: step:  49050, time: 0.219, loss: 1554.210083\n",
      "Train: step:  49060, time: 0.215, loss: 1222.690308\n",
      "Train: step:  49070, time: 0.190, loss: 1814.749268\n",
      "Train: step:  49080, time: 0.229, loss: 1479.358643\n",
      "Train: step:  49090, time: 0.190, loss: 3239.862549\n",
      "Train: step:  49100, time: 0.194, loss: 1337.124023\n",
      "Train: step:  49110, time: 0.191, loss: 942.921204\n",
      "Train: step:  49120, time: 0.232, loss: 2817.862305\n",
      "Train: step:  49130, time: 0.223, loss: 3504.654053\n",
      "Train: step:  49140, time: 0.186, loss: 3018.431885\n",
      "Train: step:  49150, time: 0.190, loss: 1468.494385\n",
      "Train: step:  49160, time: 0.212, loss: 1617.295044\n",
      "Train: step:  49170, time: 0.187, loss: 3559.131836\n",
      "Train: step:  49180, time: 0.191, loss: 1391.369995\n",
      "Train: step:  49190, time: 0.203, loss: 3478.253418\n",
      "Train: step:  49200, time: 0.185, loss: 362.945435\n",
      "Train: step:  49210, time: 0.220, loss: 1684.231323\n",
      "Train: step:  49220, time: 0.187, loss: 3260.789062\n",
      "Train: step:  49230, time: 0.185, loss: 2149.169678\n",
      "Train: step:  49240, time: 0.193, loss: 2109.635742\n",
      "Train: step:  49250, time: 0.193, loss: 1713.877808\n",
      "Train: step:  49260, time: 0.191, loss: 506.960938\n",
      "Train: step:  49270, time: 0.203, loss: 5133.155762\n",
      "Train: step:  49280, time: 0.187, loss: 738.201111\n",
      "Train: step:  49290, time: 0.198, loss: 772.368286\n",
      "Train: step:  49300, time: 0.202, loss: 1562.248657\n",
      "Train: step:  49310, time: 0.187, loss: 1663.122681\n",
      "Train: step:  49320, time: 0.227, loss: 3623.526855\n",
      "Train: step:  49330, time: 0.218, loss: 689.834839\n",
      "Train: step:  49340, time: 0.193, loss: 1368.147095\n",
      "Train: step:  49350, time: 0.232, loss: 2286.199219\n",
      "Train: step:  49360, time: 0.217, loss: 2810.532715\n",
      "Train: step:  49370, time: 0.200, loss: 1933.972290\n",
      "Train: step:  49380, time: 0.236, loss: 2156.707275\n",
      "Train: step:  49390, time: 0.187, loss: 430.844757\n",
      "Train: step:  49400, time: 0.188, loss: 715.990967\n",
      "Train: step:  49410, time: 0.185, loss: 1038.574097\n",
      "Train: step:  49420, time: 0.191, loss: 629.937683\n",
      "Train: step:  49430, time: 0.222, loss: 2822.846680\n",
      "Train: step:  49440, time: 0.184, loss: 1655.885864\n",
      "Train: step:  49450, time: 0.184, loss: 2282.596680\n",
      "Train: step:  49460, time: 0.208, loss: 3199.295654\n",
      "Train: step:  49470, time: 0.230, loss: 1677.386108\n",
      "Train: step:  49480, time: 0.192, loss: 2208.646240\n",
      "Train: step:  49490, time: 0.229, loss: 1725.274170\n",
      "Train: step:  49500, time: 0.195, loss: 2318.509521\n",
      "Train: step:  49510, time: 0.195, loss: 2809.336670\n",
      "Train: step:  49520, time: 0.198, loss: 1851.320557\n",
      "Train: step:  49530, time: 0.189, loss: 419.245758\n",
      "Train: step:  49540, time: 0.247, loss: 2814.047852\n",
      "Train: step:  49550, time: 0.191, loss: 1382.094604\n",
      "Train: step:  49560, time: 0.249, loss: 973.507629\n",
      "Train: step:  49570, time: 0.194, loss: 2599.683594\n",
      "Train: step:  49580, time: 0.186, loss: 2685.472168\n",
      "Train: step:  49590, time: 0.204, loss: 1908.494995\n",
      "Train: step:  49600, time: 0.186, loss: 1975.436646\n",
      "Train: step:  49610, time: 0.201, loss: 462.598969\n",
      "Train: step:  49620, time: 0.218, loss: 2067.378174\n",
      "Train: step:  49630, time: 0.241, loss: 2827.973633\n",
      "Train: step:  49640, time: 0.230, loss: 2698.388184\n",
      "Train: step:  49650, time: 0.187, loss: 3078.947021\n",
      "Train: step:  49660, time: 0.232, loss: 1660.645874\n",
      "Train: step:  49670, time: 0.231, loss: 1355.004150\n",
      "Train: step:  49680, time: 0.186, loss: 2099.405762\n",
      "Train: step:  49690, time: 0.193, loss: 783.232666\n",
      "Train: step:  49700, time: 0.194, loss: 2748.942871\n",
      "Train: step:  49710, time: 0.192, loss: 721.680481\n",
      "Train: step:  49720, time: 0.195, loss: 3596.433350\n",
      "Train: step:  49730, time: 0.191, loss: 1454.884033\n",
      "Train: step:  49740, time: 0.195, loss: 1328.615479\n",
      "Train: step:  49750, time: 0.198, loss: 353.728577\n",
      "Train: step:  49760, time: 0.186, loss: 797.295959\n",
      "Train: step:  49770, time: 0.191, loss: 2792.177490\n",
      "Train: step:  49780, time: 0.230, loss: 1366.910522\n",
      "Train: step:  49790, time: 0.213, loss: 929.248962\n",
      "Train: step:  49800, time: 0.191, loss: 2616.097656\n",
      "Train: step:  49810, time: 0.189, loss: 788.906189\n",
      "Train: step:  49820, time: 0.188, loss: 1148.831787\n",
      "Train: step:  49830, time: 0.187, loss: 1488.164185\n",
      "Train: step:  49840, time: 0.189, loss: 1328.053711\n",
      "Train: step:  49850, time: 0.201, loss: 2675.423828\n",
      "Train: step:  49860, time: 0.186, loss: 1979.210693\n",
      "Train: step:  49870, time: 0.229, loss: 3610.396973\n",
      "Train: step:  49880, time: 0.203, loss: 2537.379883\n",
      "Train: step:  49890, time: 0.197, loss: 2132.802246\n",
      "Train: step:  49900, time: 0.249, loss: 1019.255249\n",
      "Train: step:  49910, time: 0.224, loss: 2049.366943\n",
      "Train: step:  49920, time: 0.196, loss: 2073.618896\n",
      "Train: step:  49930, time: 0.187, loss: 1485.292725\n",
      "Train: step:  49940, time: 0.183, loss: 1809.280273\n",
      "Train: step:  49950, time: 0.217, loss: 1053.674438\n",
      "Train: step:  49960, time: 0.232, loss: 647.607117\n",
      "Train: step:  49970, time: 0.223, loss: 2502.794434\n",
      "Train: step:  49980, time: 0.192, loss: 1730.557495\n",
      "Train: step:  49990, time: 0.195, loss: 402.775330\n",
      "Train: step:  50000, time: 0.189, loss: 1124.153198\n",
      "Train: step:  50010, time: 0.195, loss: 1244.123657\n",
      "Train: step:  50020, time: 0.228, loss: 2015.531982\n",
      "Train: step:  50030, time: 0.191, loss: 3005.104980\n",
      "Train: step:  50040, time: 0.193, loss: 698.959534\n",
      "Train: step:  50050, time: 0.206, loss: 1730.540161\n",
      "Train: step:  50060, time: 0.199, loss: 738.102478\n",
      "Train: step:  50070, time: 0.189, loss: 1371.179077\n",
      "Train: step:  50080, time: 0.205, loss: 4837.938477\n",
      "Train: step:  50090, time: 0.187, loss: 2495.300049\n",
      "Train: step:  50100, time: 0.185, loss: 787.185120\n",
      "Train: step:  50110, time: 0.189, loss: 1224.868896\n",
      "Train: step:  50120, time: 0.185, loss: 1418.138428\n",
      "Train: step:  50130, time: 0.188, loss: 2824.185791\n",
      "Train: step:  50140, time: 0.191, loss: 1087.856689\n",
      "Train: step:  50150, time: 0.188, loss: 1131.927979\n",
      "Train: step:  50160, time: 0.186, loss: 1571.849854\n",
      "Train: step:  50170, time: 0.199, loss: 2573.769043\n",
      "Train: step:  50180, time: 0.198, loss: 1501.515137\n",
      "Train: step:  50190, time: 0.216, loss: 993.036987\n",
      "Train: step:  50200, time: 0.231, loss: 755.299622\n",
      "Train: step:  50210, time: 0.193, loss: 1148.714478\n",
      "Train: step:  50220, time: 0.185, loss: 2811.851807\n",
      "Train: step:  50230, time: 0.233, loss: 2076.595215\n",
      "Train: step:  50240, time: 0.189, loss: 1247.965942\n",
      "Train: step:  50250, time: 0.185, loss: 3290.705566\n",
      "Train: step:  50260, time: 0.231, loss: 1747.153564\n",
      "Train: step:  50270, time: 0.248, loss: 2782.305664\n",
      "Train: step:  50280, time: 0.229, loss: 1645.113159\n",
      "Train: step:  50290, time: 0.216, loss: 1535.500854\n",
      "Train: step:  50300, time: 0.200, loss: 1912.101685\n",
      "Train: step:  50310, time: 0.193, loss: 1173.448608\n",
      "Train: step:  50320, time: 0.216, loss: 728.438904\n",
      "Train: step:  50330, time: 0.186, loss: 2367.058105\n",
      "Train: step:  50340, time: 0.223, loss: 659.180786\n",
      "Train: step:  50350, time: 0.193, loss: 724.565063\n",
      "Train: step:  50360, time: 0.194, loss: 955.052856\n",
      "Train: step:  50370, time: 0.188, loss: 2874.356689\n",
      "Train: step:  50380, time: 0.190, loss: 765.360657\n",
      "Train: step:  50390, time: 0.189, loss: 1399.776489\n",
      "Train: step:  50400, time: 0.228, loss: 1776.158569\n",
      "Train: step:  50410, time: 0.187, loss: 1013.754944\n",
      "Train: step:  50420, time: 0.192, loss: 1661.246460\n",
      "Train: step:  50430, time: 0.195, loss: 1268.439697\n",
      "Train: step:  50440, time: 0.203, loss: 1394.082397\n",
      "Train: step:  50450, time: 0.191, loss: 1341.839722\n",
      "Train: step:  50460, time: 0.186, loss: 873.288513\n",
      "Train: step:  50470, time: 0.185, loss: 1406.300537\n",
      "Train: step:  50480, time: 0.217, loss: 893.907898\n",
      "Train: step:  50490, time: 0.192, loss: 1909.033691\n",
      "Train: step:  50500, time: 0.182, loss: 1215.223755\n",
      "Train: step:  50510, time: 0.187, loss: 517.805664\n",
      "Train: step:  50520, time: 0.202, loss: 1812.018555\n",
      "Train: step:  50530, time: 0.198, loss: 4112.165527\n",
      "Train: step:  50540, time: 0.199, loss: 3381.847656\n",
      "Train: step:  50550, time: 0.188, loss: 3728.062256\n",
      "Train: step:  50560, time: 0.188, loss: 2195.573486\n",
      "Train: step:  50570, time: 0.190, loss: 1644.894409\n",
      "Train: step:  50580, time: 0.189, loss: 1096.727905\n",
      "Train: step:  50590, time: 0.190, loss: 1219.476318\n",
      "Train: step:  50600, time: 0.193, loss: 2227.239502\n",
      "Train: step:  50610, time: 0.194, loss: 2063.891113\n",
      "Train: step:  50620, time: 0.234, loss: 801.697876\n",
      "Train: step:  50630, time: 0.216, loss: 529.263794\n",
      "Train: step:  50640, time: 0.196, loss: 898.312561\n",
      "Train: step:  50650, time: 0.218, loss: 551.700562\n",
      "Train: step:  50660, time: 0.203, loss: 2750.675781\n",
      "Train: step:  50670, time: 0.188, loss: 1324.679810\n",
      "Train: step:  50680, time: 0.196, loss: 1112.099365\n",
      "Train: step:  50690, time: 0.218, loss: 2979.215332\n",
      "Train: step:  50700, time: 0.243, loss: 2880.602539\n",
      "Train: step:  50710, time: 0.184, loss: 870.769958\n",
      "Train: step:  50720, time: 0.217, loss: 1717.017090\n",
      "Train: step:  50730, time: 0.188, loss: 890.515015\n",
      "Train: step:  50740, time: 0.230, loss: 480.504608\n",
      "Train: step:  50750, time: 0.183, loss: 577.239014\n",
      "Train: step:  50760, time: 0.234, loss: 2370.780762\n",
      "Train: step:  50770, time: 0.217, loss: 834.183044\n",
      "Train: step:  50780, time: 0.192, loss: 1737.337280\n",
      "Train: step:  50790, time: 0.184, loss: 192.917953\n",
      "Train: step:  50800, time: 0.192, loss: 382.053680\n",
      "Train: step:  50810, time: 0.187, loss: 1482.899292\n",
      "Train: step:  50820, time: 0.192, loss: 850.138123\n",
      "Train: step:  50830, time: 0.187, loss: 1711.103271\n",
      "Train: step:  50840, time: 0.191, loss: 3681.024658\n",
      "Train: step:  50850, time: 0.190, loss: 2196.128418\n",
      "Train: step:  50860, time: 0.187, loss: 571.707397\n",
      "Train: step:  50870, time: 0.192, loss: 1349.330200\n",
      "Train: step:  50880, time: 0.225, loss: 1629.615112\n",
      "Train: step:  50890, time: 0.193, loss: 2221.790527\n",
      "Train: step:  50900, time: 0.217, loss: 1360.113770\n",
      "Train: step:  50910, time: 0.193, loss: 879.646484\n",
      "Train: step:  50920, time: 0.217, loss: 1288.424561\n",
      "Train: step:  50930, time: 0.185, loss: 1218.939453\n",
      "Train: step:  50940, time: 0.225, loss: 2645.379639\n",
      "Train: step:  50950, time: 0.195, loss: 2152.654541\n",
      "Train: step:  50960, time: 0.189, loss: 1041.348389\n",
      "Train: step:  50970, time: 0.222, loss: 956.312500\n",
      "Train: step:  50980, time: 0.195, loss: 4048.313477\n",
      "Train: step:  50990, time: 0.190, loss: 1918.366211\n",
      "Train: step:  51000, time: 0.253, loss: 1747.767090\n",
      "Train: step:  51010, time: 0.189, loss: 1797.512573\n",
      "Train: step:  51020, time: 0.245, loss: 448.424988\n",
      "Train: step:  51030, time: 0.195, loss: 1655.620728\n",
      "Train: step:  51040, time: 0.193, loss: 4365.265625\n",
      "Train: step:  51050, time: 0.194, loss: 1919.988281\n",
      "Train: step:  51060, time: 0.200, loss: 493.359650\n",
      "Train: step:  51070, time: 0.187, loss: 1479.755615\n",
      "Train: step:  51080, time: 0.217, loss: 494.667053\n",
      "Train: step:  51090, time: 0.190, loss: 2195.740723\n",
      "Train: step:  51100, time: 0.200, loss: 1918.249878\n",
      "Train: step:  51110, time: 0.188, loss: 1621.866455\n",
      "Train: step:  51120, time: 0.245, loss: 3387.817871\n",
      "Train: step:  51130, time: 0.191, loss: 1952.483765\n",
      "Train: step:  51140, time: 0.184, loss: 2034.045166\n",
      "Train: step:  51150, time: 0.191, loss: 1099.339233\n",
      "Train: step:  51160, time: 0.193, loss: 1797.553589\n",
      "Train: step:  51170, time: 0.191, loss: 2084.018066\n",
      "Train: step:  51180, time: 0.192, loss: 1672.552368\n",
      "Train: step:  51190, time: 0.198, loss: 500.621368\n",
      "Train: step:  51200, time: 0.246, loss: 1956.085693\n",
      "Train: step:  51210, time: 0.221, loss: 1336.594971\n",
      "Train: step:  51220, time: 0.187, loss: 2165.282715\n",
      "Train: step:  51230, time: 0.184, loss: 2307.986572\n",
      "Train: step:  51240, time: 0.184, loss: 1679.703735\n",
      "Train: step:  51250, time: 0.194, loss: 2527.821045\n",
      "Train: step:  51260, time: 0.185, loss: 2626.568115\n",
      "Train: step:  51270, time: 0.230, loss: 1529.493164\n",
      "Train: step:  51280, time: 0.186, loss: 1889.805054\n",
      "Train: step:  51290, time: 0.242, loss: 1404.625000\n",
      "Train: step:  51300, time: 0.225, loss: 500.961884\n",
      "Train: step:  51310, time: 0.192, loss: 2767.281250\n",
      "Train: step:  51320, time: 0.218, loss: 1124.264404\n",
      "Train: step:  51330, time: 0.194, loss: 836.585022\n",
      "Train: step:  51340, time: 0.241, loss: 417.007111\n",
      "Train: step:  51350, time: 0.227, loss: 2033.948364\n",
      "Train: step:  51360, time: 0.191, loss: 1991.313477\n",
      "Train: step:  51370, time: 0.224, loss: 1375.958496\n",
      "Train: step:  51380, time: 0.217, loss: 2236.706055\n",
      "Train: step:  51390, time: 0.191, loss: 3406.853516\n",
      "Train: step:  51400, time: 0.187, loss: 1119.928833\n",
      "Train: step:  51410, time: 0.215, loss: 2913.091309\n",
      "Train: step:  51420, time: 0.190, loss: 1689.697021\n",
      "Train: step:  51430, time: 0.193, loss: 1405.075562\n",
      "Train: step:  51440, time: 0.192, loss: 1285.418213\n",
      "Train: step:  51450, time: 0.191, loss: 3834.790527\n",
      "Train: step:  51460, time: 0.233, loss: 2803.102051\n",
      "Train: step:  51470, time: 0.190, loss: 2196.495117\n",
      "Train: step:  51480, time: 0.185, loss: 1362.303711\n",
      "Train: step:  51490, time: 0.229, loss: 1417.597778\n",
      "Train: step:  51500, time: 0.192, loss: 2388.985107\n",
      "Train: step:  51510, time: 0.227, loss: 977.107239\n",
      "Train: step:  51520, time: 0.193, loss: 3471.024658\n",
      "Train: step:  51530, time: 0.215, loss: 3056.060059\n",
      "Train: step:  51540, time: 0.231, loss: 529.071167\n",
      "Train: step:  51550, time: 0.196, loss: 1513.555542\n",
      "Train: step:  51560, time: 0.195, loss: 197.328537\n",
      "Train: step:  51570, time: 0.180, loss: 1467.078369\n",
      "Train: step:  51580, time: 0.189, loss: 822.181396\n",
      "Train: step:  51590, time: 0.196, loss: 4262.244629\n",
      "Train: step:  51600, time: 0.194, loss: 619.482727\n",
      "Train: step:  51610, time: 0.217, loss: 990.281372\n",
      "Train: step:  51620, time: 0.193, loss: 2496.964600\n",
      "Train: step:  51630, time: 0.193, loss: 1334.953125\n",
      "Train: step:  51640, time: 0.195, loss: 1074.246582\n",
      "Train: step:  51650, time: 0.189, loss: 811.853333\n",
      "Train: step:  51660, time: 0.196, loss: 996.014038\n",
      "Train: step:  51670, time: 0.227, loss: 2402.420166\n",
      "Train: step:  51680, time: 0.225, loss: 1219.507080\n",
      "Train: step:  51690, time: 0.186, loss: 2421.322510\n",
      "Train: step:  51700, time: 0.196, loss: 2069.568604\n",
      "Train: step:  51710, time: 0.186, loss: 1056.713379\n",
      "Train: step:  51720, time: 0.237, loss: 3494.157227\n",
      "Train: step:  51730, time: 0.188, loss: 1800.752808\n",
      "Train: step:  51740, time: 0.190, loss: 3379.127441\n",
      "Train: step:  51750, time: 0.191, loss: 490.161011\n",
      "Train: step:  51760, time: 0.227, loss: 3066.740234\n",
      "Train: step:  51770, time: 0.193, loss: 2331.821289\n",
      "Train: step:  51780, time: 0.187, loss: 1840.947388\n",
      "Train: step:  51790, time: 0.217, loss: 749.183289\n",
      "Train: step:  51800, time: 0.197, loss: 1056.141968\n",
      "Train: step:  51810, time: 0.217, loss: 2669.699707\n",
      "Train: step:  51820, time: 0.196, loss: 2982.211182\n",
      "Train: step:  51830, time: 0.190, loss: 2229.771729\n",
      "Train: step:  51840, time: 0.196, loss: 1821.259399\n",
      "Train: step:  51850, time: 0.223, loss: 1274.331665\n",
      "Train: step:  51860, time: 0.193, loss: 1458.172119\n",
      "Train: step:  51870, time: 0.195, loss: 3178.248047\n",
      "Train: step:  51880, time: 0.193, loss: 1561.984985\n",
      "Train: step:  51890, time: 0.191, loss: 2307.999756\n",
      "Train: step:  51900, time: 0.189, loss: 2053.018799\n",
      "Train: step:  51910, time: 0.198, loss: 1525.996582\n",
      "Train: step:  51920, time: 0.217, loss: 1480.840332\n",
      "Train: step:  51930, time: 0.193, loss: 2294.078369\n",
      "Train: step:  51940, time: 0.244, loss: 1412.231323\n",
      "Train: step:  51950, time: 0.217, loss: 3376.864990\n",
      "Train: step:  51960, time: 0.233, loss: 1026.273804\n",
      "Train: step:  51970, time: 0.228, loss: 1788.576416\n",
      "Train: step:  51980, time: 0.186, loss: 1449.102905\n",
      "Train: step:  51990, time: 0.193, loss: 2299.274658\n",
      "Train: step:  52000, time: 0.227, loss: 2866.796875\n",
      "Train: step:  52010, time: 0.202, loss: 868.638611\n",
      "Train: step:  52020, time: 0.223, loss: 2179.096191\n",
      "Train: step:  52030, time: 0.188, loss: 2506.384766\n",
      "Train: step:  52040, time: 0.200, loss: 3307.789551\n",
      "Train: step:  52050, time: 0.235, loss: 1063.646362\n",
      "Train: step:  52060, time: 0.189, loss: 962.863098\n",
      "Train: step:  52070, time: 0.197, loss: 1970.303955\n",
      "Train: step:  52080, time: 0.198, loss: 1773.930298\n",
      "Train: step:  52090, time: 0.184, loss: 2266.275391\n",
      "Train: step:  52100, time: 0.185, loss: 1105.093506\n",
      "Train: step:  52110, time: 0.196, loss: 1484.116211\n",
      "Train: step:  52120, time: 0.190, loss: 912.777893\n",
      "Train: step:  52130, time: 0.190, loss: 1760.059448\n",
      "Train: step:  52140, time: 0.215, loss: 920.722717\n",
      "Train: step:  52150, time: 0.187, loss: 1973.144043\n",
      "Train: step:  52160, time: 0.184, loss: 537.992676\n",
      "Train: step:  52170, time: 0.227, loss: 3623.673584\n",
      "Train: step:  52180, time: 0.190, loss: 883.115356\n",
      "Train: step:  52190, time: 0.216, loss: 1628.241943\n",
      "Train: step:  52200, time: 0.190, loss: 1431.740112\n",
      "Train: step:  52210, time: 0.191, loss: 1774.924805\n",
      "Train: step:  52220, time: 0.201, loss: 2310.971191\n",
      "Train: step:  52230, time: 0.193, loss: 3004.379883\n",
      "Train: step:  52240, time: 0.187, loss: 1736.144043\n",
      "Train: step:  52250, time: 0.222, loss: 2573.759033\n",
      "Train: step:  52260, time: 0.191, loss: 3214.300537\n",
      "Train: step:  52270, time: 0.196, loss: 421.365112\n",
      "Train: step:  52280, time: 0.194, loss: 2783.639404\n",
      "Train: step:  52290, time: 0.185, loss: 2602.632324\n",
      "Train: step:  52300, time: 0.194, loss: 992.905945\n",
      "Train: step:  52310, time: 0.193, loss: 3348.192871\n",
      "Train: step:  52320, time: 0.196, loss: 2122.402100\n",
      "Train: step:  52330, time: 0.225, loss: 923.606384\n",
      "Train: step:  52340, time: 0.195, loss: 2527.751465\n",
      "Train: step:  52350, time: 0.196, loss: 1386.127197\n",
      "Train: step:  52360, time: 0.185, loss: 1094.534546\n",
      "Train: step:  52370, time: 0.190, loss: 665.343628\n",
      "Train: step:  52380, time: 0.188, loss: 2118.318115\n",
      "Train: step:  52390, time: 0.190, loss: 2387.827881\n",
      "Train: step:  52400, time: 0.225, loss: 2678.739014\n",
      "Train: step:  52410, time: 0.190, loss: 3185.464600\n",
      "Train: step:  52420, time: 0.188, loss: 2886.159912\n",
      "Train: step:  52430, time: 0.192, loss: 2901.735840\n",
      "Train: step:  52440, time: 0.228, loss: 1718.121826\n",
      "Train: step:  52450, time: 0.230, loss: 2922.495605\n",
      "Train: step:  52460, time: 0.210, loss: 2034.766846\n",
      "Train: step:  52470, time: 0.189, loss: 2168.950195\n",
      "Train: step:  52480, time: 0.199, loss: 1723.612305\n",
      "Train: step:  52490, time: 0.187, loss: 3727.229736\n",
      "Train: step:  52500, time: 0.195, loss: 3995.358887\n",
      "Train: step:  52510, time: 0.190, loss: 3487.512939\n",
      "Train: step:  52520, time: 0.216, loss: 383.401459\n",
      "Train: step:  52530, time: 0.226, loss: 2746.187500\n",
      "Train: step:  52540, time: 0.257, loss: 1557.792725\n",
      "Train: step:  52550, time: 0.186, loss: 2326.663574\n",
      "Train: step:  52560, time: 0.217, loss: 2116.540039\n",
      "Train: step:  52570, time: 0.218, loss: 1270.125000\n",
      "Train: step:  52580, time: 0.206, loss: 2709.973389\n",
      "Train: step:  52590, time: 0.190, loss: 3050.684082\n",
      "Train: step:  52600, time: 0.188, loss: 3492.192139\n",
      "Train: step:  52610, time: 0.186, loss: 528.597839\n",
      "Train: step:  52620, time: 0.203, loss: 1188.113403\n",
      "Train: step:  52630, time: 0.192, loss: 1437.852661\n",
      "Train: step:  52640, time: 0.196, loss: 671.846375\n",
      "Train: step:  52650, time: 0.190, loss: 1274.451294\n",
      "Train: step:  52660, time: 0.190, loss: 1633.161499\n",
      "Train: step:  52670, time: 0.186, loss: 1629.202515\n",
      "Train: step:  52680, time: 0.191, loss: 3504.371582\n",
      "Train: step:  52690, time: 0.186, loss: 1604.985962\n",
      "Train: step:  52700, time: 0.196, loss: 4248.446289\n",
      "Train: step:  52710, time: 0.190, loss: 2373.356201\n",
      "Train: step:  52720, time: 0.194, loss: 874.293518\n",
      "Train: step:  52730, time: 0.191, loss: 2062.745850\n",
      "Train: step:  52740, time: 0.181, loss: 3500.174561\n",
      "Train: step:  52750, time: 0.214, loss: 509.552490\n",
      "Train: step:  52760, time: 0.188, loss: 2143.102783\n",
      "Train: step:  52770, time: 0.250, loss: 1521.124878\n",
      "Train: step:  52780, time: 0.189, loss: 2981.766846\n",
      "Train: step:  52790, time: 0.190, loss: 2046.966431\n",
      "Train: step:  52800, time: 0.193, loss: 1354.093506\n",
      "Train: step:  52810, time: 0.195, loss: 1150.805542\n",
      "Train: step:  52820, time: 0.232, loss: 2357.284180\n",
      "Train: step:  52830, time: 0.188, loss: 1287.801147\n",
      "Train: step:  52840, time: 0.248, loss: 2872.252930\n",
      "Train: step:  52850, time: 0.227, loss: 1283.078857\n",
      "Train: step:  52860, time: 0.230, loss: 1599.976807\n",
      "Train: step:  52870, time: 0.211, loss: 1003.743164\n",
      "Train: step:  52880, time: 0.217, loss: 3506.778076\n",
      "Train: step:  52890, time: 0.200, loss: 2186.607910\n",
      "Train: step:  52900, time: 0.228, loss: 1744.691772\n",
      "Train: step:  52910, time: 0.227, loss: 1197.309326\n",
      "Train: step:  52920, time: 0.261, loss: 2483.623535\n",
      "Train: step:  52930, time: 0.226, loss: 3254.948975\n",
      "Train: step:  52940, time: 0.183, loss: 2280.812256\n",
      "Train: step:  52950, time: 0.217, loss: 3496.634277\n",
      "Train: step:  52960, time: 0.215, loss: 567.515076\n",
      "Train: step:  52970, time: 0.192, loss: 2273.401855\n",
      "Train: step:  52980, time: 0.206, loss: 4259.278320\n",
      "Train: step:  52990, time: 0.195, loss: 367.434570\n",
      "Train: step:  53000, time: 0.185, loss: 2815.293701\n",
      "Train: step:  53010, time: 0.190, loss: 2339.056641\n",
      "Train: step:  53020, time: 0.226, loss: 850.455811\n",
      "Train: step:  53030, time: 0.188, loss: 2011.730103\n",
      "Train: step:  53040, time: 0.184, loss: 2470.471680\n",
      "Train: step:  53050, time: 0.225, loss: 3987.014160\n",
      "Train: step:  53060, time: 0.188, loss: 390.059509\n",
      "Train: step:  53070, time: 0.187, loss: 2451.172852\n",
      "Train: step:  53080, time: 0.228, loss: 1467.278442\n",
      "Train: step:  53090, time: 0.188, loss: 470.633453\n",
      "Train: step:  53100, time: 0.188, loss: 852.365356\n",
      "Train: step:  53110, time: 0.192, loss: 2029.944336\n",
      "Train: step:  53120, time: 0.196, loss: 951.871033\n",
      "Train: step:  53130, time: 0.195, loss: 2660.288086\n",
      "Train: step:  53140, time: 0.209, loss: 1192.706543\n",
      "Train: step:  53150, time: 0.229, loss: 3626.544189\n",
      "Train: step:  53160, time: 0.190, loss: 784.205627\n",
      "Train: step:  53170, time: 0.217, loss: 1283.643188\n",
      "Train: step:  53180, time: 0.219, loss: 1321.198242\n",
      "Train: step:  53190, time: 0.191, loss: 769.438171\n",
      "Train: step:  53200, time: 0.217, loss: 2789.837402\n",
      "Train: step:  53210, time: 0.190, loss: 1435.909180\n",
      "Train: step:  53220, time: 0.199, loss: 1435.009766\n",
      "Train: step:  53230, time: 0.219, loss: 993.137146\n",
      "Train: step:  53240, time: 0.218, loss: 2746.632324\n",
      "Train: step:  53250, time: 0.192, loss: 946.212708\n",
      "Train: step:  53260, time: 0.191, loss: 1033.513062\n",
      "Train: step:  53270, time: 0.195, loss: 2161.060059\n",
      "Train: step:  53280, time: 0.192, loss: 1951.434692\n",
      "Train: step:  53290, time: 0.192, loss: 2346.239746\n",
      "Train: step:  53300, time: 0.192, loss: 1616.369385\n",
      "Train: step:  53310, time: 0.217, loss: 1182.668335\n",
      "Train: step:  53320, time: 0.187, loss: 3057.938232\n",
      "Train: step:  53330, time: 0.224, loss: 2903.389648\n",
      "Train: step:  53340, time: 0.191, loss: 875.646362\n",
      "Train: step:  53350, time: 0.186, loss: 1118.132324\n",
      "Train: step:  53360, time: 0.204, loss: 2389.826172\n",
      "Train: step:  53370, time: 0.188, loss: 1621.051758\n",
      "Train: step:  53380, time: 0.189, loss: 3140.943359\n",
      "Train: step:  53390, time: 0.187, loss: 2515.130615\n",
      "Train: step:  53400, time: 0.215, loss: 2409.622803\n",
      "Train: step:  53410, time: 0.194, loss: 2410.137695\n",
      "Train: step:  53420, time: 0.196, loss: 1838.305664\n",
      "Train: step:  53430, time: 0.197, loss: 1865.131470\n",
      "Train: step:  53440, time: 0.192, loss: 2153.067383\n",
      "Train: step:  53450, time: 0.189, loss: 682.521851\n",
      "Train: step:  53460, time: 0.190, loss: 2519.851807\n",
      "Train: step:  53470, time: 0.195, loss: 4183.499512\n",
      "Train: step:  53480, time: 0.198, loss: 445.119354\n",
      "Train: step:  53490, time: 0.218, loss: 1292.434937\n",
      "Train: step:  53500, time: 0.217, loss: 788.462463\n",
      "Train: step:  53510, time: 0.203, loss: 967.439148\n",
      "Train: step:  53520, time: 0.191, loss: 1756.198486\n",
      "Train: step:  53530, time: 0.200, loss: 2547.221436\n",
      "Train: step:  53540, time: 0.194, loss: 1430.241821\n",
      "Train: step:  53550, time: 0.215, loss: 1781.266113\n",
      "Train: step:  53560, time: 0.187, loss: 2129.208740\n",
      "Train: step:  53570, time: 0.184, loss: 2012.280884\n",
      "Train: step:  53580, time: 0.197, loss: 1750.235474\n",
      "Train: step:  53590, time: 0.191, loss: 1904.461914\n",
      "Train: step:  53600, time: 0.216, loss: 1688.526367\n",
      "Train: step:  53610, time: 0.198, loss: 2178.572021\n",
      "Train: step:  53620, time: 0.198, loss: 394.638123\n",
      "Train: step:  53630, time: 0.187, loss: 1351.758057\n",
      "Train: step:  53640, time: 0.182, loss: 274.399414\n",
      "Train: step:  53650, time: 0.213, loss: 2055.622314\n",
      "Train: step:  53660, time: 0.223, loss: 808.291016\n",
      "Train: step:  53670, time: 0.194, loss: 1475.500122\n",
      "Train: step:  53680, time: 0.197, loss: 2228.062500\n",
      "Train: step:  53690, time: 0.190, loss: 3049.961914\n",
      "Train: step:  53700, time: 0.191, loss: 971.120117\n",
      "Train: step:  53710, time: 0.192, loss: 5158.301270\n",
      "Train: step:  53720, time: 0.192, loss: 1326.052368\n",
      "Train: step:  53730, time: 0.191, loss: 2116.283936\n",
      "Train: step:  53740, time: 0.192, loss: 1388.404541\n",
      "Train: step:  53750, time: 0.240, loss: 2457.487793\n",
      "Train: step:  53760, time: 0.188, loss: 1448.620972\n",
      "Train: step:  53770, time: 0.239, loss: 355.831390\n",
      "Train: step:  53780, time: 0.193, loss: 1867.987549\n",
      "Train: step:  53790, time: 0.188, loss: 2846.259277\n",
      "Train: step:  53800, time: 0.188, loss: 2397.520996\n",
      "Train: step:  53810, time: 0.205, loss: 1467.713257\n",
      "Train: step:  53820, time: 0.195, loss: 892.629150\n",
      "Train: step:  53830, time: 0.186, loss: 625.766602\n",
      "Train: step:  53840, time: 0.194, loss: 1166.526245\n",
      "Train: step:  53850, time: 0.204, loss: 2192.204834\n",
      "Train: step:  53860, time: 0.188, loss: 1528.623169\n",
      "Train: step:  53870, time: 0.189, loss: 1689.174194\n",
      "Train: step:  53880, time: 0.191, loss: 663.369080\n",
      "Train: step:  53890, time: 0.220, loss: 775.277954\n",
      "Train: step:  53900, time: 0.195, loss: 1560.536499\n",
      "Train: step:  53910, time: 0.193, loss: 1451.364380\n",
      "Train: step:  53920, time: 0.251, loss: 2639.470947\n",
      "Train: step:  53930, time: 0.220, loss: 1412.184326\n",
      "Train: step:  53940, time: 0.216, loss: 2034.704590\n",
      "Train: step:  53950, time: 0.188, loss: 2514.552246\n",
      "Train: step:  53960, time: 0.200, loss: 851.881165\n",
      "Train: step:  53970, time: 0.197, loss: 1619.997314\n",
      "Train: step:  53980, time: 0.192, loss: 2722.480713\n",
      "Train: step:  53990, time: 0.188, loss: 2286.004150\n",
      "Train: step:  54000, time: 0.194, loss: 1570.175049\n",
      "Train: step:  54010, time: 0.187, loss: 3206.100098\n",
      "Train: step:  54020, time: 0.191, loss: 1868.117676\n",
      "Train: step:  54030, time: 0.187, loss: 1149.334351\n",
      "Train: step:  54040, time: 0.189, loss: 1188.811890\n",
      "Train: step:  54050, time: 0.189, loss: 2134.848633\n",
      "Train: step:  54060, time: 0.187, loss: 1347.557007\n",
      "Train: step:  54070, time: 0.205, loss: 978.122864\n",
      "Train: step:  54080, time: 0.217, loss: 1493.117676\n",
      "Train: step:  54090, time: 0.190, loss: 1485.849243\n",
      "Train: step:  54100, time: 0.191, loss: 2298.766357\n",
      "Train: step:  54110, time: 0.213, loss: 1834.673340\n",
      "Train: step:  54120, time: 0.187, loss: 4069.912842\n",
      "Train: step:  54130, time: 0.186, loss: 2093.853516\n",
      "Train: step:  54140, time: 0.189, loss: 1695.738770\n",
      "Train: step:  54150, time: 0.193, loss: 448.362030\n",
      "Train: step:  54160, time: 0.217, loss: 796.136353\n",
      "Train: step:  54170, time: 0.187, loss: 1821.224365\n",
      "Train: step:  54180, time: 0.197, loss: 2939.526855\n",
      "Train: step:  54190, time: 0.212, loss: 550.182983\n",
      "Train: step:  54200, time: 0.192, loss: 861.227966\n",
      "Train: step:  54210, time: 0.197, loss: 2400.031982\n",
      "Train: step:  54220, time: 0.219, loss: 1693.456909\n",
      "Train: step:  54230, time: 0.212, loss: 3124.057617\n",
      "Train: step:  54240, time: 0.182, loss: 1398.162598\n",
      "Train: step:  54250, time: 0.191, loss: 1047.274658\n",
      "Train: step:  54260, time: 0.211, loss: 1996.977539\n",
      "Train: step:  54270, time: 0.197, loss: 1279.767578\n",
      "Train: step:  54280, time: 0.187, loss: 1347.923340\n",
      "Train: step:  54290, time: 0.192, loss: 3029.530029\n",
      "Train: step:  54300, time: 0.190, loss: 2222.339600\n",
      "Train: step:  54310, time: 0.222, loss: 652.411804\n",
      "Train: step:  54320, time: 0.190, loss: 854.019775\n",
      "Train: step:  54330, time: 0.189, loss: 370.280212\n",
      "Train: step:  54340, time: 0.228, loss: 1407.484619\n",
      "Train: step:  54350, time: 0.193, loss: 3367.510498\n",
      "Train: step:  54360, time: 0.219, loss: 666.859131\n",
      "Train: step:  54370, time: 0.186, loss: 1526.584717\n",
      "Train: step:  54380, time: 0.236, loss: 745.872314\n",
      "Train: step:  54390, time: 0.194, loss: 359.579010\n",
      "Train: step:  54400, time: 0.183, loss: 3468.501465\n",
      "Train: step:  54410, time: 0.230, loss: 2070.329346\n",
      "Train: step:  54420, time: 0.193, loss: 3203.688477\n",
      "Train: step:  54430, time: 0.191, loss: 2478.011230\n",
      "Train: step:  54440, time: 0.187, loss: 2317.071777\n",
      "Train: step:  54450, time: 0.217, loss: 3577.041992\n",
      "Train: step:  54460, time: 0.247, loss: 2614.460205\n",
      "Train: step:  54470, time: 0.191, loss: 3407.844482\n",
      "Train: step:  54480, time: 0.192, loss: 1545.693604\n",
      "Train: step:  54490, time: 0.238, loss: 2072.262207\n",
      "Train: step:  54500, time: 0.193, loss: 1133.799561\n",
      "Train: step:  54510, time: 0.194, loss: 810.736206\n",
      "Train: step:  54520, time: 0.193, loss: 1742.770996\n",
      "Train: step:  54530, time: 0.198, loss: 1027.099731\n",
      "Train: step:  54540, time: 0.186, loss: 744.127197\n",
      "Train: step:  54550, time: 0.186, loss: 1920.745972\n",
      "Train: step:  54560, time: 0.192, loss: 590.883179\n",
      "Train: step:  54570, time: 0.196, loss: 897.070190\n",
      "Train: step:  54580, time: 0.195, loss: 2962.240967\n",
      "Train: step:  54590, time: 0.185, loss: 2823.309326\n",
      "Train: step:  54600, time: 0.189, loss: 761.709839\n",
      "Train: step:  54610, time: 0.181, loss: 2870.687500\n",
      "Train: step:  54620, time: 0.189, loss: 2103.184570\n",
      "Train: step:  54630, time: 0.218, loss: 1514.435425\n",
      "Train: step:  54640, time: 0.212, loss: 3132.693115\n",
      "Train: step:  54650, time: 0.190, loss: 1822.355347\n",
      "Train: step:  54660, time: 0.191, loss: 1441.402466\n",
      "Train: step:  54670, time: 0.190, loss: 2069.242432\n",
      "Train: step:  54680, time: 0.190, loss: 1130.127930\n",
      "Train: step:  54690, time: 0.185, loss: 1954.209229\n",
      "Train: step:  54700, time: 0.202, loss: 2892.506592\n",
      "Train: step:  54710, time: 0.252, loss: 1861.047852\n",
      "Train: step:  54720, time: 0.189, loss: 2021.999146\n",
      "Train: step:  54730, time: 0.186, loss: 1461.714966\n",
      "Train: step:  54740, time: 0.217, loss: 2159.210938\n",
      "Train: step:  54750, time: 0.193, loss: 1705.043945\n",
      "Train: step:  54760, time: 0.226, loss: 1499.648193\n",
      "Train: step:  54770, time: 0.189, loss: 2953.364502\n",
      "Train: step:  54780, time: 0.192, loss: 3796.480957\n",
      "Train: step:  54790, time: 0.194, loss: 2592.478516\n",
      "Train: step:  54800, time: 0.189, loss: 1094.407593\n",
      "Train: step:  54810, time: 0.192, loss: 1280.184326\n",
      "Train: step:  54820, time: 0.201, loss: 442.023376\n",
      "Train: step:  54830, time: 0.198, loss: 4363.258789\n",
      "Train: step:  54840, time: 0.187, loss: 1828.363525\n",
      "Train: step:  54850, time: 0.220, loss: 1890.406372\n",
      "Train: step:  54860, time: 0.186, loss: 2233.302002\n",
      "Train: step:  54870, time: 0.186, loss: 3165.246338\n",
      "Train: step:  54880, time: 0.193, loss: 4335.554199\n",
      "Train: step:  54890, time: 0.191, loss: 2135.787842\n",
      "Train: step:  54900, time: 0.196, loss: 1599.944336\n",
      "Train: step:  54910, time: 0.189, loss: 2827.428467\n",
      "Train: step:  54920, time: 0.185, loss: 2291.390869\n",
      "Train: step:  54930, time: 0.194, loss: 1944.231323\n",
      "Train: step:  54940, time: 0.195, loss: 3193.641113\n",
      "Train: step:  54950, time: 0.189, loss: 517.414978\n",
      "Train: step:  54960, time: 0.188, loss: 2304.878418\n",
      "Train: step:  54970, time: 0.215, loss: 2373.685059\n",
      "Train: step:  54980, time: 0.218, loss: 765.767517\n",
      "Train: step:  54990, time: 0.218, loss: 3788.053223\n",
      "Train: step:  55000, time: 0.188, loss: 3451.122314\n",
      "Train: step:  55010, time: 0.191, loss: 760.350952\n",
      "Train: step:  55020, time: 0.188, loss: 1354.657837\n",
      "Train: step:  55030, time: 0.229, loss: 2366.592773\n",
      "Train: step:  55040, time: 0.192, loss: 1071.625244\n",
      "Train: step:  55050, time: 0.193, loss: 1076.071167\n",
      "Train: step:  55060, time: 0.223, loss: 1579.334229\n",
      "Train: step:  55070, time: 0.190, loss: 1726.354248\n",
      "Train: step:  55080, time: 0.208, loss: 1772.490234\n",
      "Train: step:  55090, time: 0.229, loss: 1824.227539\n",
      "Train: step:  55100, time: 0.193, loss: 680.235962\n",
      "Train: step:  55110, time: 0.226, loss: 1809.238037\n",
      "Train: step:  55120, time: 0.197, loss: 2014.704346\n",
      "Train: step:  55130, time: 0.185, loss: 1872.682129\n",
      "Train: step:  55140, time: 0.195, loss: 2607.638184\n",
      "Train: step:  55150, time: 0.192, loss: 672.712280\n",
      "Train: step:  55160, time: 0.238, loss: 1856.183105\n",
      "Train: step:  55170, time: 0.180, loss: 1028.842773\n",
      "Train: step:  55180, time: 0.218, loss: 2647.929443\n",
      "Train: step:  55190, time: 0.185, loss: 1611.529907\n",
      "Train: step:  55200, time: 0.190, loss: 307.857208\n",
      "Train: step:  55210, time: 0.223, loss: 1301.848755\n",
      "Train: step:  55220, time: 0.189, loss: 2917.118164\n",
      "Train: step:  55230, time: 0.235, loss: 1300.377563\n",
      "Train: step:  55240, time: 0.222, loss: 2564.800781\n",
      "Train: step:  55250, time: 0.195, loss: 1593.876099\n",
      "Train: step:  55260, time: 0.194, loss: 1472.481079\n",
      "Train: step:  55270, time: 0.192, loss: 999.524719\n",
      "Train: step:  55280, time: 0.233, loss: 2049.100098\n",
      "Train: step:  55290, time: 0.193, loss: 1162.198853\n",
      "Train: step:  55300, time: 0.196, loss: 2398.101074\n",
      "Train: step:  55310, time: 0.191, loss: 2776.951416\n",
      "Train: step:  55320, time: 0.188, loss: 1253.156982\n",
      "Train: step:  55330, time: 0.183, loss: 1920.843140\n",
      "Train: step:  55340, time: 0.186, loss: 1755.588745\n",
      "Train: step:  55350, time: 0.253, loss: 1270.232910\n",
      "Train: step:  55360, time: 0.185, loss: 1355.514404\n",
      "Train: step:  55370, time: 0.217, loss: 1642.971680\n",
      "Train: step:  55380, time: 0.191, loss: 3416.065918\n",
      "Train: step:  55390, time: 0.218, loss: 2151.264648\n",
      "Train: step:  55400, time: 0.202, loss: 1977.401489\n",
      "Train: step:  55410, time: 0.216, loss: 530.178223\n",
      "Train: step:  55420, time: 0.188, loss: 2012.051392\n",
      "Train: step:  55430, time: 0.196, loss: 651.263367\n",
      "Train: step:  55440, time: 0.219, loss: 2702.805664\n",
      "Train: step:  55450, time: 0.242, loss: 991.553894\n",
      "Train: step:  55460, time: 0.194, loss: 1664.863892\n",
      "Train: step:  55470, time: 0.189, loss: 4025.286621\n",
      "Train: step:  55480, time: 0.234, loss: 2025.847290\n",
      "Train: step:  55490, time: 0.200, loss: 1344.625366\n",
      "Train: step:  55500, time: 0.217, loss: 1529.714233\n",
      "Train: step:  55510, time: 0.187, loss: 1411.858887\n",
      "Train: step:  55520, time: 0.193, loss: 2045.595581\n",
      "Train: step:  55530, time: 0.185, loss: 591.163208\n",
      "Train: step:  55540, time: 0.250, loss: 2150.576172\n",
      "Train: step:  55550, time: 0.212, loss: 3089.102295\n",
      "Train: step:  55560, time: 0.190, loss: 908.580933\n",
      "Train: step:  55570, time: 0.218, loss: 913.561768\n",
      "Train: step:  55580, time: 0.216, loss: 1613.217407\n",
      "Train: step:  55590, time: 0.217, loss: 1122.336304\n",
      "Train: step:  55600, time: 0.191, loss: 1476.658325\n",
      "Train: step:  55610, time: 0.234, loss: 2114.005371\n",
      "Train: step:  55620, time: 0.233, loss: 3722.810059\n",
      "Train: step:  55630, time: 0.187, loss: 3414.624268\n",
      "Train: step:  55640, time: 0.233, loss: 661.589661\n",
      "Train: step:  55650, time: 0.217, loss: 2454.018555\n",
      "Train: step:  55660, time: 0.189, loss: 314.639191\n",
      "Train: step:  55670, time: 0.193, loss: 820.568970\n",
      "Train: step:  55680, time: 0.190, loss: 2610.690430\n",
      "Train: step:  55690, time: 0.248, loss: 1060.189453\n",
      "Train: step:  55700, time: 0.201, loss: 2420.113525\n",
      "Train: step:  55710, time: 0.221, loss: 1287.675903\n",
      "Train: step:  55720, time: 0.194, loss: 1252.947021\n",
      "Train: step:  55730, time: 0.187, loss: 1345.827515\n",
      "Train: step:  55740, time: 0.192, loss: 2091.498535\n",
      "Train: step:  55750, time: 0.193, loss: 1417.189819\n",
      "Train: step:  55760, time: 0.191, loss: 2509.571533\n",
      "Train: step:  55770, time: 0.214, loss: 1197.716064\n",
      "Train: step:  55780, time: 0.188, loss: 550.630371\n",
      "Train: step:  55790, time: 0.186, loss: 1108.138306\n",
      "Train: step:  55800, time: 0.195, loss: 1818.147827\n",
      "Train: step:  55810, time: 0.191, loss: 4174.186035\n",
      "Train: step:  55820, time: 0.186, loss: 2219.238037\n",
      "Train: step:  55830, time: 0.188, loss: 1122.852539\n",
      "Train: step:  55840, time: 0.200, loss: 2719.224121\n",
      "Train: step:  55850, time: 0.188, loss: 1372.241455\n",
      "Train: step:  55860, time: 0.226, loss: 1322.127808\n",
      "Train: step:  55870, time: 0.186, loss: 2167.475098\n",
      "Train: step:  55880, time: 0.188, loss: 2323.415283\n",
      "Train: step:  55890, time: 0.232, loss: 3411.470215\n",
      "Train: step:  55900, time: 0.250, loss: 1990.014526\n",
      "Train: step:  55910, time: 0.195, loss: 2199.870605\n",
      "Train: step:  55920, time: 0.190, loss: 886.794312\n",
      "Train: step:  55930, time: 0.220, loss: 748.553833\n",
      "Train: step:  55940, time: 0.215, loss: 2313.239990\n",
      "Train: step:  55950, time: 0.193, loss: 2196.428955\n",
      "Train: step:  55960, time: 0.248, loss: 1949.954346\n",
      "Train: step:  55970, time: 0.190, loss: 2484.505371\n",
      "Train: step:  55980, time: 0.242, loss: 853.621094\n",
      "Train: step:  55990, time: 0.226, loss: 2885.476074\n",
      "Train: step:  56000, time: 0.193, loss: 2477.293213\n",
      "Train: step:  56010, time: 0.198, loss: 693.870178\n",
      "Train: step:  56020, time: 0.190, loss: 3356.152344\n",
      "Train: step:  56030, time: 0.183, loss: 1761.699829\n",
      "Train: step:  56040, time: 0.216, loss: 1493.777588\n",
      "Train: step:  56050, time: 0.195, loss: 1583.944214\n",
      "Train: step:  56060, time: 0.196, loss: 1033.360840\n",
      "Train: step:  56070, time: 0.248, loss: 2679.221436\n",
      "Train: step:  56080, time: 0.234, loss: 2262.448730\n",
      "Train: step:  56090, time: 0.184, loss: 1048.312744\n",
      "Train: step:  56100, time: 0.187, loss: 782.134583\n",
      "Train: step:  56110, time: 0.190, loss: 2893.437256\n",
      "Train: step:  56120, time: 0.202, loss: 589.078125\n",
      "Train: step:  56130, time: 0.183, loss: 607.975220\n",
      "Train: step:  56140, time: 0.220, loss: 560.368591\n",
      "Train: step:  56150, time: 0.201, loss: 301.112366\n",
      "Train: step:  56160, time: 0.195, loss: 3296.741699\n",
      "Train: step:  56170, time: 0.185, loss: 1856.533569\n",
      "Train: step:  56180, time: 0.193, loss: 2601.308594\n",
      "Train: step:  56190, time: 0.231, loss: 2275.372314\n",
      "Train: step:  56200, time: 0.246, loss: 853.036621\n",
      "Train: step:  56210, time: 0.192, loss: 2548.909668\n",
      "Train: step:  56220, time: 0.214, loss: 407.045898\n",
      "Train: step:  56230, time: 0.181, loss: 447.846680\n",
      "Train: step:  56240, time: 0.198, loss: 1333.228760\n",
      "Train: step:  56250, time: 0.224, loss: 555.095276\n",
      "Train: step:  56260, time: 0.200, loss: 267.676941\n",
      "Train: step:  56270, time: 0.221, loss: 2285.559570\n",
      "Train: step:  56280, time: 0.191, loss: 2003.554688\n",
      "Train: step:  56290, time: 0.185, loss: 2956.396973\n",
      "Train: step:  56300, time: 0.186, loss: 2362.650879\n",
      "Train: step:  56310, time: 0.184, loss: 897.190857\n",
      "Train: step:  56320, time: 0.193, loss: 1413.869507\n",
      "Train: step:  56330, time: 0.183, loss: 1170.440796\n",
      "Train: step:  56340, time: 0.187, loss: 2488.145020\n",
      "Train: step:  56350, time: 0.215, loss: 1708.383667\n",
      "Train: step:  56360, time: 0.224, loss: 2877.500977\n",
      "Train: step:  56370, time: 0.184, loss: 1624.470337\n",
      "Train: step:  56380, time: 0.195, loss: 1671.253296\n",
      "Train: step:  56390, time: 0.189, loss: 3435.722168\n",
      "Train: step:  56400, time: 0.187, loss: 3124.391113\n",
      "Train: step:  56410, time: 0.184, loss: 3890.515381\n",
      "Train: step:  56420, time: 0.218, loss: 2637.429199\n",
      "Train: step:  56430, time: 0.192, loss: 1835.440186\n",
      "Train: step:  56440, time: 0.185, loss: 320.899628\n",
      "Train: step:  56450, time: 0.207, loss: 958.184204\n",
      "Train: step:  56460, time: 0.219, loss: 2047.526978\n",
      "Train: step:  56470, time: 0.188, loss: 1007.661072\n",
      "Train: step:  56480, time: 0.202, loss: 806.648926\n",
      "Train: step:  56490, time: 0.190, loss: 1902.120483\n",
      "Train: step:  56500, time: 0.189, loss: 1900.131958\n",
      "Train: step:  56510, time: 0.189, loss: 1013.980652\n",
      "Train: step:  56520, time: 0.192, loss: 2786.445801\n",
      "Train: step:  56530, time: 0.192, loss: 2023.793213\n",
      "Train: step:  56540, time: 0.239, loss: 994.007874\n",
      "Train: step:  56550, time: 0.218, loss: 2471.595215\n",
      "Train: step:  56560, time: 0.189, loss: 1273.866943\n",
      "Train: step:  56570, time: 0.193, loss: 2987.996338\n",
      "Train: step:  56580, time: 0.191, loss: 2404.136719\n",
      "Train: step:  56590, time: 0.189, loss: 1943.039551\n",
      "Train: step:  56600, time: 0.185, loss: 549.141663\n",
      "Train: step:  56610, time: 0.219, loss: 2480.757812\n",
      "Train: step:  56620, time: 0.216, loss: 1045.716919\n",
      "Train: step:  56630, time: 0.190, loss: 2194.775879\n",
      "Train: step:  56640, time: 0.214, loss: 1212.156982\n",
      "Train: step:  56650, time: 0.218, loss: 536.953857\n",
      "Train: step:  56660, time: 0.264, loss: 2080.776855\n",
      "Train: step:  56670, time: 0.229, loss: 1634.068726\n",
      "Train: step:  56680, time: 0.230, loss: 1122.100464\n",
      "Train: step:  56690, time: 0.189, loss: 1561.884521\n",
      "Train: step:  56700, time: 0.190, loss: 3476.281982\n",
      "Train: step:  56710, time: 0.193, loss: 394.499634\n",
      "Train: step:  56720, time: 0.181, loss: 2641.124023\n",
      "Train: step:  56730, time: 0.186, loss: 2074.755615\n",
      "Train: step:  56740, time: 0.228, loss: 1574.598877\n",
      "Train: step:  56750, time: 0.187, loss: 1943.782715\n",
      "Train: step:  56760, time: 0.205, loss: 2737.668701\n",
      "Train: step:  56770, time: 0.232, loss: 1077.046875\n",
      "Train: step:  56780, time: 0.215, loss: 200.705872\n",
      "Train: step:  56790, time: 0.218, loss: 977.426453\n",
      "Train: step:  56800, time: 0.197, loss: 2441.034668\n",
      "Train: step:  56810, time: 0.189, loss: 2274.883789\n",
      "Train: step:  56820, time: 0.191, loss: 600.559814\n",
      "Train: step:  56830, time: 0.188, loss: 2371.951904\n",
      "Train: step:  56840, time: 0.228, loss: 711.608032\n",
      "Train: step:  56850, time: 0.211, loss: 2700.729004\n",
      "Train: step:  56860, time: 0.191, loss: 844.901855\n",
      "Train: step:  56870, time: 0.181, loss: 737.825928\n",
      "Train: step:  56880, time: 0.188, loss: 3766.397461\n",
      "Train: step:  56890, time: 0.228, loss: 2155.885498\n",
      "Train: step:  56900, time: 0.217, loss: 1533.017090\n",
      "Train: step:  56910, time: 0.194, loss: 2455.374023\n",
      "Train: step:  56920, time: 0.233, loss: 2278.042725\n",
      "Train: step:  56930, time: 0.192, loss: 369.694366\n",
      "Train: step:  56940, time: 0.187, loss: 3111.730469\n",
      "Train: step:  56950, time: 0.250, loss: 249.899963\n",
      "Train: step:  56960, time: 0.226, loss: 1967.746338\n",
      "Train: step:  56970, time: 0.191, loss: 849.844666\n",
      "Train: step:  56980, time: 0.212, loss: 1904.688110\n",
      "Train: step:  56990, time: 0.193, loss: 1426.059326\n",
      "Train: step:  57000, time: 0.235, loss: 2295.174316\n",
      "Train: step:  57010, time: 0.253, loss: 2214.652344\n",
      "Train: step:  57020, time: 0.187, loss: 2939.294434\n",
      "Train: step:  57030, time: 0.188, loss: 1605.919067\n",
      "Train: step:  57040, time: 0.189, loss: 2013.228882\n",
      "Train: step:  57050, time: 0.197, loss: 748.438477\n",
      "Train: step:  57060, time: 0.191, loss: 3479.505615\n",
      "Train: step:  57070, time: 0.185, loss: 602.730408\n",
      "Train: step:  57080, time: 0.223, loss: 634.570740\n",
      "Train: step:  57090, time: 0.198, loss: 1566.309570\n",
      "Train: step:  57100, time: 0.184, loss: 2432.365723\n",
      "Train: step:  57110, time: 0.216, loss: 1819.667358\n",
      "Train: step:  57120, time: 0.223, loss: 2414.481445\n",
      "Train: step:  57130, time: 0.189, loss: 850.452637\n",
      "Train: step:  57140, time: 0.194, loss: 3153.018066\n",
      "Train: step:  57150, time: 0.201, loss: 2383.025146\n",
      "Train: step:  57160, time: 0.206, loss: 1858.233154\n",
      "Train: step:  57170, time: 0.197, loss: 2809.337891\n",
      "Train: step:  57180, time: 0.187, loss: 1006.065613\n",
      "Train: step:  57190, time: 0.220, loss: 2313.092041\n",
      "Train: step:  57200, time: 0.191, loss: 1766.701904\n",
      "Train: step:  57210, time: 0.193, loss: 2762.797607\n",
      "Train: step:  57220, time: 0.219, loss: 606.410522\n",
      "Train: step:  57230, time: 0.192, loss: 1590.808472\n",
      "Train: step:  57240, time: 0.201, loss: 2290.134521\n",
      "Train: step:  57250, time: 0.189, loss: 414.695709\n",
      "Train: step:  57260, time: 0.190, loss: 1500.764893\n",
      "Train: step:  57270, time: 0.247, loss: 4071.999756\n",
      "Train: step:  57280, time: 0.189, loss: 2254.348389\n",
      "Train: step:  57290, time: 0.213, loss: 1616.710327\n",
      "Train: step:  57300, time: 0.193, loss: 947.704285\n",
      "Train: step:  57310, time: 0.192, loss: 1402.860474\n",
      "Train: step:  57320, time: 0.219, loss: 1854.012573\n",
      "Train: step:  57330, time: 0.194, loss: 1795.382446\n",
      "Train: step:  57340, time: 0.187, loss: 1388.341309\n",
      "Train: step:  57350, time: 0.188, loss: 2890.243652\n",
      "Train: step:  57360, time: 0.190, loss: 3595.919922\n",
      "Train: step:  57370, time: 0.196, loss: 2926.593262\n",
      "Train: step:  57380, time: 0.217, loss: 656.936279\n",
      "Train: step:  57390, time: 0.227, loss: 648.490784\n",
      "Train: step:  57400, time: 0.183, loss: 766.413269\n",
      "Train: step:  57410, time: 0.193, loss: 1620.089844\n",
      "Train: step:  57420, time: 0.184, loss: 1930.140015\n",
      "Train: step:  57430, time: 0.195, loss: 3532.754150\n",
      "Train: step:  57440, time: 0.187, loss: 3737.953369\n",
      "Train: step:  57450, time: 0.195, loss: 2176.772217\n",
      "Train: step:  57460, time: 0.216, loss: 2149.909912\n",
      "Train: step:  57470, time: 0.206, loss: 2481.708984\n",
      "Train: step:  57480, time: 0.192, loss: 1465.506104\n",
      "Train: step:  57490, time: 0.192, loss: 1820.192871\n",
      "Train: step:  57500, time: 0.200, loss: 4876.290039\n",
      "Train: step:  57510, time: 0.227, loss: 1115.458252\n",
      "Train: step:  57520, time: 0.184, loss: 1101.045044\n",
      "Train: step:  57530, time: 0.189, loss: 1506.889404\n",
      "Train: step:  57540, time: 0.195, loss: 1988.890381\n",
      "Train: step:  57550, time: 0.187, loss: 2400.937500\n",
      "Train: step:  57560, time: 0.200, loss: 1013.941345\n",
      "Train: step:  57570, time: 0.188, loss: 3237.930664\n",
      "Train: step:  57580, time: 0.187, loss: 1327.164551\n",
      "Train: step:  57590, time: 0.185, loss: 1369.916382\n",
      "Train: step:  57600, time: 0.216, loss: 1169.142578\n",
      "Train: step:  57610, time: 0.188, loss: 1058.929321\n",
      "Train: step:  57620, time: 0.217, loss: 1913.291138\n",
      "Train: step:  57630, time: 0.220, loss: 1492.504272\n",
      "Train: step:  57640, time: 0.190, loss: 5350.489258\n",
      "Train: step:  57650, time: 0.192, loss: 3439.646729\n",
      "Train: step:  57660, time: 0.188, loss: 1241.240967\n",
      "Train: step:  57670, time: 0.225, loss: 2883.683594\n",
      "Train: step:  57680, time: 0.187, loss: 2406.256104\n",
      "Train: step:  57690, time: 0.192, loss: 2120.344238\n",
      "Train: step:  57700, time: 0.190, loss: 2429.424561\n",
      "Train: step:  57710, time: 0.198, loss: 1852.358398\n",
      "Train: step:  57720, time: 0.193, loss: 959.685242\n",
      "Train: step:  57730, time: 0.191, loss: 1490.304321\n",
      "Train: step:  57740, time: 0.217, loss: 2423.331543\n",
      "Train: step:  57750, time: 0.184, loss: 475.360626\n",
      "Train: step:  57760, time: 0.186, loss: 3939.757812\n",
      "Train: step:  57770, time: 0.191, loss: 1158.333130\n",
      "Train: step:  57780, time: 0.197, loss: 675.745422\n",
      "Train: step:  57790, time: 0.188, loss: 3000.436523\n",
      "Train: step:  57800, time: 0.189, loss: 4353.923828\n",
      "Train: step:  57810, time: 0.193, loss: 2379.177979\n",
      "Train: step:  57820, time: 0.185, loss: 1244.164429\n",
      "Train: step:  57830, time: 0.186, loss: 588.953430\n",
      "Train: step:  57840, time: 0.184, loss: 1062.360718\n",
      "Train: step:  57850, time: 0.200, loss: 793.780823\n",
      "Train: step:  57860, time: 0.187, loss: 4236.039551\n",
      "Train: step:  57870, time: 0.193, loss: 2363.735596\n",
      "Train: step:  57880, time: 0.237, loss: 2165.039795\n",
      "Train: step:  57890, time: 0.217, loss: 2343.721191\n",
      "Train: step:  57900, time: 0.188, loss: 1742.067261\n",
      "Train: step:  57910, time: 0.186, loss: 1991.821533\n",
      "Train: step:  57920, time: 0.193, loss: 1781.345825\n",
      "Train: step:  57930, time: 0.192, loss: 1229.501709\n",
      "Train: step:  57940, time: 0.185, loss: 1266.195923\n",
      "Train: step:  57950, time: 0.216, loss: 4455.901855\n",
      "Train: step:  57960, time: 0.183, loss: 2398.436523\n",
      "Train: step:  57970, time: 0.225, loss: 580.797241\n",
      "Train: step:  57980, time: 0.197, loss: 996.517029\n",
      "Train: step:  57990, time: 0.216, loss: 2727.905762\n",
      "Train: step:  58000, time: 0.204, loss: 1939.644165\n",
      "Train: step:  58010, time: 0.192, loss: 1627.727539\n",
      "Train: step:  58020, time: 0.204, loss: 1674.518555\n",
      "Train: step:  58030, time: 0.217, loss: 2792.560303\n",
      "Train: step:  58040, time: 0.185, loss: 938.656616\n",
      "Train: step:  58050, time: 0.183, loss: 622.839417\n",
      "Train: step:  58060, time: 0.191, loss: 870.373779\n",
      "Train: step:  58070, time: 0.187, loss: 3151.633057\n",
      "Train: step:  58080, time: 0.194, loss: 1965.817749\n",
      "Train: step:  58090, time: 0.184, loss: 2360.827148\n",
      "Train: step:  58100, time: 0.189, loss: 2067.247314\n",
      "Train: step:  58110, time: 0.263, loss: 1558.378052\n",
      "Train: step:  58120, time: 0.235, loss: 379.717468\n",
      "Train: step:  58130, time: 0.189, loss: 1386.367065\n",
      "Train: step:  58140, time: 0.182, loss: 1187.603394\n",
      "Train: step:  58150, time: 0.217, loss: 2621.726074\n",
      "Train: step:  58160, time: 0.214, loss: 1474.481079\n",
      "Train: step:  58170, time: 0.189, loss: 1751.658081\n",
      "Train: step:  58180, time: 0.205, loss: 1670.952759\n",
      "Train: step:  58190, time: 0.194, loss: 662.534241\n",
      "Train: step:  58200, time: 0.191, loss: 606.620972\n",
      "Train: step:  58210, time: 0.226, loss: 1422.855835\n",
      "Train: step:  58220, time: 0.215, loss: 2188.280273\n",
      "Train: step:  58230, time: 0.183, loss: 1701.925781\n",
      "Train: step:  58240, time: 0.184, loss: 1667.006836\n",
      "Train: step:  58250, time: 0.187, loss: 1222.313232\n",
      "Train: step:  58260, time: 0.218, loss: 787.054077\n",
      "Train: step:  58270, time: 0.216, loss: 2787.158203\n",
      "Train: step:  58280, time: 0.218, loss: 576.031311\n",
      "Train: step:  58290, time: 0.215, loss: 2662.503662\n",
      "Train: step:  58300, time: 0.218, loss: 2358.933350\n",
      "Train: step:  58310, time: 0.247, loss: 2474.762451\n",
      "Train: step:  58320, time: 0.187, loss: 2051.544434\n",
      "Train: step:  58330, time: 0.185, loss: 1402.207642\n",
      "Train: step:  58340, time: 0.215, loss: 382.688171\n",
      "Train: step:  58350, time: 0.194, loss: 2544.812500\n",
      "Train: step:  58360, time: 0.195, loss: 2622.369873\n",
      "Train: step:  58370, time: 0.191, loss: 3461.952393\n",
      "Train: step:  58380, time: 0.193, loss: 1211.832031\n",
      "Train: step:  58390, time: 0.231, loss: 2240.466064\n",
      "Train: step:  58400, time: 0.190, loss: 2151.447266\n",
      "Train: step:  58410, time: 0.185, loss: 2157.268066\n",
      "Train: step:  58420, time: 0.187, loss: 3138.001953\n",
      "Train: step:  58430, time: 0.195, loss: 1526.344360\n",
      "Train: step:  58440, time: 0.197, loss: 3032.927490\n",
      "Train: step:  58450, time: 0.194, loss: 2698.690674\n",
      "Train: step:  58460, time: 0.193, loss: 1383.163940\n",
      "Train: step:  58470, time: 0.192, loss: 2322.039551\n",
      "Train: step:  58480, time: 0.203, loss: 348.712189\n",
      "Train: step:  58490, time: 0.212, loss: 1441.635010\n",
      "Train: step:  58500, time: 0.198, loss: 2348.403320\n",
      "Train: step:  58510, time: 0.196, loss: 1985.816162\n",
      "Train: step:  58520, time: 0.198, loss: 394.842316\n",
      "Train: step:  58530, time: 0.215, loss: 2146.839111\n",
      "Train: step:  58540, time: 0.219, loss: 2596.759766\n",
      "Train: step:  58550, time: 0.195, loss: 579.250793\n",
      "Train: step:  58560, time: 0.192, loss: 2955.972900\n",
      "Train: step:  58570, time: 0.201, loss: 1415.317261\n",
      "Train: step:  58580, time: 0.189, loss: 2645.263184\n",
      "Train: step:  58590, time: 0.187, loss: 2616.673096\n",
      "Train: step:  58600, time: 0.190, loss: 1731.098267\n",
      "Train: step:  58610, time: 0.207, loss: 417.477997\n",
      "Train: step:  58620, time: 0.188, loss: 381.718811\n",
      "Train: step:  58630, time: 0.190, loss: 1304.190674\n",
      "Train: step:  58640, time: 0.209, loss: 1817.251831\n",
      "Train: step:  58650, time: 0.191, loss: 915.812866\n",
      "Train: step:  58660, time: 0.231, loss: 1224.057495\n",
      "Train: step:  58670, time: 0.220, loss: 3045.903564\n",
      "Train: step:  58680, time: 0.193, loss: 1049.079956\n",
      "Train: step:  58690, time: 0.186, loss: 317.284393\n",
      "Train: step:  58700, time: 0.186, loss: 3784.333008\n",
      "Train: step:  58710, time: 0.188, loss: 424.453217\n",
      "Train: step:  58720, time: 0.200, loss: 1481.612305\n",
      "Train: step:  58730, time: 0.209, loss: 997.514709\n",
      "Train: step:  58740, time: 0.194, loss: 2799.791992\n",
      "Train: step:  58750, time: 0.192, loss: 2946.104980\n",
      "Train: step:  58760, time: 0.191, loss: 2773.722168\n",
      "Train: step:  58770, time: 0.191, loss: 1122.316406\n",
      "Train: step:  58780, time: 0.232, loss: 3259.394775\n",
      "Train: step:  58790, time: 0.198, loss: 3192.317383\n",
      "Train: step:  58800, time: 0.191, loss: 1664.184937\n",
      "Train: step:  58810, time: 0.195, loss: 2281.669922\n",
      "Train: step:  58820, time: 0.219, loss: 3923.215088\n",
      "Train: step:  58830, time: 0.189, loss: 2826.874023\n",
      "Train: step:  58840, time: 0.192, loss: 616.715210\n",
      "Train: step:  58850, time: 0.190, loss: 1107.337891\n",
      "Train: step:  58860, time: 0.208, loss: 1851.374268\n",
      "Train: step:  58870, time: 0.190, loss: 1531.170288\n",
      "Train: step:  58880, time: 0.193, loss: 770.792053\n",
      "Train: step:  58890, time: 0.215, loss: 263.061829\n",
      "Train: step:  58900, time: 0.190, loss: 1652.566895\n",
      "Train: step:  58910, time: 0.188, loss: 1047.684448\n",
      "Train: step:  58920, time: 0.190, loss: 649.114990\n",
      "Train: step:  58930, time: 0.187, loss: 3711.056396\n",
      "Train: step:  58940, time: 0.187, loss: 2500.528320\n",
      "Train: step:  58950, time: 0.191, loss: 2261.695068\n",
      "Train: step:  58960, time: 0.233, loss: 1868.882812\n",
      "Train: step:  58970, time: 0.186, loss: 2265.333496\n",
      "Train: step:  58980, time: 0.202, loss: 2780.721680\n",
      "Train: step:  58990, time: 0.235, loss: 1961.281128\n",
      "Train: step:  59000, time: 0.189, loss: 2047.100220\n",
      "Train: step:  59010, time: 0.195, loss: 872.450073\n",
      "Train: step:  59020, time: 0.191, loss: 444.476715\n",
      "Train: step:  59030, time: 0.221, loss: 1375.619751\n",
      "Train: step:  59040, time: 0.207, loss: 673.319580\n",
      "Train: step:  59050, time: 0.192, loss: 1272.664551\n",
      "Train: step:  59060, time: 0.225, loss: 495.281830\n",
      "Train: step:  59070, time: 0.217, loss: 3215.725098\n",
      "Train: step:  59080, time: 0.205, loss: 2304.427734\n",
      "Train: step:  59090, time: 0.219, loss: 2204.817871\n",
      "Train: step:  59100, time: 0.189, loss: 2801.156738\n",
      "Train: step:  59110, time: 0.187, loss: 2603.816406\n",
      "Train: step:  59120, time: 0.187, loss: 1940.992432\n",
      "Train: step:  59130, time: 0.190, loss: 520.094849\n",
      "Train: step:  59140, time: 0.226, loss: 2275.050781\n",
      "Train: step:  59150, time: 0.217, loss: 1239.232788\n",
      "Train: step:  59160, time: 0.228, loss: 1121.620728\n",
      "Train: step:  59170, time: 0.185, loss: 333.887512\n",
      "Train: step:  59180, time: 0.190, loss: 476.238129\n",
      "Train: step:  59190, time: 0.194, loss: 2425.420654\n",
      "Train: step:  59200, time: 0.220, loss: 2891.950928\n",
      "Train: step:  59210, time: 0.185, loss: 1731.199219\n",
      "Train: step:  59220, time: 0.185, loss: 1573.447266\n",
      "Train: step:  59230, time: 0.196, loss: 1417.779541\n",
      "Train: step:  59240, time: 0.191, loss: 1201.992432\n",
      "Train: step:  59250, time: 0.205, loss: 2516.264160\n",
      "Train: step:  59260, time: 0.239, loss: 1024.402588\n",
      "Train: step:  59270, time: 0.219, loss: 3398.942871\n",
      "Train: step:  59280, time: 0.195, loss: 2025.288574\n",
      "Train: step:  59290, time: 0.190, loss: 1043.245117\n",
      "Train: step:  59300, time: 0.220, loss: 2547.454590\n",
      "Train: step:  59310, time: 0.196, loss: 1591.222900\n",
      "Train: step:  59320, time: 0.200, loss: 1385.628052\n",
      "Train: step:  59330, time: 0.189, loss: 3387.323486\n",
      "Train: step:  59340, time: 0.197, loss: 1031.060547\n",
      "Train: step:  59350, time: 0.191, loss: 2986.518311\n",
      "Train: step:  59360, time: 0.192, loss: 1574.028198\n",
      "Train: step:  59370, time: 0.195, loss: 3094.480957\n",
      "Train: step:  59380, time: 0.185, loss: 2324.879639\n",
      "Train: step:  59390, time: 0.191, loss: 1486.094849\n",
      "Train: step:  59400, time: 0.187, loss: 1999.872314\n",
      "Train: step:  59410, time: 0.187, loss: 3895.788574\n",
      "Train: step:  59420, time: 0.190, loss: 3058.110840\n",
      "Train: step:  59430, time: 0.224, loss: 1373.153564\n",
      "Train: step:  59440, time: 0.194, loss: 1781.350342\n",
      "Train: step:  59450, time: 0.195, loss: 166.558838\n",
      "Train: step:  59460, time: 0.188, loss: 2800.489014\n",
      "Train: step:  59470, time: 0.199, loss: 1781.991943\n",
      "Train: step:  59480, time: 0.186, loss: 3328.568359\n",
      "Train: step:  59490, time: 0.187, loss: 1417.135742\n",
      "Train: step:  59500, time: 0.229, loss: 339.723816\n",
      "Train: step:  59510, time: 0.216, loss: 606.566406\n",
      "Train: step:  59520, time: 0.187, loss: 1992.117065\n",
      "Train: step:  59530, time: 0.186, loss: 1288.595703\n",
      "Train: step:  59540, time: 0.190, loss: 1556.990112\n",
      "Train: step:  59550, time: 0.188, loss: 864.056885\n",
      "Train: step:  59560, time: 0.186, loss: 1201.197754\n",
      "Train: step:  59570, time: 0.220, loss: 2777.303467\n",
      "Train: step:  59580, time: 0.229, loss: 862.801697\n",
      "Train: step:  59590, time: 0.189, loss: 1671.506958\n",
      "Train: step:  59600, time: 0.218, loss: 3229.993896\n",
      "Train: step:  59610, time: 0.185, loss: 2431.169922\n",
      "Train: step:  59620, time: 0.191, loss: 1672.258545\n",
      "Train: step:  59630, time: 0.187, loss: 207.269760\n",
      "Train: step:  59640, time: 0.188, loss: 1460.855347\n",
      "Train: step:  59650, time: 0.195, loss: 1320.945923\n",
      "Train: step:  59660, time: 0.197, loss: 1964.948608\n",
      "Train: step:  59670, time: 0.190, loss: 1240.978882\n",
      "Train: step:  59680, time: 0.187, loss: 2865.520996\n",
      "Train: step:  59690, time: 0.184, loss: 1812.947876\n",
      "Train: step:  59700, time: 0.210, loss: 2776.719727\n",
      "Train: step:  59710, time: 0.198, loss: 385.833832\n",
      "Train: step:  59720, time: 0.252, loss: 896.847839\n",
      "Train: step:  59730, time: 0.195, loss: 2049.185059\n",
      "Train: step:  59740, time: 0.189, loss: 856.698853\n",
      "Train: step:  59750, time: 0.188, loss: 2386.039551\n",
      "Train: step:  59760, time: 0.199, loss: 1222.295654\n",
      "Train: step:  59770, time: 0.186, loss: 1336.114624\n",
      "Train: step:  59780, time: 0.197, loss: 2406.917480\n",
      "Train: step:  59790, time: 0.214, loss: 722.454895\n",
      "Train: step:  59800, time: 0.189, loss: 429.113953\n",
      "Train: step:  59810, time: 0.201, loss: 2929.912354\n",
      "Train: step:  59820, time: 0.229, loss: 408.433685\n",
      "Train: step:  59830, time: 0.181, loss: 3340.678467\n",
      "Train: step:  59840, time: 0.193, loss: 2322.423096\n",
      "Train: step:  59850, time: 0.222, loss: 1843.543457\n",
      "Train: step:  59860, time: 0.186, loss: 2404.931396\n",
      "Train: step:  59870, time: 0.186, loss: 4912.786133\n",
      "Train: step:  59880, time: 0.190, loss: 806.278931\n",
      "Train: step:  59890, time: 0.226, loss: 3427.930664\n",
      "Train: step:  59900, time: 0.190, loss: 2234.343018\n",
      "Train: step:  59910, time: 0.217, loss: 1627.979614\n",
      "Train: step:  59920, time: 0.190, loss: 1435.446899\n",
      "Train: step:  59930, time: 0.189, loss: 2602.049561\n",
      "Train: step:  59940, time: 0.191, loss: 2604.230957\n",
      "Train: step:  59950, time: 0.190, loss: 169.201645\n",
      "Train: step:  59960, time: 0.234, loss: 1838.505615\n",
      "Train: step:  59970, time: 0.242, loss: 3370.141357\n",
      "Train: step:  59980, time: 0.198, loss: 2072.213867\n",
      "Train: step:  59990, time: 0.186, loss: 2065.882080\n",
      "Train: step:  60000, time: 0.184, loss: 2101.792969\n",
      "Train: step:  60010, time: 0.187, loss: 2158.593262\n",
      "Train: step:  60020, time: 0.187, loss: 1728.021851\n",
      "Train: step:  60030, time: 0.188, loss: 3191.689453\n",
      "Train: step:  60040, time: 0.183, loss: 1053.059570\n",
      "Train: step:  60050, time: 0.217, loss: 2258.035645\n",
      "Train: step:  60060, time: 0.227, loss: 1064.042969\n",
      "Train: step:  60070, time: 0.187, loss: 3557.291504\n",
      "Train: step:  60080, time: 0.218, loss: 3773.381592\n",
      "Train: step:  60090, time: 0.199, loss: 2043.928589\n",
      "Train: step:  60100, time: 0.242, loss: 2200.700684\n",
      "Train: step:  60110, time: 0.236, loss: 1247.886475\n",
      "Train: step:  60120, time: 0.216, loss: 755.944275\n",
      "Train: step:  60130, time: 0.190, loss: 1886.176880\n",
      "Train: step:  60140, time: 0.187, loss: 2027.404907\n",
      "Train: step:  60150, time: 0.196, loss: 3709.060791\n",
      "Train: step:  60160, time: 0.224, loss: 2519.038574\n",
      "Train: step:  60170, time: 0.188, loss: 1787.559937\n",
      "Train: step:  60180, time: 0.193, loss: 1096.275391\n",
      "Train: step:  60190, time: 0.224, loss: 846.680908\n",
      "Train: step:  60200, time: 0.183, loss: 2166.189453\n",
      "Train: step:  60210, time: 0.215, loss: 2661.330566\n",
      "Train: step:  60220, time: 0.220, loss: 3295.369873\n",
      "Train: step:  60230, time: 0.224, loss: 2259.298828\n",
      "Train: step:  60240, time: 0.186, loss: 361.980682\n",
      "Train: step:  60250, time: 0.192, loss: 1009.316895\n",
      "Train: step:  60260, time: 0.201, loss: 2720.670410\n",
      "Train: step:  60270, time: 0.187, loss: 3117.294434\n",
      "Train: step:  60280, time: 0.193, loss: 1156.977417\n",
      "Train: step:  60290, time: 0.193, loss: 2337.847656\n",
      "Train: step:  60300, time: 0.189, loss: 817.621155\n",
      "Train: step:  60310, time: 0.188, loss: 366.733307\n",
      "Train: step:  60320, time: 0.230, loss: 1148.380127\n",
      "Train: step:  60330, time: 0.184, loss: 2098.404053\n",
      "Train: step:  60340, time: 0.206, loss: 852.017456\n",
      "Train: step:  60350, time: 0.212, loss: 2415.394531\n",
      "Train: step:  60360, time: 0.185, loss: 2469.830078\n",
      "Train: step:  60370, time: 0.230, loss: 2262.779053\n",
      "Train: step:  60380, time: 0.217, loss: 919.620422\n",
      "Train: step:  60390, time: 0.194, loss: 1311.967651\n",
      "Train: step:  60400, time: 0.188, loss: 595.858032\n",
      "Train: step:  60410, time: 0.228, loss: 1129.853149\n",
      "Train: step:  60420, time: 0.190, loss: 1974.080322\n",
      "Train: step:  60430, time: 0.218, loss: 1602.657349\n",
      "Train: step:  60440, time: 0.191, loss: 1099.214722\n",
      "Train: step:  60450, time: 0.217, loss: 1360.086304\n",
      "Train: step:  60460, time: 0.188, loss: 2112.475098\n",
      "Train: step:  60470, time: 0.190, loss: 1116.333496\n",
      "Train: step:  60480, time: 0.188, loss: 222.794052\n",
      "Train: step:  60490, time: 0.191, loss: 1474.125000\n",
      "Train: step:  60500, time: 0.191, loss: 1506.197266\n",
      "Train: step:  60510, time: 0.190, loss: 2112.599609\n",
      "Train: step:  60520, time: 0.215, loss: 2368.212402\n",
      "Train: step:  60530, time: 0.222, loss: 703.293274\n",
      "Train: step:  60540, time: 0.208, loss: 2686.312256\n",
      "Train: step:  60550, time: 0.191, loss: 1823.910889\n",
      "Train: step:  60560, time: 0.192, loss: 1269.534424\n",
      "Train: step:  60570, time: 0.200, loss: 3424.273438\n",
      "Train: step:  60580, time: 0.225, loss: 2028.720093\n",
      "Train: step:  60590, time: 0.190, loss: 641.058044\n",
      "Train: step:  60600, time: 0.228, loss: 762.816101\n",
      "Train: step:  60610, time: 0.217, loss: 1948.176270\n",
      "Train: step:  60620, time: 0.219, loss: 1945.832886\n",
      "Train: step:  60630, time: 0.189, loss: 2300.406250\n",
      "Train: step:  60640, time: 0.246, loss: 3637.135742\n",
      "Train: step:  60650, time: 0.186, loss: 616.065430\n",
      "Train: step:  60660, time: 0.220, loss: 1609.142578\n",
      "Train: step:  60670, time: 0.208, loss: 2989.785400\n",
      "Train: step:  60680, time: 0.192, loss: 2451.324951\n",
      "Train: step:  60690, time: 0.197, loss: 1244.626587\n",
      "Train: step:  60700, time: 0.218, loss: 2684.818604\n",
      "Train: step:  60710, time: 0.206, loss: 1871.526611\n",
      "Train: step:  60720, time: 0.226, loss: 2396.854980\n",
      "Train: step:  60730, time: 0.199, loss: 1615.482178\n",
      "Train: step:  60740, time: 0.190, loss: 2759.062500\n",
      "Train: step:  60750, time: 0.218, loss: 1137.957520\n",
      "Train: step:  60760, time: 0.195, loss: 826.074768\n",
      "Train: step:  60770, time: 0.188, loss: 2190.562012\n",
      "Train: step:  60780, time: 0.200, loss: 2572.539795\n",
      "Train: step:  60790, time: 0.190, loss: 1045.591309\n",
      "Train: step:  60800, time: 0.193, loss: 3086.368408\n",
      "Train: step:  60810, time: 0.219, loss: 435.019714\n",
      "Train: step:  60820, time: 0.218, loss: 623.998291\n",
      "Train: step:  60830, time: 0.197, loss: 3167.044434\n",
      "Train: step:  60840, time: 0.192, loss: 1037.598877\n",
      "Train: step:  60850, time: 0.195, loss: 2981.997803\n",
      "Train: step:  60860, time: 0.188, loss: 2819.644287\n",
      "Train: step:  60870, time: 0.189, loss: 2444.034912\n",
      "Train: step:  60880, time: 0.227, loss: 1826.553101\n",
      "Train: step:  60890, time: 0.227, loss: 2236.718506\n",
      "Train: step:  60900, time: 0.200, loss: 1317.453369\n",
      "Train: step:  60910, time: 0.185, loss: 1964.499512\n",
      "Train: step:  60920, time: 0.209, loss: 1670.020020\n",
      "Train: step:  60930, time: 0.191, loss: 983.044800\n",
      "Train: step:  60940, time: 0.229, loss: 1818.828369\n",
      "Train: step:  60950, time: 0.252, loss: 970.036011\n",
      "Train: step:  60960, time: 0.196, loss: 1871.291504\n",
      "Train: step:  60970, time: 0.184, loss: 1039.925903\n",
      "Train: step:  60980, time: 0.231, loss: 2826.107910\n",
      "Train: step:  60990, time: 0.196, loss: 1991.894531\n",
      "Train: step:  61000, time: 0.189, loss: 2675.241943\n",
      "Train: step:  61010, time: 0.187, loss: 2602.028320\n",
      "Train: step:  61020, time: 0.187, loss: 3425.054199\n",
      "Train: step:  61030, time: 0.185, loss: 1728.842163\n",
      "Train: step:  61040, time: 0.185, loss: 1408.833862\n",
      "Train: step:  61050, time: 0.203, loss: 494.495911\n",
      "Train: step:  61060, time: 0.195, loss: 738.947998\n",
      "Train: step:  61070, time: 0.208, loss: 1777.307983\n",
      "Train: step:  61080, time: 0.198, loss: 517.340942\n",
      "Train: step:  61090, time: 0.205, loss: 966.493530\n",
      "Train: step:  61100, time: 0.190, loss: 1384.414429\n",
      "Train: step:  61110, time: 0.246, loss: 1314.373779\n",
      "Train: step:  61120, time: 0.203, loss: 2406.805908\n",
      "Train: step:  61130, time: 0.187, loss: 1424.192627\n",
      "Train: step:  61140, time: 0.216, loss: 1664.355713\n",
      "Train: step:  61150, time: 0.185, loss: 973.874207\n",
      "Train: step:  61160, time: 0.190, loss: 2718.559326\n",
      "Train: step:  61170, time: 0.181, loss: 2012.223022\n",
      "Train: step:  61180, time: 0.186, loss: 2306.114746\n",
      "Train: step:  61190, time: 0.190, loss: 2935.909180\n",
      "Train: step:  61200, time: 0.218, loss: 2001.540283\n",
      "Train: step:  61210, time: 0.189, loss: 1472.832642\n",
      "Train: step:  61220, time: 0.190, loss: 2000.070190\n",
      "Train: step:  61230, time: 0.192, loss: 356.073700\n",
      "Train: step:  61240, time: 0.196, loss: 1783.347778\n",
      "Train: step:  61250, time: 0.231, loss: 772.616150\n",
      "Train: step:  61260, time: 0.231, loss: 2499.010742\n",
      "Train: step:  61270, time: 0.218, loss: 4605.295410\n",
      "Train: step:  61280, time: 0.184, loss: 1562.774658\n",
      "Train: step:  61290, time: 0.195, loss: 1665.435669\n",
      "Train: step:  61300, time: 0.213, loss: 1681.579834\n",
      "Train: step:  61310, time: 0.183, loss: 3715.448242\n",
      "Train: step:  61320, time: 0.217, loss: 1970.896118\n",
      "Train: step:  61330, time: 0.191, loss: 954.995178\n",
      "Train: step:  61340, time: 0.190, loss: 1883.915649\n",
      "Train: step:  61350, time: 0.196, loss: 2469.375977\n",
      "Train: step:  61360, time: 0.246, loss: 1757.066406\n",
      "Train: step:  61370, time: 0.245, loss: 1993.102905\n",
      "Train: step:  61380, time: 0.189, loss: 420.302887\n",
      "Train: step:  61390, time: 0.254, loss: 2103.998291\n",
      "Train: step:  61400, time: 0.197, loss: 1169.733521\n",
      "Train: step:  61410, time: 0.215, loss: 1349.622681\n",
      "Train: step:  61420, time: 0.194, loss: 1227.564819\n",
      "Train: step:  61430, time: 0.237, loss: 3040.162598\n",
      "Train: step:  61440, time: 0.193, loss: 2260.833496\n",
      "Train: step:  61450, time: 0.217, loss: 1529.399292\n",
      "Train: step:  61460, time: 0.226, loss: 3074.797119\n",
      "Train: step:  61470, time: 0.191, loss: 2299.857666\n",
      "Train: step:  61480, time: 0.189, loss: 2376.430908\n",
      "Train: step:  61490, time: 0.201, loss: 556.658020\n",
      "Train: step:  61500, time: 0.193, loss: 263.180481\n",
      "Train: step:  61510, time: 0.189, loss: 1024.905640\n",
      "Train: step:  61520, time: 0.189, loss: 897.348999\n",
      "Train: step:  61530, time: 0.191, loss: 338.139008\n",
      "Train: step:  61540, time: 0.230, loss: 1707.655396\n",
      "Train: step:  61550, time: 0.190, loss: 2772.093018\n",
      "Train: step:  61560, time: 0.187, loss: 2281.481201\n",
      "Train: step:  61570, time: 0.191, loss: 3671.941162\n",
      "Train: step:  61580, time: 0.191, loss: 3377.357178\n",
      "Train: step:  61590, time: 0.266, loss: 353.898010\n",
      "Train: step:  61600, time: 0.196, loss: 3356.372559\n",
      "Train: step:  61610, time: 0.190, loss: 1672.141113\n",
      "Train: step:  61620, time: 0.190, loss: 3378.304443\n",
      "Train: step:  61630, time: 0.196, loss: 619.150146\n",
      "Train: step:  61640, time: 0.186, loss: 2131.380859\n",
      "Train: step:  61650, time: 0.199, loss: 3411.031006\n",
      "Train: step:  61660, time: 0.185, loss: 1907.442261\n",
      "Train: step:  61670, time: 0.193, loss: 994.674866\n",
      "Train: step:  61680, time: 0.194, loss: 3073.753662\n",
      "Train: step:  61690, time: 0.190, loss: 2011.825439\n",
      "Train: step:  61700, time: 0.194, loss: 1785.826660\n",
      "Train: step:  61710, time: 0.189, loss: 1361.856689\n",
      "Train: step:  61720, time: 0.186, loss: 772.845276\n",
      "Train: step:  61730, time: 0.197, loss: 2917.242676\n",
      "Train: step:  61740, time: 0.239, loss: 3925.097168\n",
      "Train: step:  61750, time: 0.219, loss: 2861.102295\n",
      "Train: step:  61760, time: 0.194, loss: 501.287262\n",
      "Train: step:  61770, time: 0.196, loss: 2275.522705\n",
      "Train: step:  61780, time: 0.212, loss: 1915.329102\n",
      "Train: step:  61790, time: 0.190, loss: 1688.124390\n",
      "Train: step:  61800, time: 0.227, loss: 511.026428\n",
      "Train: step:  61810, time: 0.243, loss: 2093.701172\n",
      "Train: step:  61820, time: 0.196, loss: 677.313416\n",
      "Train: step:  61830, time: 0.192, loss: 2772.837646\n",
      "Train: step:  61840, time: 0.190, loss: 1361.921509\n",
      "Train: step:  61850, time: 0.202, loss: 2111.584717\n",
      "Train: step:  61860, time: 0.203, loss: 678.805420\n",
      "Train: step:  61870, time: 0.189, loss: 985.279663\n",
      "Train: step:  61880, time: 0.192, loss: 1953.035889\n",
      "Train: step:  61890, time: 0.191, loss: 1055.015015\n",
      "Train: step:  61900, time: 0.188, loss: 652.248840\n",
      "Train: step:  61910, time: 0.217, loss: 2559.347168\n",
      "Train: step:  61920, time: 0.227, loss: 1521.928345\n",
      "Train: step:  61930, time: 0.196, loss: 1811.759033\n",
      "Train: step:  61940, time: 0.192, loss: 164.152771\n",
      "Train: step:  61950, time: 0.194, loss: 1096.469482\n",
      "Train: step:  61960, time: 0.196, loss: 1069.951538\n",
      "Train: step:  61970, time: 0.196, loss: 626.773376\n",
      "Train: step:  61980, time: 0.195, loss: 650.804565\n",
      "Train: step:  61990, time: 0.197, loss: 781.266602\n",
      "Train: step:  62000, time: 0.195, loss: 3383.645508\n",
      "Train: step:  62010, time: 0.216, loss: 901.380310\n",
      "Train: step:  62020, time: 0.187, loss: 5743.718750\n",
      "Train: step:  62030, time: 0.208, loss: 3400.953857\n",
      "Train: step:  62040, time: 0.218, loss: 3296.284424\n",
      "Train: step:  62050, time: 0.236, loss: 1525.142456\n",
      "Train: step:  62060, time: 0.185, loss: 3329.537598\n",
      "Train: step:  62070, time: 0.194, loss: 1200.886963\n",
      "Train: step:  62080, time: 0.205, loss: 2099.379150\n",
      "Train: step:  62090, time: 0.210, loss: 1754.764771\n",
      "Train: step:  62100, time: 0.195, loss: 1731.748047\n",
      "Train: step:  62110, time: 0.189, loss: 1271.428467\n",
      "Train: step:  62120, time: 0.186, loss: 414.554779\n",
      "Train: step:  62130, time: 0.183, loss: 1625.461548\n",
      "Train: step:  62140, time: 0.194, loss: 2382.606445\n",
      "Train: step:  62150, time: 0.194, loss: 1657.727783\n",
      "Train: step:  62160, time: 0.190, loss: 3062.995850\n",
      "Train: step:  62170, time: 0.219, loss: 1181.171509\n",
      "Train: step:  62180, time: 0.196, loss: 2592.392822\n",
      "Train: step:  62190, time: 0.197, loss: 370.133484\n",
      "Train: step:  62200, time: 0.185, loss: 1769.179321\n",
      "Train: step:  62210, time: 0.195, loss: 977.302185\n",
      "Train: step:  62220, time: 0.195, loss: 1920.526611\n",
      "Train: step:  62230, time: 0.191, loss: 727.595276\n",
      "Train: step:  62240, time: 0.189, loss: 1603.177124\n",
      "Train: step:  62250, time: 0.192, loss: 1952.643555\n",
      "Train: step:  62260, time: 0.191, loss: 2149.730957\n",
      "Train: step:  62270, time: 0.188, loss: 515.831726\n",
      "Train: step:  62280, time: 0.245, loss: 931.966248\n",
      "Train: step:  62290, time: 0.191, loss: 1552.457397\n",
      "Train: step:  62300, time: 0.215, loss: 1600.430420\n",
      "Train: step:  62310, time: 0.189, loss: 3921.962891\n",
      "Train: step:  62320, time: 0.215, loss: 1158.869141\n",
      "Train: step:  62330, time: 0.191, loss: 1762.895020\n",
      "Train: step:  62340, time: 0.199, loss: 1496.336670\n",
      "Train: step:  62350, time: 0.216, loss: 1943.903442\n",
      "Train: step:  62360, time: 0.214, loss: 1616.991333\n",
      "Train: step:  62370, time: 0.215, loss: 2309.700439\n",
      "Train: step:  62380, time: 0.194, loss: 1750.368042\n",
      "Train: step:  62390, time: 0.217, loss: 4341.892090\n",
      "Train: step:  62400, time: 0.186, loss: 2147.390381\n",
      "Train: step:  62410, time: 0.194, loss: 3024.160400\n",
      "Train: step:  62420, time: 0.185, loss: 1105.320435\n",
      "Train: step:  62430, time: 0.183, loss: 922.275513\n",
      "Train: step:  62440, time: 0.177, loss: 2371.118652\n",
      "Train: step:  62450, time: 0.194, loss: 1658.904663\n",
      "Train: step:  62460, time: 0.184, loss: 1658.653687\n",
      "Train: step:  62470, time: 0.186, loss: 2673.385010\n",
      "Train: step:  62480, time: 0.192, loss: 2006.789307\n",
      "Train: step:  62490, time: 0.188, loss: 1662.376343\n",
      "Train: step:  62500, time: 0.191, loss: 2229.479736\n",
      "Train: step:  62510, time: 0.195, loss: 870.700745\n",
      "Train: step:  62520, time: 0.195, loss: 2519.166504\n",
      "Train: step:  62530, time: 0.188, loss: 1814.196655\n",
      "Train: step:  62540, time: 0.192, loss: 1005.514282\n",
      "Train: step:  62550, time: 0.228, loss: 1607.729980\n",
      "Train: step:  62560, time: 0.188, loss: 1297.786865\n",
      "Train: step:  62570, time: 0.205, loss: 2703.406006\n",
      "Train: step:  62580, time: 0.185, loss: 1120.050659\n",
      "Train: step:  62590, time: 0.237, loss: 1649.048462\n",
      "Train: step:  62600, time: 0.191, loss: 1887.948364\n",
      "Train: step:  62610, time: 0.217, loss: 1668.227417\n",
      "Train: step:  62620, time: 0.192, loss: 2400.850830\n",
      "Train: step:  62630, time: 0.217, loss: 826.291748\n",
      "Train: step:  62640, time: 0.192, loss: 1709.181396\n",
      "Train: step:  62650, time: 0.225, loss: 1085.340576\n",
      "Train: step:  62660, time: 0.188, loss: 181.918182\n",
      "Train: step:  62670, time: 0.228, loss: 2102.288818\n",
      "Train: step:  62680, time: 0.183, loss: 775.610352\n",
      "Train: step:  62690, time: 0.233, loss: 1329.437378\n",
      "Train: step:  62700, time: 0.190, loss: 293.033234\n",
      "Train: step:  62710, time: 0.188, loss: 1707.283447\n",
      "Train: step:  62720, time: 0.191, loss: 1833.841797\n",
      "Train: step:  62730, time: 0.192, loss: 1947.244751\n",
      "Train: step:  62740, time: 0.184, loss: 948.773987\n",
      "Train: step:  62750, time: 0.209, loss: 3182.187256\n",
      "Train: step:  62760, time: 0.198, loss: 3117.373535\n",
      "Train: step:  62770, time: 0.195, loss: 2461.594727\n",
      "Train: step:  62780, time: 0.190, loss: 2049.699463\n",
      "Train: step:  62790, time: 0.187, loss: 249.554428\n",
      "Train: step:  62800, time: 0.188, loss: 3449.482666\n",
      "Train: step:  62810, time: 0.182, loss: 2729.956299\n",
      "Train: step:  62820, time: 0.244, loss: 983.803955\n",
      "Train: step:  62830, time: 0.193, loss: 1037.484375\n",
      "Train: step:  62840, time: 0.196, loss: 3083.325195\n",
      "Train: step:  62850, time: 0.197, loss: 209.755096\n",
      "Train: step:  62860, time: 0.214, loss: 4121.344238\n",
      "Train: step:  62870, time: 0.214, loss: 769.211853\n",
      "Train: step:  62880, time: 0.195, loss: 1746.133423\n",
      "Train: step:  62890, time: 0.194, loss: 1393.576294\n",
      "Train: step:  62900, time: 0.196, loss: 2021.135498\n",
      "Train: step:  62910, time: 0.191, loss: 2589.682617\n",
      "Train: step:  62920, time: 0.223, loss: 563.151184\n",
      "Train: step:  62930, time: 0.188, loss: 1857.436157\n",
      "Train: step:  62940, time: 0.189, loss: 2310.615967\n",
      "Train: step:  62950, time: 0.195, loss: 2754.813477\n",
      "Train: step:  62960, time: 0.191, loss: 3063.504150\n",
      "Train: step:  62970, time: 0.227, loss: 2436.766357\n",
      "Train: step:  62980, time: 0.192, loss: 1412.372070\n",
      "Train: step:  62990, time: 0.195, loss: 1498.751465\n",
      "Train: step:  63000, time: 0.192, loss: 1522.552490\n",
      "Train: step:  63010, time: 0.213, loss: 2659.057129\n",
      "Train: step:  63020, time: 0.189, loss: 1256.122192\n",
      "Train: step:  63030, time: 0.191, loss: 2376.648193\n",
      "Train: step:  63040, time: 0.188, loss: 2357.835449\n",
      "Train: step:  63050, time: 0.237, loss: 1849.981079\n",
      "Train: step:  63060, time: 0.189, loss: 1512.542480\n",
      "Train: step:  63070, time: 0.194, loss: 2303.667725\n",
      "Train: step:  63080, time: 0.189, loss: 3040.273438\n",
      "Train: step:  63090, time: 0.192, loss: 3752.237061\n",
      "Train: step:  63100, time: 0.187, loss: 1016.128052\n",
      "Train: step:  63110, time: 0.215, loss: 623.450928\n",
      "Train: step:  63120, time: 0.218, loss: 2029.783936\n",
      "Train: step:  63130, time: 0.190, loss: 612.949219\n",
      "Train: step:  63140, time: 0.191, loss: 2574.580811\n",
      "Train: step:  63150, time: 0.262, loss: 2233.742432\n",
      "Train: step:  63160, time: 0.201, loss: 2584.290039\n",
      "Train: step:  63170, time: 0.201, loss: 1693.796265\n",
      "Train: step:  63180, time: 0.193, loss: 812.090515\n",
      "Train: step:  63190, time: 0.216, loss: 2240.522461\n",
      "Train: step:  63200, time: 0.242, loss: 2739.997559\n",
      "Train: step:  63210, time: 0.193, loss: 573.073730\n",
      "Train: step:  63220, time: 0.217, loss: 980.717957\n",
      "Train: step:  63230, time: 0.186, loss: 819.475586\n",
      "Train: step:  63240, time: 0.188, loss: 1765.124390\n",
      "Train: step:  63250, time: 0.193, loss: 2127.486572\n",
      "Train: step:  63260, time: 0.195, loss: 1958.525146\n",
      "Train: step:  63270, time: 0.185, loss: 2887.052002\n",
      "Train: step:  63280, time: 0.191, loss: 1546.040649\n",
      "Train: step:  63290, time: 0.190, loss: 2097.744873\n",
      "Train: step:  63300, time: 0.220, loss: 1798.035034\n",
      "Train: step:  63310, time: 0.196, loss: 385.338379\n",
      "Train: step:  63320, time: 0.194, loss: 2900.856689\n",
      "Train: step:  63330, time: 0.217, loss: 1684.061035\n",
      "Train: step:  63340, time: 0.189, loss: 1372.757080\n",
      "Train: step:  63350, time: 0.192, loss: 1319.989014\n",
      "Train: step:  63360, time: 0.267, loss: 1020.796936\n",
      "Train: step:  63370, time: 0.189, loss: 646.643860\n",
      "Train: step:  63380, time: 0.218, loss: 1909.043213\n",
      "Train: step:  63390, time: 0.186, loss: 490.088287\n",
      "Train: step:  63400, time: 0.214, loss: 710.285095\n",
      "Train: step:  63410, time: 0.193, loss: 575.711670\n",
      "Train: step:  63420, time: 0.193, loss: 293.345001\n",
      "Train: step:  63430, time: 0.199, loss: 294.120148\n",
      "Train: step:  63440, time: 0.190, loss: 1587.929565\n",
      "Train: step:  63450, time: 0.191, loss: 2546.675781\n",
      "Train: step:  63460, time: 0.209, loss: 1366.623413\n",
      "Train: step:  63470, time: 0.223, loss: 897.804932\n",
      "Train: step:  63480, time: 0.205, loss: 2968.852783\n",
      "Train: step:  63490, time: 0.185, loss: 1033.397827\n",
      "Train: step:  63500, time: 0.214, loss: 2251.695557\n",
      "Train: step:  63510, time: 0.188, loss: 1257.089600\n",
      "Train: step:  63520, time: 0.231, loss: 1253.840088\n",
      "Train: step:  63530, time: 0.232, loss: 3397.633057\n",
      "Train: step:  63540, time: 0.188, loss: 772.633850\n",
      "Train: step:  63550, time: 0.234, loss: 2552.641602\n",
      "Train: step:  63560, time: 0.190, loss: 1793.086182\n",
      "Train: step:  63570, time: 0.185, loss: 720.305786\n",
      "Train: step:  63580, time: 0.212, loss: 2325.363037\n",
      "Train: step:  63590, time: 0.187, loss: 2393.135742\n",
      "Train: step:  63600, time: 0.188, loss: 853.956604\n",
      "Train: step:  63610, time: 0.188, loss: 3349.429199\n",
      "Train: step:  63620, time: 0.190, loss: 1301.273193\n",
      "Train: step:  63630, time: 0.189, loss: 1977.093140\n",
      "Train: step:  63640, time: 0.190, loss: 3042.881836\n",
      "Train: step:  63650, time: 0.205, loss: 1146.956055\n",
      "Train: step:  63660, time: 0.191, loss: 2166.660156\n",
      "Train: step:  63670, time: 0.188, loss: 1700.667358\n",
      "Train: step:  63680, time: 0.218, loss: 3820.472412\n",
      "Train: step:  63690, time: 0.257, loss: 1386.518433\n",
      "Train: step:  63700, time: 0.232, loss: 971.594971\n",
      "Train: step:  63710, time: 0.203, loss: 2856.152832\n",
      "Train: step:  63720, time: 0.195, loss: 1166.221558\n",
      "Train: step:  63730, time: 0.188, loss: 800.631531\n",
      "Train: step:  63740, time: 0.194, loss: 3358.687012\n",
      "Train: step:  63750, time: 0.211, loss: 791.856873\n",
      "Train: step:  63760, time: 0.200, loss: 1629.750488\n",
      "Train: step:  63770, time: 0.189, loss: 2578.944580\n",
      "Train: step:  63780, time: 0.208, loss: 1335.334229\n",
      "Train: step:  63790, time: 0.196, loss: 1474.175659\n",
      "Train: step:  63800, time: 0.193, loss: 3648.163086\n",
      "Train: step:  63810, time: 0.192, loss: 1605.879272\n",
      "Train: step:  63820, time: 0.231, loss: 2617.072998\n",
      "Train: step:  63830, time: 0.219, loss: 2016.078979\n",
      "Train: step:  63840, time: 0.228, loss: 1574.765259\n",
      "Train: step:  63850, time: 0.214, loss: 3704.135742\n",
      "Train: step:  63860, time: 0.188, loss: 3112.924561\n",
      "Train: step:  63870, time: 0.190, loss: 1887.875977\n",
      "Train: step:  63880, time: 0.193, loss: 1321.654297\n",
      "Train: step:  63890, time: 0.220, loss: 1670.036621\n",
      "Train: step:  63900, time: 0.189, loss: 2487.329346\n",
      "Train: step:  63910, time: 0.190, loss: 1746.202515\n",
      "Train: step:  63920, time: 0.192, loss: 2580.828857\n",
      "Train: step:  63930, time: 0.228, loss: 1507.879883\n",
      "Train: step:  63940, time: 0.236, loss: 3889.831543\n",
      "Train: step:  63950, time: 0.190, loss: 1252.815674\n",
      "Train: step:  63960, time: 0.192, loss: 1700.655396\n",
      "Train: step:  63970, time: 0.217, loss: 3307.012939\n",
      "Train: step:  63980, time: 0.188, loss: 1756.770874\n",
      "Train: step:  63990, time: 0.188, loss: 1223.343872\n",
      "Train: step:  64000, time: 0.216, loss: 809.051392\n",
      "Train: step:  64010, time: 0.191, loss: 2445.802490\n",
      "Train: step:  64020, time: 0.189, loss: 3124.777100\n",
      "Train: step:  64030, time: 0.198, loss: 1731.816650\n",
      "Train: step:  64040, time: 0.230, loss: 2020.357300\n",
      "Train: step:  64050, time: 0.228, loss: 3475.538330\n",
      "Train: step:  64060, time: 0.194, loss: 1941.690796\n",
      "Train: step:  64070, time: 0.187, loss: 942.998718\n",
      "Train: step:  64080, time: 0.204, loss: 2195.453857\n",
      "Train: step:  64090, time: 0.192, loss: 2416.936279\n",
      "Train: step:  64100, time: 0.217, loss: 3243.618896\n",
      "Train: step:  64110, time: 0.185, loss: 2858.798828\n",
      "Train: step:  64120, time: 0.196, loss: 2158.813477\n",
      "Train: step:  64130, time: 0.218, loss: 1119.570068\n",
      "Train: step:  64140, time: 0.191, loss: 2791.624023\n",
      "Train: step:  64150, time: 0.185, loss: 1805.137817\n",
      "Train: step:  64160, time: 0.183, loss: 2748.161377\n",
      "Train: step:  64170, time: 0.188, loss: 1578.916382\n",
      "Train: step:  64180, time: 0.182, loss: 492.046478\n",
      "Train: step:  64190, time: 0.207, loss: 2613.181641\n",
      "Train: step:  64200, time: 0.229, loss: 1397.463745\n",
      "Train: step:  64210, time: 0.191, loss: 2702.208252\n",
      "Train: step:  64220, time: 0.189, loss: 565.799988\n",
      "Train: step:  64230, time: 0.190, loss: 1701.527710\n",
      "Train: step:  64240, time: 0.185, loss: 1105.812378\n",
      "Train: step:  64250, time: 0.185, loss: 2579.796387\n",
      "Train: step:  64260, time: 0.226, loss: 192.667603\n",
      "Train: step:  64270, time: 0.199, loss: 217.452850\n",
      "Train: step:  64280, time: 0.215, loss: 3297.711182\n",
      "Train: step:  64290, time: 0.193, loss: 941.118225\n",
      "Train: step:  64300, time: 0.216, loss: 1635.494873\n",
      "Train: step:  64310, time: 0.194, loss: 674.120422\n",
      "Train: step:  64320, time: 0.190, loss: 2606.179932\n",
      "Train: step:  64330, time: 0.204, loss: 2272.297363\n",
      "Train: step:  64340, time: 0.186, loss: 1438.775513\n",
      "Train: step:  64350, time: 0.217, loss: 1033.267456\n",
      "Train: step:  64360, time: 0.191, loss: 1303.919800\n",
      "Train: step:  64370, time: 0.187, loss: 1996.057373\n",
      "Train: step:  64380, time: 0.233, loss: 1461.810059\n",
      "Train: step:  64390, time: 0.206, loss: 203.838440\n",
      "Train: step:  64400, time: 0.218, loss: 3568.726318\n",
      "Train: step:  64410, time: 0.196, loss: 1842.550659\n",
      "Train: step:  64420, time: 0.190, loss: 1473.656372\n",
      "Train: step:  64430, time: 0.182, loss: 1350.547729\n",
      "Train: step:  64440, time: 0.189, loss: 1710.316162\n",
      "Train: step:  64450, time: 0.203, loss: 1638.764771\n",
      "Train: step:  64460, time: 0.192, loss: 2737.770508\n",
      "Train: step:  64470, time: 0.193, loss: 3424.816650\n",
      "Train: step:  64480, time: 0.193, loss: 1241.027222\n",
      "Train: step:  64490, time: 0.190, loss: 1769.161987\n",
      "Train: step:  64500, time: 0.226, loss: 1373.140503\n",
      "Train: step:  64510, time: 0.183, loss: 1542.189453\n",
      "Train: step:  64520, time: 0.218, loss: 2812.169434\n",
      "Train: step:  64530, time: 0.226, loss: 393.181122\n",
      "Train: step:  64540, time: 0.218, loss: 1683.705200\n",
      "Train: step:  64550, time: 0.185, loss: 2160.577148\n",
      "Train: step:  64560, time: 0.188, loss: 1858.713257\n",
      "Train: step:  64570, time: 0.218, loss: 793.631653\n",
      "Train: step:  64580, time: 0.219, loss: 1955.468384\n",
      "Train: step:  64590, time: 0.188, loss: 2196.132324\n",
      "Train: step:  64600, time: 0.220, loss: 1964.513916\n",
      "Train: step:  64610, time: 0.231, loss: 2467.114990\n",
      "Train: step:  64620, time: 0.230, loss: 3416.635254\n",
      "Train: step:  64630, time: 0.186, loss: 744.695007\n",
      "Train: step:  64640, time: 0.209, loss: 1541.188110\n",
      "Train: step:  64650, time: 0.227, loss: 1787.871216\n",
      "Train: step:  64660, time: 0.191, loss: 2259.295654\n",
      "Train: step:  64670, time: 0.232, loss: 2088.381592\n",
      "Train: step:  64680, time: 0.199, loss: 3330.128906\n",
      "Train: step:  64690, time: 0.184, loss: 2575.400391\n",
      "Train: step:  64700, time: 0.191, loss: 1123.751465\n",
      "Train: step:  64710, time: 0.218, loss: 1212.331665\n",
      "Train: step:  64720, time: 0.187, loss: 242.855469\n",
      "Train: step:  64730, time: 0.196, loss: 1017.698364\n",
      "Train: step:  64740, time: 0.220, loss: 546.652832\n",
      "Train: step:  64750, time: 0.188, loss: 2001.407837\n",
      "Train: step:  64760, time: 0.189, loss: 1971.282349\n",
      "Train: step:  64770, time: 0.194, loss: 1525.064453\n",
      "Train: step:  64780, time: 0.191, loss: 2483.242676\n",
      "Train: step:  64790, time: 0.188, loss: 1139.525391\n",
      "Train: step:  64800, time: 0.190, loss: 1780.701782\n",
      "Train: step:  64810, time: 0.186, loss: 744.170776\n",
      "Train: step:  64820, time: 0.230, loss: 1010.581909\n",
      "Train: step:  64830, time: 0.200, loss: 913.909058\n",
      "Train: step:  64840, time: 0.195, loss: 1875.803101\n",
      "Train: step:  64850, time: 0.184, loss: 4252.541504\n",
      "Train: step:  64860, time: 0.192, loss: 3347.793945\n",
      "Train: step:  64870, time: 0.184, loss: 702.119263\n",
      "Train: step:  64880, time: 0.190, loss: 1923.373901\n",
      "Train: step:  64890, time: 0.197, loss: 531.699158\n",
      "Train: step:  64900, time: 0.183, loss: 465.420837\n",
      "Train: step:  64910, time: 0.190, loss: 2074.136963\n",
      "Train: step:  64920, time: 0.218, loss: 4541.703125\n",
      "Train: step:  64930, time: 0.192, loss: 428.899719\n",
      "Train: step:  64940, time: 0.187, loss: 455.690552\n",
      "Train: step:  64950, time: 0.185, loss: 2166.951904\n",
      "Train: step:  64960, time: 0.206, loss: 2927.599609\n",
      "Train: step:  64970, time: 0.186, loss: 1340.338501\n",
      "Train: step:  64980, time: 0.186, loss: 1336.819458\n",
      "Train: step:  64990, time: 0.215, loss: 1552.008301\n",
      "Train: step:  65000, time: 0.218, loss: 248.786957\n",
      "Train: step:  65010, time: 0.217, loss: 2493.456543\n",
      "Train: step:  65020, time: 0.187, loss: 870.388733\n",
      "Train: step:  65030, time: 0.214, loss: 795.885864\n",
      "Train: step:  65040, time: 0.185, loss: 2327.081055\n",
      "Train: step:  65050, time: 0.189, loss: 3031.542969\n",
      "Train: step:  65060, time: 0.196, loss: 766.656067\n",
      "Train: step:  65070, time: 0.194, loss: 2287.455811\n",
      "Train: step:  65080, time: 0.196, loss: 2399.906494\n",
      "Train: step:  65090, time: 0.236, loss: 2247.833740\n",
      "Train: step:  65100, time: 0.235, loss: 1670.940674\n",
      "Train: step:  65110, time: 0.187, loss: 1721.571045\n",
      "Train: step:  65120, time: 0.186, loss: 1075.699219\n",
      "Train: step:  65130, time: 0.184, loss: 1431.837524\n",
      "Train: step:  65140, time: 0.195, loss: 2951.681885\n",
      "Train: step:  65150, time: 0.189, loss: 2392.597412\n",
      "Train: step:  65160, time: 0.194, loss: 1813.623413\n",
      "Train: step:  65170, time: 0.186, loss: 2363.283936\n",
      "Train: step:  65180, time: 0.183, loss: 4766.600586\n",
      "Train: step:  65190, time: 0.184, loss: 710.961426\n",
      "Train: step:  65200, time: 0.186, loss: 2459.312500\n",
      "Train: step:  65210, time: 0.211, loss: 688.105103\n",
      "Train: step:  65220, time: 0.196, loss: 2828.273438\n",
      "Train: step:  65230, time: 0.188, loss: 1138.579102\n",
      "Train: step:  65240, time: 0.256, loss: 1573.327271\n",
      "Train: step:  65250, time: 0.208, loss: 638.314270\n",
      "Train: step:  65260, time: 0.192, loss: 2480.560791\n",
      "Train: step:  65270, time: 0.190, loss: 2202.350586\n",
      "Train: step:  65280, time: 0.185, loss: 2609.733643\n",
      "Train: step:  65290, time: 0.188, loss: 2285.896729\n",
      "Train: step:  65300, time: 0.187, loss: 1194.575439\n",
      "Train: step:  65310, time: 0.197, loss: 2777.734863\n",
      "Train: step:  65320, time: 0.232, loss: 1744.842407\n",
      "Train: step:  65330, time: 0.195, loss: 1476.913940\n",
      "Train: step:  65340, time: 0.197, loss: 1366.448975\n",
      "Train: step:  65350, time: 0.204, loss: 1787.085083\n",
      "Train: step:  65360, time: 0.220, loss: 1388.010132\n",
      "Train: step:  65370, time: 0.189, loss: 3128.955322\n",
      "Train: step:  65380, time: 0.195, loss: 1647.818726\n",
      "Train: step:  65390, time: 0.191, loss: 373.932587\n",
      "Train: step:  65400, time: 0.205, loss: 2984.544922\n",
      "Train: step:  65410, time: 0.200, loss: 1393.701050\n",
      "Train: step:  65420, time: 0.193, loss: 2185.696777\n",
      "Train: step:  65430, time: 0.183, loss: 855.569153\n",
      "Train: step:  65440, time: 0.189, loss: 1991.811523\n",
      "Train: step:  65450, time: 0.193, loss: 2532.139893\n",
      "Train: step:  65460, time: 0.231, loss: 1295.129272\n",
      "Train: step:  65470, time: 0.188, loss: 1981.637207\n",
      "Train: step:  65480, time: 0.201, loss: 1864.572998\n",
      "Train: step:  65490, time: 0.191, loss: 306.873169\n",
      "Train: step:  65500, time: 0.194, loss: 2740.488525\n",
      "Train: step:  65510, time: 0.193, loss: 2306.677979\n",
      "Train: step:  65520, time: 0.216, loss: 1011.640076\n",
      "Train: step:  65530, time: 0.195, loss: 2451.810547\n",
      "Train: step:  65540, time: 0.186, loss: 3185.160889\n",
      "Train: step:  65550, time: 0.186, loss: 1537.529541\n",
      "Train: step:  65560, time: 0.188, loss: 2462.212158\n",
      "Train: step:  65570, time: 0.193, loss: 3035.551758\n",
      "Train: step:  65580, time: 0.192, loss: 1882.635864\n",
      "Train: step:  65590, time: 0.227, loss: 1007.298218\n",
      "Train: step:  65600, time: 0.188, loss: 1239.882690\n",
      "Train: step:  65610, time: 0.215, loss: 2369.100830\n",
      "Train: step:  65620, time: 0.226, loss: 1637.497070\n",
      "Train: step:  65630, time: 0.190, loss: 2631.279541\n",
      "Train: step:  65640, time: 0.204, loss: 1659.201660\n",
      "Train: step:  65650, time: 0.195, loss: 437.418884\n",
      "Train: step:  65660, time: 0.197, loss: 3177.772217\n",
      "Train: step:  65670, time: 0.188, loss: 1923.827759\n",
      "Train: step:  65680, time: 0.189, loss: 1467.353027\n",
      "Train: step:  65690, time: 0.191, loss: 1439.773071\n",
      "Train: step:  65700, time: 0.198, loss: 3898.495850\n",
      "Train: step:  65710, time: 0.225, loss: 708.151184\n",
      "Train: step:  65720, time: 0.217, loss: 1543.229004\n",
      "Train: step:  65730, time: 0.229, loss: 230.064774\n",
      "Train: step:  65740, time: 0.186, loss: 1977.184204\n",
      "Train: step:  65750, time: 0.191, loss: 502.345337\n",
      "Train: step:  65760, time: 0.202, loss: 1006.063599\n",
      "Train: step:  65770, time: 0.218, loss: 2372.437012\n",
      "Train: step:  65780, time: 0.183, loss: 313.199646\n",
      "Train: step:  65790, time: 0.217, loss: 828.644104\n",
      "Train: step:  65800, time: 0.205, loss: 655.561890\n",
      "Train: step:  65810, time: 0.189, loss: 1597.322876\n",
      "Train: step:  65820, time: 0.226, loss: 3631.255615\n",
      "Train: step:  65830, time: 0.216, loss: 404.189697\n",
      "Train: step:  65840, time: 0.196, loss: 1802.258423\n",
      "Train: step:  65850, time: 0.220, loss: 2390.042725\n",
      "Train: step:  65860, time: 0.229, loss: 1630.764160\n",
      "Train: step:  65870, time: 0.194, loss: 1731.564087\n",
      "Train: step:  65880, time: 0.195, loss: 1783.336182\n",
      "Train: step:  65890, time: 0.193, loss: 875.187622\n",
      "Train: step:  65900, time: 0.187, loss: 983.304871\n",
      "Train: step:  65910, time: 0.216, loss: 1407.816772\n",
      "Train: step:  65920, time: 0.235, loss: 1696.951172\n",
      "Train: step:  65930, time: 0.201, loss: 2298.764160\n",
      "Train: step:  65940, time: 0.191, loss: 994.463074\n",
      "Train: step:  65950, time: 0.187, loss: 412.742676\n",
      "Train: step:  65960, time: 0.187, loss: 1511.583740\n",
      "Train: step:  65970, time: 0.231, loss: 3292.446533\n",
      "Train: step:  65980, time: 0.203, loss: 1901.315796\n",
      "Train: step:  65990, time: 0.183, loss: 3155.231201\n",
      "Train: step:  66000, time: 0.192, loss: 2126.393066\n",
      "Train: step:  66010, time: 0.189, loss: 924.249268\n",
      "Train: step:  66020, time: 0.188, loss: 2524.430664\n",
      "Train: step:  66030, time: 0.188, loss: 1120.288452\n",
      "Train: step:  66040, time: 0.182, loss: 1506.885742\n",
      "Train: step:  66050, time: 0.221, loss: 2099.306396\n",
      "Train: step:  66060, time: 0.193, loss: 706.599365\n",
      "Train: step:  66070, time: 0.204, loss: 2257.150391\n",
      "Train: step:  66080, time: 0.196, loss: 789.112671\n",
      "Train: step:  66090, time: 0.186, loss: 1329.278198\n",
      "Train: step:  66100, time: 0.228, loss: 1685.232056\n",
      "Train: step:  66110, time: 0.222, loss: 1683.107666\n",
      "Train: step:  66120, time: 0.202, loss: 1794.265259\n",
      "Train: step:  66130, time: 0.185, loss: 2025.430420\n",
      "Train: step:  66140, time: 0.218, loss: 2884.382080\n",
      "Train: step:  66150, time: 0.193, loss: 1648.917236\n",
      "Train: step:  66160, time: 0.223, loss: 997.460388\n",
      "Train: step:  66170, time: 0.217, loss: 1597.381348\n",
      "Train: step:  66180, time: 0.226, loss: 402.274475\n",
      "Train: step:  66190, time: 0.192, loss: 1403.409546\n",
      "Train: step:  66200, time: 0.216, loss: 1418.037354\n",
      "Train: step:  66210, time: 0.195, loss: 2471.953125\n",
      "Train: step:  66220, time: 0.226, loss: 2053.332520\n",
      "Train: step:  66230, time: 0.190, loss: 1178.875610\n",
      "Train: step:  66240, time: 0.249, loss: 2249.985840\n",
      "Train: step:  66250, time: 0.219, loss: 2690.038330\n",
      "Train: step:  66260, time: 0.194, loss: 2522.647461\n",
      "Train: step:  66270, time: 0.184, loss: 799.140808\n",
      "Train: step:  66280, time: 0.190, loss: 832.781494\n",
      "Train: step:  66290, time: 0.204, loss: 2128.896729\n",
      "Train: step:  66300, time: 0.223, loss: 2212.191406\n",
      "Train: step:  66310, time: 0.193, loss: 2579.908936\n",
      "Train: step:  66320, time: 0.185, loss: 2842.094482\n",
      "Train: step:  66330, time: 0.190, loss: 980.377930\n",
      "Train: step:  66340, time: 0.217, loss: 1353.498779\n",
      "Train: step:  66350, time: 0.218, loss: 1398.346680\n",
      "Train: step:  66360, time: 0.232, loss: 1811.831055\n",
      "Train: step:  66370, time: 0.229, loss: 1804.016357\n",
      "Train: step:  66380, time: 0.192, loss: 1916.931885\n",
      "Train: step:  66390, time: 0.197, loss: 654.906189\n",
      "Train: step:  66400, time: 0.198, loss: 899.041626\n",
      "Train: step:  66410, time: 0.227, loss: 1021.410339\n",
      "Train: step:  66420, time: 0.254, loss: 1023.470886\n",
      "Train: step:  66430, time: 0.207, loss: 1756.248413\n",
      "Train: step:  66440, time: 0.186, loss: 2535.145508\n",
      "Train: step:  66450, time: 0.186, loss: 2010.100586\n",
      "Train: step:  66460, time: 0.218, loss: 3372.215576\n",
      "Train: step:  66470, time: 0.192, loss: 3392.164307\n",
      "Train: step:  66480, time: 0.187, loss: 4561.216309\n",
      "Train: step:  66490, time: 0.190, loss: 2467.388428\n",
      "Train: step:  66500, time: 0.199, loss: 4714.573242\n",
      "Train: step:  66510, time: 0.217, loss: 1986.996094\n",
      "Train: step:  66520, time: 0.216, loss: 2401.806885\n",
      "Train: step:  66530, time: 0.187, loss: 1504.588745\n",
      "Train: step:  66540, time: 0.191, loss: 2025.394287\n",
      "Train: step:  66550, time: 0.183, loss: 2105.495361\n",
      "Train: step:  66560, time: 0.204, loss: 2950.591553\n",
      "Train: step:  66570, time: 0.204, loss: 2647.334473\n",
      "Train: step:  66580, time: 0.193, loss: 1955.461792\n",
      "Train: step:  66590, time: 0.187, loss: 2274.768555\n",
      "Train: step:  66600, time: 0.233, loss: 3481.978027\n",
      "Train: step:  66610, time: 0.185, loss: 2538.418945\n",
      "Train: step:  66620, time: 0.223, loss: 1040.968262\n",
      "Train: step:  66630, time: 0.199, loss: 3342.919922\n",
      "Train: step:  66640, time: 0.224, loss: 2035.294678\n",
      "Train: step:  66650, time: 0.195, loss: 1019.689697\n",
      "Train: step:  66660, time: 0.218, loss: 246.011810\n",
      "Train: step:  66670, time: 0.216, loss: 506.688782\n",
      "Train: step:  66680, time: 0.230, loss: 2494.277100\n",
      "Train: step:  66690, time: 0.209, loss: 1411.616821\n",
      "Train: step:  66700, time: 0.217, loss: 1939.915527\n",
      "Train: step:  66710, time: 0.187, loss: 2502.258545\n",
      "Train: step:  66720, time: 0.187, loss: 2290.064941\n",
      "Train: step:  66730, time: 0.222, loss: 2740.994873\n",
      "Train: step:  66740, time: 0.193, loss: 2225.032959\n",
      "Train: step:  66750, time: 0.189, loss: 2399.634766\n",
      "Train: step:  66760, time: 0.220, loss: 865.228821\n",
      "Train: step:  66770, time: 0.219, loss: 2291.255615\n",
      "Train: step:  66780, time: 0.228, loss: 1741.984375\n",
      "Train: step:  66790, time: 0.194, loss: 1854.974365\n",
      "Train: step:  66800, time: 0.192, loss: 1946.208740\n",
      "Train: step:  66810, time: 0.211, loss: 2748.585693\n",
      "Train: step:  66820, time: 0.193, loss: 549.492188\n",
      "Train: step:  66830, time: 0.196, loss: 1928.048950\n",
      "Train: step:  66840, time: 0.178, loss: 1859.760010\n",
      "Train: step:  66850, time: 0.217, loss: 1429.195435\n",
      "Train: step:  66860, time: 0.256, loss: 681.665527\n",
      "Train: step:  66870, time: 0.217, loss: 1423.452148\n",
      "Train: step:  66880, time: 0.188, loss: 1554.792725\n",
      "Train: step:  66890, time: 0.194, loss: 1166.881836\n",
      "Train: step:  66900, time: 0.196, loss: 581.353455\n",
      "Train: step:  66910, time: 0.201, loss: 1713.827148\n",
      "Train: step:  66920, time: 0.216, loss: 1018.509644\n",
      "Train: step:  66930, time: 0.194, loss: 2485.801025\n",
      "Train: step:  66940, time: 0.196, loss: 1939.340698\n",
      "Train: step:  66950, time: 0.188, loss: 644.569641\n",
      "Train: step:  66960, time: 0.196, loss: 1866.138428\n",
      "Train: step:  66970, time: 0.216, loss: 2582.557861\n",
      "Train: step:  66980, time: 0.218, loss: 2918.808838\n",
      "Train: step:  66990, time: 0.196, loss: 1801.183716\n",
      "Train: step:  67000, time: 0.226, loss: 1870.257812\n",
      "Train: step:  67010, time: 0.202, loss: 3151.263428\n",
      "Train: step:  67020, time: 0.196, loss: 2458.612061\n",
      "Train: step:  67030, time: 0.210, loss: 2924.476562\n",
      "Train: step:  67040, time: 0.227, loss: 2509.799072\n",
      "Train: step:  67050, time: 0.189, loss: 2875.989990\n",
      "Train: step:  67060, time: 0.192, loss: 866.129517\n",
      "Train: step:  67070, time: 0.231, loss: 1488.977661\n",
      "Train: step:  67080, time: 0.226, loss: 2134.538818\n",
      "Train: step:  67090, time: 0.195, loss: 2249.301025\n",
      "Train: step:  67100, time: 0.217, loss: 810.749878\n",
      "Train: step:  67110, time: 0.194, loss: 1620.893921\n",
      "Train: step:  67120, time: 0.200, loss: 1594.527466\n",
      "Train: step:  67130, time: 0.217, loss: 4075.903320\n",
      "Train: step:  67140, time: 0.247, loss: 2331.189453\n",
      "Train: step:  67150, time: 0.192, loss: 742.395874\n",
      "Train: step:  67160, time: 0.188, loss: 2526.798584\n",
      "Train: step:  67170, time: 0.197, loss: 2814.530518\n",
      "Train: step:  67180, time: 0.194, loss: 1219.423706\n",
      "Train: step:  67190, time: 0.233, loss: 1080.039795\n",
      "Train: step:  67200, time: 0.222, loss: 3463.794922\n",
      "Train: step:  67210, time: 0.188, loss: 809.491089\n",
      "Train: step:  67220, time: 0.187, loss: 1385.163452\n",
      "Train: step:  67230, time: 0.252, loss: 2438.378418\n",
      "Train: step:  67240, time: 0.193, loss: 743.938293\n",
      "Train: step:  67250, time: 0.181, loss: 1091.870361\n",
      "Train: step:  67260, time: 0.187, loss: 1916.309326\n",
      "Train: step:  67270, time: 0.186, loss: 1378.693237\n",
      "Train: step:  67280, time: 0.217, loss: 1060.111084\n",
      "Train: step:  67290, time: 0.184, loss: 1973.430542\n",
      "Train: step:  67300, time: 0.197, loss: 3143.818604\n",
      "Train: step:  67310, time: 0.197, loss: 447.544067\n",
      "Train: step:  67320, time: 0.193, loss: 950.280518\n",
      "Train: step:  67330, time: 0.191, loss: 4163.891602\n",
      "Train: step:  67340, time: 0.227, loss: 955.699707\n",
      "Train: step:  67350, time: 0.192, loss: 2121.407227\n",
      "Train: step:  67360, time: 0.193, loss: 2459.847656\n",
      "Train: step:  67370, time: 0.218, loss: 2220.975342\n",
      "Train: step:  67380, time: 0.206, loss: 571.078613\n",
      "Train: step:  67390, time: 0.211, loss: 246.439667\n",
      "Train: step:  67400, time: 0.192, loss: 1574.771729\n",
      "Train: step:  67410, time: 0.216, loss: 2595.036133\n",
      "Train: step:  67420, time: 0.187, loss: 498.360016\n",
      "Train: step:  67430, time: 0.219, loss: 1704.345947\n",
      "Train: step:  67440, time: 0.191, loss: 1151.798462\n",
      "Train: step:  67450, time: 0.196, loss: 1648.019287\n",
      "Train: step:  67460, time: 0.195, loss: 1830.593506\n",
      "Train: step:  67470, time: 0.188, loss: 3451.702881\n",
      "Train: step:  67480, time: 0.249, loss: 313.128510\n",
      "Train: step:  67490, time: 0.186, loss: 359.651855\n",
      "Train: step:  67500, time: 0.228, loss: 1711.759766\n",
      "Train: step:  67510, time: 0.192, loss: 387.910583\n",
      "Train: step:  67520, time: 0.184, loss: 2019.916138\n",
      "Train: step:  67530, time: 0.216, loss: 1691.346924\n",
      "Train: step:  67540, time: 0.227, loss: 2201.875977\n",
      "Train: step:  67550, time: 0.250, loss: 366.293091\n",
      "Train: step:  67560, time: 0.218, loss: 3362.391602\n",
      "Train: step:  67570, time: 0.225, loss: 1907.197632\n",
      "Train: step:  67580, time: 0.220, loss: 3200.776611\n",
      "Train: step:  67590, time: 0.191, loss: 3290.484375\n",
      "Train: step:  67600, time: 0.192, loss: 1742.373413\n",
      "Train: step:  67610, time: 0.244, loss: 2084.982910\n",
      "Train: step:  67620, time: 0.245, loss: 1915.925659\n",
      "Train: step:  67630, time: 0.219, loss: 3497.282227\n",
      "Train: step:  67640, time: 0.215, loss: 1998.358032\n",
      "Train: step:  67650, time: 0.217, loss: 1865.229370\n",
      "Train: step:  67660, time: 0.189, loss: 2284.319336\n",
      "Train: step:  67670, time: 0.187, loss: 3179.017334\n",
      "Train: step:  67680, time: 0.189, loss: 1245.489990\n",
      "Train: step:  67690, time: 0.230, loss: 2284.376953\n",
      "Train: step:  67700, time: 0.193, loss: 2044.053955\n",
      "Train: step:  67710, time: 0.212, loss: 1016.239136\n",
      "Train: step:  67720, time: 0.185, loss: 2472.163574\n",
      "Train: step:  67730, time: 0.229, loss: 1380.585327\n",
      "Train: step:  67740, time: 0.186, loss: 1715.189575\n",
      "Train: step:  67750, time: 0.263, loss: 224.989014\n",
      "Train: step:  67760, time: 0.186, loss: 406.569824\n",
      "Train: step:  67770, time: 0.185, loss: 3020.535400\n",
      "Train: step:  67780, time: 0.214, loss: 662.385864\n",
      "Train: step:  67790, time: 0.192, loss: 2285.198242\n",
      "Train: step:  67800, time: 0.183, loss: 2384.292236\n",
      "Train: step:  67810, time: 0.199, loss: 1491.121704\n",
      "Train: step:  67820, time: 0.218, loss: 2363.844971\n",
      "Train: step:  67830, time: 0.216, loss: 1885.758179\n",
      "Train: step:  67840, time: 0.199, loss: 1931.796143\n",
      "Train: step:  67850, time: 0.220, loss: 1125.477905\n",
      "Train: step:  67860, time: 0.185, loss: 666.312622\n",
      "Train: step:  67870, time: 0.194, loss: 173.574188\n",
      "Train: step:  67880, time: 0.226, loss: 2999.066162\n",
      "Train: step:  67890, time: 0.186, loss: 2212.463867\n",
      "Train: step:  67900, time: 0.218, loss: 2200.904541\n",
      "Train: step:  67910, time: 0.217, loss: 2005.208008\n",
      "Train: step:  67920, time: 0.186, loss: 2750.656494\n",
      "Train: step:  67930, time: 0.188, loss: 1717.603638\n",
      "Train: step:  67940, time: 0.197, loss: 3719.635010\n",
      "Train: step:  67950, time: 0.217, loss: 2867.670898\n",
      "Train: step:  67960, time: 0.185, loss: 2487.125488\n",
      "Train: step:  67970, time: 0.193, loss: 523.328796\n",
      "Train: step:  67980, time: 0.217, loss: 1589.288086\n",
      "Train: step:  67990, time: 0.237, loss: 1931.498535\n",
      "Train: step:  68000, time: 0.198, loss: 4391.562988\n",
      "Train: step:  68010, time: 0.185, loss: 1778.776001\n",
      "Train: step:  68020, time: 0.226, loss: 2012.781006\n",
      "Train: step:  68030, time: 0.216, loss: 1007.301086\n",
      "Train: step:  68040, time: 0.188, loss: 1762.369263\n",
      "Train: step:  68050, time: 0.180, loss: 2507.504639\n",
      "Train: step:  68060, time: 0.205, loss: 1115.988159\n",
      "Train: step:  68070, time: 0.203, loss: 2341.165771\n",
      "Train: step:  68080, time: 0.187, loss: 2404.651855\n",
      "Train: step:  68090, time: 0.196, loss: 2475.160400\n",
      "Train: step:  68100, time: 0.245, loss: 2015.093140\n",
      "Train: step:  68110, time: 0.236, loss: 3449.800049\n",
      "Train: step:  68120, time: 0.216, loss: 2198.287354\n",
      "Train: step:  68130, time: 0.186, loss: 2001.076172\n",
      "Train: step:  68140, time: 0.204, loss: 1715.979248\n",
      "Train: step:  68150, time: 0.217, loss: 3146.183594\n",
      "Train: step:  68160, time: 0.234, loss: 2215.721924\n",
      "Train: step:  68170, time: 0.228, loss: 2874.131836\n",
      "Train: step:  68180, time: 0.194, loss: 290.400696\n",
      "Train: step:  68190, time: 0.190, loss: 718.209229\n",
      "Train: step:  68200, time: 0.186, loss: 1008.312622\n",
      "Train: step:  68210, time: 0.217, loss: 2055.809326\n",
      "Train: step:  68220, time: 0.250, loss: 1530.774536\n",
      "Train: step:  68230, time: 0.216, loss: 3462.178955\n",
      "Train: step:  68240, time: 0.222, loss: 2278.646240\n",
      "Train: step:  68250, time: 0.227, loss: 1757.602295\n",
      "Train: step:  68260, time: 0.188, loss: 1839.616089\n",
      "Train: step:  68270, time: 0.227, loss: 843.645996\n",
      "Train: step:  68280, time: 0.186, loss: 2153.139648\n",
      "Train: step:  68290, time: 0.206, loss: 2629.132812\n",
      "Train: step:  68300, time: 0.188, loss: 2251.436279\n",
      "Train: step:  68310, time: 0.217, loss: 1697.144531\n",
      "Train: step:  68320, time: 0.187, loss: 1695.577271\n",
      "Train: step:  68330, time: 0.188, loss: 1369.872925\n",
      "Train: step:  68340, time: 0.185, loss: 2039.010010\n",
      "Train: step:  68350, time: 0.187, loss: 2224.681885\n",
      "Train: step:  68360, time: 0.244, loss: 2273.495850\n",
      "Train: step:  68370, time: 0.216, loss: 2950.961670\n",
      "Train: step:  68380, time: 0.223, loss: 3117.510742\n",
      "Train: step:  68390, time: 0.196, loss: 2699.194580\n",
      "Train: step:  68400, time: 0.191, loss: 2276.901611\n",
      "Train: step:  68410, time: 0.217, loss: 3051.977295\n",
      "Train: step:  68420, time: 0.186, loss: 2067.898193\n",
      "Train: step:  68430, time: 0.218, loss: 1813.245361\n",
      "Train: step:  68440, time: 0.189, loss: 2403.292480\n",
      "Train: step:  68450, time: 0.193, loss: 2478.171143\n",
      "Train: step:  68460, time: 0.196, loss: 1348.703247\n",
      "Train: step:  68470, time: 0.186, loss: 1358.558838\n",
      "Train: step:  68480, time: 0.183, loss: 1248.743286\n",
      "Train: step:  68490, time: 0.217, loss: 714.849670\n",
      "Train: step:  68500, time: 0.188, loss: 502.566772\n",
      "Train: step:  68510, time: 0.243, loss: 1428.245361\n",
      "Train: step:  68520, time: 0.197, loss: 634.692871\n",
      "Train: step:  68530, time: 0.189, loss: 2329.612305\n",
      "Train: step:  68540, time: 0.190, loss: 1051.377930\n",
      "Train: step:  68550, time: 0.194, loss: 3002.663818\n",
      "Train: step:  68560, time: 0.230, loss: 840.515686\n",
      "Train: step:  68570, time: 0.215, loss: 1529.932007\n",
      "Train: step:  68580, time: 0.188, loss: 2751.787354\n",
      "Train: step:  68590, time: 0.182, loss: 2929.955322\n",
      "Train: step:  68600, time: 0.191, loss: 2283.978516\n",
      "Train: step:  68610, time: 0.189, loss: 520.744812\n",
      "Train: step:  68620, time: 0.203, loss: 1845.464722\n",
      "Train: step:  68630, time: 0.188, loss: 1074.481201\n",
      "Train: step:  68640, time: 0.220, loss: 1035.877441\n",
      "Train: step:  68650, time: 0.205, loss: 2796.794922\n",
      "Train: step:  68660, time: 0.194, loss: 1614.195435\n",
      "Train: step:  68670, time: 0.218, loss: 1386.401978\n",
      "Train: step:  68680, time: 0.210, loss: 867.882812\n",
      "Train: step:  68690, time: 0.191, loss: 653.190308\n",
      "Train: step:  68700, time: 0.198, loss: 2485.240723\n",
      "Train: step:  68710, time: 0.190, loss: 1478.043701\n",
      "Train: step:  68720, time: 0.198, loss: 2656.405273\n",
      "Train: step:  68730, time: 0.189, loss: 1711.925049\n",
      "Train: step:  68740, time: 0.190, loss: 2088.093994\n",
      "Train: step:  68750, time: 0.186, loss: 682.590698\n",
      "Train: step:  68760, time: 0.188, loss: 199.259415\n",
      "Train: step:  68770, time: 0.194, loss: 2136.404541\n",
      "Train: step:  68780, time: 0.185, loss: 1976.991577\n",
      "Train: step:  68790, time: 0.191, loss: 1412.481445\n",
      "Train: step:  68800, time: 0.191, loss: 430.878723\n",
      "Train: step:  68810, time: 0.192, loss: 618.124878\n",
      "Train: step:  68820, time: 0.227, loss: 2260.437012\n",
      "Train: step:  68830, time: 0.192, loss: 3115.051514\n",
      "Train: step:  68840, time: 0.196, loss: 709.845642\n",
      "Train: step:  68850, time: 0.219, loss: 1944.550171\n",
      "Train: step:  68860, time: 0.191, loss: 1236.774536\n",
      "Train: step:  68870, time: 0.221, loss: 1897.628296\n",
      "Train: step:  68880, time: 0.192, loss: 1773.446289\n",
      "Train: step:  68890, time: 0.184, loss: 697.354736\n",
      "Train: step:  68900, time: 0.187, loss: 710.439514\n",
      "Train: step:  68910, time: 0.230, loss: 846.986694\n",
      "Train: step:  68920, time: 0.185, loss: 773.655151\n",
      "Train: step:  68930, time: 0.187, loss: 2221.002930\n",
      "Train: step:  68940, time: 0.188, loss: 1415.635132\n",
      "Train: step:  68950, time: 0.187, loss: 492.076782\n",
      "Train: step:  68960, time: 0.196, loss: 1263.872803\n",
      "Train: step:  68970, time: 0.193, loss: 2176.069580\n",
      "Train: step:  68980, time: 0.204, loss: 1612.893066\n",
      "Train: step:  68990, time: 0.186, loss: 1405.391968\n",
      "Train: step:  69000, time: 0.191, loss: 1324.886719\n",
      "Train: step:  69010, time: 0.185, loss: 350.466156\n",
      "Train: step:  69020, time: 0.191, loss: 2229.425537\n",
      "Train: step:  69030, time: 0.208, loss: 885.884766\n",
      "Train: step:  69040, time: 0.185, loss: 3656.047852\n",
      "Train: step:  69050, time: 0.185, loss: 3913.571533\n",
      "Train: step:  69060, time: 0.191, loss: 2417.943115\n",
      "Train: step:  69070, time: 0.193, loss: 2629.541260\n",
      "Train: step:  69080, time: 0.187, loss: 1627.382446\n",
      "Train: step:  69090, time: 0.196, loss: 2153.116455\n",
      "Train: step:  69100, time: 0.186, loss: 1262.328857\n",
      "Train: step:  69110, time: 0.194, loss: 1296.610840\n",
      "Train: step:  69120, time: 0.201, loss: 2961.810059\n",
      "Train: step:  69130, time: 0.191, loss: 2204.482910\n",
      "Train: step:  69140, time: 0.239, loss: 2254.124023\n",
      "Train: step:  69150, time: 0.191, loss: 1594.868042\n",
      "Train: step:  69160, time: 0.190, loss: 1409.148438\n",
      "Train: step:  69170, time: 0.230, loss: 2344.282715\n",
      "Train: step:  69180, time: 0.191, loss: 2760.928467\n",
      "Train: step:  69190, time: 0.192, loss: 992.356201\n",
      "Train: step:  69200, time: 0.194, loss: 1427.010132\n",
      "Train: step:  69210, time: 0.219, loss: 1671.965210\n",
      "Train: step:  69220, time: 0.220, loss: 3350.514648\n",
      "Train: step:  69230, time: 0.217, loss: 2249.008301\n",
      "Train: step:  69240, time: 0.217, loss: 1585.446899\n",
      "Train: step:  69250, time: 0.187, loss: 661.129395\n",
      "Train: step:  69260, time: 0.214, loss: 310.364288\n",
      "Train: step:  69270, time: 0.191, loss: 1207.405518\n",
      "Train: step:  69280, time: 0.228, loss: 1364.221313\n",
      "Train: step:  69290, time: 0.197, loss: 2656.104980\n",
      "Train: step:  69300, time: 0.196, loss: 523.412231\n",
      "Train: step:  69310, time: 0.187, loss: 2613.483887\n",
      "Train: step:  69320, time: 0.193, loss: 2145.348633\n",
      "Train: step:  69330, time: 0.181, loss: 517.083984\n",
      "Train: step:  69340, time: 0.214, loss: 2881.861572\n",
      "Train: step:  69350, time: 0.224, loss: 988.692505\n",
      "Train: step:  69360, time: 0.186, loss: 2711.147217\n",
      "Train: step:  69370, time: 0.188, loss: 2645.344727\n",
      "Train: step:  69380, time: 0.194, loss: 2278.320068\n",
      "Train: step:  69390, time: 0.187, loss: 712.838135\n",
      "Train: step:  69400, time: 0.214, loss: 2251.453369\n",
      "Train: step:  69410, time: 0.228, loss: 1689.863525\n",
      "Train: step:  69420, time: 0.216, loss: 2888.333740\n",
      "Train: step:  69430, time: 0.222, loss: 1135.854248\n",
      "Train: step:  69440, time: 0.188, loss: 995.476196\n",
      "Train: step:  69450, time: 0.188, loss: 3352.071289\n",
      "Train: step:  69460, time: 0.194, loss: 828.944458\n",
      "Train: step:  69470, time: 0.193, loss: 332.879364\n",
      "Train: step:  69480, time: 0.193, loss: 1718.922607\n",
      "Train: step:  69490, time: 0.231, loss: 393.562103\n",
      "Train: step:  69500, time: 0.225, loss: 2990.406982\n",
      "Train: step:  69510, time: 0.232, loss: 1053.992798\n",
      "Train: step:  69520, time: 0.228, loss: 2219.927490\n",
      "Train: step:  69530, time: 0.191, loss: 2430.157471\n",
      "Train: step:  69540, time: 0.190, loss: 2312.055420\n",
      "Train: step:  69550, time: 0.227, loss: 362.469330\n",
      "Train: step:  69560, time: 0.208, loss: 3014.382812\n",
      "Train: step:  69570, time: 0.186, loss: 2065.682373\n",
      "Train: step:  69580, time: 0.216, loss: 240.806610\n",
      "Train: step:  69590, time: 0.227, loss: 3531.291260\n",
      "Train: step:  69600, time: 0.182, loss: 922.426392\n",
      "Train: step:  69610, time: 0.215, loss: 2094.798584\n",
      "Train: step:  69620, time: 0.229, loss: 1412.550781\n",
      "Train: step:  69630, time: 0.217, loss: 2071.866455\n",
      "Train: step:  69640, time: 0.201, loss: 2264.095459\n",
      "Train: step:  69650, time: 0.196, loss: 2463.485107\n",
      "Train: step:  69660, time: 0.229, loss: 5839.833008\n",
      "Train: step:  69670, time: 0.190, loss: 3894.906982\n",
      "Train: step:  69680, time: 0.190, loss: 1742.956055\n",
      "Train: step:  69690, time: 0.191, loss: 2404.446533\n",
      "Train: step:  69700, time: 0.219, loss: 763.177734\n",
      "Train: step:  69710, time: 0.197, loss: 2305.101318\n",
      "Train: step:  69720, time: 0.218, loss: 2198.887695\n",
      "Train: step:  69730, time: 0.195, loss: 747.229309\n",
      "Train: step:  69740, time: 0.195, loss: 256.148010\n",
      "Train: step:  69750, time: 0.197, loss: 2624.324463\n",
      "Train: step:  69760, time: 0.192, loss: 2223.199707\n",
      "Train: step:  69770, time: 0.217, loss: 1787.138916\n",
      "Train: step:  69780, time: 0.217, loss: 342.585999\n",
      "Train: step:  69790, time: 0.186, loss: 2415.523682\n",
      "Train: step:  69800, time: 0.193, loss: 3075.407227\n",
      "Train: step:  69810, time: 0.192, loss: 2244.458740\n",
      "Train: step:  69820, time: 0.217, loss: 3099.806641\n",
      "Train: step:  69830, time: 0.191, loss: 346.994171\n",
      "Train: step:  69840, time: 0.186, loss: 1535.834961\n",
      "Train: step:  69850, time: 0.211, loss: 599.557190\n",
      "Train: step:  69860, time: 0.190, loss: 1108.443237\n",
      "Train: step:  69870, time: 0.192, loss: 1118.533691\n",
      "Train: step:  69880, time: 0.190, loss: 1269.300781\n",
      "Train: step:  69890, time: 0.227, loss: 2297.378906\n",
      "Train: step:  69900, time: 0.190, loss: 1139.147339\n",
      "Train: step:  69910, time: 0.196, loss: 2772.769043\n",
      "Train: step:  69920, time: 0.214, loss: 1620.636353\n",
      "Train: step:  69930, time: 0.183, loss: 943.943359\n",
      "Train: step:  69940, time: 0.189, loss: 2170.574219\n",
      "Train: step:  69950, time: 0.190, loss: 4999.960449\n",
      "Train: step:  69960, time: 0.185, loss: 2744.756836\n",
      "Train: step:  69970, time: 0.216, loss: 1259.053467\n",
      "Train: step:  69980, time: 0.189, loss: 2550.410156\n",
      "Train: step:  69990, time: 0.185, loss: 1850.836792\n",
      "Train: step:  70000, time: 0.218, loss: 3084.219727\n",
      "Train: step:  70010, time: 0.215, loss: 3906.391846\n",
      "Train: step:  70020, time: 0.247, loss: 137.961639\n",
      "Train: step:  70030, time: 0.219, loss: 464.108917\n",
      "Train: step:  70040, time: 0.189, loss: 1884.353394\n",
      "Train: step:  70050, time: 0.187, loss: 889.173035\n",
      "Train: step:  70060, time: 0.184, loss: 2119.073975\n",
      "Train: step:  70070, time: 0.193, loss: 1347.355469\n",
      "Train: step:  70080, time: 0.189, loss: 804.465942\n",
      "Train: step:  70090, time: 0.216, loss: 2181.974854\n",
      "Train: step:  70100, time: 0.209, loss: 1451.512207\n",
      "Train: step:  70110, time: 0.187, loss: 424.570221\n",
      "Train: step:  70120, time: 0.201, loss: 2071.937012\n",
      "Train: step:  70130, time: 0.197, loss: 457.098785\n",
      "Train: step:  70140, time: 0.186, loss: 1521.107910\n",
      "Train: step:  70150, time: 0.219, loss: 1637.248779\n",
      "Train: step:  70160, time: 0.189, loss: 2031.311646\n",
      "Train: step:  70170, time: 0.198, loss: 725.766235\n",
      "Train: step:  70180, time: 0.217, loss: 1515.611816\n",
      "Train: step:  70190, time: 0.196, loss: 1833.465942\n",
      "Train: step:  70200, time: 0.185, loss: 1605.544556\n",
      "Train: step:  70210, time: 0.189, loss: 2315.462646\n",
      "Train: step:  70220, time: 0.183, loss: 2421.676758\n",
      "Train: step:  70230, time: 0.227, loss: 2017.651001\n",
      "Train: step:  70240, time: 0.218, loss: 2120.913086\n",
      "Train: step:  70250, time: 0.194, loss: 1168.314697\n",
      "Train: step:  70260, time: 0.197, loss: 451.177765\n",
      "Train: step:  70270, time: 0.231, loss: 1339.851562\n",
      "Train: step:  70280, time: 0.187, loss: 1095.279663\n",
      "Train: step:  70290, time: 0.187, loss: 1289.231079\n",
      "Train: step:  70300, time: 0.182, loss: 1318.755859\n",
      "Train: step:  70310, time: 0.192, loss: 1866.493896\n",
      "Train: step:  70320, time: 0.218, loss: 734.657776\n",
      "Train: step:  70330, time: 0.198, loss: 1783.661255\n",
      "Train: step:  70340, time: 0.191, loss: 2092.852295\n",
      "Train: step:  70350, time: 0.193, loss: 2209.446777\n",
      "Train: step:  70360, time: 0.217, loss: 594.837219\n",
      "Train: step:  70370, time: 0.216, loss: 2284.094727\n",
      "Train: step:  70380, time: 0.188, loss: 2663.317627\n",
      "Train: step:  70390, time: 0.223, loss: 1372.353149\n",
      "Train: step:  70400, time: 0.216, loss: 2505.310547\n",
      "Train: step:  70410, time: 0.203, loss: 1102.198975\n",
      "Train: step:  70420, time: 0.228, loss: 1371.687744\n",
      "Train: step:  70430, time: 0.192, loss: 1930.995850\n",
      "Train: step:  70440, time: 0.186, loss: 3098.662598\n",
      "Train: step:  70450, time: 0.255, loss: 870.476990\n",
      "Train: step:  70460, time: 0.187, loss: 275.806641\n",
      "Train: step:  70470, time: 0.184, loss: 430.218384\n",
      "Train: step:  70480, time: 0.191, loss: 2786.871582\n",
      "Train: step:  70490, time: 0.216, loss: 911.090088\n",
      "Train: step:  70500, time: 0.215, loss: 2299.634277\n",
      "Train: step:  70510, time: 0.187, loss: 878.951599\n",
      "Train: step:  70520, time: 0.192, loss: 2993.200195\n",
      "Train: step:  70530, time: 0.217, loss: 1188.030762\n",
      "Train: step:  70540, time: 0.196, loss: 1986.710327\n",
      "Train: step:  70550, time: 0.227, loss: 1766.434082\n",
      "Train: step:  70560, time: 0.230, loss: 2945.494629\n",
      "Train: step:  70570, time: 0.185, loss: 1747.521240\n",
      "Train: step:  70580, time: 0.193, loss: 1014.586243\n",
      "Train: step:  70590, time: 0.194, loss: 1763.711548\n",
      "Train: step:  70600, time: 0.192, loss: 2387.253174\n",
      "Train: step:  70610, time: 0.228, loss: 1457.718384\n",
      "Train: step:  70620, time: 0.239, loss: 1425.473145\n",
      "Train: step:  70630, time: 0.228, loss: 1801.035156\n",
      "Train: step:  70640, time: 0.194, loss: 2090.759033\n",
      "Train: step:  70650, time: 0.229, loss: 727.280029\n",
      "Train: step:  70660, time: 0.193, loss: 1251.404785\n",
      "Train: step:  70670, time: 0.216, loss: 2149.015625\n",
      "Train: step:  70680, time: 0.187, loss: 4096.045898\n",
      "Train: step:  70690, time: 0.214, loss: 1840.767578\n",
      "Train: step:  70700, time: 0.187, loss: 2471.176514\n",
      "Train: step:  70710, time: 0.215, loss: 1859.527222\n",
      "Train: step:  70720, time: 0.234, loss: 3063.376465\n",
      "Train: step:  70730, time: 0.215, loss: 1851.026001\n",
      "Train: step:  70740, time: 0.190, loss: 1111.002441\n",
      "Train: step:  70750, time: 0.205, loss: 791.616638\n",
      "Train: step:  70760, time: 0.198, loss: 1406.023438\n",
      "Train: step:  70770, time: 0.238, loss: 2617.979980\n",
      "Train: step:  70780, time: 0.240, loss: 567.849548\n",
      "Train: step:  70790, time: 0.244, loss: 970.666382\n",
      "Train: step:  70800, time: 0.202, loss: 486.319275\n",
      "Train: step:  70810, time: 0.189, loss: 1341.260254\n",
      "Train: step:  70820, time: 0.191, loss: 2610.472656\n",
      "Train: step:  70830, time: 0.197, loss: 1179.040649\n",
      "Train: step:  70840, time: 0.215, loss: 2398.789551\n",
      "Train: step:  70850, time: 0.195, loss: 2756.883301\n",
      "Train: step:  70860, time: 0.191, loss: 1274.596436\n",
      "Train: step:  70870, time: 0.202, loss: 966.943420\n",
      "Train: step:  70880, time: 0.196, loss: 1690.513916\n",
      "Train: step:  70890, time: 0.193, loss: 2215.754395\n",
      "Train: step:  70900, time: 0.225, loss: 1728.958008\n",
      "Train: step:  70910, time: 0.193, loss: 1544.360352\n",
      "Train: step:  70920, time: 0.239, loss: 1521.905273\n",
      "Train: step:  70930, time: 0.188, loss: 2758.299072\n",
      "Train: step:  70940, time: 0.223, loss: 1250.269409\n",
      "Train: step:  70950, time: 0.221, loss: 2213.364990\n",
      "Train: step:  70960, time: 0.226, loss: 3151.085938\n",
      "Train: step:  70970, time: 0.198, loss: 1264.687378\n",
      "Train: step:  70980, time: 0.195, loss: 1984.644653\n",
      "Train: step:  70990, time: 0.197, loss: 932.716492\n",
      "Train: step:  71000, time: 0.224, loss: 935.730591\n",
      "Train: step:  71010, time: 0.209, loss: 3073.921143\n",
      "Train: step:  71020, time: 0.220, loss: 2214.592041\n",
      "Train: step:  71030, time: 0.209, loss: 632.403809\n",
      "Train: step:  71040, time: 0.198, loss: 1679.153320\n",
      "Train: step:  71050, time: 0.228, loss: 2332.882568\n",
      "Train: step:  71060, time: 0.193, loss: 2576.770996\n",
      "Train: step:  71070, time: 0.235, loss: 2275.933594\n",
      "Train: step:  71080, time: 0.224, loss: 403.376984\n",
      "Train: step:  71090, time: 0.236, loss: 1389.075317\n",
      "Train: step:  71100, time: 0.195, loss: 2932.412842\n",
      "Train: step:  71110, time: 0.200, loss: 2456.106934\n",
      "Train: step:  71120, time: 0.232, loss: 2730.943848\n",
      "Train: step:  71130, time: 0.220, loss: 1373.564941\n",
      "Train: step:  71140, time: 0.227, loss: 1981.411255\n",
      "Train: step:  71150, time: 0.192, loss: 1638.764526\n",
      "Train: step:  71160, time: 0.237, loss: 2390.061768\n",
      "Train: step:  71170, time: 0.208, loss: 2527.271484\n",
      "Train: step:  71180, time: 0.223, loss: 2441.303955\n",
      "Train: step:  71190, time: 0.233, loss: 2211.504395\n",
      "Train: step:  71200, time: 0.240, loss: 1210.956177\n",
      "Train: step:  71210, time: 0.199, loss: 428.096710\n",
      "Train: step:  71220, time: 0.194, loss: 2026.584961\n",
      "Train: step:  71230, time: 0.232, loss: 1586.730835\n",
      "Train: step:  71240, time: 0.237, loss: 1158.286743\n",
      "Train: step:  71250, time: 0.192, loss: 2013.730835\n",
      "Train: step:  71260, time: 0.201, loss: 2247.524658\n",
      "Train: step:  71270, time: 0.248, loss: 1814.402710\n",
      "Train: step:  71280, time: 0.200, loss: 1850.358765\n",
      "Train: step:  71290, time: 0.240, loss: 2185.406982\n",
      "Train: step:  71300, time: 0.238, loss: 1719.556030\n",
      "Train: step:  71310, time: 0.199, loss: 1416.975830\n",
      "Train: step:  71320, time: 0.204, loss: 405.922546\n",
      "Train: step:  71330, time: 0.194, loss: 3532.817871\n",
      "Train: step:  71340, time: 0.214, loss: 4067.221924\n",
      "Train: step:  71350, time: 0.192, loss: 1454.254639\n",
      "Train: step:  71360, time: 0.194, loss: 4093.534668\n",
      "Train: step:  71370, time: 0.196, loss: 1193.705933\n",
      "Train: step:  71380, time: 0.224, loss: 1078.583740\n",
      "Train: step:  71390, time: 0.224, loss: 2520.993652\n",
      "Train: step:  71400, time: 0.206, loss: 1894.656494\n",
      "Train: step:  71410, time: 0.213, loss: 1140.720825\n",
      "Train: step:  71420, time: 0.191, loss: 2427.064941\n",
      "Train: step:  71430, time: 0.193, loss: 2150.130615\n",
      "Train: step:  71440, time: 0.190, loss: 2240.984863\n",
      "Train: step:  71450, time: 0.190, loss: 2026.636597\n",
      "Train: step:  71460, time: 0.225, loss: 727.394531\n",
      "Train: step:  71470, time: 0.216, loss: 2160.185547\n",
      "Train: step:  71480, time: 0.197, loss: 719.939697\n",
      "Train: step:  71490, time: 0.229, loss: 1447.733398\n",
      "Train: step:  71500, time: 0.200, loss: 1161.691528\n",
      "Train: step:  71510, time: 0.194, loss: 1715.416260\n",
      "Train: step:  71520, time: 0.194, loss: 451.515900\n",
      "Train: step:  71530, time: 0.186, loss: 1597.246826\n",
      "Train: step:  71540, time: 0.194, loss: 2314.230225\n",
      "Train: step:  71550, time: 0.233, loss: 1677.730957\n",
      "Train: step:  71560, time: 0.195, loss: 745.871155\n",
      "Train: step:  71570, time: 0.205, loss: 2015.201050\n",
      "Train: step:  71580, time: 0.193, loss: 1220.864990\n",
      "Train: step:  71590, time: 0.234, loss: 1171.217529\n",
      "Train: step:  71600, time: 0.240, loss: 1712.557983\n",
      "Train: step:  71610, time: 0.196, loss: 823.644226\n",
      "Train: step:  71620, time: 0.192, loss: 1881.036011\n",
      "Train: step:  71630, time: 0.196, loss: 829.826721\n",
      "Train: step:  71640, time: 0.203, loss: 2556.407471\n",
      "Train: step:  71650, time: 0.220, loss: 983.231567\n",
      "Train: step:  71660, time: 0.190, loss: 778.113403\n",
      "Train: step:  71670, time: 0.244, loss: 3209.194580\n",
      "Train: step:  71680, time: 0.230, loss: 1381.120728\n",
      "Train: step:  71690, time: 0.227, loss: 2248.027344\n",
      "Train: step:  71700, time: 0.236, loss: 1827.025513\n",
      "Train: step:  71710, time: 0.187, loss: 1663.704102\n",
      "Train: step:  71720, time: 0.226, loss: 1249.715088\n",
      "Train: step:  71730, time: 0.215, loss: 1531.369019\n",
      "Train: step:  71740, time: 0.207, loss: 1702.902344\n",
      "Train: step:  71750, time: 0.196, loss: 1148.005371\n",
      "Train: step:  71760, time: 0.229, loss: 1404.336914\n",
      "Train: step:  71770, time: 0.197, loss: 822.623047\n",
      "Train: step:  71780, time: 0.197, loss: 2124.727295\n",
      "Train: step:  71790, time: 0.208, loss: 2859.085449\n",
      "Train: step:  71800, time: 0.202, loss: 1669.464355\n",
      "Train: step:  71810, time: 0.194, loss: 1665.927612\n",
      "Train: step:  71820, time: 0.190, loss: 1556.099854\n",
      "Train: step:  71830, time: 0.192, loss: 1751.291260\n",
      "Train: step:  71840, time: 0.226, loss: 2682.804688\n",
      "Train: step:  71850, time: 0.235, loss: 1355.912476\n",
      "Train: step:  71860, time: 0.197, loss: 466.480072\n",
      "Train: step:  71870, time: 0.192, loss: 2389.409180\n",
      "Train: step:  71880, time: 0.239, loss: 1539.411987\n",
      "Train: step:  71890, time: 0.201, loss: 1001.373901\n",
      "Train: step:  71900, time: 0.221, loss: 2804.174316\n",
      "Train: step:  71910, time: 0.224, loss: 750.605164\n",
      "Train: step:  71920, time: 0.198, loss: 1833.222046\n",
      "Train: step:  71930, time: 0.201, loss: 1876.338745\n",
      "Train: step:  71940, time: 0.257, loss: 1877.149414\n",
      "Train: step:  71950, time: 0.246, loss: 2547.222656\n",
      "Train: step:  71960, time: 0.215, loss: 1186.270386\n",
      "Train: step:  71970, time: 0.196, loss: 902.877197\n",
      "Train: step:  71980, time: 0.192, loss: 1202.488525\n",
      "Train: step:  71990, time: 0.198, loss: 1924.499268\n",
      "Train: step:  72000, time: 0.254, loss: 1007.153931\n",
      "Train: step:  72010, time: 0.189, loss: 2315.393311\n",
      "Train: step:  72020, time: 0.194, loss: 1714.918457\n",
      "Train: step:  72030, time: 0.195, loss: 2392.217041\n",
      "Train: step:  72040, time: 0.274, loss: 1853.666504\n",
      "Train: step:  72050, time: 0.194, loss: 1494.980103\n",
      "Train: step:  72060, time: 0.221, loss: 678.087402\n",
      "Train: step:  72070, time: 0.223, loss: 2828.110596\n",
      "Train: step:  72080, time: 0.224, loss: 2251.418945\n",
      "Train: step:  72090, time: 0.233, loss: 1213.870239\n",
      "Train: step:  72100, time: 0.201, loss: 1180.908691\n",
      "Train: step:  72110, time: 0.197, loss: 3906.100586\n",
      "Train: step:  72120, time: 0.190, loss: 1406.156738\n",
      "Train: step:  72130, time: 0.202, loss: 2401.508057\n",
      "Train: step:  72140, time: 0.236, loss: 1834.858643\n",
      "Train: step:  72150, time: 0.224, loss: 1790.513306\n",
      "Train: step:  72160, time: 0.219, loss: 1522.999390\n",
      "Train: step:  72170, time: 0.222, loss: 2203.325928\n",
      "Train: step:  72180, time: 0.226, loss: 1352.658447\n",
      "Train: step:  72190, time: 0.193, loss: 3142.381592\n",
      "Train: step:  72200, time: 0.203, loss: 2989.416504\n",
      "Train: step:  72210, time: 0.221, loss: 1410.326294\n",
      "Train: step:  72220, time: 0.223, loss: 376.473297\n",
      "Train: step:  72230, time: 0.219, loss: 489.867218\n",
      "Train: step:  72240, time: 0.197, loss: 1232.958862\n",
      "Train: step:  72250, time: 0.235, loss: 1802.429688\n",
      "Train: step:  72260, time: 0.220, loss: 1365.661133\n",
      "Train: step:  72270, time: 0.203, loss: 2568.939697\n",
      "Train: step:  72280, time: 0.197, loss: 2348.643799\n",
      "Train: step:  72290, time: 0.191, loss: 589.888428\n",
      "Train: step:  72300, time: 0.224, loss: 2232.640625\n",
      "Train: step:  72310, time: 0.193, loss: 2589.933350\n",
      "Train: step:  72320, time: 0.221, loss: 685.024231\n",
      "Train: step:  72330, time: 0.208, loss: 1086.253906\n",
      "Train: step:  72340, time: 0.203, loss: 842.380127\n",
      "Train: step:  72350, time: 0.201, loss: 1875.104614\n",
      "Train: step:  72360, time: 0.224, loss: 1915.977173\n",
      "Train: step:  72370, time: 0.193, loss: 1090.754272\n",
      "Train: step:  72380, time: 0.238, loss: 2775.456299\n",
      "Train: step:  72390, time: 0.217, loss: 1921.405884\n",
      "Train: step:  72400, time: 0.195, loss: 4613.275879\n",
      "Train: step:  72410, time: 0.217, loss: 1179.348877\n",
      "Train: step:  72420, time: 0.228, loss: 975.913147\n",
      "Train: step:  72430, time: 0.185, loss: 1418.711548\n",
      "Train: step:  72440, time: 0.188, loss: 322.998535\n",
      "Train: step:  72450, time: 0.192, loss: 424.105804\n",
      "Train: step:  72460, time: 0.194, loss: 1112.457153\n",
      "Train: step:  72470, time: 0.230, loss: 2585.182861\n",
      "Train: step:  72480, time: 0.199, loss: 3488.174316\n",
      "Train: step:  72490, time: 0.204, loss: 1100.692993\n",
      "Train: step:  72500, time: 0.184, loss: 1034.836914\n",
      "Train: step:  72510, time: 0.187, loss: 801.109009\n",
      "Train: step:  72520, time: 0.192, loss: 2809.061523\n",
      "Train: step:  72530, time: 0.188, loss: 1602.963867\n",
      "Train: step:  72540, time: 0.183, loss: 1648.032593\n",
      "Train: step:  72550, time: 0.193, loss: 1820.753662\n",
      "Train: step:  72560, time: 0.237, loss: 2484.830566\n",
      "Train: step:  72570, time: 0.185, loss: 1825.478149\n",
      "Train: step:  72580, time: 0.185, loss: 3127.768799\n",
      "Train: step:  72590, time: 0.231, loss: 2099.063232\n",
      "Train: step:  72600, time: 0.195, loss: 2220.373779\n",
      "Train: step:  72610, time: 0.190, loss: 974.840881\n",
      "Train: step:  72620, time: 0.191, loss: 1087.893799\n",
      "Train: step:  72630, time: 0.191, loss: 1506.889160\n",
      "Train: step:  72640, time: 0.229, loss: 2308.181152\n",
      "Train: step:  72650, time: 0.196, loss: 1361.146484\n",
      "Train: step:  72660, time: 0.233, loss: 1360.420410\n",
      "Train: step:  72670, time: 0.191, loss: 718.319153\n",
      "Train: step:  72680, time: 0.215, loss: 1692.726807\n",
      "Train: step:  72690, time: 0.186, loss: 1076.163574\n",
      "Train: step:  72700, time: 0.234, loss: 1148.130493\n",
      "Train: step:  72710, time: 0.193, loss: 2648.822754\n",
      "Train: step:  72720, time: 0.186, loss: 1226.266846\n",
      "Train: step:  72730, time: 0.189, loss: 1537.833008\n",
      "Train: step:  72740, time: 0.189, loss: 391.745972\n",
      "Train: step:  72750, time: 0.185, loss: 2526.320801\n",
      "Train: step:  72760, time: 0.185, loss: 1533.398926\n",
      "Train: step:  72770, time: 0.217, loss: 2200.170166\n",
      "Train: step:  72780, time: 0.185, loss: 1317.011963\n",
      "Train: step:  72790, time: 0.184, loss: 1762.662598\n",
      "Train: step:  72800, time: 0.217, loss: 2624.010986\n",
      "Train: step:  72810, time: 0.184, loss: 2537.852783\n",
      "Train: step:  72820, time: 0.217, loss: 1775.148071\n",
      "Train: step:  72830, time: 0.217, loss: 1567.524902\n",
      "Train: step:  72840, time: 0.189, loss: 1692.972290\n",
      "Train: step:  72850, time: 0.265, loss: 1734.228760\n",
      "Train: step:  72860, time: 0.191, loss: 2831.455566\n",
      "Train: step:  72870, time: 0.184, loss: 613.704346\n",
      "Train: step:  72880, time: 0.219, loss: 1195.754517\n",
      "Train: step:  72890, time: 0.188, loss: 459.793488\n",
      "Train: step:  72900, time: 0.224, loss: 300.010040\n",
      "Train: step:  72910, time: 0.182, loss: 2726.307373\n",
      "Train: step:  72920, time: 0.222, loss: 416.917603\n",
      "Train: step:  72930, time: 0.220, loss: 537.258118\n",
      "Train: step:  72940, time: 0.184, loss: 404.898285\n",
      "Train: step:  72950, time: 0.216, loss: 1261.526611\n",
      "Train: step:  72960, time: 0.188, loss: 1926.170044\n",
      "Train: step:  72970, time: 0.188, loss: 580.288696\n",
      "Train: step:  72980, time: 0.217, loss: 1479.993896\n",
      "Train: step:  72990, time: 0.188, loss: 1306.525024\n",
      "Train: step:  73000, time: 0.191, loss: 1190.038940\n",
      "Train: step:  73010, time: 0.192, loss: 1244.608276\n",
      "Train: step:  73020, time: 0.194, loss: 786.634705\n",
      "Train: step:  73030, time: 0.229, loss: 230.901077\n",
      "Train: step:  73040, time: 0.195, loss: 1306.674805\n",
      "Train: step:  73050, time: 0.216, loss: 1361.315552\n",
      "Train: step:  73060, time: 0.228, loss: 1760.698120\n",
      "Train: step:  73070, time: 0.227, loss: 1576.511108\n",
      "Train: step:  73080, time: 0.217, loss: 1284.423828\n",
      "Train: step:  73090, time: 0.219, loss: 884.726685\n",
      "Train: step:  73100, time: 0.217, loss: 754.418396\n",
      "Train: step:  73110, time: 0.199, loss: 636.667053\n",
      "Train: step:  73120, time: 0.188, loss: 883.238831\n",
      "Train: step:  73130, time: 0.214, loss: 785.758911\n",
      "Train: step:  73140, time: 0.216, loss: 1868.897827\n",
      "Train: step:  73150, time: 0.218, loss: 1932.965210\n",
      "Train: step:  73160, time: 0.193, loss: 2975.155273\n",
      "Train: step:  73170, time: 0.190, loss: 1481.342529\n",
      "Train: step:  73180, time: 0.218, loss: 319.589355\n",
      "Train: step:  73190, time: 0.191, loss: 1454.164307\n",
      "Train: step:  73200, time: 0.237, loss: 959.253418\n",
      "Train: step:  73210, time: 0.232, loss: 1613.634399\n",
      "Train: step:  73220, time: 0.213, loss: 884.988098\n",
      "Train: step:  73230, time: 0.206, loss: 1532.199463\n",
      "Train: step:  73240, time: 0.186, loss: 2492.254395\n",
      "Train: step:  73250, time: 0.213, loss: 2525.868652\n",
      "Train: step:  73260, time: 0.194, loss: 3399.306641\n",
      "Train: step:  73270, time: 0.193, loss: 625.149048\n",
      "Train: step:  73280, time: 0.228, loss: 1175.677612\n",
      "Train: step:  73290, time: 0.188, loss: 1006.839783\n",
      "Train: step:  73300, time: 0.192, loss: 829.329834\n",
      "Train: step:  73310, time: 0.193, loss: 2189.194824\n",
      "Train: step:  73320, time: 0.188, loss: 1454.090454\n",
      "Train: step:  73330, time: 0.190, loss: 1492.044312\n",
      "Train: step:  73340, time: 0.197, loss: 3361.995361\n",
      "Train: step:  73350, time: 0.194, loss: 2924.116455\n",
      "Train: step:  73360, time: 0.190, loss: 3350.997803\n",
      "Train: step:  73370, time: 0.223, loss: 544.802979\n",
      "Train: step:  73380, time: 0.183, loss: 3136.624268\n",
      "Train: step:  73390, time: 0.185, loss: 709.824158\n",
      "Train: step:  73400, time: 0.184, loss: 2014.665039\n",
      "Train: step:  73410, time: 0.187, loss: 974.150635\n",
      "Train: step:  73420, time: 0.183, loss: 1580.113281\n",
      "Train: step:  73430, time: 0.211, loss: 1735.794556\n",
      "Train: step:  73440, time: 0.196, loss: 2977.230225\n",
      "Train: step:  73450, time: 0.200, loss: 2978.945557\n",
      "Train: step:  73460, time: 0.189, loss: 4212.695312\n",
      "Train: step:  73470, time: 0.189, loss: 3089.167725\n",
      "Train: step:  73480, time: 0.190, loss: 1154.687744\n",
      "Train: step:  73490, time: 0.236, loss: 3126.416016\n",
      "Train: step:  73500, time: 0.227, loss: 3019.900635\n",
      "Train: step:  73510, time: 0.196, loss: 1586.633301\n",
      "Train: step:  73520, time: 0.189, loss: 3249.243408\n",
      "Train: step:  73530, time: 0.195, loss: 1648.423706\n",
      "Train: step:  73540, time: 0.216, loss: 2402.479980\n",
      "Train: step:  73550, time: 0.193, loss: 1426.440430\n",
      "Train: step:  73560, time: 0.187, loss: 1127.229248\n",
      "Train: step:  73570, time: 0.190, loss: 679.615051\n",
      "Train: step:  73580, time: 0.193, loss: 4197.847656\n",
      "Train: step:  73590, time: 0.230, loss: 1436.894043\n",
      "Train: step:  73600, time: 0.187, loss: 2594.343994\n",
      "Train: step:  73610, time: 0.206, loss: 2195.476807\n",
      "Train: step:  73620, time: 0.201, loss: 2595.537842\n",
      "Train: step:  73630, time: 0.192, loss: 614.321472\n",
      "Train: step:  73640, time: 0.195, loss: 413.687256\n",
      "Train: step:  73650, time: 0.188, loss: 1862.126831\n",
      "Train: step:  73660, time: 0.207, loss: 962.831177\n",
      "Train: step:  73670, time: 0.190, loss: 2378.991699\n",
      "Train: step:  73680, time: 0.190, loss: 1968.546021\n",
      "Train: step:  73690, time: 0.188, loss: 1519.393677\n",
      "Train: step:  73700, time: 0.190, loss: 170.958740\n",
      "Train: step:  73710, time: 0.188, loss: 2143.395020\n",
      "Train: step:  73720, time: 0.218, loss: 1425.362549\n",
      "Train: step:  73730, time: 0.223, loss: 1638.839600\n",
      "Train: step:  73740, time: 0.252, loss: 3607.027832\n",
      "Train: step:  73750, time: 0.194, loss: 1863.935425\n",
      "Train: step:  73760, time: 0.215, loss: 2531.058350\n",
      "Train: step:  73770, time: 0.194, loss: 1457.704956\n",
      "Train: step:  73780, time: 0.221, loss: 1547.307861\n",
      "Train: step:  73790, time: 0.186, loss: 2810.096680\n",
      "Train: step:  73800, time: 0.193, loss: 3933.754639\n",
      "Train: step:  73810, time: 0.186, loss: 342.933197\n",
      "Train: step:  73820, time: 0.217, loss: 1538.049805\n",
      "Train: step:  73830, time: 0.190, loss: 741.000793\n",
      "Train: step:  73840, time: 0.219, loss: 3918.486572\n",
      "Train: step:  73850, time: 0.190, loss: 2289.063965\n",
      "Train: step:  73860, time: 0.185, loss: 2423.443359\n",
      "Train: step:  73870, time: 0.187, loss: 3796.926025\n",
      "Train: step:  73880, time: 0.196, loss: 1235.263794\n",
      "Train: step:  73890, time: 0.216, loss: 1764.973267\n",
      "Train: step:  73900, time: 0.216, loss: 3147.714844\n",
      "Train: step:  73910, time: 0.187, loss: 2098.127197\n",
      "Train: step:  73920, time: 0.189, loss: 2070.220703\n",
      "Train: step:  73930, time: 0.218, loss: 1875.377563\n",
      "Train: step:  73940, time: 0.185, loss: 1046.346069\n",
      "Train: step:  73950, time: 0.187, loss: 1350.452026\n",
      "Train: step:  73960, time: 0.218, loss: 2457.275146\n",
      "Train: step:  73970, time: 0.219, loss: 1201.267578\n",
      "Train: step:  73980, time: 0.227, loss: 3254.454346\n",
      "Train: step:  73990, time: 0.223, loss: 420.820862\n",
      "Train: step:  74000, time: 0.190, loss: 1262.307129\n",
      "Train: step:  74010, time: 0.185, loss: 1432.094360\n",
      "Train: step:  74020, time: 0.187, loss: 1203.318115\n",
      "Train: step:  74030, time: 0.216, loss: 1298.096924\n",
      "Train: step:  74040, time: 0.195, loss: 1227.590820\n",
      "Train: step:  74050, time: 0.221, loss: 2319.869629\n",
      "Train: step:  74060, time: 0.216, loss: 1123.895752\n",
      "Train: step:  74070, time: 0.189, loss: 505.009827\n",
      "Train: step:  74080, time: 0.190, loss: 3237.900879\n",
      "Train: step:  74090, time: 0.189, loss: 1421.559692\n",
      "Train: step:  74100, time: 0.188, loss: 1406.091187\n",
      "Train: step:  74110, time: 0.195, loss: 1255.848633\n",
      "Train: step:  74120, time: 0.189, loss: 2534.595215\n",
      "Train: step:  74130, time: 0.191, loss: 2496.406494\n",
      "Train: step:  74140, time: 0.193, loss: 1722.513794\n",
      "Train: step:  74150, time: 0.196, loss: 2084.624023\n",
      "Train: step:  74160, time: 0.189, loss: 1220.427856\n",
      "Train: step:  74170, time: 0.194, loss: 936.153198\n",
      "Train: step:  74180, time: 0.229, loss: 1909.333862\n",
      "Train: step:  74190, time: 0.189, loss: 931.206055\n",
      "Train: step:  74200, time: 0.186, loss: 1467.263062\n",
      "Train: step:  74210, time: 0.184, loss: 3511.315918\n",
      "Train: step:  74220, time: 0.252, loss: 2478.079590\n",
      "Train: step:  74230, time: 0.193, loss: 1626.492554\n",
      "Train: step:  74240, time: 0.190, loss: 841.772705\n",
      "Train: step:  74250, time: 0.190, loss: 2058.405518\n",
      "Train: step:  74260, time: 0.225, loss: 660.067200\n",
      "Train: step:  74270, time: 0.190, loss: 1522.453125\n",
      "Train: step:  74280, time: 0.191, loss: 2360.371826\n",
      "Train: step:  74290, time: 0.185, loss: 2602.062012\n",
      "Train: step:  74300, time: 0.185, loss: 1528.613892\n",
      "Train: step:  74310, time: 0.196, loss: 708.936890\n",
      "Train: step:  74320, time: 0.191, loss: 1824.599243\n",
      "Train: step:  74330, time: 0.189, loss: 2324.512695\n",
      "Train: step:  74340, time: 0.184, loss: 1695.323975\n",
      "Train: step:  74350, time: 0.241, loss: 2198.408203\n",
      "Train: step:  74360, time: 0.216, loss: 1373.702148\n",
      "Train: step:  74370, time: 0.214, loss: 1531.423584\n",
      "Train: step:  74380, time: 0.217, loss: 1008.001953\n",
      "Train: step:  74390, time: 0.251, loss: 788.138428\n",
      "Train: step:  74400, time: 0.228, loss: 3208.054199\n",
      "Train: step:  74410, time: 0.187, loss: 3111.958008\n",
      "Train: step:  74420, time: 0.195, loss: 370.843964\n",
      "Train: step:  74430, time: 0.194, loss: 413.874451\n",
      "Train: step:  74440, time: 0.222, loss: 2481.164062\n",
      "Train: step:  74450, time: 0.196, loss: 1671.443726\n",
      "Train: step:  74460, time: 0.223, loss: 2512.115723\n",
      "Train: step:  74470, time: 0.190, loss: 169.300308\n",
      "Train: step:  74480, time: 0.225, loss: 2027.787354\n",
      "Train: step:  74490, time: 0.232, loss: 3082.541016\n",
      "Train: step:  74500, time: 0.186, loss: 1837.871216\n",
      "Train: step:  74510, time: 0.210, loss: 1477.292847\n",
      "Train: step:  74520, time: 0.217, loss: 1083.374268\n",
      "Train: step:  74530, time: 0.185, loss: 3837.804932\n",
      "Train: step:  74540, time: 0.193, loss: 234.335480\n",
      "Train: step:  74550, time: 0.226, loss: 3101.219482\n",
      "Train: step:  74560, time: 0.229, loss: 430.867706\n",
      "Train: step:  74570, time: 0.190, loss: 487.334442\n",
      "Train: step:  74580, time: 0.201, loss: 2527.524414\n",
      "Train: step:  74590, time: 0.212, loss: 2659.239014\n",
      "Train: step:  74600, time: 0.192, loss: 2399.684814\n",
      "Train: step:  74610, time: 0.227, loss: 2172.859619\n",
      "Train: step:  74620, time: 0.186, loss: 2931.583252\n",
      "Train: step:  74630, time: 0.188, loss: 1709.629150\n",
      "Train: step:  74640, time: 0.258, loss: 1935.249268\n",
      "Train: step:  74650, time: 0.226, loss: 2849.597656\n",
      "Train: step:  74660, time: 0.194, loss: 897.614319\n",
      "Train: step:  74670, time: 0.216, loss: 1844.223267\n",
      "Train: step:  74680, time: 0.190, loss: 2037.845825\n",
      "Train: step:  74690, time: 0.207, loss: 3620.121826\n",
      "Train: step:  74700, time: 0.202, loss: 1497.756104\n",
      "Train: step:  74710, time: 0.218, loss: 1496.173340\n",
      "Train: step:  74720, time: 0.188, loss: 3940.468506\n",
      "Train: step:  74730, time: 0.188, loss: 1492.632324\n",
      "Train: step:  74740, time: 0.225, loss: 2609.548828\n",
      "Train: step:  74750, time: 0.219, loss: 3651.402344\n",
      "Train: step:  74760, time: 0.219, loss: 3305.117920\n",
      "Train: step:  74770, time: 0.218, loss: 1350.543091\n",
      "Train: step:  74780, time: 0.189, loss: 3135.465088\n",
      "Train: step:  74790, time: 0.185, loss: 2676.705566\n",
      "Train: step:  74800, time: 0.218, loss: 1792.510742\n",
      "Train: step:  74810, time: 0.229, loss: 908.639160\n",
      "Train: step:  74820, time: 0.228, loss: 1930.272217\n",
      "Train: step:  74830, time: 0.222, loss: 2153.391602\n",
      "Train: step:  74840, time: 0.225, loss: 859.530762\n",
      "Train: step:  74850, time: 0.215, loss: 1326.832275\n",
      "Train: step:  74860, time: 0.235, loss: 536.204834\n",
      "Train: step:  74870, time: 0.189, loss: 1185.839844\n",
      "Train: step:  74880, time: 0.198, loss: 1267.328979\n",
      "Train: step:  74890, time: 0.219, loss: 2055.581543\n",
      "Train: step:  74900, time: 0.187, loss: 1613.437988\n",
      "Train: step:  74910, time: 0.218, loss: 814.527893\n",
      "Train: step:  74920, time: 0.230, loss: 3271.886230\n",
      "Train: step:  74930, time: 0.217, loss: 2676.619141\n",
      "Train: step:  74940, time: 0.189, loss: 298.924500\n",
      "Train: step:  74950, time: 0.191, loss: 596.468201\n",
      "Train: step:  74960, time: 0.190, loss: 580.968384\n",
      "Train: step:  74970, time: 0.216, loss: 1747.054932\n",
      "Train: step:  74980, time: 0.193, loss: 1836.761230\n",
      "Train: step:  74990, time: 0.206, loss: 3138.015381\n",
      "Train: step:  75000, time: 0.216, loss: 998.316284\n",
      "Train: step:  75010, time: 0.210, loss: 1925.084595\n",
      "Train: step:  75020, time: 0.220, loss: 1205.322144\n",
      "Train: step:  75030, time: 0.223, loss: 727.845337\n",
      "Train: step:  75040, time: 0.195, loss: 2562.218750\n",
      "Train: step:  75050, time: 0.235, loss: 1834.493774\n",
      "Train: step:  75060, time: 0.185, loss: 961.724548\n",
      "Train: step:  75070, time: 0.217, loss: 1170.559692\n",
      "Train: step:  75080, time: 0.198, loss: 1606.354614\n",
      "Train: step:  75090, time: 0.209, loss: 481.263550\n",
      "Train: step:  75100, time: 0.204, loss: 1461.645752\n",
      "Train: step:  75110, time: 0.183, loss: 3040.467529\n",
      "Train: step:  75120, time: 0.240, loss: 738.028320\n",
      "Train: step:  75130, time: 0.232, loss: 1144.193359\n",
      "Train: step:  75140, time: 0.188, loss: 2126.099854\n",
      "Train: step:  75150, time: 0.186, loss: 1237.440308\n",
      "Train: step:  75160, time: 0.239, loss: 3565.959961\n",
      "Train: step:  75170, time: 0.189, loss: 809.856934\n",
      "Train: step:  75180, time: 0.188, loss: 2206.887207\n",
      "Train: step:  75190, time: 0.216, loss: 1567.279053\n",
      "Train: step:  75200, time: 0.232, loss: 2393.788330\n",
      "Train: step:  75210, time: 0.232, loss: 2727.893555\n",
      "Train: step:  75220, time: 0.186, loss: 2555.164307\n",
      "Train: step:  75230, time: 0.185, loss: 1164.749023\n",
      "Train: step:  75240, time: 0.218, loss: 2245.078125\n",
      "Train: step:  75250, time: 0.187, loss: 1039.889038\n",
      "Train: step:  75260, time: 0.184, loss: 1849.302246\n",
      "Train: step:  75270, time: 0.185, loss: 982.072388\n",
      "Train: step:  75280, time: 0.208, loss: 2542.207275\n",
      "Train: step:  75290, time: 0.225, loss: 3245.511475\n",
      "Train: step:  75300, time: 0.192, loss: 430.558563\n",
      "Train: step:  75310, time: 0.187, loss: 1021.063049\n",
      "Train: step:  75320, time: 0.214, loss: 886.963745\n",
      "Train: step:  75330, time: 0.263, loss: 1292.260132\n",
      "Train: step:  75340, time: 0.194, loss: 1911.957886\n",
      "Train: step:  75350, time: 0.188, loss: 1845.521973\n",
      "Train: step:  75360, time: 0.224, loss: 1422.124146\n",
      "Train: step:  75370, time: 0.181, loss: 1464.773438\n",
      "Train: step:  75380, time: 0.189, loss: 1195.976074\n",
      "Train: step:  75390, time: 0.188, loss: 987.700317\n",
      "Train: step:  75400, time: 0.187, loss: 1839.018188\n",
      "Train: step:  75410, time: 0.190, loss: 3647.937744\n",
      "Train: step:  75420, time: 0.231, loss: 2848.717285\n",
      "Train: step:  75430, time: 0.189, loss: 362.352905\n",
      "Train: step:  75440, time: 0.192, loss: 2557.217529\n",
      "Train: step:  75450, time: 0.185, loss: 759.204773\n",
      "Train: step:  75460, time: 0.231, loss: 3038.192871\n",
      "Train: step:  75470, time: 0.188, loss: 3117.749268\n",
      "Train: step:  75480, time: 0.187, loss: 2359.360107\n",
      "Train: step:  75490, time: 0.188, loss: 2668.128906\n",
      "Train: step:  75500, time: 0.187, loss: 3353.252930\n",
      "Train: step:  75510, time: 0.193, loss: 3025.604492\n",
      "Train: step:  75520, time: 0.215, loss: 1215.504150\n",
      "Train: step:  75530, time: 0.212, loss: 1053.178223\n",
      "Train: step:  75540, time: 0.191, loss: 463.822784\n",
      "Train: step:  75550, time: 0.193, loss: 2074.369873\n",
      "Train: step:  75560, time: 0.186, loss: 1977.931030\n",
      "Train: step:  75570, time: 0.218, loss: 4673.041504\n",
      "Train: step:  75580, time: 0.239, loss: 1568.015503\n",
      "Train: step:  75590, time: 0.193, loss: 1508.901978\n",
      "Train: step:  75600, time: 0.188, loss: 2288.774902\n",
      "Train: step:  75610, time: 0.195, loss: 1348.225098\n",
      "Train: step:  75620, time: 0.183, loss: 1747.767700\n",
      "Train: step:  75630, time: 0.221, loss: 1403.369751\n",
      "Train: step:  75640, time: 0.188, loss: 3105.295166\n",
      "Train: step:  75650, time: 0.186, loss: 1129.158081\n",
      "Train: step:  75660, time: 0.261, loss: 650.651184\n",
      "Train: step:  75670, time: 0.217, loss: 3005.326904\n",
      "Train: step:  75680, time: 0.215, loss: 269.835632\n",
      "Train: step:  75690, time: 0.186, loss: 2945.497559\n",
      "Train: step:  75700, time: 0.190, loss: 1680.428467\n",
      "Train: step:  75710, time: 0.195, loss: 841.667419\n",
      "Train: step:  75720, time: 0.188, loss: 1968.779785\n",
      "Train: step:  75730, time: 0.188, loss: 1439.416870\n",
      "Train: step:  75740, time: 0.189, loss: 364.653839\n",
      "Train: step:  75750, time: 0.215, loss: 2044.659790\n",
      "Train: step:  75760, time: 0.195, loss: 3110.789307\n",
      "Train: step:  75770, time: 0.207, loss: 3096.175781\n",
      "Train: step:  75780, time: 0.233, loss: 379.688660\n",
      "Train: step:  75790, time: 0.194, loss: 2438.329346\n",
      "Train: step:  75800, time: 0.190, loss: 1062.930786\n",
      "Train: step:  75810, time: 0.186, loss: 2793.228027\n",
      "Train: step:  75820, time: 0.186, loss: 1615.151978\n",
      "Train: step:  75830, time: 0.190, loss: 2157.362793\n",
      "Train: step:  75840, time: 0.192, loss: 1487.845703\n",
      "Train: step:  75850, time: 0.185, loss: 1964.130859\n",
      "Train: step:  75860, time: 0.183, loss: 2654.242920\n",
      "Train: step:  75870, time: 0.187, loss: 4005.761475\n",
      "Train: step:  75880, time: 0.234, loss: 2345.178467\n",
      "Train: step:  75890, time: 0.192, loss: 2364.704590\n",
      "Train: step:  75900, time: 0.194, loss: 2475.258301\n",
      "Train: step:  75910, time: 0.198, loss: 741.734009\n",
      "Train: step:  75920, time: 0.193, loss: 1767.177490\n",
      "Train: step:  75930, time: 0.192, loss: 1234.093872\n",
      "Train: step:  75940, time: 0.191, loss: 1911.531738\n",
      "Train: step:  75950, time: 0.233, loss: 2308.163818\n",
      "Train: step:  75960, time: 0.190, loss: 2366.581787\n",
      "Train: step:  75970, time: 0.200, loss: 1581.674561\n",
      "Train: step:  75980, time: 0.190, loss: 1064.097412\n",
      "Train: step:  75990, time: 0.182, loss: 1375.651245\n",
      "Train: step:  76000, time: 0.201, loss: 3039.005127\n",
      "Train: step:  76010, time: 0.192, loss: 1399.476685\n",
      "Train: step:  76020, time: 0.197, loss: 1166.091431\n",
      "Train: step:  76030, time: 0.185, loss: 1624.695068\n",
      "Train: step:  76040, time: 0.231, loss: 2538.499023\n",
      "Train: step:  76050, time: 0.190, loss: 2788.968018\n",
      "Train: step:  76060, time: 0.192, loss: 1786.518066\n",
      "Train: step:  76070, time: 0.185, loss: 1798.414673\n",
      "Train: step:  76080, time: 0.217, loss: 657.201538\n",
      "Train: step:  76090, time: 0.217, loss: 3637.730469\n",
      "Train: step:  76100, time: 0.186, loss: 5733.399414\n",
      "Train: step:  76110, time: 0.187, loss: 530.463501\n",
      "Train: step:  76120, time: 0.188, loss: 2870.883545\n",
      "Train: step:  76130, time: 0.208, loss: 1825.467773\n",
      "Train: step:  76140, time: 0.180, loss: 853.328613\n",
      "Train: step:  76150, time: 0.225, loss: 2426.227051\n",
      "Train: step:  76160, time: 0.199, loss: 2113.132080\n",
      "Train: step:  76170, time: 0.190, loss: 622.108704\n",
      "Train: step:  76180, time: 0.188, loss: 2481.106934\n",
      "Train: step:  76190, time: 0.217, loss: 1884.800293\n",
      "Train: step:  76200, time: 0.187, loss: 2648.478760\n",
      "Train: step:  76210, time: 0.187, loss: 380.878571\n",
      "Train: step:  76220, time: 0.252, loss: 480.605255\n",
      "Train: step:  76230, time: 0.190, loss: 705.682922\n",
      "Train: step:  76240, time: 0.192, loss: 1936.034180\n",
      "Train: step:  76250, time: 0.187, loss: 2157.004883\n",
      "Train: step:  76260, time: 0.217, loss: 877.551331\n",
      "Train: step:  76270, time: 0.183, loss: 1607.688477\n",
      "Train: step:  76280, time: 0.187, loss: 365.060455\n",
      "Train: step:  76290, time: 0.220, loss: 452.307251\n",
      "Train: step:  76300, time: 0.187, loss: 2142.967041\n",
      "Train: step:  76310, time: 0.189, loss: 927.090149\n",
      "Train: step:  76320, time: 0.208, loss: 2486.164062\n",
      "Train: step:  76330, time: 0.194, loss: 2427.442139\n",
      "Train: step:  76340, time: 0.184, loss: 910.752502\n",
      "Train: step:  76350, time: 0.197, loss: 2744.980957\n",
      "Train: step:  76360, time: 0.213, loss: 859.751465\n",
      "Train: step:  76370, time: 0.187, loss: 1423.628906\n",
      "Train: step:  76380, time: 0.185, loss: 1739.123413\n",
      "Train: step:  76390, time: 0.218, loss: 1956.792114\n",
      "Train: step:  76400, time: 0.185, loss: 2092.129150\n",
      "Train: step:  76410, time: 0.184, loss: 1753.541626\n",
      "Train: step:  76420, time: 0.215, loss: 1354.040649\n",
      "Train: step:  76430, time: 0.227, loss: 1071.576172\n",
      "Train: step:  76440, time: 0.188, loss: 1523.653564\n",
      "Train: step:  76450, time: 0.185, loss: 2298.178223\n",
      "Train: step:  76460, time: 0.196, loss: 2643.214600\n",
      "Train: step:  76470, time: 0.218, loss: 1581.338379\n",
      "Train: step:  76480, time: 0.192, loss: 2448.059814\n",
      "Train: step:  76490, time: 0.212, loss: 5055.787598\n",
      "Train: step:  76500, time: 0.194, loss: 1468.173218\n",
      "Train: step:  76510, time: 0.186, loss: 1825.031006\n",
      "Train: step:  76520, time: 0.191, loss: 1319.674194\n",
      "Train: step:  76530, time: 0.216, loss: 1525.474976\n",
      "Train: step:  76540, time: 0.190, loss: 1424.577148\n",
      "Train: step:  76550, time: 0.187, loss: 2428.877197\n",
      "Train: step:  76560, time: 0.218, loss: 951.078125\n",
      "Train: step:  76570, time: 0.191, loss: 1063.369873\n",
      "Train: step:  76580, time: 0.227, loss: 761.127747\n",
      "Train: step:  76590, time: 0.192, loss: 4020.945312\n",
      "Train: step:  76600, time: 0.242, loss: 1714.459106\n",
      "Train: step:  76610, time: 0.188, loss: 2525.933350\n",
      "Train: step:  76620, time: 0.185, loss: 2305.695801\n",
      "Train: step:  76630, time: 0.187, loss: 708.998352\n",
      "Train: step:  76640, time: 0.213, loss: 1200.101318\n",
      "Train: step:  76650, time: 0.188, loss: 1506.118652\n",
      "Train: step:  76660, time: 0.191, loss: 2137.899414\n",
      "Train: step:  76670, time: 0.189, loss: 640.850708\n",
      "Train: step:  76680, time: 0.190, loss: 1484.513916\n",
      "Train: step:  76690, time: 0.188, loss: 1801.224609\n",
      "Train: step:  76700, time: 0.190, loss: 2240.338135\n",
      "Train: step:  76710, time: 0.219, loss: 2969.649170\n",
      "Train: step:  76720, time: 0.234, loss: 2300.186768\n",
      "Train: step:  76730, time: 0.190, loss: 1142.751587\n",
      "Train: step:  76740, time: 0.190, loss: 3032.632568\n",
      "Train: step:  76750, time: 0.241, loss: 1000.688538\n",
      "Train: step:  76760, time: 0.226, loss: 2091.828125\n",
      "Train: step:  76770, time: 0.190, loss: 2447.255615\n",
      "Train: step:  76780, time: 0.193, loss: 1880.322266\n",
      "Train: step:  76790, time: 0.193, loss: 2601.159912\n",
      "Train: step:  76800, time: 0.194, loss: 3010.908691\n",
      "Train: step:  76810, time: 0.197, loss: 1659.235596\n",
      "Train: step:  76820, time: 0.189, loss: 1677.097534\n",
      "Train: step:  76830, time: 0.188, loss: 734.418701\n",
      "Train: step:  76840, time: 0.189, loss: 2071.054932\n",
      "Train: step:  76850, time: 0.191, loss: 639.310791\n",
      "Train: step:  76860, time: 0.228, loss: 902.965454\n",
      "Train: step:  76870, time: 0.233, loss: 5388.519531\n",
      "Train: step:  76880, time: 0.192, loss: 1410.483521\n",
      "Train: step:  76890, time: 0.186, loss: 1322.179932\n",
      "Train: step:  76900, time: 0.216, loss: 985.757874\n",
      "Train: step:  76910, time: 0.201, loss: 2903.287109\n",
      "Train: step:  76920, time: 0.188, loss: 1756.208862\n",
      "Train: step:  76930, time: 0.186, loss: 554.966125\n",
      "Train: step:  76940, time: 0.192, loss: 3543.119141\n",
      "Train: step:  76950, time: 0.228, loss: 595.411621\n",
      "Train: step:  76960, time: 0.195, loss: 2225.076416\n",
      "Train: step:  76970, time: 0.190, loss: 2259.187744\n",
      "Train: step:  76980, time: 0.229, loss: 1831.292236\n",
      "Train: step:  76990, time: 0.218, loss: 1122.994995\n",
      "Train: step:  77000, time: 0.217, loss: 2246.469971\n",
      "Train: step:  77010, time: 0.215, loss: 1988.559204\n",
      "Train: step:  77020, time: 0.193, loss: 4080.925781\n",
      "Train: step:  77030, time: 0.189, loss: 4515.366211\n",
      "Train: step:  77040, time: 0.191, loss: 922.153137\n",
      "Train: step:  77050, time: 0.229, loss: 1319.674683\n",
      "Train: step:  77060, time: 0.233, loss: 1195.898315\n",
      "Train: step:  77070, time: 0.187, loss: 1625.944336\n",
      "Train: step:  77080, time: 0.216, loss: 1357.406494\n",
      "Train: step:  77090, time: 0.195, loss: 2105.592285\n",
      "Train: step:  77100, time: 0.197, loss: 1377.576660\n",
      "Train: step:  77110, time: 0.217, loss: 3089.890869\n",
      "Train: step:  77120, time: 0.228, loss: 632.140869\n",
      "Train: step:  77130, time: 0.217, loss: 1799.001953\n",
      "Train: step:  77140, time: 0.189, loss: 1661.955200\n",
      "Train: step:  77150, time: 0.226, loss: 480.298889\n",
      "Train: step:  77160, time: 0.220, loss: 2470.257812\n",
      "Train: step:  77170, time: 0.192, loss: 3055.779785\n",
      "Train: step:  77180, time: 0.193, loss: 502.065338\n",
      "Train: step:  77190, time: 0.191, loss: 2826.961914\n",
      "Train: step:  77200, time: 0.188, loss: 1465.234863\n",
      "Train: step:  77210, time: 0.226, loss: 665.467712\n",
      "Train: step:  77220, time: 0.186, loss: 1448.776123\n",
      "Train: step:  77230, time: 0.184, loss: 628.017883\n",
      "Train: step:  77240, time: 0.189, loss: 5423.892090\n",
      "Train: step:  77250, time: 0.186, loss: 910.752930\n",
      "Train: step:  77260, time: 0.189, loss: 1675.817993\n",
      "Train: step:  77270, time: 0.227, loss: 2828.009521\n",
      "Train: step:  77280, time: 0.194, loss: 2445.784180\n",
      "Train: step:  77290, time: 0.224, loss: 1076.553223\n",
      "Train: step:  77300, time: 0.193, loss: 2606.220459\n",
      "Train: step:  77310, time: 0.202, loss: 2053.156494\n",
      "Train: step:  77320, time: 0.200, loss: 3114.270752\n",
      "Train: step:  77330, time: 0.199, loss: 1589.754639\n",
      "Train: step:  77340, time: 0.217, loss: 2579.763184\n",
      "Train: step:  77350, time: 0.190, loss: 2177.681396\n",
      "Train: step:  77360, time: 0.209, loss: 2854.842773\n",
      "Train: step:  77370, time: 0.225, loss: 1311.584351\n",
      "Train: step:  77380, time: 0.224, loss: 546.825562\n",
      "Train: step:  77390, time: 0.192, loss: 1436.476440\n",
      "Train: step:  77400, time: 0.201, loss: 305.302795\n",
      "Train: step:  77410, time: 0.224, loss: 869.069702\n",
      "Train: step:  77420, time: 0.238, loss: 1654.681396\n",
      "Train: step:  77430, time: 0.233, loss: 1861.683716\n",
      "Train: step:  77440, time: 0.225, loss: 1523.930786\n",
      "Train: step:  77450, time: 0.222, loss: 1356.827759\n",
      "Train: step:  77460, time: 0.194, loss: 2317.723389\n",
      "Train: step:  77470, time: 0.213, loss: 1596.243896\n",
      "Train: step:  77480, time: 0.196, loss: 361.751556\n",
      "Train: step:  77490, time: 0.231, loss: 2508.882568\n",
      "Train: step:  77500, time: 0.188, loss: 2478.453125\n",
      "Train: step:  77510, time: 0.195, loss: 2871.208740\n",
      "Train: step:  77520, time: 0.222, loss: 2958.532715\n",
      "Train: step:  77530, time: 0.202, loss: 2082.359863\n",
      "Train: step:  77540, time: 0.191, loss: 248.268890\n",
      "Train: step:  77550, time: 0.201, loss: 2591.949951\n",
      "Train: step:  77560, time: 0.196, loss: 1285.223511\n",
      "Train: step:  77570, time: 0.190, loss: 2875.522461\n",
      "Train: step:  77580, time: 0.229, loss: 1114.166870\n",
      "Train: step:  77590, time: 0.193, loss: 1921.927490\n",
      "Train: step:  77600, time: 0.220, loss: 2850.927490\n",
      "Train: step:  77610, time: 0.207, loss: 1983.744385\n",
      "Train: step:  77620, time: 0.234, loss: 1114.048096\n",
      "Train: step:  77630, time: 0.218, loss: 1947.620850\n",
      "Train: step:  77640, time: 0.192, loss: 914.932251\n",
      "Train: step:  77650, time: 0.222, loss: 2904.266357\n",
      "Train: step:  77660, time: 0.226, loss: 2930.995361\n",
      "Train: step:  77670, time: 0.190, loss: 288.260437\n",
      "Train: step:  77680, time: 0.191, loss: 1089.691284\n",
      "Train: step:  77690, time: 0.190, loss: 1876.129272\n",
      "Train: step:  77700, time: 0.237, loss: 1606.588501\n",
      "Train: step:  77710, time: 0.225, loss: 1852.129639\n",
      "Train: step:  77720, time: 0.250, loss: 2296.333984\n",
      "Train: step:  77730, time: 0.232, loss: 2454.316895\n",
      "Train: step:  77740, time: 0.196, loss: 3110.233643\n",
      "Train: step:  77750, time: 0.194, loss: 1570.070190\n",
      "Train: step:  77760, time: 0.190, loss: 3446.188721\n",
      "Train: step:  77770, time: 0.231, loss: 2390.803711\n",
      "Train: step:  77780, time: 0.188, loss: 2337.142334\n",
      "Train: step:  77790, time: 0.239, loss: 639.155151\n",
      "Train: step:  77800, time: 0.195, loss: 935.339294\n",
      "Train: step:  77810, time: 0.200, loss: 1242.044312\n",
      "Train: step:  77820, time: 0.252, loss: 2545.947754\n",
      "Train: step:  77830, time: 0.195, loss: 2083.504150\n",
      "Train: step:  77840, time: 0.194, loss: 4722.836426\n",
      "Train: step:  77850, time: 0.187, loss: 1211.212158\n",
      "Train: step:  77860, time: 0.192, loss: 2639.485352\n",
      "Train: step:  77870, time: 0.190, loss: 2242.044189\n",
      "Train: step:  77880, time: 0.217, loss: 2039.960815\n",
      "Train: step:  77890, time: 0.223, loss: 1654.824341\n",
      "Train: step:  77900, time: 0.185, loss: 1551.462769\n",
      "Train: step:  77910, time: 0.184, loss: 3068.947266\n",
      "Train: step:  77920, time: 0.188, loss: 1718.206787\n",
      "Train: step:  77930, time: 0.226, loss: 1386.947998\n",
      "Train: step:  77940, time: 0.228, loss: 1994.817139\n",
      "Train: step:  77950, time: 0.224, loss: 2661.742432\n",
      "Train: step:  77960, time: 0.186, loss: 1001.085693\n",
      "Train: step:  77970, time: 0.187, loss: 1083.441406\n",
      "Train: step:  77980, time: 0.228, loss: 2066.275879\n",
      "Train: step:  77990, time: 0.217, loss: 4495.395996\n",
      "Train: step:  78000, time: 0.194, loss: 590.336060\n",
      "Train: step:  78010, time: 0.184, loss: 2195.537842\n",
      "Train: step:  78020, time: 0.203, loss: 1211.337280\n",
      "Train: step:  78030, time: 0.219, loss: 1466.987183\n",
      "Train: step:  78040, time: 0.189, loss: 1776.720337\n",
      "Train: step:  78050, time: 0.184, loss: 1101.466919\n",
      "Train: step:  78060, time: 0.227, loss: 2311.938232\n",
      "Train: step:  78070, time: 0.190, loss: 1321.999268\n",
      "Train: step:  78080, time: 0.186, loss: 1297.310059\n",
      "Train: step:  78090, time: 0.183, loss: 1294.534668\n",
      "Train: step:  78100, time: 0.217, loss: 2660.232666\n",
      "Train: step:  78110, time: 0.190, loss: 457.198914\n",
      "Train: step:  78120, time: 0.218, loss: 1498.904175\n",
      "Train: step:  78130, time: 0.187, loss: 1647.953369\n",
      "Train: step:  78140, time: 0.184, loss: 2301.814941\n",
      "Train: step:  78150, time: 0.209, loss: 2297.164307\n",
      "Train: step:  78160, time: 0.199, loss: 642.851379\n",
      "Train: step:  78170, time: 0.210, loss: 1762.717041\n",
      "Train: step:  78180, time: 0.190, loss: 685.797180\n",
      "Train: step:  78190, time: 0.201, loss: 1420.113892\n",
      "Train: step:  78200, time: 0.185, loss: 1636.051880\n",
      "Train: step:  78210, time: 0.217, loss: 2358.217041\n",
      "Train: step:  78220, time: 0.188, loss: 2754.806396\n",
      "Train: step:  78230, time: 0.202, loss: 745.650391\n",
      "Train: step:  78240, time: 0.194, loss: 2446.408447\n",
      "Train: step:  78250, time: 0.192, loss: 1566.235474\n",
      "Train: step:  78260, time: 0.185, loss: 2236.517578\n",
      "Train: step:  78270, time: 0.209, loss: 1283.834229\n",
      "Train: step:  78280, time: 0.229, loss: 1381.614502\n",
      "Train: step:  78290, time: 0.211, loss: 3221.369629\n",
      "Train: step:  78300, time: 0.184, loss: 1489.023438\n",
      "Train: step:  78310, time: 0.226, loss: 1408.648438\n",
      "Train: step:  78320, time: 0.226, loss: 1757.350708\n",
      "Train: step:  78330, time: 0.250, loss: 815.501160\n",
      "Train: step:  78340, time: 0.222, loss: 1315.821655\n",
      "Train: step:  78350, time: 0.185, loss: 1229.935303\n",
      "Train: step:  78360, time: 0.190, loss: 1812.893677\n",
      "Train: step:  78370, time: 0.238, loss: 2064.298340\n",
      "Train: step:  78380, time: 0.190, loss: 2408.950439\n",
      "Train: step:  78390, time: 0.215, loss: 2459.550781\n",
      "Train: step:  78400, time: 0.187, loss: 2631.213867\n",
      "Train: step:  78410, time: 0.217, loss: 1122.981201\n",
      "Train: step:  78420, time: 0.212, loss: 1744.674561\n",
      "Train: step:  78430, time: 0.238, loss: 2214.193115\n",
      "Train: step:  78440, time: 0.199, loss: 856.995056\n",
      "Train: step:  78450, time: 0.215, loss: 2615.597656\n",
      "Train: step:  78460, time: 0.219, loss: 472.278503\n",
      "Train: step:  78470, time: 0.219, loss: 2120.209961\n",
      "Train: step:  78480, time: 0.193, loss: 1428.840332\n",
      "Train: step:  78490, time: 0.187, loss: 2174.849121\n",
      "Train: step:  78500, time: 0.210, loss: 2394.298828\n",
      "Train: step:  78510, time: 0.219, loss: 1733.449219\n",
      "Train: step:  78520, time: 0.234, loss: 920.465881\n",
      "Train: step:  78530, time: 0.227, loss: 284.422516\n",
      "Train: step:  78540, time: 0.195, loss: 1576.492065\n",
      "Train: step:  78550, time: 0.196, loss: 1895.383057\n",
      "Train: step:  78560, time: 0.211, loss: 1483.765503\n",
      "Train: step:  78570, time: 0.187, loss: 864.792297\n",
      "Train: step:  78580, time: 0.216, loss: 1947.664307\n",
      "Train: step:  78590, time: 0.226, loss: 3021.220947\n",
      "Train: step:  78600, time: 0.218, loss: 1421.915039\n",
      "Train: step:  78610, time: 0.218, loss: 1115.327393\n",
      "Train: step:  78620, time: 0.192, loss: 1308.575562\n",
      "Train: step:  78630, time: 0.201, loss: 2066.666992\n",
      "Train: step:  78640, time: 0.190, loss: 693.183716\n",
      "Train: step:  78650, time: 0.235, loss: 815.809570\n",
      "Train: step:  78660, time: 0.192, loss: 1272.927368\n",
      "Train: step:  78670, time: 0.189, loss: 3746.375244\n",
      "Train: step:  78680, time: 0.218, loss: 416.043152\n",
      "Train: step:  78690, time: 0.189, loss: 1357.631348\n",
      "Train: step:  78700, time: 0.217, loss: 2027.009888\n",
      "Train: step:  78710, time: 0.216, loss: 1778.102051\n",
      "Train: step:  78720, time: 0.185, loss: 1057.081909\n",
      "Train: step:  78730, time: 0.215, loss: 1876.671753\n",
      "Train: step:  78740, time: 0.221, loss: 1066.205078\n",
      "Train: step:  78750, time: 0.189, loss: 1174.698853\n",
      "Train: step:  78760, time: 0.198, loss: 2510.921875\n",
      "Train: step:  78770, time: 0.187, loss: 2999.690430\n",
      "Train: step:  78780, time: 0.191, loss: 1159.889526\n",
      "Train: step:  78790, time: 0.192, loss: 1447.467163\n",
      "Train: step:  78800, time: 0.189, loss: 410.135468\n",
      "Train: step:  78810, time: 0.189, loss: 932.056946\n",
      "Train: step:  78820, time: 0.215, loss: 1378.473267\n",
      "Train: step:  78830, time: 0.228, loss: 2762.397217\n",
      "Train: step:  78840, time: 0.186, loss: 4715.928711\n",
      "Train: step:  78850, time: 0.186, loss: 2611.347412\n",
      "Train: step:  78860, time: 0.189, loss: 427.363556\n",
      "Train: step:  78870, time: 0.202, loss: 2112.279297\n",
      "Train: step:  78880, time: 0.210, loss: 1577.209351\n",
      "Train: step:  78890, time: 0.188, loss: 844.688538\n",
      "Train: step:  78900, time: 0.233, loss: 2173.599854\n",
      "Train: step:  78910, time: 0.218, loss: 1538.456543\n",
      "Train: step:  78920, time: 0.194, loss: 196.603073\n",
      "Train: step:  78930, time: 0.189, loss: 1167.962524\n",
      "Train: step:  78940, time: 0.218, loss: 2068.873047\n",
      "Train: step:  78950, time: 0.207, loss: 770.646851\n",
      "Train: step:  78960, time: 0.192, loss: 1852.513550\n",
      "Train: step:  78970, time: 0.218, loss: 1309.902222\n",
      "Train: step:  78980, time: 0.183, loss: 2530.423584\n",
      "Train: step:  78990, time: 0.206, loss: 2110.005859\n",
      "Train: step:  79000, time: 0.210, loss: 271.720490\n",
      "Train: step:  79010, time: 0.186, loss: 1013.108643\n",
      "Train: step:  79020, time: 0.193, loss: 2852.696289\n",
      "Train: step:  79030, time: 0.191, loss: 2207.950928\n",
      "Train: step:  79040, time: 0.207, loss: 2956.454834\n",
      "Train: step:  79050, time: 0.213, loss: 2821.618408\n",
      "Train: step:  79060, time: 0.221, loss: 1248.433838\n",
      "Train: step:  79070, time: 0.223, loss: 4364.280762\n",
      "Train: step:  79080, time: 0.188, loss: 2916.508301\n",
      "Train: step:  79090, time: 0.186, loss: 1380.108154\n",
      "Train: step:  79100, time: 0.220, loss: 1821.659668\n",
      "Train: step:  79110, time: 0.226, loss: 360.209625\n",
      "Train: step:  79120, time: 0.222, loss: 2647.738525\n",
      "Train: step:  79130, time: 0.210, loss: 1447.140869\n",
      "Train: step:  79140, time: 0.227, loss: 3232.939697\n",
      "Train: step:  79150, time: 0.216, loss: 1736.379517\n",
      "Train: step:  79160, time: 0.214, loss: 1452.846802\n",
      "Train: step:  79170, time: 0.248, loss: 842.625732\n",
      "Train: step:  79180, time: 0.229, loss: 2768.715576\n",
      "Train: step:  79190, time: 0.189, loss: 2125.099854\n",
      "Train: step:  79200, time: 0.227, loss: 1551.289673\n",
      "Train: step:  79210, time: 0.217, loss: 1618.457275\n",
      "Train: step:  79220, time: 0.217, loss: 2377.133057\n",
      "Train: step:  79230, time: 0.240, loss: 4122.284180\n",
      "Train: step:  79240, time: 0.191, loss: 2101.174072\n",
      "Train: step:  79250, time: 0.183, loss: 1238.905396\n",
      "Train: step:  79260, time: 0.238, loss: 648.480103\n",
      "Train: step:  79270, time: 0.192, loss: 1075.397583\n",
      "Train: step:  79280, time: 0.227, loss: 2108.329590\n",
      "Train: step:  79290, time: 0.243, loss: 1084.233521\n",
      "Train: step:  79300, time: 0.185, loss: 425.595306\n",
      "Train: step:  79310, time: 0.217, loss: 2649.637451\n",
      "Train: step:  79320, time: 0.189, loss: 2385.139404\n",
      "Train: step:  79330, time: 0.217, loss: 631.948792\n",
      "Train: step:  79340, time: 0.189, loss: 2813.123291\n",
      "Train: step:  79350, time: 0.219, loss: 3815.533203\n",
      "Train: step:  79360, time: 0.190, loss: 3151.802979\n",
      "Train: step:  79370, time: 0.234, loss: 658.990112\n",
      "Train: step:  79380, time: 0.191, loss: 2302.065430\n",
      "Train: step:  79390, time: 0.192, loss: 1870.608154\n",
      "Train: step:  79400, time: 0.194, loss: 1639.639282\n",
      "Train: step:  79410, time: 0.192, loss: 1424.376953\n",
      "Train: step:  79420, time: 0.193, loss: 667.277649\n",
      "Train: step:  79430, time: 0.194, loss: 2594.375244\n",
      "Train: step:  79440, time: 0.230, loss: 1984.223999\n",
      "Train: step:  79450, time: 0.217, loss: 1224.610840\n",
      "Train: step:  79460, time: 0.217, loss: 1749.891968\n",
      "Train: step:  79470, time: 0.216, loss: 3545.891846\n",
      "Train: step:  79480, time: 0.207, loss: 1877.753662\n",
      "Train: step:  79490, time: 0.217, loss: 1695.086792\n",
      "Train: step:  79500, time: 0.192, loss: 2554.756104\n",
      "Train: step:  79510, time: 0.192, loss: 3760.011230\n",
      "Train: step:  79520, time: 0.188, loss: 907.053528\n",
      "Train: step:  79530, time: 0.189, loss: 3313.561768\n",
      "Train: step:  79540, time: 0.239, loss: 3924.858398\n",
      "Train: step:  79550, time: 0.194, loss: 3728.683105\n",
      "Train: step:  79560, time: 0.225, loss: 1581.519653\n",
      "Train: step:  79570, time: 0.185, loss: 905.672729\n",
      "Train: step:  79580, time: 0.185, loss: 1037.123047\n",
      "Train: step:  79590, time: 0.191, loss: 1791.976074\n",
      "Train: step:  79600, time: 0.192, loss: 399.419189\n",
      "Train: step:  79610, time: 0.197, loss: 3133.814453\n",
      "Train: step:  79620, time: 0.191, loss: 2126.176514\n",
      "Train: step:  79630, time: 0.191, loss: 761.330750\n",
      "Train: step:  79640, time: 0.219, loss: 2649.158936\n",
      "Train: step:  79650, time: 0.250, loss: 1840.848389\n",
      "Train: step:  79660, time: 0.217, loss: 1144.608765\n",
      "Train: step:  79670, time: 0.192, loss: 1636.466919\n",
      "Train: step:  79680, time: 0.183, loss: 1402.670776\n",
      "Train: step:  79690, time: 0.216, loss: 2815.913330\n",
      "Train: step:  79700, time: 0.191, loss: 2669.880859\n",
      "Train: step:  79710, time: 0.193, loss: 1500.543579\n",
      "Train: step:  79720, time: 0.221, loss: 349.474182\n",
      "Train: step:  79730, time: 0.189, loss: 1560.553101\n",
      "Train: step:  79740, time: 0.186, loss: 1023.730347\n",
      "Train: step:  79750, time: 0.218, loss: 1012.681702\n",
      "Train: step:  79760, time: 0.188, loss: 486.427185\n",
      "Train: step:  79770, time: 0.200, loss: 1832.657104\n",
      "Train: step:  79780, time: 0.191, loss: 1966.140747\n",
      "Train: step:  79790, time: 0.188, loss: 1027.360962\n",
      "Train: step:  79800, time: 0.198, loss: 2435.591797\n",
      "Train: step:  79810, time: 0.227, loss: 2164.816406\n",
      "Train: step:  79820, time: 0.230, loss: 978.439148\n",
      "Train: step:  79830, time: 0.188, loss: 1097.017578\n",
      "Train: step:  79840, time: 0.190, loss: 1426.765503\n",
      "Train: step:  79850, time: 0.229, loss: 1571.180908\n",
      "Train: step:  79860, time: 0.227, loss: 2261.058105\n",
      "Train: step:  79870, time: 0.187, loss: 2339.770020\n",
      "Train: step:  79880, time: 0.187, loss: 3326.140137\n",
      "Train: step:  79890, time: 0.246, loss: 1876.507446\n",
      "Train: step:  79900, time: 0.231, loss: 5170.719238\n",
      "Train: step:  79910, time: 0.184, loss: 2110.870117\n",
      "Train: step:  79920, time: 0.216, loss: 2199.415039\n",
      "Train: step:  79930, time: 0.227, loss: 2608.863281\n",
      "Train: step:  79940, time: 0.194, loss: 1984.780029\n",
      "Train: step:  79950, time: 0.216, loss: 4280.469727\n",
      "Train: step:  79960, time: 0.216, loss: 960.305603\n",
      "Train: step:  79970, time: 0.189, loss: 633.291504\n",
      "Train: step:  79980, time: 0.189, loss: 2341.292969\n",
      "Train: step:  79990, time: 0.200, loss: 1640.098389\n",
      "Train: step:  80000, time: 0.227, loss: 1362.920044\n",
      "Train: step:  80010, time: 0.188, loss: 1165.818115\n",
      "Train: step:  80020, time: 0.190, loss: 1802.298584\n",
      "Train: step:  80030, time: 0.185, loss: 2156.520508\n",
      "Train: step:  80040, time: 0.184, loss: 921.182190\n",
      "Train: step:  80050, time: 0.190, loss: 3644.151367\n",
      "Train: step:  80060, time: 0.189, loss: 3953.519531\n",
      "Train: step:  80070, time: 0.189, loss: 1316.736694\n",
      "Train: step:  80080, time: 0.188, loss: 1441.276245\n",
      "Train: step:  80090, time: 0.193, loss: 695.849487\n",
      "Train: step:  80100, time: 0.188, loss: 2064.010498\n",
      "Train: step:  80110, time: 0.242, loss: 1404.004150\n",
      "Train: step:  80120, time: 0.194, loss: 2659.288574\n",
      "Train: step:  80130, time: 0.191, loss: 1194.584473\n",
      "Train: step:  80140, time: 0.191, loss: 2745.296631\n",
      "Train: step:  80150, time: 0.195, loss: 1642.752808\n",
      "Train: step:  80160, time: 0.248, loss: 2987.121094\n",
      "Train: step:  80170, time: 0.193, loss: 737.051758\n",
      "Train: step:  80180, time: 0.231, loss: 1591.390503\n",
      "Train: step:  80190, time: 0.198, loss: 2001.311646\n",
      "Train: step:  80200, time: 0.188, loss: 1876.804077\n",
      "Train: step:  80210, time: 0.219, loss: 567.953430\n",
      "Train: step:  80220, time: 0.218, loss: 1933.335083\n",
      "Train: step:  80230, time: 0.188, loss: 1064.962646\n",
      "Train: step:  80240, time: 0.195, loss: 1061.917236\n",
      "Train: step:  80250, time: 0.203, loss: 1494.781372\n",
      "Train: step:  80260, time: 0.191, loss: 2596.037598\n",
      "Train: step:  80270, time: 0.191, loss: 3772.526855\n",
      "Train: step:  80280, time: 0.188, loss: 1558.462158\n",
      "Train: step:  80290, time: 0.196, loss: 627.429626\n",
      "Train: step:  80300, time: 0.198, loss: 1371.110229\n",
      "Train: step:  80310, time: 0.193, loss: 1241.436646\n",
      "Train: step:  80320, time: 0.208, loss: 2000.037598\n",
      "Train: step:  80330, time: 0.223, loss: 2973.708496\n",
      "Train: step:  80340, time: 0.219, loss: 2737.289795\n",
      "Train: step:  80350, time: 0.252, loss: 1753.974609\n",
      "Train: step:  80360, time: 0.231, loss: 1898.247314\n",
      "Train: step:  80370, time: 0.217, loss: 2092.746582\n",
      "Train: step:  80380, time: 0.218, loss: 1772.263672\n",
      "Train: step:  80390, time: 0.195, loss: 1097.367920\n",
      "Train: step:  80400, time: 0.181, loss: 3723.597656\n",
      "Train: step:  80410, time: 0.189, loss: 2958.416992\n",
      "Train: step:  80420, time: 0.218, loss: 669.335083\n",
      "Train: step:  80430, time: 0.193, loss: 1384.849243\n",
      "Train: step:  80440, time: 0.192, loss: 698.584656\n",
      "Train: step:  80450, time: 0.192, loss: 952.684326\n",
      "Train: step:  80460, time: 0.187, loss: 500.125977\n",
      "Train: step:  80470, time: 0.189, loss: 1671.521240\n",
      "Train: step:  80480, time: 0.233, loss: 1449.475098\n",
      "Train: step:  80490, time: 0.244, loss: 948.828979\n",
      "Train: step:  80500, time: 0.216, loss: 2340.296875\n",
      "Train: step:  80510, time: 0.218, loss: 2780.042969\n",
      "Train: step:  80520, time: 0.191, loss: 734.822449\n",
      "Train: step:  80530, time: 0.223, loss: 1995.793945\n",
      "Train: step:  80540, time: 0.217, loss: 2935.153809\n",
      "Train: step:  80550, time: 0.214, loss: 804.470032\n",
      "Train: step:  80560, time: 0.217, loss: 2644.603760\n",
      "Train: step:  80570, time: 0.217, loss: 1906.672363\n",
      "Train: step:  80580, time: 0.197, loss: 1276.162231\n",
      "Train: step:  80590, time: 0.200, loss: 2250.836426\n",
      "Train: step:  80600, time: 0.257, loss: 342.899872\n",
      "Train: step:  80610, time: 0.187, loss: 2965.959473\n",
      "Train: step:  80620, time: 0.224, loss: 1819.530640\n",
      "Train: step:  80630, time: 0.192, loss: 2990.658691\n",
      "Train: step:  80640, time: 0.192, loss: 887.209778\n",
      "Train: step:  80650, time: 0.189, loss: 2670.705566\n",
      "Train: step:  80660, time: 0.217, loss: 2094.436035\n",
      "Train: step:  80670, time: 0.186, loss: 547.584900\n",
      "Train: step:  80680, time: 0.192, loss: 1658.870117\n",
      "Train: step:  80690, time: 0.204, loss: 2211.574219\n",
      "Train: step:  80700, time: 0.223, loss: 1749.722046\n",
      "Train: step:  80710, time: 0.205, loss: 3277.483154\n",
      "Train: step:  80720, time: 0.196, loss: 1618.728149\n",
      "Train: step:  80730, time: 0.196, loss: 2208.451416\n",
      "Train: step:  80740, time: 0.203, loss: 2032.648315\n",
      "Train: step:  80750, time: 0.224, loss: 1736.780029\n",
      "Train: step:  80760, time: 0.200, loss: 1082.187866\n",
      "Train: step:  80770, time: 0.193, loss: 1710.766357\n",
      "Train: step:  80780, time: 0.198, loss: 780.452271\n",
      "Train: step:  80790, time: 0.192, loss: 2290.914551\n",
      "Train: step:  80800, time: 0.226, loss: 968.711121\n",
      "Train: step:  80810, time: 0.203, loss: 2236.946289\n",
      "Train: step:  80820, time: 0.194, loss: 2819.411133\n",
      "Train: step:  80830, time: 0.188, loss: 692.602234\n",
      "Train: step:  80840, time: 0.193, loss: 2455.512695\n",
      "Train: step:  80850, time: 0.216, loss: 2655.975830\n",
      "Train: step:  80860, time: 0.225, loss: 2297.492432\n",
      "Train: step:  80870, time: 0.189, loss: 1956.511719\n",
      "Train: step:  80880, time: 0.206, loss: 1823.989624\n",
      "Train: step:  80890, time: 0.188, loss: 1729.666016\n",
      "Train: step:  80900, time: 0.226, loss: 2667.891113\n",
      "Train: step:  80910, time: 0.213, loss: 1602.622925\n",
      "Train: step:  80920, time: 0.193, loss: 1005.488464\n",
      "Train: step:  80930, time: 0.236, loss: 3178.834229\n",
      "Train: step:  80940, time: 0.193, loss: 2911.688477\n",
      "Train: step:  80950, time: 0.188, loss: 2188.461182\n",
      "Train: step:  80960, time: 0.229, loss: 3001.059814\n",
      "Train: step:  80970, time: 0.238, loss: 1439.565063\n",
      "Train: step:  80980, time: 0.194, loss: 1679.954224\n",
      "Train: step:  80990, time: 0.221, loss: 1226.016968\n",
      "Train: step:  81000, time: 0.190, loss: 1838.210327\n",
      "Train: step:  81010, time: 0.194, loss: 1896.669922\n",
      "Train: step:  81020, time: 0.220, loss: 3135.510254\n",
      "Train: step:  81030, time: 0.188, loss: 540.122314\n",
      "Train: step:  81040, time: 0.231, loss: 2567.915527\n",
      "Train: step:  81050, time: 0.232, loss: 1366.867798\n",
      "Train: step:  81060, time: 0.187, loss: 2926.493164\n",
      "Train: step:  81070, time: 0.189, loss: 1002.133484\n",
      "Train: step:  81080, time: 0.294, loss: 2036.176880\n",
      "Train: step:  81090, time: 0.220, loss: 2495.567627\n",
      "Train: step:  81100, time: 0.196, loss: 1738.604248\n",
      "Train: step:  81110, time: 0.200, loss: 2459.480957\n",
      "Train: step:  81120, time: 0.190, loss: 3021.622070\n",
      "Train: step:  81130, time: 0.231, loss: 1649.015137\n",
      "Train: step:  81140, time: 0.191, loss: 2753.464844\n",
      "Train: step:  81150, time: 0.193, loss: 706.472961\n",
      "Train: step:  81160, time: 0.185, loss: 974.793274\n",
      "Train: step:  81170, time: 0.193, loss: 2478.396973\n",
      "Train: step:  81180, time: 0.198, loss: 1805.307251\n",
      "Train: step:  81190, time: 0.232, loss: 3105.550781\n",
      "Train: step:  81200, time: 0.192, loss: 638.959778\n",
      "Train: step:  81210, time: 0.192, loss: 1390.942383\n",
      "Train: step:  81220, time: 0.202, loss: 707.316406\n",
      "Train: step:  81230, time: 0.192, loss: 2731.848145\n",
      "Train: step:  81240, time: 0.184, loss: 1237.178833\n",
      "Train: step:  81250, time: 0.219, loss: 324.802185\n",
      "Train: step:  81260, time: 0.249, loss: 640.412842\n",
      "Train: step:  81270, time: 0.220, loss: 710.203186\n",
      "Train: step:  81280, time: 0.189, loss: 1734.357544\n",
      "Train: step:  81290, time: 0.193, loss: 566.506714\n",
      "Train: step:  81300, time: 0.193, loss: 1269.469482\n",
      "Train: step:  81310, time: 0.188, loss: 1275.964600\n",
      "Train: step:  81320, time: 0.184, loss: 260.060974\n",
      "Train: step:  81330, time: 0.188, loss: 1104.100464\n",
      "Train: step:  81340, time: 0.230, loss: 961.529114\n",
      "Train: step:  81350, time: 0.231, loss: 1386.590576\n",
      "Train: step:  81360, time: 0.191, loss: 934.663940\n",
      "Train: step:  81370, time: 0.190, loss: 1909.567139\n",
      "Train: step:  81380, time: 0.195, loss: 1628.558472\n",
      "Train: step:  81390, time: 0.191, loss: 3928.142578\n",
      "Train: step:  81400, time: 0.217, loss: 2976.130371\n",
      "Train: step:  81410, time: 0.191, loss: 1019.578735\n",
      "Train: step:  81420, time: 0.187, loss: 2840.997803\n",
      "Train: step:  81430, time: 0.229, loss: 2426.204834\n",
      "Train: step:  81440, time: 0.221, loss: 1604.608398\n",
      "Train: step:  81450, time: 0.188, loss: 908.402039\n",
      "Train: step:  81460, time: 0.233, loss: 1105.344360\n",
      "Train: step:  81470, time: 0.207, loss: 2888.077881\n",
      "Train: step:  81480, time: 0.186, loss: 1358.952393\n",
      "Train: step:  81490, time: 0.191, loss: 1611.114258\n",
      "Train: step:  81500, time: 0.198, loss: 3771.756592\n",
      "Train: step:  81510, time: 0.189, loss: 1770.333496\n",
      "Train: step:  81520, time: 0.197, loss: 613.056091\n",
      "Train: step:  81530, time: 0.214, loss: 2521.800293\n",
      "Train: step:  81540, time: 0.196, loss: 1763.493164\n",
      "Train: step:  81550, time: 0.189, loss: 385.572693\n",
      "Train: step:  81560, time: 0.192, loss: 1904.577515\n",
      "Train: step:  81570, time: 0.188, loss: 1530.163086\n",
      "Train: step:  81580, time: 0.192, loss: 1267.940796\n",
      "Train: step:  81590, time: 0.228, loss: 4496.797852\n",
      "Train: step:  81600, time: 0.191, loss: 140.607651\n",
      "Train: step:  81610, time: 0.193, loss: 1456.271484\n",
      "Train: step:  81620, time: 0.226, loss: 1751.096802\n",
      "Train: step:  81630, time: 0.183, loss: 293.869659\n",
      "Train: step:  81640, time: 0.194, loss: 659.092651\n",
      "Train: step:  81650, time: 0.196, loss: 3245.597412\n",
      "Train: step:  81660, time: 0.220, loss: 752.566223\n",
      "Train: step:  81670, time: 0.237, loss: 1023.642151\n",
      "Train: step:  81680, time: 0.220, loss: 1998.154663\n",
      "Train: step:  81690, time: 0.188, loss: 2318.091797\n",
      "Train: step:  81700, time: 0.225, loss: 1247.035156\n",
      "Train: step:  81710, time: 0.196, loss: 443.876953\n",
      "Train: step:  81720, time: 0.199, loss: 2266.263428\n",
      "Train: step:  81730, time: 0.206, loss: 1521.210938\n",
      "Train: step:  81740, time: 0.219, loss: 2145.489746\n",
      "Train: step:  81750, time: 0.199, loss: 1353.472168\n",
      "Train: step:  81760, time: 0.221, loss: 1277.448242\n",
      "Train: step:  81770, time: 0.204, loss: 1916.783813\n",
      "Train: step:  81780, time: 0.223, loss: 1871.647583\n",
      "Train: step:  81790, time: 0.191, loss: 2373.620605\n",
      "Train: step:  81800, time: 0.265, loss: 1816.024780\n",
      "Train: step:  81810, time: 0.201, loss: 1649.644775\n",
      "Train: step:  81820, time: 0.194, loss: 2180.269043\n",
      "Train: step:  81830, time: 0.192, loss: 2869.995605\n",
      "Train: step:  81840, time: 0.191, loss: 1050.817505\n",
      "Train: step:  81850, time: 0.227, loss: 2485.250244\n",
      "Train: step:  81860, time: 0.196, loss: 2017.212036\n",
      "Train: step:  81870, time: 0.192, loss: 902.363770\n",
      "Train: step:  81880, time: 0.189, loss: 2835.925049\n",
      "Train: step:  81890, time: 0.220, loss: 1431.387451\n",
      "Train: step:  81900, time: 0.190, loss: 1025.804565\n",
      "Train: step:  81910, time: 0.195, loss: 575.130737\n",
      "Train: step:  81920, time: 0.190, loss: 1497.583374\n",
      "Train: step:  81930, time: 0.186, loss: 1791.773438\n",
      "Train: step:  81940, time: 0.195, loss: 4050.411621\n",
      "Train: step:  81950, time: 0.190, loss: 2372.967041\n",
      "Train: step:  81960, time: 0.191, loss: 1213.020020\n",
      "Train: step:  81970, time: 0.191, loss: 1210.844604\n",
      "Train: step:  81980, time: 0.219, loss: 932.509277\n",
      "Train: step:  81990, time: 0.216, loss: 2497.060791\n",
      "Train: step:  82000, time: 0.221, loss: 305.615814\n",
      "Train: step:  82010, time: 0.189, loss: 1704.449829\n",
      "Train: step:  82020, time: 0.218, loss: 208.794678\n",
      "Train: step:  82030, time: 0.192, loss: 446.895935\n",
      "Train: step:  82040, time: 0.189, loss: 2392.571533\n",
      "Train: step:  82050, time: 0.233, loss: 2620.484375\n",
      "Train: step:  82060, time: 0.221, loss: 1894.529297\n",
      "Train: step:  82070, time: 0.192, loss: 2480.614990\n",
      "Train: step:  82080, time: 0.199, loss: 1026.680908\n",
      "Train: step:  82090, time: 0.188, loss: 1525.163696\n",
      "Train: step:  82100, time: 0.193, loss: 1898.856079\n",
      "Train: step:  82110, time: 0.187, loss: 1155.063477\n",
      "Train: step:  82120, time: 0.195, loss: 3307.963623\n",
      "Train: step:  82130, time: 0.196, loss: 2476.464111\n",
      "Train: step:  82140, time: 0.197, loss: 556.201843\n",
      "Train: step:  82150, time: 0.195, loss: 1838.978882\n",
      "Train: step:  82160, time: 0.195, loss: 834.625488\n",
      "Train: step:  82170, time: 0.185, loss: 713.333740\n",
      "Train: step:  82180, time: 0.195, loss: 349.166077\n",
      "Train: step:  82190, time: 0.194, loss: 3533.680908\n",
      "Train: step:  82200, time: 0.191, loss: 2656.476318\n",
      "Train: step:  82210, time: 0.228, loss: 2591.948975\n",
      "Train: step:  82220, time: 0.235, loss: 1060.871582\n",
      "Train: step:  82230, time: 0.226, loss: 481.886536\n",
      "Train: step:  82240, time: 0.237, loss: 2110.465576\n",
      "Train: step:  82250, time: 0.225, loss: 2748.080566\n",
      "Train: step:  82260, time: 0.185, loss: 3127.606201\n",
      "Train: step:  82270, time: 0.189, loss: 2310.454102\n",
      "Train: step:  82280, time: 0.183, loss: 3121.920166\n",
      "Train: step:  82290, time: 0.189, loss: 2806.515869\n",
      "Train: step:  82300, time: 0.194, loss: 628.266724\n",
      "Train: step:  82310, time: 0.207, loss: 1653.883911\n",
      "Train: step:  82320, time: 0.194, loss: 1277.872681\n",
      "Train: step:  82330, time: 0.217, loss: 1508.568481\n",
      "Train: step:  82340, time: 0.196, loss: 901.815063\n",
      "Train: step:  82350, time: 0.199, loss: 1649.016602\n",
      "Train: step:  82360, time: 0.224, loss: 1882.228882\n",
      "Train: step:  82370, time: 0.189, loss: 1359.640137\n",
      "Train: step:  82380, time: 0.194, loss: 1576.565308\n",
      "Train: step:  82390, time: 0.202, loss: 3163.425781\n",
      "Train: step:  82400, time: 0.191, loss: 1515.836304\n",
      "Train: step:  82410, time: 0.219, loss: 2013.639526\n",
      "Train: step:  82420, time: 0.225, loss: 1323.336670\n",
      "Train: step:  82430, time: 0.220, loss: 1247.654663\n",
      "Train: step:  82440, time: 0.196, loss: 981.944458\n",
      "Train: step:  82450, time: 0.225, loss: 2241.187256\n",
      "Train: step:  82460, time: 0.234, loss: 1308.101929\n",
      "Train: step:  82470, time: 0.233, loss: 965.821716\n",
      "Train: step:  82480, time: 0.198, loss: 2595.215332\n",
      "Train: step:  82490, time: 0.232, loss: 2283.767090\n",
      "Train: step:  82500, time: 0.216, loss: 3416.307617\n",
      "Train: step:  82510, time: 0.190, loss: 1450.672974\n",
      "Train: step:  82520, time: 0.228, loss: 2051.491211\n",
      "Train: step:  82530, time: 0.197, loss: 4225.638672\n",
      "Train: step:  82540, time: 0.192, loss: 3007.393555\n",
      "Train: step:  82550, time: 0.223, loss: 666.108643\n",
      "Train: step:  82560, time: 0.202, loss: 384.802765\n",
      "Train: step:  82570, time: 0.197, loss: 2626.007324\n",
      "Train: step:  82580, time: 0.190, loss: 1983.352173\n",
      "Train: step:  82590, time: 0.220, loss: 353.910156\n",
      "Train: step:  82600, time: 0.208, loss: 1246.224609\n",
      "Train: step:  82610, time: 0.191, loss: 1151.516724\n",
      "Train: step:  82620, time: 0.219, loss: 1512.100342\n",
      "Train: step:  82630, time: 0.223, loss: 1598.245605\n",
      "Train: step:  82640, time: 0.217, loss: 713.814758\n",
      "Train: step:  82650, time: 0.218, loss: 4237.235352\n",
      "Train: step:  82660, time: 0.190, loss: 1992.044922\n",
      "Train: step:  82670, time: 0.187, loss: 3419.636719\n",
      "Train: step:  82680, time: 0.194, loss: 1438.458496\n",
      "Train: step:  82690, time: 0.230, loss: 2608.388184\n",
      "Train: step:  82700, time: 0.269, loss: 3273.471436\n",
      "Train: step:  82710, time: 0.210, loss: 2047.374390\n",
      "Train: step:  82720, time: 0.234, loss: 2611.556152\n",
      "Train: step:  82730, time: 0.217, loss: 3451.794922\n",
      "Train: step:  82740, time: 0.196, loss: 2274.125244\n",
      "Train: step:  82750, time: 0.192, loss: 2629.311279\n",
      "Train: step:  82760, time: 0.195, loss: 3003.677246\n",
      "Train: step:  82770, time: 0.240, loss: 1686.317505\n",
      "Train: step:  82780, time: 0.229, loss: 3177.386475\n",
      "Train: step:  82790, time: 0.209, loss: 1927.989868\n",
      "Train: step:  82800, time: 0.193, loss: 722.595764\n",
      "Train: step:  82810, time: 0.184, loss: 997.298218\n",
      "Train: step:  82820, time: 0.189, loss: 712.882690\n",
      "Train: step:  82830, time: 0.230, loss: 1422.984619\n",
      "Train: step:  82840, time: 0.191, loss: 1057.721924\n",
      "Train: step:  82850, time: 0.199, loss: 1954.361084\n",
      "Train: step:  82860, time: 0.191, loss: 2553.498779\n",
      "Train: step:  82870, time: 0.209, loss: 1099.290283\n",
      "Train: step:  82880, time: 0.189, loss: 1916.743042\n",
      "Train: step:  82890, time: 0.217, loss: 2213.739990\n",
      "Train: step:  82900, time: 0.188, loss: 1637.129272\n",
      "Train: step:  82910, time: 0.250, loss: 1548.003784\n",
      "Train: step:  82920, time: 0.235, loss: 613.581970\n",
      "Train: step:  82930, time: 0.244, loss: 2823.003174\n",
      "Train: step:  82940, time: 0.188, loss: 1953.765015\n",
      "Train: step:  82950, time: 0.207, loss: 1724.283203\n",
      "Train: step:  82960, time: 0.218, loss: 1195.822510\n",
      "Train: step:  82970, time: 0.189, loss: 2137.118652\n",
      "Train: step:  82980, time: 0.205, loss: 1678.570068\n",
      "Train: step:  82990, time: 0.186, loss: 312.489105\n",
      "Train: step:  83000, time: 0.206, loss: 2112.109863\n",
      "Train: step:  83010, time: 0.216, loss: 660.321289\n",
      "Train: step:  83020, time: 0.228, loss: 529.131897\n",
      "Train: step:  83030, time: 0.233, loss: 2412.462402\n",
      "Train: step:  83040, time: 0.187, loss: 2999.625732\n",
      "Train: step:  83050, time: 0.186, loss: 474.329010\n",
      "Train: step:  83060, time: 0.227, loss: 2482.194336\n",
      "Train: step:  83070, time: 0.186, loss: 3023.828857\n",
      "Train: step:  83080, time: 0.190, loss: 2468.360840\n",
      "Train: step:  83090, time: 0.225, loss: 1773.002197\n",
      "Train: step:  83100, time: 0.195, loss: 3096.643311\n",
      "Train: step:  83110, time: 0.195, loss: 1422.138062\n",
      "Train: step:  83120, time: 0.229, loss: 3400.132080\n",
      "Train: step:  83130, time: 0.196, loss: 1529.669434\n",
      "Train: step:  83140, time: 0.183, loss: 2162.989258\n",
      "Train: step:  83150, time: 0.238, loss: 2600.241699\n",
      "Train: step:  83160, time: 0.193, loss: 2196.242188\n",
      "Train: step:  83170, time: 0.201, loss: 1836.500244\n",
      "Train: step:  83180, time: 0.190, loss: 742.175415\n",
      "Train: step:  83190, time: 0.183, loss: 1067.689819\n",
      "Train: step:  83200, time: 0.191, loss: 711.053894\n",
      "Train: step:  83210, time: 0.218, loss: 1164.997314\n",
      "Train: step:  83220, time: 0.191, loss: 1847.545288\n",
      "Train: step:  83230, time: 0.198, loss: 2248.892090\n",
      "Train: step:  83240, time: 0.186, loss: 2234.461914\n",
      "Train: step:  83250, time: 0.190, loss: 5213.668457\n",
      "Train: step:  83260, time: 0.187, loss: 3197.191650\n",
      "Train: step:  83270, time: 0.228, loss: 2596.190918\n",
      "Train: step:  83280, time: 0.207, loss: 1049.625732\n",
      "Train: step:  83290, time: 0.193, loss: 2565.619873\n",
      "Train: step:  83300, time: 0.211, loss: 2995.082764\n",
      "Train: step:  83310, time: 0.190, loss: 2083.988525\n",
      "Train: step:  83320, time: 0.195, loss: 2297.012939\n",
      "Train: step:  83330, time: 0.205, loss: 2144.341309\n",
      "Train: step:  83340, time: 0.183, loss: 2266.162109\n",
      "Train: step:  83350, time: 0.190, loss: 942.097290\n",
      "Train: step:  83360, time: 0.192, loss: 4014.165527\n",
      "Train: step:  83370, time: 0.252, loss: 1549.445557\n",
      "Train: step:  83380, time: 0.188, loss: 3803.730957\n",
      "Train: step:  83390, time: 0.218, loss: 2828.158447\n",
      "Train: step:  83400, time: 0.198, loss: 583.453003\n",
      "Train: step:  83410, time: 0.202, loss: 1471.089844\n",
      "Train: step:  83420, time: 0.211, loss: 608.972656\n",
      "Train: step:  83430, time: 0.204, loss: 730.190857\n",
      "Train: step:  83440, time: 0.198, loss: 1341.494019\n",
      "Train: step:  83450, time: 0.187, loss: 1480.124512\n",
      "Train: step:  83460, time: 0.213, loss: 1329.798218\n",
      "Train: step:  83470, time: 0.217, loss: 1711.009888\n",
      "Train: step:  83480, time: 0.197, loss: 3267.344971\n",
      "Train: step:  83490, time: 0.188, loss: 1771.397217\n",
      "Train: step:  83500, time: 0.197, loss: 1626.690918\n",
      "Train: step:  83510, time: 0.196, loss: 1933.551147\n",
      "Train: step:  83520, time: 0.189, loss: 1160.203491\n",
      "Train: step:  83530, time: 0.190, loss: 1631.733276\n",
      "Train: step:  83540, time: 0.188, loss: 586.774292\n",
      "Train: step:  83550, time: 0.197, loss: 1960.703491\n",
      "Train: step:  83560, time: 0.186, loss: 1231.107666\n",
      "Train: step:  83570, time: 0.207, loss: 660.641663\n",
      "Train: step:  83580, time: 0.199, loss: 2008.320435\n",
      "Train: step:  83590, time: 0.194, loss: 3444.750244\n",
      "Train: step:  83600, time: 0.233, loss: 2474.603516\n",
      "Train: step:  83610, time: 0.210, loss: 2112.546875\n",
      "Train: step:  83620, time: 0.230, loss: 1649.817993\n",
      "Train: step:  83630, time: 0.218, loss: 1953.746704\n",
      "Train: step:  83640, time: 0.230, loss: 723.777466\n",
      "Train: step:  83650, time: 0.230, loss: 3151.756836\n",
      "Train: step:  83660, time: 0.187, loss: 2108.931152\n",
      "Train: step:  83670, time: 0.219, loss: 2372.210205\n",
      "Train: step:  83680, time: 0.188, loss: 2795.272217\n",
      "Train: step:  83690, time: 0.224, loss: 4128.928223\n",
      "Train: step:  83700, time: 0.216, loss: 1877.177856\n",
      "Train: step:  83710, time: 0.234, loss: 1951.102539\n",
      "Train: step:  83720, time: 0.216, loss: 1750.828491\n",
      "Train: step:  83730, time: 0.191, loss: 1445.900391\n",
      "Train: step:  83740, time: 0.199, loss: 1037.871338\n",
      "Train: step:  83750, time: 0.191, loss: 2408.044434\n",
      "Train: step:  83760, time: 0.206, loss: 859.015991\n",
      "Train: step:  83770, time: 0.189, loss: 1092.365112\n",
      "Train: step:  83780, time: 0.218, loss: 1682.406128\n",
      "Train: step:  83790, time: 0.186, loss: 1036.020630\n",
      "Train: step:  83800, time: 0.191, loss: 392.247650\n",
      "Train: step:  83810, time: 0.237, loss: 1297.032349\n",
      "Train: step:  83820, time: 0.195, loss: 944.353882\n",
      "Train: step:  83830, time: 0.192, loss: 676.404846\n",
      "Train: step:  83840, time: 0.194, loss: 2208.815674\n",
      "Train: step:  83850, time: 0.191, loss: 1486.615601\n",
      "Train: step:  83860, time: 0.215, loss: 681.739563\n",
      "Train: step:  83870, time: 0.186, loss: 2367.286621\n",
      "Train: step:  83880, time: 0.189, loss: 902.099060\n",
      "Train: step:  83890, time: 0.183, loss: 2393.510498\n",
      "Train: step:  83900, time: 0.188, loss: 3360.072998\n",
      "Train: step:  83910, time: 0.187, loss: 1536.964844\n",
      "Train: step:  83920, time: 0.247, loss: 2071.166260\n",
      "Train: step:  83930, time: 0.242, loss: 2953.325684\n",
      "Train: step:  83940, time: 0.209, loss: 3168.627686\n",
      "Train: step:  83950, time: 0.227, loss: 2151.360352\n",
      "Train: step:  83960, time: 0.208, loss: 1144.693359\n",
      "Train: step:  83970, time: 0.195, loss: 3932.602295\n",
      "Train: step:  83980, time: 0.232, loss: 530.702393\n",
      "Train: step:  83990, time: 0.209, loss: 2427.611084\n",
      "Train: step:  84000, time: 0.185, loss: 1583.896729\n",
      "Train: step:  84010, time: 0.229, loss: 2135.345703\n",
      "Train: step:  84020, time: 0.204, loss: 2186.671143\n",
      "Train: step:  84030, time: 0.190, loss: 2465.235840\n",
      "Train: step:  84040, time: 0.198, loss: 3737.560547\n",
      "Train: step:  84050, time: 0.195, loss: 3735.336914\n",
      "Train: step:  84060, time: 0.219, loss: 2763.731445\n",
      "Train: step:  84070, time: 0.217, loss: 2282.455566\n",
      "Train: step:  84080, time: 0.191, loss: 1126.407104\n",
      "Train: step:  84090, time: 0.191, loss: 167.896530\n",
      "Train: step:  84100, time: 0.194, loss: 1618.944702\n",
      "Train: step:  84110, time: 0.215, loss: 518.417419\n",
      "Train: step:  84120, time: 0.190, loss: 2101.456787\n",
      "Train: step:  84130, time: 0.191, loss: 1764.995239\n",
      "Train: step:  84140, time: 0.186, loss: 2658.312988\n",
      "Train: step:  84150, time: 0.207, loss: 997.199158\n",
      "Train: step:  84160, time: 0.200, loss: 1060.209473\n",
      "Train: step:  84170, time: 0.209, loss: 3196.028809\n",
      "Train: step:  84180, time: 0.226, loss: 1778.493530\n",
      "Train: step:  84190, time: 0.229, loss: 3649.619873\n",
      "Train: step:  84200, time: 0.187, loss: 1895.400391\n",
      "Train: step:  84210, time: 0.183, loss: 1711.608521\n",
      "Train: step:  84220, time: 0.190, loss: 2093.890625\n",
      "Train: step:  84230, time: 0.189, loss: 1384.089600\n",
      "Train: step:  84240, time: 0.193, loss: 2172.744141\n",
      "Train: step:  84250, time: 0.185, loss: 1555.491455\n",
      "Train: step:  84260, time: 0.226, loss: 5034.634766\n",
      "Train: step:  84270, time: 0.190, loss: 3070.652344\n",
      "Train: step:  84280, time: 0.191, loss: 2743.288818\n",
      "Train: step:  84290, time: 0.204, loss: 2846.150635\n",
      "Train: step:  84300, time: 0.190, loss: 1744.648682\n",
      "Train: step:  84310, time: 0.229, loss: 1248.708740\n",
      "Train: step:  84320, time: 0.196, loss: 1853.897339\n",
      "Train: step:  84330, time: 0.227, loss: 2228.773438\n",
      "Train: step:  84340, time: 0.231, loss: 2860.914062\n",
      "Train: step:  84350, time: 0.188, loss: 1161.782959\n",
      "Train: step:  84360, time: 0.197, loss: 1450.555664\n",
      "Train: step:  84370, time: 0.195, loss: 2163.271973\n",
      "Train: step:  84380, time: 0.244, loss: 2060.726318\n",
      "Train: step:  84390, time: 0.218, loss: 1413.464111\n",
      "Train: step:  84400, time: 0.213, loss: 1544.174072\n",
      "Train: step:  84410, time: 0.196, loss: 1372.570068\n",
      "Train: step:  84420, time: 0.193, loss: 777.612793\n",
      "Train: step:  84430, time: 0.217, loss: 2730.059570\n",
      "Train: step:  84440, time: 0.190, loss: 1515.705566\n",
      "Train: step:  84450, time: 0.198, loss: 2418.135010\n",
      "Train: step:  84460, time: 0.220, loss: 2478.202148\n",
      "Train: step:  84470, time: 0.234, loss: 2511.384277\n",
      "Train: step:  84480, time: 0.187, loss: 682.360718\n",
      "Train: step:  84490, time: 0.198, loss: 3517.715088\n",
      "Train: step:  84500, time: 0.229, loss: 718.767212\n",
      "Train: step:  84510, time: 0.191, loss: 1807.472046\n",
      "Train: step:  84520, time: 0.189, loss: 2307.444336\n",
      "Train: step:  84530, time: 0.197, loss: 2183.334717\n",
      "Train: step:  84540, time: 0.214, loss: 2368.360840\n",
      "Train: step:  84550, time: 0.195, loss: 1541.819824\n",
      "Train: step:  84560, time: 0.220, loss: 365.994171\n",
      "Train: step:  84570, time: 0.186, loss: 1041.272095\n",
      "Train: step:  84580, time: 0.192, loss: 2249.587158\n",
      "Train: step:  84590, time: 0.243, loss: 2689.689453\n",
      "Train: step:  84600, time: 0.196, loss: 903.430115\n",
      "Train: step:  84610, time: 0.188, loss: 1847.233765\n",
      "Train: step:  84620, time: 0.212, loss: 3654.957275\n",
      "Train: step:  84630, time: 0.219, loss: 2306.608398\n",
      "Train: step:  84640, time: 0.217, loss: 532.855896\n",
      "Train: step:  84650, time: 0.229, loss: 698.626282\n",
      "Train: step:  84660, time: 0.190, loss: 646.797607\n",
      "Train: step:  84670, time: 0.200, loss: 640.872864\n",
      "Train: step:  84680, time: 0.181, loss: 1640.195923\n",
      "Train: step:  84690, time: 0.203, loss: 1379.827881\n",
      "Train: step:  84700, time: 0.228, loss: 673.523743\n",
      "Train: step:  84710, time: 0.193, loss: 736.894409\n",
      "Train: step:  84720, time: 0.191, loss: 1969.732422\n",
      "Train: step:  84730, time: 0.216, loss: 721.618713\n",
      "Train: step:  84740, time: 0.182, loss: 1351.852783\n",
      "Train: step:  84750, time: 0.189, loss: 2977.978027\n",
      "Train: step:  84760, time: 0.193, loss: 2369.904541\n",
      "Train: step:  84770, time: 0.188, loss: 2306.711914\n",
      "Train: step:  84780, time: 0.227, loss: 1656.849731\n",
      "Train: step:  84790, time: 0.213, loss: 509.016174\n",
      "Train: step:  84800, time: 0.231, loss: 499.108246\n",
      "Train: step:  84810, time: 0.195, loss: 1898.370239\n",
      "Train: step:  84820, time: 0.182, loss: 1569.140137\n",
      "Train: step:  84830, time: 0.200, loss: 2314.121826\n",
      "Train: step:  84840, time: 0.191, loss: 651.631897\n",
      "Train: step:  84850, time: 0.185, loss: 1216.466431\n",
      "Train: step:  84860, time: 0.221, loss: 415.826904\n",
      "Train: step:  84870, time: 0.185, loss: 487.731873\n",
      "Train: step:  84880, time: 0.234, loss: 1348.663818\n",
      "Train: step:  84890, time: 0.192, loss: 1346.076782\n",
      "Train: step:  84900, time: 0.228, loss: 1657.096191\n",
      "Train: step:  84910, time: 0.186, loss: 1691.647339\n",
      "Train: step:  84920, time: 0.232, loss: 1465.066772\n",
      "Train: step:  84930, time: 0.190, loss: 1289.126465\n",
      "Train: step:  84940, time: 0.219, loss: 1066.818115\n",
      "Train: step:  84950, time: 0.195, loss: 1654.309204\n",
      "Train: step:  84960, time: 0.196, loss: 1580.816895\n",
      "Train: step:  84970, time: 0.196, loss: 1724.644775\n",
      "Train: step:  84980, time: 0.187, loss: 1335.946411\n",
      "Train: step:  84990, time: 0.275, loss: 1739.507935\n",
      "Train: step:  85000, time: 0.225, loss: 2019.963501\n",
      "Train: step:  85010, time: 0.215, loss: 2568.138916\n",
      "Train: step:  85020, time: 0.190, loss: 1981.755859\n",
      "Train: step:  85030, time: 0.191, loss: 1513.569458\n",
      "Train: step:  85040, time: 0.208, loss: 3277.554932\n",
      "Train: step:  85050, time: 0.186, loss: 3022.107666\n",
      "Train: step:  85060, time: 0.204, loss: 3422.686279\n",
      "Train: step:  85070, time: 0.215, loss: 1232.171387\n",
      "Train: step:  85080, time: 0.226, loss: 2363.842773\n",
      "Train: step:  85090, time: 0.201, loss: 1033.702271\n",
      "Train: step:  85100, time: 0.205, loss: 1149.070557\n",
      "Train: step:  85110, time: 0.193, loss: 1672.253662\n",
      "Train: step:  85120, time: 0.196, loss: 5583.500488\n",
      "Train: step:  85130, time: 0.205, loss: 1018.364929\n",
      "Train: step:  85140, time: 0.187, loss: 1024.605225\n",
      "Train: step:  85150, time: 0.194, loss: 1409.056763\n",
      "Train: step:  85160, time: 0.183, loss: 2292.969482\n",
      "Train: step:  85170, time: 0.199, loss: 2452.635010\n",
      "Train: step:  85180, time: 0.207, loss: 2330.461426\n",
      "Train: step:  85190, time: 0.193, loss: 2248.521973\n",
      "Train: step:  85200, time: 0.192, loss: 1356.209106\n",
      "Train: step:  85210, time: 0.185, loss: 2253.223633\n",
      "Train: step:  85220, time: 0.206, loss: 988.736755\n",
      "Train: step:  85230, time: 0.245, loss: 1217.422485\n",
      "Train: step:  85240, time: 0.214, loss: 1608.725586\n",
      "Train: step:  85250, time: 0.227, loss: 1955.932251\n",
      "Train: step:  85260, time: 0.194, loss: 2576.933105\n",
      "Train: step:  85270, time: 0.213, loss: 1957.732788\n",
      "Train: step:  85280, time: 0.221, loss: 2557.432861\n",
      "Train: step:  85290, time: 0.185, loss: 2930.217041\n",
      "Train: step:  85300, time: 0.192, loss: 1233.860840\n",
      "Train: step:  85310, time: 0.185, loss: 1915.019165\n",
      "Train: step:  85320, time: 0.217, loss: 986.591309\n",
      "Train: step:  85330, time: 0.228, loss: 2401.086914\n",
      "Train: step:  85340, time: 0.186, loss: 1758.262085\n",
      "Train: step:  85350, time: 0.227, loss: 1685.750122\n",
      "Train: step:  85360, time: 0.230, loss: 960.779907\n",
      "Train: step:  85370, time: 0.192, loss: 2560.597900\n",
      "Train: step:  85380, time: 0.184, loss: 2068.102783\n",
      "Train: step:  85390, time: 0.188, loss: 1324.561157\n",
      "Train: step:  85400, time: 0.231, loss: 702.445312\n",
      "Train: step:  85410, time: 0.228, loss: 2000.365356\n",
      "Train: step:  85420, time: 0.194, loss: 2239.609619\n",
      "Train: step:  85430, time: 0.188, loss: 3040.635010\n",
      "Train: step:  85440, time: 0.194, loss: 2319.481445\n",
      "Train: step:  85450, time: 0.189, loss: 1434.742310\n",
      "Train: step:  85460, time: 0.193, loss: 2082.689209\n",
      "Train: step:  85470, time: 0.218, loss: 3604.933105\n",
      "Train: step:  85480, time: 0.214, loss: 2376.831299\n",
      "Train: step:  85490, time: 0.192, loss: 2310.531250\n",
      "Train: step:  85500, time: 0.182, loss: 862.863708\n",
      "Train: step:  85510, time: 0.189, loss: 2281.403564\n",
      "Train: step:  85520, time: 0.227, loss: 2670.680908\n",
      "Train: step:  85530, time: 0.191, loss: 668.237488\n",
      "Train: step:  85540, time: 0.190, loss: 1322.069336\n",
      "Train: step:  85550, time: 0.220, loss: 1644.673828\n",
      "Train: step:  85560, time: 0.188, loss: 2500.354492\n",
      "Train: step:  85570, time: 0.228, loss: 1444.991699\n",
      "Train: step:  85580, time: 0.190, loss: 2923.347412\n",
      "Train: step:  85590, time: 0.202, loss: 2750.803467\n",
      "Train: step:  85600, time: 0.232, loss: 1175.333862\n",
      "Train: step:  85610, time: 0.191, loss: 2005.609497\n",
      "Train: step:  85620, time: 0.215, loss: 2564.649658\n",
      "Train: step:  85630, time: 0.227, loss: 2341.462402\n",
      "Train: step:  85640, time: 0.188, loss: 2116.197021\n",
      "Train: step:  85650, time: 0.223, loss: 697.529114\n",
      "Train: step:  85660, time: 0.184, loss: 1596.917969\n",
      "Train: step:  85670, time: 0.227, loss: 1901.286255\n",
      "Train: step:  85680, time: 0.228, loss: 1308.071899\n",
      "Train: step:  85690, time: 0.199, loss: 1860.693481\n",
      "Train: step:  85700, time: 0.243, loss: 1142.591309\n",
      "Train: step:  85710, time: 0.218, loss: 584.423096\n",
      "Train: step:  85720, time: 0.189, loss: 1183.623657\n",
      "Train: step:  85730, time: 0.232, loss: 232.625916\n",
      "Train: step:  85740, time: 0.216, loss: 2609.416016\n",
      "Train: step:  85750, time: 0.217, loss: 689.085999\n",
      "Train: step:  85760, time: 0.196, loss: 2506.406738\n",
      "Train: step:  85770, time: 0.236, loss: 650.946289\n",
      "Train: step:  85780, time: 0.194, loss: 1818.919067\n",
      "Train: step:  85790, time: 0.189, loss: 1997.286377\n",
      "Train: step:  85800, time: 0.184, loss: 926.164185\n",
      "Train: step:  85810, time: 0.239, loss: 1027.618408\n",
      "Train: step:  85820, time: 0.182, loss: 1480.487671\n",
      "Train: step:  85830, time: 0.228, loss: 2068.801270\n",
      "Train: step:  85840, time: 0.223, loss: 1053.958496\n",
      "Train: step:  85850, time: 0.188, loss: 1229.149170\n",
      "Train: step:  85860, time: 0.185, loss: 2758.991455\n",
      "Train: step:  85870, time: 0.192, loss: 1367.667114\n",
      "Train: step:  85880, time: 0.219, loss: 1399.450439\n",
      "Train: step:  85890, time: 0.235, loss: 2110.141602\n",
      "Train: step:  85900, time: 0.218, loss: 2137.168213\n",
      "Train: step:  85910, time: 0.218, loss: 1214.405151\n",
      "Train: step:  85920, time: 0.218, loss: 1447.620483\n",
      "Train: step:  85930, time: 0.191, loss: 870.394287\n",
      "Train: step:  85940, time: 0.188, loss: 897.974365\n",
      "Train: step:  85950, time: 0.192, loss: 1238.440674\n",
      "Train: step:  85960, time: 0.194, loss: 2692.950439\n",
      "Train: step:  85970, time: 0.204, loss: 1501.623413\n",
      "Train: step:  85980, time: 0.181, loss: 2718.009277\n",
      "Train: step:  85990, time: 0.186, loss: 1095.500732\n",
      "Train: step:  86000, time: 0.199, loss: 1340.133667\n",
      "Train: step:  86010, time: 0.194, loss: 2093.731934\n",
      "Train: step:  86020, time: 0.218, loss: 2647.274902\n",
      "Train: step:  86030, time: 0.225, loss: 1765.646362\n",
      "Train: step:  86040, time: 0.193, loss: 3584.625244\n",
      "Train: step:  86050, time: 0.188, loss: 2010.698975\n",
      "Train: step:  86060, time: 0.231, loss: 1634.766968\n",
      "Train: step:  86070, time: 0.248, loss: 992.409729\n",
      "Train: step:  86080, time: 0.204, loss: 1996.501831\n",
      "Train: step:  86090, time: 0.186, loss: 2566.942627\n",
      "Train: step:  86100, time: 0.188, loss: 1026.654907\n",
      "Train: step:  86110, time: 0.203, loss: 2579.379395\n",
      "Train: step:  86120, time: 0.218, loss: 2058.874023\n",
      "Train: step:  86130, time: 0.194, loss: 3392.008545\n",
      "Train: step:  86140, time: 0.196, loss: 1861.766968\n",
      "Train: step:  86150, time: 0.222, loss: 3408.958496\n",
      "Train: step:  86160, time: 0.217, loss: 2534.480713\n",
      "Train: step:  86170, time: 0.217, loss: 2283.582275\n",
      "Train: step:  86180, time: 0.187, loss: 1424.978027\n",
      "Train: step:  86190, time: 0.231, loss: 1726.675903\n",
      "Train: step:  86200, time: 0.195, loss: 1686.779785\n",
      "Train: step:  86210, time: 0.188, loss: 821.354126\n",
      "Train: step:  86220, time: 0.189, loss: 712.116333\n",
      "Train: step:  86230, time: 0.260, loss: 2640.504395\n",
      "Train: step:  86240, time: 0.241, loss: 407.996063\n",
      "Train: step:  86250, time: 0.216, loss: 2832.171387\n",
      "Train: step:  86260, time: 0.193, loss: 1569.244629\n",
      "Train: step:  86270, time: 0.225, loss: 4316.393066\n",
      "Train: step:  86280, time: 0.237, loss: 1144.218018\n",
      "Train: step:  86290, time: 0.194, loss: 2060.381592\n",
      "Train: step:  86300, time: 0.196, loss: 330.312897\n",
      "Train: step:  86310, time: 0.189, loss: 2781.633789\n",
      "Train: step:  86320, time: 0.196, loss: 1861.617310\n",
      "Train: step:  86330, time: 0.193, loss: 1481.262451\n",
      "Train: step:  86340, time: 0.191, loss: 762.206299\n",
      "Train: step:  86350, time: 0.213, loss: 1560.800415\n",
      "Train: step:  86360, time: 0.198, loss: 1611.869385\n",
      "Train: step:  86370, time: 0.198, loss: 1779.430664\n",
      "Train: step:  86380, time: 0.195, loss: 2333.760986\n",
      "Train: step:  86390, time: 0.220, loss: 485.167145\n",
      "Train: step:  86400, time: 0.219, loss: 2748.958740\n",
      "Train: step:  86410, time: 0.191, loss: 2629.430176\n",
      "Train: step:  86420, time: 0.193, loss: 2276.523682\n",
      "Train: step:  86430, time: 0.213, loss: 2004.845947\n",
      "Train: step:  86440, time: 0.228, loss: 893.014343\n",
      "Train: step:  86450, time: 0.244, loss: 1682.615967\n",
      "Train: step:  86460, time: 0.196, loss: 1070.126953\n",
      "Train: step:  86470, time: 0.202, loss: 1366.886108\n",
      "Train: step:  86480, time: 0.221, loss: 4082.664307\n",
      "Train: step:  86490, time: 0.232, loss: 401.172150\n",
      "Train: step:  86500, time: 0.198, loss: 1699.986816\n",
      "Train: step:  86510, time: 0.200, loss: 2396.220947\n",
      "Train: step:  86520, time: 0.218, loss: 897.124268\n",
      "Train: step:  86530, time: 0.192, loss: 197.300674\n",
      "Train: step:  86540, time: 0.233, loss: 2323.468018\n",
      "Train: step:  86550, time: 0.229, loss: 3762.828857\n",
      "Train: step:  86560, time: 0.194, loss: 2164.498535\n",
      "Train: step:  86570, time: 0.196, loss: 2959.455811\n",
      "Train: step:  86580, time: 0.197, loss: 421.221069\n",
      "Train: step:  86590, time: 0.220, loss: 2435.926514\n",
      "Train: step:  86600, time: 0.230, loss: 2679.864990\n",
      "Train: step:  86610, time: 0.200, loss: 1091.510376\n",
      "Train: step:  86620, time: 0.193, loss: 1592.557251\n",
      "Train: step:  86630, time: 0.233, loss: 3125.093994\n",
      "Train: step:  86640, time: 0.209, loss: 1661.332886\n",
      "Train: step:  86650, time: 0.188, loss: 2171.082764\n",
      "Train: step:  86660, time: 0.195, loss: 2514.661133\n",
      "Train: step:  86670, time: 0.219, loss: 2326.143555\n",
      "Train: step:  86680, time: 0.236, loss: 2064.074707\n",
      "Train: step:  86690, time: 0.247, loss: 2469.098877\n",
      "Train: step:  86700, time: 0.199, loss: 1697.850586\n",
      "Train: step:  86710, time: 0.217, loss: 1759.891357\n",
      "Train: step:  86720, time: 0.203, loss: 1535.774536\n",
      "Train: step:  86730, time: 0.238, loss: 434.146973\n",
      "Train: step:  86740, time: 0.197, loss: 370.033539\n",
      "Train: step:  86750, time: 0.192, loss: 1662.593506\n",
      "Train: step:  86760, time: 0.229, loss: 1226.441284\n",
      "Train: step:  86770, time: 0.261, loss: 1604.178223\n",
      "Train: step:  86780, time: 0.238, loss: 420.341400\n",
      "Train: step:  86790, time: 0.219, loss: 1623.608032\n",
      "Train: step:  86800, time: 0.193, loss: 2937.779297\n",
      "Train: step:  86810, time: 0.208, loss: 1812.685547\n",
      "Train: step:  86820, time: 0.220, loss: 2484.953613\n",
      "Train: step:  86830, time: 0.218, loss: 1708.299316\n",
      "Train: step:  86840, time: 0.199, loss: 999.331238\n",
      "Train: step:  86850, time: 0.198, loss: 506.835480\n",
      "Train: step:  86860, time: 0.216, loss: 1587.350220\n",
      "Train: step:  86870, time: 0.217, loss: 2074.260498\n",
      "Train: step:  86880, time: 0.198, loss: 1670.504150\n",
      "Train: step:  86890, time: 0.216, loss: 1380.096558\n",
      "Train: step:  86900, time: 0.193, loss: 1870.620117\n",
      "Train: step:  86910, time: 0.192, loss: 2626.182861\n",
      "Train: step:  86920, time: 0.192, loss: 1734.458252\n",
      "Train: step:  86930, time: 0.189, loss: 1045.265625\n",
      "Train: step:  86940, time: 0.188, loss: 1270.948730\n",
      "Train: step:  86950, time: 0.235, loss: 2164.889160\n",
      "Train: step:  86960, time: 0.219, loss: 1375.223633\n",
      "Train: step:  86970, time: 0.201, loss: 1507.011719\n",
      "Train: step:  86980, time: 0.205, loss: 1853.040161\n",
      "Train: step:  86990, time: 0.219, loss: 2565.772949\n",
      "Train: step:  87000, time: 0.197, loss: 1792.210693\n",
      "Train: step:  87010, time: 0.197, loss: 1486.054932\n",
      "Train: step:  87020, time: 0.254, loss: 2509.469238\n",
      "Train: step:  87030, time: 0.196, loss: 1271.096436\n",
      "Train: step:  87040, time: 0.218, loss: 2477.544434\n",
      "Train: step:  87050, time: 0.188, loss: 1295.181885\n",
      "Train: step:  87060, time: 0.194, loss: 428.696320\n",
      "Train: step:  87070, time: 0.220, loss: 1862.948364\n",
      "Train: step:  87080, time: 0.198, loss: 3202.632812\n",
      "Train: step:  87090, time: 0.189, loss: 1398.915283\n",
      "Train: step:  87100, time: 0.235, loss: 2177.903320\n",
      "Train: step:  87110, time: 0.231, loss: 2405.364990\n",
      "Train: step:  87120, time: 0.188, loss: 2073.008545\n",
      "Train: step:  87130, time: 0.219, loss: 563.369385\n",
      "Train: step:  87140, time: 0.189, loss: 702.161011\n",
      "Train: step:  87150, time: 0.196, loss: 1064.232788\n",
      "Train: step:  87160, time: 0.226, loss: 4047.204834\n",
      "Train: step:  87170, time: 0.229, loss: 313.327881\n",
      "Train: step:  87180, time: 0.190, loss: 2449.489014\n",
      "Train: step:  87190, time: 0.203, loss: 249.810898\n",
      "Train: step:  87200, time: 0.253, loss: 3212.847412\n",
      "Train: step:  87210, time: 0.231, loss: 1660.580322\n",
      "Train: step:  87220, time: 0.244, loss: 803.613220\n",
      "Train: step:  87230, time: 0.196, loss: 141.131149\n",
      "Train: step:  87240, time: 0.215, loss: 2236.500244\n",
      "Train: step:  87250, time: 0.253, loss: 3136.590088\n",
      "Train: step:  87260, time: 0.231, loss: 1839.003174\n",
      "Train: step:  87270, time: 0.222, loss: 1712.703369\n",
      "Train: step:  87280, time: 0.220, loss: 1755.944458\n",
      "Train: step:  87290, time: 0.223, loss: 1137.079956\n",
      "Train: step:  87300, time: 0.235, loss: 2491.185303\n",
      "Train: step:  87310, time: 0.188, loss: 2716.699707\n",
      "Train: step:  87320, time: 0.197, loss: 1997.283691\n",
      "Train: step:  87330, time: 0.194, loss: 2831.052002\n",
      "Train: step:  87340, time: 0.227, loss: 1394.107910\n",
      "Train: step:  87350, time: 0.194, loss: 2711.362793\n",
      "Train: step:  87360, time: 0.203, loss: 1242.191406\n",
      "Train: step:  87370, time: 0.221, loss: 2239.112061\n",
      "Train: step:  87380, time: 0.190, loss: 614.765076\n",
      "Train: step:  87390, time: 0.218, loss: 1608.061523\n",
      "Train: step:  87400, time: 0.221, loss: 339.566528\n",
      "Train: step:  87410, time: 0.195, loss: 1882.188232\n",
      "Train: step:  87420, time: 0.223, loss: 2913.698975\n",
      "Train: step:  87430, time: 0.193, loss: 904.374390\n",
      "Train: step:  87440, time: 0.194, loss: 4607.760742\n",
      "Train: step:  87450, time: 0.194, loss: 1194.730835\n",
      "Train: step:  87460, time: 0.224, loss: 3420.034424\n",
      "Train: step:  87470, time: 0.197, loss: 807.732056\n",
      "Train: step:  87480, time: 0.221, loss: 2218.437500\n",
      "Train: step:  87490, time: 0.194, loss: 1682.905884\n",
      "Train: step:  87500, time: 0.197, loss: 2457.041992\n",
      "Train: step:  87510, time: 0.255, loss: 1550.351685\n",
      "Train: step:  87520, time: 0.197, loss: 377.487122\n",
      "Train: step:  87530, time: 0.194, loss: 749.912292\n",
      "Train: step:  87540, time: 0.190, loss: 1443.544678\n",
      "Train: step:  87550, time: 0.215, loss: 1104.916748\n",
      "Train: step:  87560, time: 0.193, loss: 2081.643066\n",
      "Train: step:  87570, time: 0.227, loss: 2296.177490\n",
      "Train: step:  87580, time: 0.193, loss: 388.785156\n",
      "Train: step:  87590, time: 0.218, loss: 422.397400\n",
      "Train: step:  87600, time: 0.188, loss: 2734.345215\n",
      "Train: step:  87610, time: 0.217, loss: 1507.561157\n",
      "Train: step:  87620, time: 0.227, loss: 1279.330200\n",
      "Train: step:  87630, time: 0.223, loss: 658.504944\n",
      "Train: step:  87640, time: 0.199, loss: 2056.400635\n",
      "Train: step:  87650, time: 0.190, loss: 652.076843\n",
      "Train: step:  87660, time: 0.217, loss: 2785.830322\n",
      "Train: step:  87670, time: 0.192, loss: 1335.934448\n",
      "Train: step:  87680, time: 0.228, loss: 2566.904785\n",
      "Train: step:  87690, time: 0.195, loss: 1598.619019\n",
      "Train: step:  87700, time: 0.192, loss: 560.698059\n",
      "Train: step:  87710, time: 0.198, loss: 618.729065\n",
      "Train: step:  87720, time: 0.202, loss: 2352.579834\n",
      "Train: step:  87730, time: 0.193, loss: 2137.675049\n",
      "Train: step:  87740, time: 0.230, loss: 2438.173828\n",
      "Train: step:  87750, time: 0.189, loss: 1724.085327\n",
      "Train: step:  87760, time: 0.207, loss: 2179.442383\n",
      "Train: step:  87770, time: 0.223, loss: 1388.070190\n",
      "Train: step:  87780, time: 0.198, loss: 1754.921265\n",
      "Train: step:  87790, time: 0.198, loss: 2145.984131\n",
      "Train: step:  87800, time: 0.204, loss: 1383.031128\n",
      "Train: step:  87810, time: 0.244, loss: 295.919617\n",
      "Train: step:  87820, time: 0.198, loss: 570.469360\n",
      "Train: step:  87830, time: 0.191, loss: 3172.869385\n",
      "Train: step:  87840, time: 0.218, loss: 2908.525635\n",
      "Train: step:  87850, time: 0.192, loss: 1348.656250\n",
      "Train: step:  87860, time: 0.198, loss: 3081.162842\n",
      "Train: step:  87870, time: 0.212, loss: 2434.246094\n",
      "Train: step:  87880, time: 0.195, loss: 552.947266\n",
      "Train: step:  87890, time: 0.230, loss: 1847.169800\n",
      "Train: step:  87900, time: 0.192, loss: 437.019196\n",
      "Train: step:  87910, time: 0.195, loss: 1790.979370\n",
      "Train: step:  87920, time: 0.186, loss: 2232.564697\n",
      "Train: step:  87930, time: 0.191, loss: 2101.269043\n",
      "Train: step:  87940, time: 0.192, loss: 1566.793457\n",
      "Train: step:  87950, time: 0.198, loss: 1116.641846\n",
      "Train: step:  87960, time: 0.193, loss: 3095.279053\n",
      "Train: step:  87970, time: 0.194, loss: 2814.639648\n",
      "Train: step:  87980, time: 0.188, loss: 577.139709\n",
      "Train: step:  87990, time: 0.202, loss: 2912.043457\n",
      "Train: step:  88000, time: 0.221, loss: 1985.563232\n",
      "Train: step:  88010, time: 0.210, loss: 1339.518921\n",
      "Train: step:  88020, time: 0.195, loss: 2807.970215\n",
      "Train: step:  88030, time: 0.186, loss: 1887.894287\n",
      "Train: step:  88040, time: 0.233, loss: 1326.527710\n",
      "Train: step:  88050, time: 0.196, loss: 1500.215210\n",
      "Train: step:  88060, time: 0.195, loss: 3522.363770\n",
      "Train: step:  88070, time: 0.187, loss: 405.453613\n",
      "Train: step:  88080, time: 0.192, loss: 1426.146606\n",
      "Train: step:  88090, time: 0.188, loss: 1395.699707\n",
      "Train: step:  88100, time: 0.188, loss: 1991.098145\n",
      "Train: step:  88110, time: 0.190, loss: 507.382355\n",
      "Train: step:  88120, time: 0.193, loss: 2190.292969\n",
      "Train: step:  88130, time: 0.229, loss: 2106.436768\n",
      "Train: step:  88140, time: 0.192, loss: 914.180664\n",
      "Train: step:  88150, time: 0.189, loss: 2681.698242\n",
      "Train: step:  88160, time: 0.202, loss: 1130.578125\n",
      "Train: step:  88170, time: 0.190, loss: 1561.482422\n",
      "Train: step:  88180, time: 0.194, loss: 2401.600098\n",
      "Train: step:  88190, time: 0.225, loss: 1056.690430\n",
      "Train: step:  88200, time: 0.194, loss: 1320.067139\n",
      "Train: step:  88210, time: 0.198, loss: 1550.869019\n",
      "Train: step:  88220, time: 0.194, loss: 2586.781250\n",
      "Train: step:  88230, time: 0.195, loss: 2795.538574\n",
      "Train: step:  88240, time: 0.221, loss: 810.087769\n",
      "Train: step:  88250, time: 0.219, loss: 2097.280762\n",
      "Train: step:  88260, time: 0.193, loss: 1817.084839\n",
      "Train: step:  88270, time: 0.222, loss: 1302.740112\n",
      "Train: step:  88280, time: 0.196, loss: 1127.360474\n",
      "Train: step:  88290, time: 0.187, loss: 4341.552246\n",
      "Train: step:  88300, time: 0.194, loss: 645.012024\n",
      "Train: step:  88310, time: 0.189, loss: 2148.833008\n",
      "Train: step:  88320, time: 0.200, loss: 901.364319\n",
      "Train: step:  88330, time: 0.189, loss: 2266.559082\n",
      "Train: step:  88340, time: 0.195, loss: 1340.129883\n",
      "Train: step:  88350, time: 0.186, loss: 1314.059937\n",
      "Train: step:  88360, time: 0.201, loss: 2160.713623\n",
      "Train: step:  88370, time: 0.199, loss: 1974.187378\n",
      "Train: step:  88380, time: 0.218, loss: 901.546875\n",
      "Train: step:  88390, time: 0.193, loss: 667.332886\n",
      "Train: step:  88400, time: 0.217, loss: 1854.761475\n",
      "Train: step:  88410, time: 0.201, loss: 1300.481079\n",
      "Train: step:  88420, time: 0.215, loss: 1369.403564\n",
      "Train: step:  88430, time: 0.218, loss: 2432.833984\n",
      "Train: step:  88440, time: 0.193, loss: 1048.964478\n",
      "Train: step:  88450, time: 0.188, loss: 1782.550171\n",
      "Train: step:  88460, time: 0.190, loss: 958.799011\n",
      "Train: step:  88470, time: 0.190, loss: 501.689362\n",
      "Train: step:  88480, time: 0.189, loss: 432.005920\n",
      "Train: step:  88490, time: 0.230, loss: 1743.312622\n",
      "Train: step:  88500, time: 0.217, loss: 1786.495850\n",
      "Train: step:  88510, time: 0.217, loss: 567.408691\n",
      "Train: step:  88520, time: 0.231, loss: 2787.458008\n",
      "Train: step:  88530, time: 0.218, loss: 2595.143799\n",
      "Train: step:  88540, time: 0.187, loss: 3214.269043\n",
      "Train: step:  88550, time: 0.196, loss: 1932.324707\n",
      "Train: step:  88560, time: 0.218, loss: 1907.921387\n",
      "Train: step:  88570, time: 0.209, loss: 2193.937988\n",
      "Train: step:  88580, time: 0.219, loss: 1697.687134\n",
      "Train: step:  88590, time: 0.195, loss: 2234.749268\n",
      "Train: step:  88600, time: 0.226, loss: 3990.725586\n",
      "Train: step:  88610, time: 0.198, loss: 1877.702637\n",
      "Train: step:  88620, time: 0.196, loss: 3233.732422\n",
      "Train: step:  88630, time: 0.218, loss: 1983.523926\n",
      "Train: step:  88640, time: 0.228, loss: 468.490967\n",
      "Train: step:  88650, time: 0.216, loss: 1879.201294\n",
      "Train: step:  88660, time: 0.196, loss: 3611.695068\n",
      "Train: step:  88670, time: 0.218, loss: 1202.150879\n",
      "Train: step:  88680, time: 0.208, loss: 2995.838379\n",
      "Train: step:  88690, time: 0.217, loss: 2160.463379\n",
      "Train: step:  88700, time: 0.193, loss: 2078.242920\n",
      "Train: step:  88710, time: 0.199, loss: 1567.549683\n",
      "Train: step:  88720, time: 0.199, loss: 1901.047974\n",
      "Train: step:  88730, time: 0.191, loss: 283.063629\n",
      "Train: step:  88740, time: 0.193, loss: 2630.506592\n",
      "Train: step:  88750, time: 0.228, loss: 1522.931519\n",
      "Train: step:  88760, time: 0.214, loss: 2410.702148\n",
      "Train: step:  88770, time: 0.195, loss: 2378.898438\n",
      "Train: step:  88780, time: 0.195, loss: 526.796692\n",
      "Train: step:  88790, time: 0.239, loss: 1660.853027\n",
      "Train: step:  88800, time: 0.196, loss: 3394.019043\n",
      "Train: step:  88810, time: 0.192, loss: 397.735046\n",
      "Train: step:  88820, time: 0.190, loss: 1828.352417\n",
      "Train: step:  88830, time: 0.193, loss: 1884.815674\n",
      "Train: step:  88840, time: 0.218, loss: 1031.513184\n",
      "Train: step:  88850, time: 0.195, loss: 2456.497559\n",
      "Train: step:  88860, time: 0.206, loss: 3292.418457\n",
      "Train: step:  88870, time: 0.197, loss: 1944.851074\n",
      "Train: step:  88880, time: 0.233, loss: 3003.131836\n",
      "Train: step:  88890, time: 0.217, loss: 2357.458252\n",
      "Train: step:  88900, time: 0.189, loss: 875.978516\n",
      "Train: step:  88910, time: 0.198, loss: 3811.929199\n",
      "Train: step:  88920, time: 0.225, loss: 4443.886230\n",
      "Train: step:  88930, time: 0.203, loss: 2723.905029\n",
      "Train: step:  88940, time: 0.187, loss: 1067.594360\n",
      "Train: step:  88950, time: 0.193, loss: 2771.448975\n",
      "Train: step:  88960, time: 0.199, loss: 2122.327637\n",
      "Train: step:  88970, time: 0.187, loss: 400.843140\n",
      "Train: step:  88980, time: 0.194, loss: 2256.382324\n",
      "Train: step:  88990, time: 0.193, loss: 2641.965332\n",
      "Train: step:  89000, time: 0.196, loss: 2098.587646\n",
      "Train: step:  89010, time: 0.234, loss: 3272.156006\n",
      "Train: step:  89020, time: 0.217, loss: 957.573547\n",
      "Train: step:  89030, time: 0.197, loss: 1578.428223\n",
      "Train: step:  89040, time: 0.196, loss: 2569.603271\n",
      "Train: step:  89050, time: 0.189, loss: 1830.316406\n",
      "Train: step:  89060, time: 0.198, loss: 268.719788\n",
      "Train: step:  89070, time: 0.237, loss: 2554.283203\n",
      "Train: step:  89080, time: 0.214, loss: 2624.854492\n",
      "Train: step:  89090, time: 0.202, loss: 1555.947266\n",
      "Train: step:  89100, time: 0.238, loss: 2244.438965\n",
      "Train: step:  89110, time: 0.227, loss: 1142.273193\n",
      "Train: step:  89120, time: 0.218, loss: 3467.374512\n",
      "Train: step:  89130, time: 0.222, loss: 3825.864258\n",
      "Train: step:  89140, time: 0.218, loss: 2075.613770\n",
      "Train: step:  89150, time: 0.194, loss: 2481.990479\n",
      "Train: step:  89160, time: 0.219, loss: 1339.554810\n",
      "Train: step:  89170, time: 0.217, loss: 1823.137573\n",
      "Train: step:  89180, time: 0.195, loss: 3099.492432\n",
      "Train: step:  89190, time: 0.196, loss: 2066.232422\n",
      "Train: step:  89200, time: 0.214, loss: 2041.068237\n",
      "Train: step:  89210, time: 0.226, loss: 2009.950562\n",
      "Train: step:  89220, time: 0.192, loss: 269.160522\n",
      "Train: step:  89230, time: 0.190, loss: 656.201843\n",
      "Train: step:  89240, time: 0.196, loss: 1532.009277\n",
      "Train: step:  89250, time: 0.231, loss: 3341.576904\n",
      "Train: step:  89260, time: 0.232, loss: 1572.798218\n",
      "Train: step:  89270, time: 0.220, loss: 752.885925\n",
      "Train: step:  89280, time: 0.220, loss: 2454.610107\n",
      "Train: step:  89290, time: 0.219, loss: 1808.103394\n",
      "Train: step:  89300, time: 0.198, loss: 431.341034\n",
      "Train: step:  89310, time: 0.234, loss: 1808.687500\n",
      "Train: step:  89320, time: 0.190, loss: 1746.598022\n",
      "Train: step:  89330, time: 0.227, loss: 821.276306\n",
      "Train: step:  89340, time: 0.188, loss: 2208.155273\n",
      "Train: step:  89350, time: 0.192, loss: 2463.403809\n",
      "Train: step:  89360, time: 0.195, loss: 1372.257935\n",
      "Train: step:  89370, time: 0.190, loss: 922.507263\n",
      "Train: step:  89380, time: 0.190, loss: 1999.880615\n",
      "Train: step:  89390, time: 0.230, loss: 1193.919067\n",
      "Train: step:  89400, time: 0.242, loss: 3092.281982\n",
      "Train: step:  89410, time: 0.224, loss: 1050.392700\n",
      "Train: step:  89420, time: 0.192, loss: 3475.891602\n",
      "Train: step:  89430, time: 0.195, loss: 1746.191650\n",
      "Train: step:  89440, time: 0.196, loss: 1697.202637\n",
      "Train: step:  89450, time: 0.209, loss: 1613.491455\n",
      "Train: step:  89460, time: 0.190, loss: 2612.542480\n",
      "Train: step:  89470, time: 0.189, loss: 2765.394287\n",
      "Train: step:  89480, time: 0.194, loss: 958.706177\n",
      "Train: step:  89490, time: 0.231, loss: 2226.712646\n",
      "Train: step:  89500, time: 0.196, loss: 2154.731445\n",
      "Train: step:  89510, time: 0.216, loss: 221.533813\n",
      "Train: step:  89520, time: 0.190, loss: 1840.104736\n",
      "Train: step:  89530, time: 0.218, loss: 1205.941895\n",
      "Train: step:  89540, time: 0.193, loss: 3528.396973\n",
      "Train: step:  89550, time: 0.202, loss: 1027.210571\n",
      "Train: step:  89560, time: 0.192, loss: 1334.231934\n",
      "Train: step:  89570, time: 0.217, loss: 2205.459961\n",
      "Train: step:  89580, time: 0.216, loss: 1245.834839\n",
      "Train: step:  89590, time: 0.195, loss: 2941.262695\n",
      "Train: step:  89600, time: 0.195, loss: 1972.569336\n",
      "Train: step:  89610, time: 0.217, loss: 2071.939941\n",
      "Train: step:  89620, time: 0.188, loss: 601.294983\n",
      "Train: step:  89630, time: 0.202, loss: 2402.062744\n",
      "Train: step:  89640, time: 0.186, loss: 354.651733\n",
      "Train: step:  89650, time: 0.218, loss: 1505.819214\n",
      "Train: step:  89660, time: 0.191, loss: 3678.623047\n",
      "Train: step:  89670, time: 0.204, loss: 1066.447754\n",
      "Train: step:  89680, time: 0.193, loss: 2453.689209\n",
      "Train: step:  89690, time: 0.192, loss: 2101.242920\n",
      "Train: step:  89700, time: 0.188, loss: 853.566162\n",
      "Train: step:  89710, time: 0.201, loss: 1159.237915\n",
      "Train: step:  89720, time: 0.219, loss: 2710.975586\n",
      "Train: step:  89730, time: 0.193, loss: 2059.151123\n",
      "Train: step:  89740, time: 0.193, loss: 2869.620117\n",
      "Train: step:  89750, time: 0.196, loss: 1864.389282\n",
      "Train: step:  89760, time: 0.210, loss: 1546.020752\n",
      "Train: step:  89770, time: 0.223, loss: 841.388611\n",
      "Train: step:  89780, time: 0.194, loss: 2109.406250\n",
      "Train: step:  89790, time: 0.217, loss: 3409.681641\n",
      "Train: step:  89800, time: 0.232, loss: 2541.957275\n",
      "Train: step:  89810, time: 0.192, loss: 2476.136963\n",
      "Train: step:  89820, time: 0.201, loss: 1152.977783\n",
      "Train: step:  89830, time: 0.214, loss: 2166.905029\n",
      "Train: step:  89840, time: 0.195, loss: 3597.518066\n",
      "Train: step:  89850, time: 0.217, loss: 2016.272827\n",
      "Train: step:  89860, time: 0.193, loss: 1907.504639\n",
      "Train: step:  89870, time: 0.196, loss: 1238.041504\n",
      "Train: step:  89880, time: 0.198, loss: 2392.216553\n",
      "Train: step:  89890, time: 0.232, loss: 449.365753\n",
      "Train: step:  89900, time: 0.218, loss: 666.704041\n",
      "Train: step:  89910, time: 0.196, loss: 2580.794922\n",
      "Train: step:  89920, time: 0.191, loss: 2688.174561\n",
      "Train: step:  89930, time: 0.195, loss: 1678.507446\n",
      "Train: step:  89940, time: 0.218, loss: 3234.905518\n",
      "Train: step:  89950, time: 0.199, loss: 1336.243896\n",
      "Train: step:  89960, time: 0.191, loss: 2453.643066\n",
      "Train: step:  89970, time: 0.195, loss: 2756.038818\n",
      "Train: step:  89980, time: 0.197, loss: 3136.476562\n",
      "Train: step:  89990, time: 0.198, loss: 1675.575439\n",
      "Train: step:  90000, time: 0.194, loss: 2832.965332\n",
      "Train: step:  90010, time: 0.192, loss: 2939.271973\n",
      "Train: step:  90020, time: 0.224, loss: 2561.424072\n",
      "Train: step:  90030, time: 0.188, loss: 1612.689697\n",
      "Train: step:  90040, time: 0.191, loss: 3391.558105\n",
      "Train: step:  90050, time: 0.191, loss: 1168.919922\n",
      "Train: step:  90060, time: 0.218, loss: 1847.364868\n",
      "Train: step:  90070, time: 0.198, loss: 1703.201904\n",
      "Train: step:  90080, time: 0.193, loss: 3122.942871\n",
      "Train: step:  90090, time: 0.201, loss: 1977.961792\n",
      "Train: step:  90100, time: 0.231, loss: 2366.524902\n",
      "Train: step:  90110, time: 0.195, loss: 2324.594727\n",
      "Train: step:  90120, time: 0.198, loss: 1557.051270\n",
      "Train: step:  90130, time: 0.201, loss: 1413.991455\n",
      "Train: step:  90140, time: 0.216, loss: 1300.216797\n",
      "Train: step:  90150, time: 0.193, loss: 4398.866699\n",
      "Train: step:  90160, time: 0.245, loss: 1028.956299\n",
      "Train: step:  90170, time: 0.219, loss: 2688.004150\n",
      "Train: step:  90180, time: 0.196, loss: 1403.492188\n",
      "Train: step:  90190, time: 0.191, loss: 1565.604126\n",
      "Train: step:  90200, time: 0.229, loss: 864.272949\n",
      "Train: step:  90210, time: 0.194, loss: 1343.673584\n",
      "Train: step:  90220, time: 0.228, loss: 3296.455566\n",
      "Train: step:  90230, time: 0.202, loss: 623.626526\n",
      "Train: step:  90240, time: 0.218, loss: 2337.470703\n",
      "Train: step:  90250, time: 0.225, loss: 949.468933\n",
      "Train: step:  90260, time: 0.223, loss: 581.270813\n",
      "Train: step:  90270, time: 0.194, loss: 549.350647\n",
      "Train: step:  90280, time: 0.222, loss: 1029.739624\n",
      "Train: step:  90290, time: 0.193, loss: 461.412109\n",
      "Train: step:  90300, time: 0.243, loss: 447.251190\n",
      "Train: step:  90310, time: 0.191, loss: 1237.748291\n",
      "Train: step:  90320, time: 0.226, loss: 2585.533203\n",
      "Train: step:  90330, time: 0.236, loss: 1057.893066\n",
      "Train: step:  90340, time: 0.212, loss: 2687.289795\n",
      "Train: step:  90350, time: 0.216, loss: 2552.949463\n",
      "Train: step:  90360, time: 0.198, loss: 257.459320\n",
      "Train: step:  90370, time: 0.192, loss: 906.164429\n",
      "Train: step:  90380, time: 0.215, loss: 2941.207275\n",
      "Train: step:  90390, time: 0.195, loss: 2143.629883\n",
      "Train: step:  90400, time: 0.220, loss: 2260.614258\n",
      "Train: step:  90410, time: 0.192, loss: 1608.146484\n",
      "Train: step:  90420, time: 0.251, loss: 573.468384\n",
      "Train: step:  90430, time: 0.216, loss: 1893.084473\n",
      "Train: step:  90440, time: 0.219, loss: 789.119263\n",
      "Train: step:  90450, time: 0.195, loss: 3623.006104\n",
      "Train: step:  90460, time: 0.235, loss: 2164.458740\n",
      "Train: step:  90470, time: 0.188, loss: 2502.817627\n",
      "Train: step:  90480, time: 0.189, loss: 3313.361816\n",
      "Train: step:  90490, time: 0.193, loss: 2842.315430\n",
      "Train: step:  90500, time: 0.227, loss: 2436.012207\n",
      "Train: step:  90510, time: 0.199, loss: 676.663269\n",
      "Train: step:  90520, time: 0.192, loss: 996.277771\n",
      "Train: step:  90530, time: 0.231, loss: 1771.410767\n",
      "Train: step:  90540, time: 0.204, loss: 656.149780\n",
      "Train: step:  90550, time: 0.198, loss: 1707.358643\n",
      "Train: step:  90560, time: 0.194, loss: 1126.267944\n",
      "Train: step:  90570, time: 0.192, loss: 2027.962891\n",
      "Train: step:  90580, time: 0.189, loss: 618.457703\n",
      "Train: step:  90590, time: 0.216, loss: 1855.764282\n",
      "Train: step:  90600, time: 0.196, loss: 439.316589\n",
      "Train: step:  90610, time: 0.224, loss: 2045.353638\n",
      "Train: step:  90620, time: 0.220, loss: 2640.223145\n",
      "Train: step:  90630, time: 0.218, loss: 1013.590454\n",
      "Train: step:  90640, time: 0.187, loss: 1654.265991\n",
      "Train: step:  90650, time: 0.199, loss: 1852.730225\n",
      "Train: step:  90660, time: 0.187, loss: 2696.831543\n",
      "Train: step:  90670, time: 0.189, loss: 2029.473999\n",
      "Train: step:  90680, time: 0.217, loss: 1361.910889\n",
      "Train: step:  90690, time: 0.191, loss: 1199.195312\n",
      "Train: step:  90700, time: 0.190, loss: 1371.918823\n",
      "Train: step:  90710, time: 0.194, loss: 231.201675\n",
      "Train: step:  90720, time: 0.196, loss: 1653.940552\n",
      "Train: step:  90730, time: 0.195, loss: 529.751709\n",
      "Train: step:  90740, time: 0.230, loss: 2974.325439\n",
      "Train: step:  90750, time: 0.197, loss: 2475.280762\n",
      "Train: step:  90760, time: 0.192, loss: 2243.950439\n",
      "Train: step:  90770, time: 0.200, loss: 582.895996\n",
      "Train: step:  90780, time: 0.224, loss: 2351.506348\n",
      "Train: step:  90790, time: 0.191, loss: 2461.542725\n",
      "Train: step:  90800, time: 0.189, loss: 2553.069336\n",
      "Train: step:  90810, time: 0.188, loss: 2351.171143\n",
      "Train: step:  90820, time: 0.193, loss: 1039.019775\n",
      "Train: step:  90830, time: 0.204, loss: 1440.399292\n",
      "Train: step:  90840, time: 0.201, loss: 2501.774414\n",
      "Train: step:  90850, time: 0.194, loss: 1252.701538\n",
      "Train: step:  90860, time: 0.219, loss: 773.039307\n",
      "Train: step:  90870, time: 0.195, loss: 3094.199219\n",
      "Train: step:  90880, time: 0.216, loss: 3767.956787\n",
      "Train: step:  90890, time: 0.193, loss: 2018.984863\n",
      "Train: step:  90900, time: 0.198, loss: 3666.126221\n",
      "Train: step:  90910, time: 0.226, loss: 1626.962280\n",
      "Train: step:  90920, time: 0.228, loss: 3323.592041\n",
      "Train: step:  90930, time: 0.192, loss: 2637.740479\n",
      "Train: step:  90940, time: 0.231, loss: 739.091431\n",
      "Train: step:  90950, time: 0.220, loss: 1017.382812\n",
      "Train: step:  90960, time: 0.191, loss: 2511.753662\n",
      "Train: step:  90970, time: 0.200, loss: 524.994507\n",
      "Train: step:  90980, time: 0.192, loss: 1895.909668\n",
      "Train: step:  90990, time: 0.197, loss: 1258.327881\n",
      "Train: step:  91000, time: 0.185, loss: 636.006836\n",
      "Train: step:  91010, time: 0.193, loss: 2081.094238\n",
      "Train: step:  91020, time: 0.218, loss: 1006.533508\n",
      "Train: step:  91030, time: 0.197, loss: 1215.428101\n",
      "Train: step:  91040, time: 0.191, loss: 1029.286133\n",
      "Train: step:  91050, time: 0.216, loss: 2148.900879\n",
      "Train: step:  91060, time: 0.226, loss: 3072.264893\n",
      "Train: step:  91070, time: 0.195, loss: 2544.426758\n",
      "Train: step:  91080, time: 0.218, loss: 3061.120361\n",
      "Train: step:  91090, time: 0.218, loss: 2334.594971\n",
      "Train: step:  91100, time: 0.186, loss: 2151.442871\n",
      "Train: step:  91110, time: 0.194, loss: 1671.189331\n",
      "Train: step:  91120, time: 0.194, loss: 739.046448\n",
      "Train: step:  91130, time: 0.190, loss: 941.990540\n",
      "Train: step:  91140, time: 0.188, loss: 1802.820190\n",
      "Train: step:  91150, time: 0.238, loss: 657.696594\n",
      "Train: step:  91160, time: 0.238, loss: 1824.091309\n",
      "Train: step:  91170, time: 0.200, loss: 1206.103882\n",
      "Train: step:  91180, time: 0.192, loss: 1114.354980\n",
      "Train: step:  91190, time: 0.193, loss: 234.287399\n",
      "Train: step:  91200, time: 0.198, loss: 1271.726685\n",
      "Train: step:  91210, time: 0.200, loss: 1154.934814\n",
      "Train: step:  91220, time: 0.199, loss: 756.903076\n",
      "Train: step:  91230, time: 0.219, loss: 1145.653687\n",
      "Train: step:  91240, time: 0.237, loss: 1042.852417\n",
      "Train: step:  91250, time: 0.221, loss: 1038.563477\n",
      "Train: step:  91260, time: 0.242, loss: 2584.755615\n",
      "Train: step:  91270, time: 0.192, loss: 1387.079956\n",
      "Train: step:  91280, time: 0.219, loss: 2460.420654\n",
      "Train: step:  91290, time: 0.188, loss: 3102.000000\n",
      "Train: step:  91300, time: 0.196, loss: 2483.656494\n",
      "Train: step:  91310, time: 0.197, loss: 2632.227783\n",
      "Train: step:  91320, time: 0.197, loss: 2063.329102\n",
      "Train: step:  91330, time: 0.197, loss: 993.856750\n",
      "Train: step:  91340, time: 0.216, loss: 3597.955078\n",
      "Train: step:  91350, time: 0.221, loss: 2481.948730\n",
      "Train: step:  91360, time: 0.233, loss: 1764.639771\n",
      "Train: step:  91370, time: 0.200, loss: 765.491577\n",
      "Train: step:  91380, time: 0.189, loss: 1885.800415\n",
      "Train: step:  91390, time: 0.197, loss: 2768.315430\n",
      "Train: step:  91400, time: 0.189, loss: 1016.262512\n",
      "Train: step:  91410, time: 0.188, loss: 1393.950317\n",
      "Train: step:  91420, time: 0.196, loss: 1253.482544\n",
      "Train: step:  91430, time: 0.189, loss: 5024.863281\n",
      "Train: step:  91440, time: 0.195, loss: 782.383911\n",
      "Train: step:  91450, time: 0.192, loss: 2865.369629\n",
      "Train: step:  91460, time: 0.210, loss: 708.454163\n",
      "Train: step:  91470, time: 0.189, loss: 2178.161133\n",
      "Train: step:  91480, time: 0.209, loss: 982.914368\n",
      "Train: step:  91490, time: 0.200, loss: 1155.767944\n",
      "Train: step:  91500, time: 0.204, loss: 2577.287598\n",
      "Train: step:  91510, time: 0.191, loss: 2830.876221\n",
      "Train: step:  91520, time: 0.219, loss: 2762.298828\n",
      "Train: step:  91530, time: 0.199, loss: 1987.542358\n",
      "Train: step:  91540, time: 0.191, loss: 2249.298828\n",
      "Train: step:  91550, time: 0.230, loss: 2106.959961\n",
      "Train: step:  91560, time: 0.216, loss: 2031.108521\n",
      "Train: step:  91570, time: 0.190, loss: 1842.327515\n",
      "Train: step:  91580, time: 0.198, loss: 2366.002441\n",
      "Train: step:  91590, time: 0.193, loss: 2660.135742\n",
      "Train: step:  91600, time: 0.215, loss: 2678.186279\n",
      "Train: step:  91610, time: 0.219, loss: 1610.711548\n",
      "Train: step:  91620, time: 0.197, loss: 1839.839966\n",
      "Train: step:  91630, time: 0.191, loss: 1588.566406\n",
      "Train: step:  91640, time: 0.203, loss: 4079.485596\n",
      "Train: step:  91650, time: 0.190, loss: 654.378662\n",
      "Train: step:  91660, time: 0.217, loss: 418.162903\n",
      "Train: step:  91670, time: 0.222, loss: 1515.617310\n",
      "Train: step:  91680, time: 0.190, loss: 1666.679565\n",
      "Train: step:  91690, time: 0.188, loss: 2183.693115\n",
      "Train: step:  91700, time: 0.191, loss: 3548.047119\n",
      "Train: step:  91710, time: 0.211, loss: 484.447540\n",
      "Train: step:  91720, time: 0.194, loss: 3651.316162\n",
      "Train: step:  91730, time: 0.217, loss: 2824.533936\n",
      "Train: step:  91740, time: 0.232, loss: 1503.113770\n",
      "Train: step:  91750, time: 0.191, loss: 1825.841187\n",
      "Train: step:  91760, time: 0.194, loss: 673.648193\n",
      "Train: step:  91770, time: 0.195, loss: 2286.552246\n",
      "Train: step:  91780, time: 0.193, loss: 1698.331421\n",
      "Train: step:  91790, time: 0.206, loss: 1548.655396\n",
      "Train: step:  91800, time: 0.191, loss: 2332.440674\n",
      "Train: step:  91810, time: 0.193, loss: 1756.078857\n",
      "Train: step:  91820, time: 0.221, loss: 1290.199341\n",
      "Train: step:  91830, time: 0.196, loss: 3274.726807\n",
      "Train: step:  91840, time: 0.189, loss: 2365.906982\n",
      "Train: step:  91850, time: 0.227, loss: 1020.921631\n",
      "Train: step:  91860, time: 0.199, loss: 2019.235107\n",
      "Train: step:  91870, time: 0.215, loss: 835.226624\n",
      "Train: step:  91880, time: 0.192, loss: 828.021057\n",
      "Train: step:  91890, time: 0.200, loss: 2252.404785\n",
      "Train: step:  91900, time: 0.193, loss: 912.510254\n",
      "Train: step:  91910, time: 0.249, loss: 2457.477783\n",
      "Train: step:  91920, time: 0.195, loss: 1818.993042\n",
      "Train: step:  91930, time: 0.195, loss: 351.792633\n",
      "Train: step:  91940, time: 0.188, loss: 855.500305\n",
      "Train: step:  91950, time: 0.192, loss: 2752.103271\n",
      "Train: step:  91960, time: 0.192, loss: 1158.922363\n",
      "Train: step:  91970, time: 0.191, loss: 2913.051758\n",
      "Train: step:  91980, time: 0.190, loss: 2177.154053\n",
      "Train: step:  91990, time: 0.203, loss: 2649.297852\n",
      "Train: step:  92000, time: 0.193, loss: 1759.709595\n",
      "Train: step:  92010, time: 0.195, loss: 1238.775024\n",
      "Train: step:  92020, time: 0.244, loss: 3622.978027\n",
      "Train: step:  92030, time: 0.188, loss: 1056.619019\n",
      "Train: step:  92040, time: 0.199, loss: 3255.169189\n",
      "Train: step:  92050, time: 0.211, loss: 1023.079224\n",
      "Train: step:  92060, time: 0.219, loss: 2154.165527\n",
      "Train: step:  92070, time: 0.220, loss: 2868.057129\n",
      "Train: step:  92080, time: 0.193, loss: 2140.896484\n",
      "Train: step:  92090, time: 0.231, loss: 2113.679688\n",
      "Train: step:  92100, time: 0.228, loss: 2539.806885\n",
      "Train: step:  92110, time: 0.196, loss: 1297.809814\n",
      "Train: step:  92120, time: 0.197, loss: 1129.085693\n",
      "Train: step:  92130, time: 0.218, loss: 1848.805176\n",
      "Train: step:  92140, time: 0.218, loss: 896.423889\n",
      "Train: step:  92150, time: 0.192, loss: 1895.687988\n",
      "Train: step:  92160, time: 0.191, loss: 2072.454346\n",
      "Train: step:  92170, time: 0.226, loss: 2427.101074\n",
      "Train: step:  92180, time: 0.234, loss: 1916.942505\n",
      "Train: step:  92190, time: 0.193, loss: 576.953735\n",
      "Train: step:  92200, time: 0.191, loss: 1603.648560\n",
      "Train: step:  92210, time: 0.203, loss: 1202.331787\n",
      "Train: step:  92220, time: 0.188, loss: 1859.275757\n",
      "Train: step:  92230, time: 0.193, loss: 1218.635864\n",
      "Train: step:  92240, time: 0.191, loss: 3260.004150\n",
      "Train: step:  92250, time: 0.191, loss: 523.956970\n",
      "Train: step:  92260, time: 0.190, loss: 584.530151\n",
      "Train: step:  92270, time: 0.228, loss: 1024.033081\n",
      "Train: step:  92280, time: 0.189, loss: 3036.876221\n",
      "Train: step:  92290, time: 0.191, loss: 2597.436279\n",
      "Train: step:  92300, time: 0.196, loss: 779.709534\n",
      "Train: step:  92310, time: 0.201, loss: 834.472961\n",
      "Train: step:  92320, time: 0.190, loss: 2409.241699\n",
      "Train: step:  92330, time: 0.194, loss: 786.878418\n",
      "Train: step:  92340, time: 0.198, loss: 1063.585693\n",
      "Train: step:  92350, time: 0.190, loss: 2724.075195\n",
      "Train: step:  92360, time: 0.193, loss: 601.122437\n",
      "Train: step:  92370, time: 0.215, loss: 1528.377686\n",
      "Train: step:  92380, time: 0.192, loss: 2006.908325\n",
      "Train: step:  92390, time: 0.197, loss: 3397.278564\n",
      "Train: step:  92400, time: 0.217, loss: 225.976746\n",
      "Train: step:  92410, time: 0.192, loss: 482.789948\n",
      "Train: step:  92420, time: 0.196, loss: 1968.387329\n",
      "Train: step:  92430, time: 0.189, loss: 1031.107178\n",
      "Train: step:  92440, time: 0.200, loss: 1598.513916\n",
      "Train: step:  92450, time: 0.233, loss: 1811.262695\n",
      "Train: step:  92460, time: 0.197, loss: 883.736023\n",
      "Train: step:  92470, time: 0.215, loss: 2433.834229\n",
      "Train: step:  92480, time: 0.194, loss: 1159.136475\n",
      "Train: step:  92490, time: 0.194, loss: 591.175781\n",
      "Train: step:  92500, time: 0.199, loss: 1373.288574\n",
      "Train: step:  92510, time: 0.192, loss: 1234.216064\n",
      "Train: step:  92520, time: 0.215, loss: 942.418274\n",
      "Train: step:  92530, time: 0.195, loss: 3113.774414\n",
      "Train: step:  92540, time: 0.198, loss: 2196.588623\n",
      "Train: step:  92550, time: 0.204, loss: 1470.473999\n",
      "Train: step:  92560, time: 0.205, loss: 937.991760\n",
      "Train: step:  92570, time: 0.233, loss: 3288.554688\n",
      "Train: step:  92580, time: 0.213, loss: 1569.668945\n",
      "Train: step:  92590, time: 0.253, loss: 3217.536621\n",
      "Train: step:  92600, time: 0.201, loss: 1595.203979\n",
      "Train: step:  92610, time: 0.217, loss: 878.221863\n",
      "Train: step:  92620, time: 0.196, loss: 2236.879395\n",
      "Train: step:  92630, time: 0.188, loss: 2193.083496\n",
      "Train: step:  92640, time: 0.231, loss: 1476.012329\n",
      "Train: step:  92650, time: 0.193, loss: 2804.798828\n",
      "Train: step:  92660, time: 0.218, loss: 2148.904541\n",
      "Train: step:  92670, time: 0.193, loss: 2349.713135\n",
      "Train: step:  92680, time: 0.227, loss: 3362.508545\n",
      "Train: step:  92690, time: 0.191, loss: 1882.679321\n",
      "Train: step:  92700, time: 0.194, loss: 1646.169678\n",
      "Train: step:  92710, time: 0.198, loss: 4013.895264\n",
      "Train: step:  92720, time: 0.198, loss: 718.726196\n",
      "Train: step:  92730, time: 0.199, loss: 2009.568604\n",
      "Train: step:  92740, time: 0.188, loss: 843.280701\n",
      "Train: step:  92750, time: 0.196, loss: 591.854004\n",
      "Train: step:  92760, time: 0.224, loss: 2948.659424\n",
      "Train: step:  92770, time: 0.237, loss: 256.871490\n",
      "Train: step:  92780, time: 0.195, loss: 1970.999634\n",
      "Train: step:  92790, time: 0.196, loss: 1203.687866\n",
      "Train: step:  92800, time: 0.218, loss: 130.565933\n",
      "Train: step:  92810, time: 0.214, loss: 2330.257324\n",
      "Train: step:  92820, time: 0.216, loss: 4196.750977\n",
      "Train: step:  92830, time: 0.217, loss: 2808.311523\n",
      "Train: step:  92840, time: 0.194, loss: 1960.492554\n",
      "Train: step:  92850, time: 0.199, loss: 2054.688721\n",
      "Train: step:  92860, time: 0.218, loss: 2320.802734\n",
      "Train: step:  92870, time: 0.192, loss: 875.914429\n",
      "Train: step:  92880, time: 0.220, loss: 2995.081055\n",
      "Train: step:  92890, time: 0.201, loss: 2157.377197\n",
      "Train: step:  92900, time: 0.191, loss: 2908.479980\n",
      "Train: step:  92910, time: 0.194, loss: 2971.826172\n",
      "Train: step:  92920, time: 0.211, loss: 1286.132202\n",
      "Train: step:  92930, time: 0.196, loss: 3614.925293\n",
      "Train: step:  92940, time: 0.191, loss: 399.455658\n",
      "Train: step:  92950, time: 0.190, loss: 3265.611572\n",
      "Train: step:  92960, time: 0.235, loss: 1096.392578\n",
      "Train: step:  92970, time: 0.191, loss: 2937.722900\n",
      "Train: step:  92980, time: 0.226, loss: 2255.775635\n",
      "Train: step:  92990, time: 0.198, loss: 1868.477051\n",
      "Train: step:  93000, time: 0.201, loss: 2354.552490\n",
      "Train: step:  93010, time: 0.219, loss: 2905.474854\n",
      "Train: step:  93020, time: 0.226, loss: 980.494202\n",
      "Train: step:  93030, time: 0.194, loss: 541.607605\n",
      "Train: step:  93040, time: 0.222, loss: 1429.776123\n",
      "Train: step:  93050, time: 0.209, loss: 1953.149414\n",
      "Train: step:  93060, time: 0.191, loss: 1169.534058\n",
      "Train: step:  93070, time: 0.197, loss: 1374.848755\n",
      "Train: step:  93080, time: 0.217, loss: 2176.843262\n",
      "Train: step:  93090, time: 0.218, loss: 3071.930908\n",
      "Train: step:  93100, time: 0.192, loss: 1525.545532\n",
      "Train: step:  93110, time: 0.188, loss: 1385.673218\n",
      "Train: step:  93120, time: 0.191, loss: 407.916534\n",
      "Train: step:  93130, time: 0.190, loss: 1801.156738\n",
      "Train: step:  93140, time: 0.189, loss: 1908.762329\n",
      "Train: step:  93150, time: 0.192, loss: 1917.539795\n",
      "Train: step:  93160, time: 0.190, loss: 2055.714111\n",
      "Train: step:  93170, time: 0.204, loss: 1868.421631\n",
      "Train: step:  93180, time: 0.192, loss: 2296.646729\n",
      "Train: step:  93190, time: 0.194, loss: 2414.836182\n",
      "Train: step:  93200, time: 0.195, loss: 406.933716\n",
      "Train: step:  93210, time: 0.193, loss: 2427.578613\n",
      "Train: step:  93220, time: 0.190, loss: 3186.585205\n",
      "Train: step:  93230, time: 0.189, loss: 3123.201172\n",
      "Train: step:  93240, time: 0.216, loss: 1345.779297\n",
      "Train: step:  93250, time: 0.220, loss: 2163.739502\n",
      "Train: step:  93260, time: 0.222, loss: 2701.244141\n",
      "Train: step:  93270, time: 0.227, loss: 1514.269897\n",
      "Train: step:  93280, time: 0.245, loss: 266.784607\n",
      "Train: step:  93290, time: 0.204, loss: 356.130676\n",
      "Train: step:  93300, time: 0.193, loss: 3562.014404\n",
      "Train: step:  93310, time: 0.193, loss: 1672.065063\n",
      "Train: step:  93320, time: 0.191, loss: 2128.620117\n",
      "Train: step:  93330, time: 0.187, loss: 2302.223633\n",
      "Train: step:  93340, time: 0.195, loss: 2321.623291\n",
      "Train: step:  93350, time: 0.229, loss: 2101.136230\n",
      "Train: step:  93360, time: 0.223, loss: 1323.637939\n",
      "Train: step:  93370, time: 0.199, loss: 2600.572510\n",
      "Train: step:  93380, time: 0.193, loss: 3005.201660\n",
      "Train: step:  93390, time: 0.205, loss: 1456.312012\n",
      "Train: step:  93400, time: 0.231, loss: 715.279968\n",
      "Train: step:  93410, time: 0.220, loss: 2688.207520\n",
      "Train: step:  93420, time: 0.195, loss: 3468.341797\n",
      "Train: step:  93430, time: 0.194, loss: 757.212219\n",
      "Train: step:  93440, time: 0.186, loss: 546.886414\n",
      "Train: step:  93450, time: 0.218, loss: 5625.522949\n",
      "Train: step:  93460, time: 0.195, loss: 2633.826416\n",
      "Train: step:  93470, time: 0.227, loss: 2379.937012\n",
      "Train: step:  93480, time: 0.190, loss: 959.566345\n",
      "Train: step:  93490, time: 0.218, loss: 3459.547119\n",
      "Train: step:  93500, time: 0.196, loss: 676.500183\n",
      "Train: step:  93510, time: 0.210, loss: 288.849792\n",
      "Train: step:  93520, time: 0.217, loss: 2339.864990\n",
      "Train: step:  93530, time: 0.195, loss: 2081.946045\n",
      "Train: step:  93540, time: 0.195, loss: 1018.683044\n",
      "Train: step:  93550, time: 0.194, loss: 2045.812256\n",
      "Train: step:  93560, time: 0.190, loss: 1532.955811\n",
      "Train: step:  93570, time: 0.191, loss: 2867.265869\n",
      "Train: step:  93580, time: 0.201, loss: 1185.319458\n",
      "Train: step:  93590, time: 0.199, loss: 1148.784546\n",
      "Train: step:  93600, time: 0.230, loss: 1023.022644\n",
      "Train: step:  93610, time: 0.193, loss: 1269.993652\n",
      "Train: step:  93620, time: 0.221, loss: 2181.092285\n",
      "Train: step:  93630, time: 0.195, loss: 1425.088867\n",
      "Train: step:  93640, time: 0.216, loss: 409.539032\n",
      "Train: step:  93650, time: 0.193, loss: 1317.614136\n",
      "Train: step:  93660, time: 0.217, loss: 3613.850098\n",
      "Train: step:  93670, time: 0.192, loss: 1616.851807\n",
      "Train: step:  93680, time: 0.197, loss: 2831.887207\n",
      "Train: step:  93690, time: 0.190, loss: 217.262161\n",
      "Train: step:  93700, time: 0.225, loss: 2341.771973\n",
      "Train: step:  93710, time: 0.195, loss: 2363.947998\n",
      "Train: step:  93720, time: 0.238, loss: 843.475037\n",
      "Train: step:  93730, time: 0.195, loss: 2773.040527\n",
      "Train: step:  93740, time: 0.232, loss: 2395.254883\n",
      "Train: step:  93750, time: 0.242, loss: 1703.008911\n",
      "Train: step:  93760, time: 0.194, loss: 1569.449097\n",
      "Train: step:  93770, time: 0.188, loss: 2503.659668\n",
      "Train: step:  93780, time: 0.189, loss: 753.468872\n",
      "Train: step:  93790, time: 0.198, loss: 2017.681641\n",
      "Train: step:  93800, time: 0.191, loss: 1835.049561\n",
      "Train: step:  93810, time: 0.194, loss: 1603.609863\n",
      "Train: step:  93820, time: 0.231, loss: 2443.370850\n",
      "Train: step:  93830, time: 0.194, loss: 1708.307739\n",
      "Train: step:  93840, time: 0.195, loss: 3148.592041\n",
      "Train: step:  93850, time: 0.191, loss: 1711.917847\n",
      "Train: step:  93860, time: 0.214, loss: 1722.747070\n",
      "Train: step:  93870, time: 0.193, loss: 3244.909180\n",
      "Train: step:  93880, time: 0.192, loss: 175.078018\n",
      "Train: step:  93890, time: 0.217, loss: 1114.812134\n",
      "Train: step:  93900, time: 0.198, loss: 2076.210938\n",
      "Train: step:  93910, time: 0.216, loss: 1222.565308\n",
      "Train: step:  93920, time: 0.201, loss: 3094.543701\n",
      "Train: step:  93930, time: 0.192, loss: 1847.513916\n",
      "Train: step:  93940, time: 0.212, loss: 2989.222656\n",
      "Train: step:  93950, time: 0.196, loss: 1303.671143\n",
      "Train: step:  93960, time: 0.189, loss: 1315.400391\n",
      "Train: step:  93970, time: 0.219, loss: 595.632202\n",
      "Train: step:  93980, time: 0.190, loss: 798.388062\n",
      "Train: step:  93990, time: 0.229, loss: 2634.800049\n",
      "Train: step:  94000, time: 0.236, loss: 1167.636108\n",
      "Train: step:  94010, time: 0.186, loss: 440.357086\n",
      "Train: step:  94020, time: 0.196, loss: 2084.885986\n",
      "Train: step:  94030, time: 0.198, loss: 1803.497925\n",
      "Train: step:  94040, time: 0.230, loss: 1575.904541\n",
      "Train: step:  94050, time: 0.193, loss: 3043.019043\n",
      "Train: step:  94060, time: 0.200, loss: 1656.293457\n",
      "Train: step:  94070, time: 0.195, loss: 482.323822\n",
      "Train: step:  94080, time: 0.217, loss: 1774.038574\n",
      "Train: step:  94090, time: 0.210, loss: 2041.416626\n",
      "Train: step:  94100, time: 0.191, loss: 775.359192\n",
      "Train: step:  94110, time: 0.189, loss: 1113.349121\n",
      "Train: step:  94120, time: 0.193, loss: 2985.721191\n",
      "Train: step:  94130, time: 0.186, loss: 2892.431152\n",
      "Train: step:  94140, time: 0.219, loss: 1936.880249\n",
      "Train: step:  94150, time: 0.221, loss: 2204.892822\n",
      "Train: step:  94160, time: 0.197, loss: 2141.904785\n",
      "Train: step:  94170, time: 0.196, loss: 1918.572998\n",
      "Train: step:  94180, time: 0.232, loss: 996.782959\n",
      "Train: step:  94190, time: 0.193, loss: 1292.271851\n",
      "Train: step:  94200, time: 0.219, loss: 2491.797852\n",
      "Train: step:  94210, time: 0.193, loss: 2401.215332\n",
      "Train: step:  94220, time: 0.237, loss: 4074.094482\n",
      "Train: step:  94230, time: 0.200, loss: 669.856567\n",
      "Train: step:  94240, time: 0.195, loss: 1883.927490\n",
      "Train: step:  94250, time: 0.196, loss: 1254.083374\n",
      "Train: step:  94260, time: 0.196, loss: 1353.674316\n",
      "Train: step:  94270, time: 0.203, loss: 1265.913086\n",
      "Train: step:  94280, time: 0.236, loss: 3469.263916\n",
      "Train: step:  94290, time: 0.248, loss: 1257.617432\n",
      "Train: step:  94300, time: 0.211, loss: 874.269348\n",
      "Train: step:  94310, time: 0.218, loss: 1795.535889\n",
      "Train: step:  94320, time: 0.206, loss: 3427.291016\n",
      "Train: step:  94330, time: 0.193, loss: 1821.133911\n",
      "Train: step:  94340, time: 0.189, loss: 2013.645752\n",
      "Train: step:  94350, time: 0.190, loss: 1388.787720\n",
      "Train: step:  94360, time: 0.190, loss: 1438.868652\n",
      "Train: step:  94370, time: 0.190, loss: 1769.521362\n",
      "Train: step:  94380, time: 0.202, loss: 2803.551270\n",
      "Train: step:  94390, time: 0.196, loss: 2188.338379\n",
      "Train: step:  94400, time: 0.202, loss: 2740.543457\n",
      "Train: step:  94410, time: 0.193, loss: 2357.846436\n",
      "Train: step:  94420, time: 0.196, loss: 1506.341309\n",
      "Train: step:  94430, time: 0.191, loss: 1598.730957\n",
      "Train: step:  94440, time: 0.189, loss: 3405.640869\n",
      "Train: step:  94450, time: 0.196, loss: 1788.604492\n",
      "Train: step:  94460, time: 0.197, loss: 999.666199\n",
      "Train: step:  94470, time: 0.202, loss: 3465.848389\n",
      "Train: step:  94480, time: 0.220, loss: 2046.181396\n",
      "Train: step:  94490, time: 0.194, loss: 1503.009888\n",
      "Train: step:  94500, time: 0.192, loss: 2493.177002\n",
      "Train: step:  94510, time: 0.189, loss: 3137.082031\n",
      "Train: step:  94520, time: 0.193, loss: 2868.824219\n",
      "Train: step:  94530, time: 0.192, loss: 1815.505615\n",
      "Train: step:  94540, time: 0.220, loss: 2549.060791\n",
      "Train: step:  94550, time: 0.193, loss: 3563.351074\n",
      "Train: step:  94560, time: 0.190, loss: 1009.049927\n",
      "Train: step:  94570, time: 0.193, loss: 817.846191\n",
      "Train: step:  94580, time: 0.194, loss: 1706.261719\n",
      "Train: step:  94590, time: 0.195, loss: 3308.577148\n",
      "Train: step:  94600, time: 0.199, loss: 1336.614990\n",
      "Train: step:  94610, time: 0.188, loss: 1440.594116\n",
      "Train: step:  94620, time: 0.192, loss: 1249.459595\n",
      "Train: step:  94630, time: 0.235, loss: 2207.987305\n",
      "Train: step:  94640, time: 0.225, loss: 1538.994263\n",
      "Train: step:  94650, time: 0.225, loss: 903.191101\n",
      "Train: step:  94660, time: 0.201, loss: 820.193054\n",
      "Train: step:  94670, time: 0.187, loss: 792.820374\n",
      "Train: step:  94680, time: 0.230, loss: 419.632111\n",
      "Train: step:  94690, time: 0.195, loss: 463.154297\n",
      "Train: step:  94700, time: 0.189, loss: 2159.785889\n",
      "Train: step:  94710, time: 0.221, loss: 2478.906494\n",
      "Train: step:  94720, time: 0.192, loss: 3191.114502\n",
      "Train: step:  94730, time: 0.187, loss: 2636.238281\n",
      "Train: step:  94740, time: 0.197, loss: 1009.394165\n",
      "Train: step:  94750, time: 0.193, loss: 1053.838257\n",
      "Train: step:  94760, time: 0.198, loss: 1062.106201\n",
      "Train: step:  94770, time: 0.222, loss: 1301.288086\n",
      "Train: step:  94780, time: 0.207, loss: 1030.627930\n",
      "Train: step:  94790, time: 0.192, loss: 1936.673950\n",
      "Train: step:  94800, time: 0.194, loss: 2208.839600\n",
      "Train: step:  94810, time: 0.212, loss: 2033.852295\n",
      "Train: step:  94820, time: 0.201, loss: 2260.201904\n",
      "Train: step:  94830, time: 0.204, loss: 1528.420898\n",
      "Train: step:  94840, time: 0.233, loss: 1044.024780\n",
      "Train: step:  94850, time: 0.193, loss: 621.241089\n",
      "Train: step:  94860, time: 0.199, loss: 2193.140137\n",
      "Train: step:  94870, time: 0.192, loss: 1743.626831\n",
      "Train: step:  94880, time: 0.195, loss: 1902.106689\n",
      "Train: step:  94890, time: 0.199, loss: 3123.044189\n",
      "Train: step:  94900, time: 0.195, loss: 1163.730591\n",
      "Train: step:  94910, time: 0.196, loss: 1974.980103\n",
      "Train: step:  94920, time: 0.227, loss: 3957.978516\n",
      "Train: step:  94930, time: 0.194, loss: 1393.947266\n",
      "Train: step:  94940, time: 0.201, loss: 2270.280762\n",
      "Train: step:  94950, time: 0.221, loss: 1026.920898\n",
      "Train: step:  94960, time: 0.201, loss: 2673.282227\n",
      "Train: step:  94970, time: 0.217, loss: 1521.434937\n",
      "Train: step:  94980, time: 0.215, loss: 4590.521973\n",
      "Train: step:  94990, time: 0.201, loss: 1921.902466\n",
      "Train: step:  95000, time: 0.215, loss: 2129.731445\n",
      "Train: step:  95010, time: 0.198, loss: 2529.546387\n",
      "Train: step:  95020, time: 0.219, loss: 1963.129028\n",
      "Train: step:  95030, time: 0.190, loss: 609.340454\n",
      "Train: step:  95040, time: 0.206, loss: 1609.032227\n",
      "Train: step:  95050, time: 0.215, loss: 799.913879\n",
      "Train: step:  95060, time: 0.204, loss: 777.375244\n",
      "Train: step:  95070, time: 0.201, loss: 970.165588\n",
      "Train: step:  95080, time: 0.236, loss: 1218.008179\n",
      "Train: step:  95090, time: 0.199, loss: 2460.464355\n",
      "Train: step:  95100, time: 0.191, loss: 2694.619873\n",
      "Train: step:  95110, time: 0.196, loss: 2490.685303\n",
      "Train: step:  95120, time: 0.194, loss: 1395.837036\n",
      "Train: step:  95130, time: 0.217, loss: 1478.178711\n",
      "Train: step:  95140, time: 0.219, loss: 1243.573730\n",
      "Train: step:  95150, time: 0.226, loss: 1288.390747\n",
      "Train: step:  95160, time: 0.215, loss: 971.087585\n",
      "Train: step:  95170, time: 0.193, loss: 1992.831055\n",
      "Train: step:  95180, time: 0.188, loss: 1795.337280\n",
      "Train: step:  95190, time: 0.225, loss: 404.416809\n",
      "Train: step:  95200, time: 0.197, loss: 1715.808838\n",
      "Train: step:  95210, time: 0.217, loss: 983.857849\n",
      "Train: step:  95220, time: 0.193, loss: 2694.213135\n",
      "Train: step:  95230, time: 0.199, loss: 2597.991211\n",
      "Train: step:  95240, time: 0.228, loss: 2164.190674\n",
      "Train: step:  95250, time: 0.189, loss: 1094.716919\n",
      "Train: step:  95260, time: 0.200, loss: 2539.372803\n",
      "Train: step:  95270, time: 0.188, loss: 1038.954590\n",
      "Train: step:  95280, time: 0.223, loss: 2904.008057\n",
      "Train: step:  95290, time: 0.191, loss: 2815.432129\n",
      "Train: step:  95300, time: 0.201, loss: 2219.052246\n",
      "Train: step:  95310, time: 0.230, loss: 4621.487793\n",
      "Train: step:  95320, time: 0.196, loss: 1679.451416\n",
      "Train: step:  95330, time: 0.189, loss: 1413.750244\n",
      "Train: step:  95340, time: 0.190, loss: 3646.123535\n",
      "Train: step:  95350, time: 0.217, loss: 3053.598633\n",
      "Train: step:  95360, time: 0.191, loss: 2722.244873\n",
      "Train: step:  95370, time: 0.189, loss: 2015.506104\n",
      "Train: step:  95380, time: 0.200, loss: 2914.438477\n",
      "Train: step:  95390, time: 0.193, loss: 1552.310669\n",
      "Train: step:  95400, time: 0.188, loss: 2358.843262\n",
      "Train: step:  95410, time: 0.261, loss: 2183.846436\n",
      "Train: step:  95420, time: 0.227, loss: 2029.110840\n",
      "Train: step:  95430, time: 0.203, loss: 2570.467773\n",
      "Train: step:  95440, time: 0.194, loss: 2456.377441\n",
      "Train: step:  95450, time: 0.218, loss: 1538.125244\n",
      "Train: step:  95460, time: 0.227, loss: 3034.624512\n",
      "Train: step:  95470, time: 0.196, loss: 1816.794312\n",
      "Train: step:  95480, time: 0.206, loss: 3766.270264\n",
      "Train: step:  95490, time: 0.215, loss: 1727.057617\n",
      "Train: step:  95500, time: 0.202, loss: 1964.705322\n",
      "Train: step:  95510, time: 0.205, loss: 2131.356934\n",
      "Train: step:  95520, time: 0.189, loss: 2902.188965\n",
      "Train: step:  95530, time: 0.216, loss: 1500.156616\n",
      "Train: step:  95540, time: 0.193, loss: 1580.897827\n",
      "Train: step:  95550, time: 0.219, loss: 2503.112061\n",
      "Train: step:  95560, time: 0.228, loss: 2262.711914\n",
      "Train: step:  95570, time: 0.221, loss: 1270.846313\n",
      "Train: step:  95580, time: 0.188, loss: 919.084778\n",
      "Train: step:  95590, time: 0.196, loss: 2265.563477\n",
      "Train: step:  95600, time: 0.196, loss: 1189.431274\n",
      "Train: step:  95610, time: 0.217, loss: 3024.564209\n",
      "Train: step:  95620, time: 0.201, loss: 1481.516357\n",
      "Train: step:  95630, time: 0.219, loss: 2051.257568\n",
      "Train: step:  95640, time: 0.197, loss: 3389.433105\n",
      "Train: step:  95650, time: 0.230, loss: 2894.713135\n",
      "Train: step:  95660, time: 0.216, loss: 1481.332397\n",
      "Train: step:  95670, time: 0.239, loss: 1166.340454\n",
      "Train: step:  95680, time: 0.242, loss: 1790.988037\n",
      "Train: step:  95690, time: 0.193, loss: 1891.248413\n",
      "Train: step:  95700, time: 0.188, loss: 1715.367065\n",
      "Train: step:  95710, time: 0.217, loss: 2598.823242\n",
      "Train: step:  95720, time: 0.214, loss: 1476.742188\n",
      "Train: step:  95730, time: 0.201, loss: 2231.711914\n",
      "Train: step:  95740, time: 0.196, loss: 1719.086304\n",
      "Train: step:  95750, time: 0.201, loss: 1771.764282\n",
      "Train: step:  95760, time: 0.193, loss: 2848.690430\n",
      "Train: step:  95770, time: 0.190, loss: 3627.667969\n",
      "Train: step:  95780, time: 0.197, loss: 2226.549805\n",
      "Train: step:  95790, time: 0.191, loss: 2469.785645\n",
      "Train: step:  95800, time: 0.191, loss: 2102.447510\n",
      "Train: step:  95810, time: 0.196, loss: 1270.983643\n",
      "Train: step:  95820, time: 0.232, loss: 616.721802\n",
      "Train: step:  95830, time: 0.203, loss: 1013.950562\n",
      "Train: step:  95840, time: 0.199, loss: 2734.244629\n",
      "Train: step:  95850, time: 0.196, loss: 2073.788574\n",
      "Train: step:  95860, time: 0.193, loss: 2791.086182\n",
      "Train: step:  95870, time: 0.191, loss: 3130.568848\n",
      "Train: step:  95880, time: 0.199, loss: 1075.230347\n",
      "Train: step:  95890, time: 0.185, loss: 1953.034790\n",
      "Train: step:  95900, time: 0.228, loss: 1195.379272\n",
      "Train: step:  95910, time: 0.197, loss: 2687.874512\n",
      "Train: step:  95920, time: 0.187, loss: 1799.684937\n",
      "Train: step:  95930, time: 0.193, loss: 2211.282959\n",
      "Train: step:  95940, time: 0.192, loss: 2750.461914\n",
      "Train: step:  95950, time: 0.217, loss: 536.332825\n",
      "Train: step:  95960, time: 0.195, loss: 2149.358154\n",
      "Train: step:  95970, time: 0.201, loss: 833.157166\n",
      "Train: step:  95980, time: 0.189, loss: 276.522797\n",
      "Train: step:  95990, time: 0.199, loss: 1910.715698\n",
      "Train: step:  96000, time: 0.188, loss: 674.540466\n",
      "Train: step:  96010, time: 0.192, loss: 1428.394531\n",
      "Train: step:  96020, time: 0.219, loss: 2961.867676\n",
      "Train: step:  96030, time: 0.195, loss: 1261.368286\n",
      "Train: step:  96040, time: 0.225, loss: 641.704102\n",
      "Train: step:  96050, time: 0.224, loss: 1623.399536\n",
      "Train: step:  96060, time: 0.216, loss: 1707.387573\n",
      "Train: step:  96070, time: 0.196, loss: 1725.852295\n",
      "Train: step:  96080, time: 0.186, loss: 2608.427246\n",
      "Train: step:  96090, time: 0.199, loss: 3085.291504\n",
      "Train: step:  96100, time: 0.230, loss: 2008.509155\n",
      "Train: step:  96110, time: 0.212, loss: 1034.189087\n",
      "Train: step:  96120, time: 0.220, loss: 1715.222046\n",
      "Train: step:  96130, time: 0.216, loss: 1342.523071\n",
      "Train: step:  96140, time: 0.217, loss: 2033.771973\n",
      "Train: step:  96150, time: 0.230, loss: 827.499817\n",
      "Train: step:  96160, time: 0.245, loss: 2414.468262\n",
      "Train: step:  96170, time: 0.199, loss: 1258.435913\n",
      "Train: step:  96180, time: 0.219, loss: 584.196716\n",
      "Train: step:  96190, time: 0.189, loss: 1558.482788\n",
      "Train: step:  96200, time: 0.190, loss: 742.720825\n",
      "Train: step:  96210, time: 0.187, loss: 2564.945801\n",
      "Train: step:  96220, time: 0.198, loss: 1809.248047\n",
      "Train: step:  96230, time: 0.226, loss: 3401.210449\n",
      "Train: step:  96240, time: 0.195, loss: 861.585632\n",
      "Train: step:  96250, time: 0.196, loss: 2219.867432\n",
      "Train: step:  96260, time: 0.198, loss: 1787.466187\n",
      "Train: step:  96270, time: 0.197, loss: 2176.331787\n",
      "Train: step:  96280, time: 0.197, loss: 360.009094\n",
      "Train: step:  96290, time: 0.197, loss: 3175.313721\n",
      "Train: step:  96300, time: 0.196, loss: 3184.278076\n",
      "Train: step:  96310, time: 0.188, loss: 3318.671631\n",
      "Train: step:  96320, time: 0.193, loss: 2363.226318\n",
      "Train: step:  96330, time: 0.191, loss: 1217.772583\n",
      "Train: step:  96340, time: 0.234, loss: 2940.747070\n",
      "Train: step:  96350, time: 0.222, loss: 3821.601562\n",
      "Train: step:  96360, time: 0.208, loss: 1595.135986\n",
      "Train: step:  96370, time: 0.194, loss: 919.270813\n",
      "Train: step:  96380, time: 0.231, loss: 1844.888672\n",
      "Train: step:  96390, time: 0.224, loss: 1723.600586\n",
      "Train: step:  96400, time: 0.190, loss: 788.306335\n",
      "Train: step:  96410, time: 0.192, loss: 2260.879883\n",
      "Train: step:  96420, time: 0.189, loss: 1090.566284\n",
      "Train: step:  96430, time: 0.203, loss: 924.278259\n",
      "Train: step:  96440, time: 0.231, loss: 1684.877319\n",
      "Train: step:  96450, time: 0.202, loss: 2481.844482\n",
      "Train: step:  96460, time: 0.191, loss: 1364.410400\n",
      "Train: step:  96470, time: 0.229, loss: 2535.162598\n",
      "Train: step:  96480, time: 0.201, loss: 3428.787598\n",
      "Train: step:  96490, time: 0.220, loss: 2551.359863\n",
      "Train: step:  96500, time: 0.206, loss: 2085.336182\n",
      "Train: step:  96510, time: 0.229, loss: 771.030640\n",
      "Train: step:  96520, time: 0.207, loss: 1750.961792\n",
      "Train: step:  96530, time: 0.220, loss: 1676.144897\n",
      "Train: step:  96540, time: 0.203, loss: 910.294373\n",
      "Train: step:  96550, time: 0.198, loss: 1288.474854\n",
      "Train: step:  96560, time: 0.238, loss: 533.472473\n",
      "Train: step:  96570, time: 0.195, loss: 1780.117188\n",
      "Train: step:  96580, time: 0.220, loss: 765.716736\n",
      "Train: step:  96590, time: 0.212, loss: 1687.545410\n",
      "Train: step:  96600, time: 0.204, loss: 2029.136353\n",
      "Train: step:  96610, time: 0.232, loss: 1361.756958\n",
      "Train: step:  96620, time: 0.196, loss: 932.337524\n",
      "Train: step:  96630, time: 0.200, loss: 1162.404541\n",
      "Train: step:  96640, time: 0.218, loss: 1198.331299\n",
      "Train: step:  96650, time: 0.219, loss: 631.517517\n",
      "Train: step:  96660, time: 0.191, loss: 607.036011\n",
      "Train: step:  96670, time: 0.219, loss: 1677.467773\n",
      "Train: step:  96680, time: 0.239, loss: 1814.863892\n",
      "Train: step:  96690, time: 0.194, loss: 2199.104492\n",
      "Train: step:  96700, time: 0.203, loss: 2000.988770\n",
      "Train: step:  96710, time: 0.198, loss: 3103.797607\n",
      "Train: step:  96720, time: 0.208, loss: 2464.406250\n",
      "Train: step:  96730, time: 0.213, loss: 2368.495361\n",
      "Train: step:  96740, time: 0.218, loss: 642.193604\n",
      "Train: step:  96750, time: 0.196, loss: 2813.189697\n",
      "Train: step:  96760, time: 0.210, loss: 2388.180908\n",
      "Train: step:  96770, time: 0.213, loss: 429.354095\n",
      "Train: step:  96780, time: 0.198, loss: 1986.833862\n",
      "Train: step:  96790, time: 0.217, loss: 3327.768799\n",
      "Train: step:  96800, time: 0.226, loss: 879.739563\n",
      "Train: step:  96810, time: 0.198, loss: 2861.999512\n",
      "Train: step:  96820, time: 0.195, loss: 2796.013916\n",
      "Train: step:  96830, time: 0.190, loss: 2842.945557\n",
      "Train: step:  96840, time: 0.193, loss: 703.829468\n",
      "Train: step:  96850, time: 0.194, loss: 2710.518799\n",
      "Train: step:  96860, time: 0.228, loss: 3159.598633\n",
      "Train: step:  96870, time: 0.208, loss: 2221.542969\n",
      "Train: step:  96880, time: 0.213, loss: 449.354980\n",
      "Train: step:  96890, time: 0.208, loss: 2536.904053\n",
      "Train: step:  96900, time: 0.206, loss: 1952.816772\n",
      "Train: step:  96910, time: 0.201, loss: 3766.146729\n",
      "Train: step:  96920, time: 0.219, loss: 1780.349609\n",
      "Train: step:  96930, time: 0.194, loss: 774.070007\n",
      "Train: step:  96940, time: 0.219, loss: 1179.723999\n",
      "Train: step:  96950, time: 0.195, loss: 1741.253662\n",
      "Train: step:  96960, time: 0.198, loss: 4164.536621\n",
      "Train: step:  96970, time: 0.217, loss: 466.249298\n",
      "Train: step:  96980, time: 0.198, loss: 982.700928\n",
      "Train: step:  96990, time: 0.192, loss: 707.276672\n",
      "Train: step:  97000, time: 0.223, loss: 2858.224854\n",
      "Train: step:  97010, time: 0.192, loss: 682.227356\n",
      "Train: step:  97020, time: 0.218, loss: 260.608551\n",
      "Train: step:  97030, time: 0.212, loss: 1687.424561\n",
      "Train: step:  97040, time: 0.218, loss: 1963.709106\n",
      "Train: step:  97050, time: 0.215, loss: 670.193848\n",
      "Train: step:  97060, time: 0.236, loss: 2972.897705\n",
      "Train: step:  97070, time: 0.197, loss: 1392.456543\n",
      "Train: step:  97080, time: 0.190, loss: 2672.140869\n",
      "Train: step:  97090, time: 0.240, loss: 1855.228271\n",
      "Train: step:  97100, time: 0.198, loss: 2731.072021\n",
      "Train: step:  97110, time: 0.218, loss: 662.876404\n",
      "Train: step:  97120, time: 0.218, loss: 1273.290405\n",
      "Train: step:  97130, time: 0.193, loss: 1009.914978\n",
      "Train: step:  97140, time: 0.196, loss: 1897.266846\n",
      "Train: step:  97150, time: 0.186, loss: 2043.831543\n",
      "Train: step:  97160, time: 0.225, loss: 3120.406982\n",
      "Train: step:  97170, time: 0.196, loss: 1452.266479\n",
      "Train: step:  97180, time: 0.230, loss: 1478.514404\n",
      "Train: step:  97190, time: 0.202, loss: 2573.193359\n",
      "Train: step:  97200, time: 0.228, loss: 3464.873047\n",
      "Train: step:  97210, time: 0.194, loss: 1449.561523\n",
      "Train: step:  97220, time: 0.198, loss: 1602.397461\n",
      "Train: step:  97230, time: 0.218, loss: 2333.162109\n",
      "Train: step:  97240, time: 0.219, loss: 1567.457520\n",
      "Train: step:  97250, time: 0.187, loss: 1628.696533\n",
      "Train: step:  97260, time: 0.206, loss: 1076.354858\n",
      "Train: step:  97270, time: 0.195, loss: 1490.080078\n",
      "Train: step:  97280, time: 0.228, loss: 451.947235\n",
      "Train: step:  97290, time: 0.196, loss: 2438.264404\n",
      "Train: step:  97300, time: 0.190, loss: 914.774231\n",
      "Train: step:  97310, time: 0.225, loss: 1626.272949\n",
      "Train: step:  97320, time: 0.200, loss: 2414.438721\n",
      "Train: step:  97330, time: 0.220, loss: 1900.912720\n",
      "Train: step:  97340, time: 0.218, loss: 1014.511902\n",
      "Train: step:  97350, time: 0.218, loss: 1837.527466\n",
      "Train: step:  97360, time: 0.191, loss: 957.072083\n",
      "Train: step:  97370, time: 0.196, loss: 3229.354004\n",
      "Train: step:  97380, time: 0.193, loss: 2328.126465\n",
      "Train: step:  97390, time: 0.197, loss: 3236.426025\n",
      "Train: step:  97400, time: 0.225, loss: 2412.472900\n",
      "Train: step:  97410, time: 0.190, loss: 472.523804\n",
      "Train: step:  97420, time: 0.196, loss: 2014.992676\n",
      "Train: step:  97430, time: 0.232, loss: 897.894775\n",
      "Train: step:  97440, time: 0.200, loss: 2155.297852\n",
      "Train: step:  97450, time: 0.195, loss: 587.008057\n",
      "Train: step:  97460, time: 0.198, loss: 1434.593872\n",
      "Train: step:  97470, time: 0.199, loss: 2515.814697\n",
      "Train: step:  97480, time: 0.197, loss: 1120.086914\n",
      "Train: step:  97490, time: 0.219, loss: 775.687134\n",
      "Train: step:  97500, time: 0.194, loss: 2349.682861\n",
      "Train: step:  97510, time: 0.195, loss: 832.459534\n",
      "Train: step:  97520, time: 0.215, loss: 942.435486\n",
      "Train: step:  97530, time: 0.222, loss: 898.049377\n",
      "Train: step:  97540, time: 0.199, loss: 2370.551025\n",
      "Train: step:  97550, time: 0.191, loss: 1656.381836\n",
      "Train: step:  97560, time: 0.254, loss: 3345.653564\n",
      "Train: step:  97570, time: 0.190, loss: 1421.822266\n",
      "Train: step:  97580, time: 0.195, loss: 1938.604126\n",
      "Train: step:  97590, time: 0.203, loss: 3198.317871\n",
      "Train: step:  97600, time: 0.216, loss: 2010.602661\n",
      "Train: step:  97610, time: 0.198, loss: 1846.566895\n",
      "Train: step:  97620, time: 0.190, loss: 1705.507202\n",
      "Train: step:  97630, time: 0.198, loss: 4129.447754\n",
      "Train: step:  97640, time: 0.192, loss: 1221.028442\n",
      "Train: step:  97650, time: 0.190, loss: 804.535645\n",
      "Train: step:  97660, time: 0.195, loss: 1441.228394\n",
      "Train: step:  97670, time: 0.218, loss: 2658.963623\n",
      "Train: step:  97680, time: 0.218, loss: 1377.288452\n",
      "Train: step:  97690, time: 0.198, loss: 2905.800049\n",
      "Train: step:  97700, time: 0.243, loss: 1410.384155\n",
      "Train: step:  97710, time: 0.235, loss: 1433.744629\n",
      "Train: step:  97720, time: 0.216, loss: 2669.675049\n",
      "Train: step:  97730, time: 0.218, loss: 3014.274658\n",
      "Train: step:  97740, time: 0.247, loss: 366.050903\n",
      "Train: step:  97750, time: 0.235, loss: 1518.690552\n",
      "Train: step:  97760, time: 0.187, loss: 1918.944824\n",
      "Train: step:  97770, time: 0.234, loss: 3253.175537\n",
      "Train: step:  97780, time: 0.191, loss: 1349.070435\n",
      "Train: step:  97790, time: 0.219, loss: 543.106445\n",
      "Train: step:  97800, time: 0.221, loss: 787.074280\n",
      "Train: step:  97810, time: 0.221, loss: 799.195312\n",
      "Train: step:  97820, time: 0.197, loss: 1696.234619\n",
      "Train: step:  97830, time: 0.236, loss: 2896.029541\n",
      "Train: step:  97840, time: 0.230, loss: 1308.357056\n",
      "Train: step:  97850, time: 0.242, loss: 1301.717896\n",
      "Train: step:  97860, time: 0.196, loss: 831.456543\n",
      "Train: step:  97870, time: 0.198, loss: 3431.305420\n",
      "Train: step:  97880, time: 0.199, loss: 2126.072998\n",
      "Train: step:  97890, time: 0.221, loss: 1590.421509\n",
      "Train: step:  97900, time: 0.204, loss: 1609.134277\n",
      "Train: step:  97910, time: 0.217, loss: 1016.661499\n",
      "Train: step:  97920, time: 0.230, loss: 2673.282471\n",
      "Train: step:  97930, time: 0.233, loss: 1039.810425\n",
      "Train: step:  97940, time: 0.193, loss: 1012.453308\n",
      "Train: step:  97950, time: 0.189, loss: 410.509308\n",
      "Train: step:  97960, time: 0.195, loss: 1750.134033\n",
      "Train: step:  97970, time: 0.205, loss: 1967.057739\n",
      "Train: step:  97980, time: 0.216, loss: 521.230164\n",
      "Train: step:  97990, time: 0.226, loss: 1125.118286\n",
      "Train: step:  98000, time: 0.194, loss: 1325.870483\n",
      "Train: step:  98010, time: 0.191, loss: 2622.785645\n",
      "Train: step:  98020, time: 0.207, loss: 576.264099\n",
      "Train: step:  98030, time: 0.217, loss: 1516.072388\n",
      "Train: step:  98040, time: 0.199, loss: 2453.683105\n",
      "Train: step:  98050, time: 0.232, loss: 606.507996\n",
      "Train: step:  98060, time: 0.196, loss: 745.373596\n",
      "Train: step:  98070, time: 0.239, loss: 1368.703003\n",
      "Train: step:  98080, time: 0.217, loss: 1171.831787\n",
      "Train: step:  98090, time: 0.191, loss: 2895.330811\n",
      "Train: step:  98100, time: 0.193, loss: 1609.352173\n",
      "Train: step:  98110, time: 0.234, loss: 1484.556030\n",
      "Train: step:  98120, time: 0.205, loss: 1692.988770\n",
      "Train: step:  98130, time: 0.191, loss: 3147.168945\n",
      "Train: step:  98140, time: 0.195, loss: 998.307434\n",
      "Train: step:  98150, time: 0.229, loss: 2332.848145\n",
      "Train: step:  98160, time: 0.227, loss: 3502.409668\n",
      "Train: step:  98170, time: 0.228, loss: 411.196930\n",
      "Train: step:  98180, time: 0.215, loss: 908.478882\n",
      "Train: step:  98190, time: 0.194, loss: 2219.935791\n",
      "Train: step:  98200, time: 0.194, loss: 1687.005737\n",
      "Train: step:  98210, time: 0.240, loss: 3262.185791\n",
      "Train: step:  98220, time: 0.208, loss: 1830.640015\n",
      "Train: step:  98230, time: 0.219, loss: 508.993774\n",
      "Train: step:  98240, time: 0.194, loss: 1767.075806\n",
      "Train: step:  98250, time: 0.229, loss: 1280.124512\n",
      "Train: step:  98260, time: 0.226, loss: 2156.155518\n",
      "Train: step:  98270, time: 0.192, loss: 3005.705566\n",
      "Train: step:  98280, time: 0.229, loss: 1323.644775\n",
      "Train: step:  98290, time: 0.190, loss: 358.126129\n",
      "Train: step:  98300, time: 0.199, loss: 2253.326904\n",
      "Train: step:  98310, time: 0.197, loss: 1216.972290\n",
      "Train: step:  98320, time: 0.229, loss: 2311.361816\n",
      "Train: step:  98330, time: 0.190, loss: 523.380310\n",
      "Train: step:  98340, time: 0.242, loss: 2511.479004\n",
      "Train: step:  98350, time: 0.217, loss: 1639.622070\n",
      "Train: step:  98360, time: 0.218, loss: 2642.836670\n",
      "Train: step:  98370, time: 0.191, loss: 2115.313477\n",
      "Train: step:  98380, time: 0.200, loss: 1999.149292\n",
      "Train: step:  98390, time: 0.191, loss: 2663.209717\n",
      "Train: step:  98400, time: 0.231, loss: 696.681641\n",
      "Train: step:  98410, time: 0.227, loss: 1289.876465\n",
      "Train: step:  98420, time: 0.218, loss: 1219.305908\n",
      "Train: step:  98430, time: 0.218, loss: 2482.357666\n",
      "Train: step:  98440, time: 0.195, loss: 2772.013916\n",
      "Train: step:  98450, time: 0.193, loss: 2686.792725\n",
      "Train: step:  98460, time: 0.227, loss: 1941.776367\n",
      "Train: step:  98470, time: 0.199, loss: 2086.770020\n",
      "Train: step:  98480, time: 0.210, loss: 495.629730\n",
      "Train: step:  98490, time: 0.188, loss: 2586.521973\n",
      "Train: step:  98500, time: 0.228, loss: 2015.312012\n",
      "Train: step:  98510, time: 0.205, loss: 3078.407227\n",
      "Train: step:  98520, time: 0.223, loss: 1482.489502\n",
      "Train: step:  98530, time: 0.225, loss: 1294.974731\n",
      "Train: step:  98540, time: 0.203, loss: 1023.179260\n",
      "Train: step:  98550, time: 0.195, loss: 3922.048096\n",
      "Train: step:  98560, time: 0.194, loss: 3985.648193\n",
      "Train: step:  98570, time: 0.211, loss: 2036.278931\n",
      "Train: step:  98580, time: 0.188, loss: 944.271973\n",
      "Train: step:  98590, time: 0.218, loss: 1594.832886\n",
      "Train: step:  98600, time: 0.194, loss: 2410.406738\n",
      "Train: step:  98610, time: 0.218, loss: 901.538452\n",
      "Train: step:  98620, time: 0.201, loss: 2783.480957\n",
      "Train: step:  98630, time: 0.193, loss: 2854.996582\n",
      "Train: step:  98640, time: 0.190, loss: 2570.212402\n",
      "Train: step:  98650, time: 0.227, loss: 2774.926514\n",
      "Train: step:  98660, time: 0.197, loss: 604.928284\n",
      "Train: step:  98670, time: 0.197, loss: 1351.179077\n",
      "Train: step:  98680, time: 0.230, loss: 3118.318848\n",
      "Train: step:  98690, time: 0.212, loss: 932.470886\n",
      "Train: step:  98700, time: 0.198, loss: 1897.467041\n",
      "Train: step:  98710, time: 0.193, loss: 2742.286621\n",
      "Train: step:  98720, time: 0.219, loss: 1693.637207\n",
      "Train: step:  98730, time: 0.194, loss: 2705.661377\n",
      "Train: step:  98740, time: 0.189, loss: 960.902771\n",
      "Train: step:  98750, time: 0.192, loss: 1546.176025\n",
      "Train: step:  98760, time: 0.220, loss: 1735.352783\n",
      "Train: step:  98770, time: 0.197, loss: 2316.233154\n",
      "Train: step:  98780, time: 0.189, loss: 3797.027100\n",
      "Train: step:  98790, time: 0.218, loss: 2074.680420\n",
      "Train: step:  98800, time: 0.188, loss: 1802.272827\n",
      "Train: step:  98810, time: 0.202, loss: 1902.585449\n",
      "Train: step:  98820, time: 0.196, loss: 1723.709839\n",
      "Train: step:  98830, time: 0.197, loss: 3617.806885\n",
      "Train: step:  98840, time: 0.219, loss: 2390.592285\n",
      "Train: step:  98850, time: 0.198, loss: 1721.407104\n",
      "Train: step:  98860, time: 0.215, loss: 2038.576782\n",
      "Train: step:  98870, time: 0.196, loss: 1742.006470\n",
      "Train: step:  98880, time: 0.194, loss: 2369.543457\n",
      "Train: step:  98890, time: 0.216, loss: 1708.563965\n",
      "Train: step:  98900, time: 0.195, loss: 2460.712402\n",
      "Train: step:  98910, time: 0.195, loss: 2542.195312\n",
      "Train: step:  98920, time: 0.197, loss: 1978.420410\n",
      "Train: step:  98930, time: 0.229, loss: 1539.934448\n",
      "Train: step:  98940, time: 0.195, loss: 1857.699707\n",
      "Train: step:  98950, time: 0.200, loss: 2176.301514\n",
      "Train: step:  98960, time: 0.215, loss: 2014.068970\n",
      "Train: step:  98970, time: 0.212, loss: 595.849304\n",
      "Train: step:  98980, time: 0.193, loss: 1591.223267\n",
      "Train: step:  98990, time: 0.227, loss: 1339.380371\n",
      "Train: step:  99000, time: 0.203, loss: 2250.389404\n",
      "Train: step:  99010, time: 0.190, loss: 2709.220215\n",
      "Train: step:  99020, time: 0.195, loss: 930.364380\n",
      "Train: step:  99030, time: 0.222, loss: 1265.772339\n",
      "Train: step:  99040, time: 0.212, loss: 752.437500\n",
      "Train: step:  99050, time: 0.216, loss: 1614.660156\n",
      "Train: step:  99060, time: 0.224, loss: 2478.253418\n",
      "Train: step:  99070, time: 0.193, loss: 2014.823364\n",
      "Train: step:  99080, time: 0.218, loss: 1151.588379\n",
      "Train: step:  99090, time: 0.192, loss: 2156.939697\n",
      "Train: step:  99100, time: 0.231, loss: 562.793640\n",
      "Train: step:  99110, time: 0.194, loss: 250.583389\n",
      "Train: step:  99120, time: 0.202, loss: 3463.945557\n",
      "Train: step:  99130, time: 0.193, loss: 2249.934326\n",
      "Train: step:  99140, time: 0.225, loss: 1690.386597\n",
      "Train: step:  99150, time: 0.230, loss: 1372.871948\n",
      "Train: step:  99160, time: 0.221, loss: 1228.536255\n",
      "Train: step:  99170, time: 0.221, loss: 2263.577881\n",
      "Train: step:  99180, time: 0.195, loss: 3442.124023\n",
      "Train: step:  99190, time: 0.234, loss: 6990.443848\n",
      "Train: step:  99200, time: 0.207, loss: 1861.911743\n",
      "Train: step:  99210, time: 0.233, loss: 949.831726\n",
      "Train: step:  99220, time: 0.212, loss: 1275.550659\n",
      "Train: step:  99230, time: 0.239, loss: 891.547180\n",
      "Train: step:  99240, time: 0.208, loss: 3113.763428\n",
      "Train: step:  99250, time: 0.199, loss: 1099.535156\n",
      "Train: step:  99260, time: 0.246, loss: 3487.110352\n",
      "Train: step:  99270, time: 0.199, loss: 2478.843262\n",
      "Train: step:  99280, time: 0.188, loss: 1911.324707\n",
      "Train: step:  99290, time: 0.212, loss: 3282.788086\n",
      "Train: step:  99300, time: 0.197, loss: 3541.193604\n",
      "Train: step:  99310, time: 0.218, loss: 2679.106445\n",
      "Train: step:  99320, time: 0.202, loss: 967.921387\n",
      "Train: step:  99330, time: 0.211, loss: 458.555054\n",
      "Train: step:  99340, time: 0.197, loss: 1828.786377\n",
      "Train: step:  99350, time: 0.195, loss: 2820.126221\n",
      "Train: step:  99360, time: 0.198, loss: 790.643005\n",
      "Train: step:  99370, time: 0.193, loss: 3682.790527\n",
      "Train: step:  99380, time: 0.191, loss: 2047.973389\n",
      "Train: step:  99390, time: 0.187, loss: 2302.437012\n",
      "Train: step:  99400, time: 0.189, loss: 2750.890869\n",
      "Train: step:  99410, time: 0.193, loss: 1566.592407\n",
      "Train: step:  99420, time: 0.187, loss: 683.061707\n",
      "Train: step:  99430, time: 0.187, loss: 1498.385620\n",
      "Train: step:  99440, time: 0.230, loss: 1774.723511\n",
      "Train: step:  99450, time: 0.222, loss: 1281.752686\n",
      "Train: step:  99460, time: 0.195, loss: 507.134857\n",
      "Train: step:  99470, time: 0.218, loss: 2185.170654\n",
      "Train: step:  99480, time: 0.216, loss: 2061.883545\n",
      "Train: step:  99490, time: 0.195, loss: 2607.680176\n",
      "Train: step:  99500, time: 0.229, loss: 858.427612\n",
      "Train: step:  99510, time: 0.224, loss: 939.124756\n",
      "Train: step:  99520, time: 0.220, loss: 1450.758545\n",
      "Train: step:  99530, time: 0.202, loss: 3673.601318\n",
      "Train: step:  99540, time: 0.223, loss: 2620.594238\n",
      "Train: step:  99550, time: 0.190, loss: 357.574615\n",
      "Train: step:  99560, time: 0.192, loss: 1527.296509\n",
      "Train: step:  99570, time: 0.217, loss: 549.918091\n",
      "Train: step:  99580, time: 0.198, loss: 687.935608\n",
      "Train: step:  99590, time: 0.229, loss: 814.869751\n",
      "Train: step:  99600, time: 0.203, loss: 1996.982422\n",
      "Train: step:  99610, time: 0.219, loss: 1793.034302\n",
      "Train: step:  99620, time: 0.188, loss: 835.026001\n",
      "Train: step:  99630, time: 0.224, loss: 1440.150391\n",
      "Train: step:  99640, time: 0.218, loss: 1360.454468\n",
      "Train: step:  99650, time: 0.202, loss: 1842.832275\n",
      "Train: step:  99660, time: 0.222, loss: 2482.897217\n",
      "Train: step:  99670, time: 0.198, loss: 786.343323\n",
      "Train: step:  99680, time: 0.196, loss: 1733.055054\n",
      "Train: step:  99690, time: 0.189, loss: 989.398865\n",
      "Train: step:  99700, time: 0.215, loss: 510.375610\n",
      "Train: step:  99710, time: 0.194, loss: 994.954529\n",
      "Train: step:  99720, time: 0.237, loss: 1008.699768\n",
      "Train: step:  99730, time: 0.230, loss: 946.523743\n",
      "Train: step:  99740, time: 0.220, loss: 2767.512695\n",
      "Train: step:  99750, time: 0.197, loss: 614.702576\n",
      "Train: step:  99760, time: 0.197, loss: 2314.653320\n",
      "Train: step:  99770, time: 0.196, loss: 1331.768311\n",
      "Train: step:  99780, time: 0.226, loss: 2166.658936\n",
      "Train: step:  99790, time: 0.237, loss: 659.098633\n",
      "Train: step:  99800, time: 0.225, loss: 2164.356934\n",
      "Train: step:  99810, time: 0.213, loss: 1224.815186\n",
      "Train: step:  99820, time: 0.191, loss: 1047.222900\n",
      "Train: step:  99830, time: 0.193, loss: 1891.216797\n",
      "Train: step:  99840, time: 0.211, loss: 3071.864990\n",
      "Train: step:  99850, time: 0.190, loss: 489.513031\n",
      "Train: step:  99860, time: 0.216, loss: 2102.885986\n",
      "Train: step:  99870, time: 0.229, loss: 1787.958008\n",
      "Train: step:  99880, time: 0.195, loss: 2273.587158\n",
      "Train: step:  99890, time: 0.218, loss: 789.814148\n",
      "Train: step:  99900, time: 0.199, loss: 1355.627686\n",
      "Train: step:  99910, time: 0.208, loss: 1886.379272\n",
      "Train: step:  99920, time: 0.230, loss: 2032.282104\n",
      "Train: step:  99930, time: 0.192, loss: 1626.746216\n",
      "Train: step:  99940, time: 0.218, loss: 2537.422119\n",
      "Train: step:  99950, time: 0.229, loss: 2699.563232\n",
      "Train: step:  99960, time: 0.209, loss: 3104.818604\n",
      "Train: step:  99970, time: 0.193, loss: 485.390350\n",
      "Train: step:  99980, time: 0.193, loss: 1559.149780\n",
      "Train: step:  99990, time: 0.232, loss: 4467.764648\n",
      "Train: step: 100000, time: 0.218, loss: 2221.705811\n",
      "Train: step: 100010, time: 0.190, loss: 2446.719971\n",
      "Train: step: 100020, time: 0.193, loss: 580.530884\n",
      "Train: step: 100030, time: 0.214, loss: 2273.817139\n",
      "Train: step: 100040, time: 0.194, loss: 3103.923584\n",
      "Train: step: 100050, time: 0.200, loss: 1161.416748\n",
      "Train: step: 100060, time: 0.218, loss: 1335.832764\n",
      "Train: step: 100070, time: 0.208, loss: 3904.678955\n",
      "Train: step: 100080, time: 0.197, loss: 2539.037109\n",
      "Train: step: 100090, time: 0.194, loss: 1789.789917\n",
      "Train: step: 100100, time: 0.194, loss: 701.093140\n",
      "Train: step: 100110, time: 0.234, loss: 1969.988281\n",
      "Train: step: 100120, time: 0.195, loss: 1900.762085\n",
      "Train: step: 100130, time: 0.245, loss: 2664.336914\n",
      "Train: step: 100140, time: 0.201, loss: 1491.336304\n",
      "Train: step: 100150, time: 0.215, loss: 1971.452026\n",
      "Train: step: 100160, time: 0.198, loss: 1497.216797\n",
      "Train: step: 100170, time: 0.226, loss: 4403.768555\n",
      "Train: step: 100180, time: 0.196, loss: 876.553711\n",
      "Train: step: 100190, time: 0.204, loss: 2764.934326\n",
      "Train: step: 100200, time: 0.194, loss: 2254.804443\n",
      "Train: step: 100210, time: 0.190, loss: 1196.656372\n",
      "Train: step: 100220, time: 0.200, loss: 551.767883\n",
      "Train: step: 100230, time: 0.193, loss: 4431.605957\n",
      "Train: step: 100240, time: 0.190, loss: 3423.674072\n",
      "Train: step: 100250, time: 0.223, loss: 2419.455811\n",
      "Train: step: 100260, time: 0.195, loss: 3338.628662\n",
      "Train: step: 100270, time: 0.197, loss: 670.867432\n",
      "Train: step: 100280, time: 0.245, loss: 2704.953125\n",
      "Train: step: 100290, time: 0.191, loss: 2477.758301\n",
      "Train: step: 100300, time: 0.190, loss: 1152.502808\n",
      "Train: step: 100310, time: 0.197, loss: 1694.437500\n",
      "Train: step: 100320, time: 0.217, loss: 2789.274658\n",
      "Train: step: 100330, time: 0.191, loss: 585.453491\n",
      "Train: step: 100340, time: 0.188, loss: 177.482422\n",
      "Train: step: 100350, time: 0.229, loss: 2739.322998\n",
      "Train: step: 100360, time: 0.224, loss: 636.136108\n",
      "Train: step: 100370, time: 0.227, loss: 404.322296\n",
      "Train: step: 100380, time: 0.192, loss: 3266.744873\n",
      "Train: step: 100390, time: 0.192, loss: 865.652039\n",
      "Train: step: 100400, time: 0.242, loss: 1362.376831\n",
      "Train: step: 100410, time: 0.216, loss: 933.263550\n",
      "Train: step: 100420, time: 0.198, loss: 1492.082031\n",
      "Train: step: 100430, time: 0.197, loss: 350.217438\n",
      "Train: step: 100440, time: 0.218, loss: 1938.341187\n",
      "Train: step: 100450, time: 0.194, loss: 1837.184692\n",
      "Train: step: 100460, time: 0.218, loss: 2340.365234\n",
      "Train: step: 100470, time: 0.196, loss: 3210.267822\n",
      "Train: step: 100480, time: 0.196, loss: 2753.685791\n",
      "Train: step: 100490, time: 0.218, loss: 1077.035522\n",
      "Train: step: 100500, time: 0.203, loss: 1036.642090\n",
      "Train: step: 100510, time: 0.228, loss: 1483.419800\n",
      "Train: step: 100520, time: 0.227, loss: 3085.453613\n",
      "Train: step: 100530, time: 0.193, loss: 3809.591064\n",
      "Train: step: 100540, time: 0.219, loss: 1029.764282\n",
      "Train: step: 100550, time: 0.189, loss: 1872.422729\n",
      "Train: step: 100560, time: 0.194, loss: 2864.784180\n",
      "Train: step: 100570, time: 0.229, loss: 3633.533691\n",
      "Train: step: 100580, time: 0.193, loss: 822.279541\n",
      "Train: step: 100590, time: 0.215, loss: 2602.734863\n",
      "Train: step: 100600, time: 0.191, loss: 3017.868896\n",
      "Train: step: 100610, time: 0.220, loss: 2100.455078\n",
      "Train: step: 100620, time: 0.191, loss: 2690.885498\n",
      "Train: step: 100630, time: 0.202, loss: 3123.070557\n",
      "Train: step: 100640, time: 0.196, loss: 1860.225952\n",
      "Train: step: 100650, time: 0.211, loss: 1620.437988\n",
      "Train: step: 100660, time: 0.214, loss: 674.221863\n",
      "Train: step: 100670, time: 0.221, loss: 2255.823975\n",
      "Train: step: 100680, time: 0.201, loss: 1332.422729\n",
      "Train: step: 100690, time: 0.196, loss: 328.039307\n",
      "Train: step: 100700, time: 0.219, loss: 2226.330322\n",
      "Train: step: 100710, time: 0.216, loss: 2359.754639\n",
      "Train: step: 100720, time: 0.215, loss: 747.757812\n",
      "Train: step: 100730, time: 0.229, loss: 1353.036377\n",
      "Train: step: 100740, time: 0.230, loss: 872.772339\n",
      "Train: step: 100750, time: 0.190, loss: 652.759216\n",
      "Train: step: 100760, time: 0.192, loss: 814.295166\n",
      "Train: step: 100770, time: 0.225, loss: 3382.701904\n",
      "Train: step: 100780, time: 0.211, loss: 435.435059\n",
      "Train: step: 100790, time: 0.198, loss: 3443.608398\n",
      "Train: step: 100800, time: 0.195, loss: 639.001160\n",
      "Train: step: 100810, time: 0.196, loss: 657.845764\n",
      "Train: step: 100820, time: 0.224, loss: 1331.332642\n",
      "Train: step: 100830, time: 0.191, loss: 1652.558960\n",
      "Train: step: 100840, time: 0.198, loss: 1129.247437\n",
      "Train: step: 100850, time: 0.188, loss: 1637.162842\n",
      "Train: step: 100860, time: 0.193, loss: 1520.986694\n",
      "Train: step: 100870, time: 0.195, loss: 2577.499268\n",
      "Train: step: 100880, time: 0.196, loss: 2374.735840\n",
      "Train: step: 100890, time: 0.193, loss: 1143.854492\n",
      "Train: step: 100900, time: 0.198, loss: 2153.563477\n",
      "Train: step: 100910, time: 0.220, loss: 1385.064941\n",
      "Train: step: 100920, time: 0.194, loss: 2682.099121\n",
      "Train: step: 100930, time: 0.229, loss: 1691.901367\n",
      "Train: step: 100940, time: 0.232, loss: 1804.597168\n",
      "Train: step: 100950, time: 0.211, loss: 1738.310303\n",
      "Train: step: 100960, time: 0.194, loss: 1438.849854\n",
      "Train: step: 100970, time: 0.226, loss: 2431.222656\n",
      "Train: step: 100980, time: 0.194, loss: 264.479553\n",
      "Train: step: 100990, time: 0.215, loss: 746.724182\n",
      "Train: step: 101000, time: 0.198, loss: 417.761536\n",
      "Train: step: 101010, time: 0.203, loss: 2333.567627\n",
      "Train: step: 101020, time: 0.217, loss: 1371.494995\n",
      "Train: step: 101030, time: 0.207, loss: 1770.032227\n",
      "Train: step: 101040, time: 0.190, loss: 4227.477539\n",
      "Train: step: 101050, time: 0.208, loss: 1441.994995\n",
      "Train: step: 101060, time: 0.224, loss: 3612.241699\n",
      "Train: step: 101070, time: 0.197, loss: 1388.037964\n",
      "Train: step: 101080, time: 0.195, loss: 1447.661743\n",
      "Train: step: 101090, time: 0.206, loss: 2433.416992\n",
      "Train: step: 101100, time: 0.236, loss: 930.497314\n",
      "Train: step: 101110, time: 0.228, loss: 856.151672\n",
      "Train: step: 101120, time: 0.185, loss: 2179.431885\n",
      "Train: step: 101130, time: 0.212, loss: 2927.155518\n",
      "Train: step: 101140, time: 0.215, loss: 3555.423096\n",
      "Train: step: 101150, time: 0.230, loss: 679.365479\n",
      "Train: step: 101160, time: 0.237, loss: 2574.404297\n",
      "Train: step: 101170, time: 0.193, loss: 1538.778564\n",
      "Train: step: 101180, time: 0.194, loss: 1615.973633\n",
      "Train: step: 101190, time: 0.219, loss: 2335.979980\n",
      "Train: step: 101200, time: 0.196, loss: 567.507629\n",
      "Train: step: 101210, time: 0.194, loss: 1300.707642\n",
      "Train: step: 101220, time: 0.215, loss: 3102.865723\n",
      "Train: step: 101230, time: 0.207, loss: 3823.801270\n",
      "Train: step: 101240, time: 0.205, loss: 3617.890381\n",
      "Train: step: 101250, time: 0.220, loss: 2866.625488\n",
      "Train: step: 101260, time: 0.198, loss: 1241.833984\n",
      "Train: step: 101270, time: 0.198, loss: 3403.015625\n",
      "Train: step: 101280, time: 0.190, loss: 3344.911621\n",
      "Train: step: 101290, time: 0.192, loss: 1939.638062\n",
      "Train: step: 101300, time: 0.196, loss: 2430.208496\n",
      "Train: step: 101310, time: 0.206, loss: 1812.289795\n",
      "Train: step: 101320, time: 0.216, loss: 2338.687012\n",
      "Train: step: 101330, time: 0.198, loss: 2812.594971\n",
      "Train: step: 101340, time: 0.194, loss: 3615.950195\n",
      "Train: step: 101350, time: 0.221, loss: 2762.174072\n",
      "Train: step: 101360, time: 0.185, loss: 728.963562\n",
      "Train: step: 101370, time: 0.216, loss: 959.591736\n",
      "Train: step: 101380, time: 0.216, loss: 2216.662354\n",
      "Train: step: 101390, time: 0.201, loss: 539.225586\n",
      "Train: step: 101400, time: 0.191, loss: 5257.125977\n",
      "Train: step: 101410, time: 0.223, loss: 2638.251221\n",
      "Train: step: 101420, time: 0.220, loss: 1100.782715\n",
      "Train: step: 101430, time: 0.195, loss: 2145.743164\n",
      "Train: step: 101440, time: 0.195, loss: 1899.562256\n",
      "Train: step: 101450, time: 0.227, loss: 976.641968\n",
      "Train: step: 101460, time: 0.198, loss: 1312.174438\n",
      "Train: step: 101470, time: 0.191, loss: 1000.481812\n",
      "Train: step: 101480, time: 0.205, loss: 3133.590576\n",
      "Train: step: 101490, time: 0.204, loss: 975.464966\n",
      "Train: step: 101500, time: 0.199, loss: 3629.328369\n",
      "Train: step: 101510, time: 0.207, loss: 915.156677\n",
      "Train: step: 101520, time: 0.226, loss: 3028.384766\n",
      "Train: step: 101530, time: 0.195, loss: 1661.133667\n",
      "Train: step: 101540, time: 0.195, loss: 3600.521973\n",
      "Train: step: 101550, time: 0.219, loss: 1092.199341\n",
      "Train: step: 101560, time: 0.196, loss: 724.424438\n",
      "Train: step: 101570, time: 0.234, loss: 815.547791\n",
      "Train: step: 101580, time: 0.195, loss: 1106.624878\n",
      "Train: step: 101590, time: 0.198, loss: 2127.976318\n",
      "Train: step: 101600, time: 0.200, loss: 317.237244\n",
      "Train: step: 101610, time: 0.196, loss: 4022.277832\n",
      "Train: step: 101620, time: 0.226, loss: 233.225052\n",
      "Train: step: 101630, time: 0.194, loss: 2962.614258\n",
      "Train: step: 101640, time: 0.201, loss: 669.379395\n",
      "Train: step: 101650, time: 0.195, loss: 1816.468506\n",
      "Train: step: 101660, time: 0.196, loss: 1515.095337\n",
      "Train: step: 101670, time: 0.203, loss: 854.901855\n",
      "Train: step: 101680, time: 0.198, loss: 1558.223145\n",
      "Train: step: 101690, time: 0.216, loss: 3661.254883\n",
      "Train: step: 101700, time: 0.218, loss: 2362.646240\n",
      "Train: step: 101710, time: 0.208, loss: 2191.072510\n",
      "Train: step: 101720, time: 0.193, loss: 1009.709106\n",
      "Train: step: 101730, time: 0.196, loss: 2458.306152\n",
      "Train: step: 101740, time: 0.193, loss: 1481.974365\n",
      "Train: step: 101750, time: 0.215, loss: 2245.508301\n",
      "Train: step: 101760, time: 0.235, loss: 2921.842285\n",
      "Train: step: 101770, time: 0.196, loss: 481.417755\n",
      "Train: step: 101780, time: 0.225, loss: 2582.326660\n",
      "Train: step: 101790, time: 0.194, loss: 2709.042969\n",
      "Train: step: 101800, time: 0.192, loss: 599.514771\n",
      "Train: step: 101810, time: 0.218, loss: 2610.017578\n",
      "Train: step: 101820, time: 0.198, loss: 2810.148682\n",
      "Train: step: 101830, time: 0.188, loss: 1754.642578\n",
      "Train: step: 101840, time: 0.191, loss: 2117.979736\n",
      "Train: step: 101850, time: 0.187, loss: 1886.097656\n",
      "Train: step: 101860, time: 0.190, loss: 1711.871216\n",
      "Train: step: 101870, time: 0.231, loss: 2128.095215\n",
      "Train: step: 101880, time: 0.195, loss: 3836.360107\n",
      "Train: step: 101890, time: 0.195, loss: 1789.557373\n",
      "Train: step: 101900, time: 0.204, loss: 2756.764648\n",
      "Train: step: 101910, time: 0.198, loss: 2271.446289\n",
      "Train: step: 101920, time: 0.193, loss: 480.625092\n",
      "Train: step: 101930, time: 0.226, loss: 3193.437744\n",
      "Train: step: 101940, time: 0.226, loss: 4132.052734\n",
      "Train: step: 101950, time: 0.191, loss: 1028.732056\n",
      "Train: step: 101960, time: 0.195, loss: 1489.390869\n",
      "Train: step: 101970, time: 0.218, loss: 941.131897\n",
      "Train: step: 101980, time: 0.241, loss: 2514.546387\n",
      "Train: step: 101990, time: 0.218, loss: 948.409058\n",
      "Train: step: 102000, time: 0.192, loss: 440.488678\n",
      "Train: step: 102010, time: 0.193, loss: 1574.334717\n",
      "Train: step: 102020, time: 0.189, loss: 3498.632080\n",
      "Train: step: 102030, time: 0.192, loss: 514.273621\n",
      "Train: step: 102040, time: 0.193, loss: 1306.036011\n",
      "Train: step: 102050, time: 0.195, loss: 2999.026367\n",
      "Train: step: 102060, time: 0.191, loss: 2121.165039\n",
      "Train: step: 102070, time: 0.247, loss: 167.618881\n",
      "Train: step: 102080, time: 0.227, loss: 1478.779785\n",
      "Train: step: 102090, time: 0.195, loss: 1179.112305\n",
      "Train: step: 102100, time: 0.218, loss: 1657.213745\n",
      "Train: step: 102110, time: 0.223, loss: 805.513306\n",
      "Train: step: 102120, time: 0.212, loss: 2481.145020\n",
      "Train: step: 102130, time: 0.199, loss: 4311.954590\n",
      "Train: step: 102140, time: 0.190, loss: 1654.440918\n",
      "Train: step: 102150, time: 0.196, loss: 379.311127\n",
      "Train: step: 102160, time: 0.232, loss: 1194.914185\n",
      "Train: step: 102170, time: 0.200, loss: 619.786438\n",
      "Train: step: 102180, time: 0.227, loss: 1273.288940\n",
      "Train: step: 102190, time: 0.219, loss: 1320.921631\n",
      "Train: step: 102200, time: 0.194, loss: 2282.690918\n",
      "Train: step: 102210, time: 0.222, loss: 746.307495\n",
      "Train: step: 102220, time: 0.188, loss: 2129.011719\n",
      "Train: step: 102230, time: 0.198, loss: 781.845581\n",
      "Train: step: 102240, time: 0.189, loss: 414.232849\n",
      "Train: step: 102250, time: 0.214, loss: 2044.765625\n",
      "Train: step: 102260, time: 0.219, loss: 1314.298706\n",
      "Train: step: 102270, time: 0.215, loss: 1190.999756\n",
      "Train: step: 102280, time: 0.192, loss: 2748.676270\n",
      "Train: step: 102290, time: 0.220, loss: 179.215515\n",
      "Train: step: 102300, time: 0.218, loss: 2440.974609\n",
      "Train: step: 102310, time: 0.203, loss: 1137.896362\n",
      "Train: step: 102320, time: 0.237, loss: 860.315002\n",
      "Train: step: 102330, time: 0.226, loss: 1643.440552\n",
      "Train: step: 102340, time: 0.220, loss: 3372.290527\n",
      "Train: step: 102350, time: 0.226, loss: 1984.659424\n",
      "Train: step: 102360, time: 0.214, loss: 2151.396484\n",
      "Train: step: 102370, time: 0.227, loss: 370.315125\n",
      "Train: step: 102380, time: 0.222, loss: 2742.653076\n",
      "Train: step: 102390, time: 0.229, loss: 465.730225\n",
      "Train: step: 102400, time: 0.220, loss: 4403.318359\n",
      "Train: step: 102410, time: 0.230, loss: 3480.373779\n",
      "Train: step: 102420, time: 0.234, loss: 1292.066406\n",
      "Train: step: 102430, time: 0.218, loss: 956.506165\n",
      "Train: step: 102440, time: 0.190, loss: 2043.301514\n",
      "Train: step: 102450, time: 0.194, loss: 1754.031128\n",
      "Train: step: 102460, time: 0.192, loss: 1686.226685\n",
      "Train: step: 102470, time: 0.188, loss: 1922.580444\n",
      "Train: step: 102480, time: 0.191, loss: 1595.852905\n",
      "Train: step: 102490, time: 0.200, loss: 2956.597900\n",
      "Train: step: 102500, time: 0.217, loss: 626.662598\n",
      "Train: step: 102510, time: 0.197, loss: 915.864380\n",
      "Train: step: 102520, time: 0.193, loss: 1860.907959\n",
      "Train: step: 102530, time: 0.199, loss: 1173.618286\n",
      "Train: step: 102540, time: 0.220, loss: 2927.014404\n",
      "Train: step: 102550, time: 0.232, loss: 2003.321167\n",
      "Train: step: 102560, time: 0.215, loss: 2689.446045\n",
      "Train: step: 102570, time: 0.219, loss: 1756.878418\n",
      "Train: step: 102580, time: 0.202, loss: 1928.807373\n",
      "Train: step: 102590, time: 0.191, loss: 1019.689209\n",
      "Train: step: 102600, time: 0.195, loss: 2999.477539\n",
      "Train: step: 102610, time: 0.214, loss: 2392.109375\n",
      "Train: step: 102620, time: 0.205, loss: 1731.924683\n",
      "Train: step: 102630, time: 0.190, loss: 1444.646973\n",
      "Train: step: 102640, time: 0.197, loss: 3123.888672\n",
      "Train: step: 102650, time: 0.188, loss: 353.053314\n",
      "Train: step: 102660, time: 0.191, loss: 2335.754883\n",
      "Train: step: 102670, time: 0.218, loss: 2914.346191\n",
      "Train: step: 102680, time: 0.223, loss: 1904.946655\n",
      "Train: step: 102690, time: 0.228, loss: 1510.902466\n",
      "Train: step: 102700, time: 0.191, loss: 939.111023\n",
      "Train: step: 102710, time: 0.192, loss: 2333.998535\n",
      "Train: step: 102720, time: 0.215, loss: 1606.029175\n",
      "Train: step: 102730, time: 0.229, loss: 2187.337646\n",
      "Train: step: 102740, time: 0.199, loss: 2543.113770\n",
      "Train: step: 102750, time: 0.194, loss: 613.159546\n",
      "Train: step: 102760, time: 0.212, loss: 462.431030\n",
      "Train: step: 102770, time: 0.194, loss: 1524.662720\n",
      "Train: step: 102780, time: 0.242, loss: 2107.922119\n",
      "Train: step: 102790, time: 0.214, loss: 1362.115601\n",
      "Train: step: 102800, time: 0.196, loss: 2199.001709\n",
      "Train: step: 102810, time: 0.196, loss: 2239.901123\n",
      "Train: step: 102820, time: 0.197, loss: 3178.789062\n",
      "Train: step: 102830, time: 0.239, loss: 2112.494873\n",
      "Train: step: 102840, time: 0.192, loss: 2260.774414\n",
      "Train: step: 102850, time: 0.198, loss: 1367.919434\n",
      "Train: step: 102860, time: 0.199, loss: 247.530777\n",
      "Train: step: 102870, time: 0.199, loss: 3156.256348\n",
      "Train: step: 102880, time: 0.191, loss: 2016.383057\n",
      "Train: step: 102890, time: 0.195, loss: 1048.875000\n",
      "Train: step: 102900, time: 0.193, loss: 2212.533203\n",
      "Train: step: 102910, time: 0.189, loss: 785.526367\n",
      "Train: step: 102920, time: 0.190, loss: 1419.028687\n",
      "Train: step: 102930, time: 0.190, loss: 1500.206543\n",
      "Train: step: 102940, time: 0.192, loss: 2041.620239\n",
      "Train: step: 102950, time: 0.195, loss: 1903.119629\n",
      "Train: step: 102960, time: 0.218, loss: 3186.699463\n",
      "Train: step: 102970, time: 0.201, loss: 2669.428711\n",
      "Train: step: 102980, time: 0.245, loss: 1590.077637\n",
      "Train: step: 102990, time: 0.191, loss: 402.652985\n",
      "Train: step: 103000, time: 0.229, loss: 697.857361\n",
      "Train: step: 103010, time: 0.192, loss: 1932.843262\n",
      "Train: step: 103020, time: 0.196, loss: 471.061096\n",
      "Train: step: 103030, time: 0.217, loss: 1174.996216\n",
      "Train: step: 103040, time: 0.215, loss: 1769.300781\n",
      "Train: step: 103050, time: 0.200, loss: 579.149658\n",
      "Train: step: 103060, time: 0.216, loss: 1250.497070\n",
      "Train: step: 103070, time: 0.201, loss: 1028.017334\n",
      "Train: step: 103080, time: 0.190, loss: 2191.140381\n",
      "Train: step: 103090, time: 0.190, loss: 1105.995483\n",
      "Train: step: 103100, time: 0.186, loss: 2293.332764\n",
      "Train: step: 103110, time: 0.193, loss: 1905.417603\n",
      "Train: step: 103120, time: 0.189, loss: 1946.798950\n",
      "Train: step: 103130, time: 0.197, loss: 547.334229\n",
      "Train: step: 103140, time: 0.199, loss: 598.948792\n",
      "Train: step: 103150, time: 0.211, loss: 1354.102783\n",
      "Train: step: 103160, time: 0.217, loss: 666.362915\n",
      "Train: step: 103170, time: 0.227, loss: 1854.459961\n",
      "Train: step: 103180, time: 0.195, loss: 679.728821\n",
      "Train: step: 103190, time: 0.219, loss: 261.639496\n",
      "Train: step: 103200, time: 0.216, loss: 1659.539429\n",
      "Train: step: 103210, time: 0.231, loss: 1328.558838\n",
      "Train: step: 103220, time: 0.238, loss: 663.084778\n",
      "Train: step: 103230, time: 0.197, loss: 2990.031250\n",
      "Train: step: 103240, time: 0.204, loss: 1427.259033\n",
      "Train: step: 103250, time: 0.214, loss: 1443.643433\n",
      "Train: step: 103260, time: 0.200, loss: 1954.157227\n",
      "Train: step: 103270, time: 0.197, loss: 1648.546753\n",
      "Train: step: 103280, time: 0.225, loss: 2444.730957\n",
      "Train: step: 103290, time: 0.196, loss: 1346.474121\n",
      "Train: step: 103300, time: 0.210, loss: 2427.988281\n",
      "Train: step: 103310, time: 0.198, loss: 1511.888672\n",
      "Train: step: 103320, time: 0.194, loss: 5281.719238\n",
      "Train: step: 103330, time: 0.192, loss: 1772.825317\n",
      "Train: step: 103340, time: 0.195, loss: 396.980591\n",
      "Train: step: 103350, time: 0.220, loss: 2468.364990\n",
      "Train: step: 103360, time: 0.194, loss: 3067.482422\n",
      "Train: step: 103370, time: 0.195, loss: 2310.340088\n",
      "Train: step: 103380, time: 0.194, loss: 2164.016602\n",
      "Train: step: 103390, time: 0.203, loss: 833.755615\n",
      "Train: step: 103400, time: 0.200, loss: 3447.300781\n",
      "Train: step: 103410, time: 0.194, loss: 2957.828125\n",
      "Train: step: 103420, time: 0.189, loss: 3738.940430\n",
      "Train: step: 103430, time: 0.193, loss: 1822.337402\n",
      "Train: step: 103440, time: 0.209, loss: 1098.963623\n",
      "Train: step: 103450, time: 0.231, loss: 1711.161255\n",
      "Train: step: 103460, time: 0.219, loss: 1203.705933\n",
      "Train: step: 103470, time: 0.202, loss: 2672.961182\n",
      "Train: step: 103480, time: 0.219, loss: 1638.299927\n",
      "Train: step: 103490, time: 0.189, loss: 2268.565918\n",
      "Train: step: 103500, time: 0.201, loss: 1714.409668\n",
      "Train: step: 103510, time: 0.192, loss: 936.534119\n",
      "Train: step: 103520, time: 0.196, loss: 3983.258545\n",
      "Train: step: 103530, time: 0.215, loss: 544.331543\n",
      "Train: step: 103540, time: 0.200, loss: 1248.621460\n",
      "Train: step: 103550, time: 0.237, loss: 2125.892334\n",
      "Train: step: 103560, time: 0.224, loss: 3663.057129\n",
      "Train: step: 103570, time: 0.233, loss: 2236.193359\n",
      "Train: step: 103580, time: 0.198, loss: 1705.133423\n",
      "Train: step: 103590, time: 0.189, loss: 437.056274\n",
      "Train: step: 103600, time: 0.211, loss: 2049.106689\n",
      "Train: step: 103610, time: 0.223, loss: 933.531494\n",
      "Train: step: 103620, time: 0.185, loss: 1324.093750\n",
      "Train: step: 103630, time: 0.194, loss: 1856.886353\n",
      "Train: step: 103640, time: 0.208, loss: 1575.598999\n",
      "Train: step: 103650, time: 0.197, loss: 1916.376831\n",
      "Train: step: 103660, time: 0.200, loss: 2024.789307\n",
      "Train: step: 103670, time: 0.198, loss: 1765.485229\n",
      "Train: step: 103680, time: 0.219, loss: 2717.665039\n",
      "Train: step: 103690, time: 0.192, loss: 1915.183838\n",
      "Train: step: 103700, time: 0.193, loss: 3063.643311\n",
      "Train: step: 103710, time: 0.192, loss: 1701.317383\n",
      "Train: step: 103720, time: 0.203, loss: 1709.336304\n",
      "Train: step: 103730, time: 0.194, loss: 2973.685059\n",
      "Train: step: 103740, time: 0.195, loss: 1316.128296\n",
      "Train: step: 103750, time: 0.211, loss: 1359.522583\n",
      "Train: step: 103760, time: 0.195, loss: 404.903137\n",
      "Train: step: 103770, time: 0.194, loss: 2552.656738\n",
      "Train: step: 103780, time: 0.189, loss: 2748.529053\n",
      "Train: step: 103790, time: 0.198, loss: 1880.917847\n",
      "Train: step: 103800, time: 0.196, loss: 1673.400024\n",
      "Train: step: 103810, time: 0.193, loss: 3241.031494\n",
      "Train: step: 103820, time: 0.219, loss: 1682.150513\n",
      "Train: step: 103830, time: 0.192, loss: 1816.285889\n",
      "Train: step: 103840, time: 0.219, loss: 1725.108521\n",
      "Train: step: 103850, time: 0.192, loss: 1557.257446\n",
      "Train: step: 103860, time: 0.240, loss: 2114.583496\n",
      "Train: step: 103870, time: 0.192, loss: 1104.552368\n",
      "Train: step: 103880, time: 0.234, loss: 1264.319214\n",
      "Train: step: 103890, time: 0.218, loss: 1402.574951\n",
      "Train: step: 103900, time: 0.195, loss: 1931.447876\n",
      "Train: step: 103910, time: 0.220, loss: 1998.782715\n",
      "Train: step: 103920, time: 0.228, loss: 315.170197\n",
      "Train: step: 103930, time: 0.216, loss: 1115.804077\n",
      "Train: step: 103940, time: 0.231, loss: 2998.516357\n",
      "Train: step: 103950, time: 0.192, loss: 2547.304199\n",
      "Train: step: 103960, time: 0.194, loss: 1462.754761\n",
      "Train: step: 103970, time: 0.201, loss: 1785.843140\n",
      "Train: step: 103980, time: 0.194, loss: 1982.937134\n",
      "Train: step: 103990, time: 0.191, loss: 827.840210\n",
      "Train: step: 104000, time: 0.187, loss: 1711.082520\n",
      "Train: step: 104010, time: 0.191, loss: 3780.091309\n",
      "Train: step: 104020, time: 0.187, loss: 1884.979980\n",
      "Train: step: 104030, time: 0.186, loss: 2217.004639\n",
      "Train: step: 104040, time: 0.260, loss: 2518.450928\n",
      "Train: step: 104050, time: 0.191, loss: 2598.435059\n",
      "Train: step: 104060, time: 0.195, loss: 603.755249\n",
      "Train: step: 104070, time: 0.197, loss: 2041.666260\n",
      "Train: step: 104080, time: 0.195, loss: 1821.564453\n",
      "Train: step: 104090, time: 0.199, loss: 2105.322510\n",
      "Train: step: 104100, time: 0.188, loss: 2010.297241\n",
      "Train: step: 104110, time: 0.196, loss: 1603.073975\n",
      "Train: step: 104120, time: 0.195, loss: 1750.697266\n",
      "Train: step: 104130, time: 0.195, loss: 2043.234741\n",
      "Train: step: 104140, time: 0.218, loss: 1033.426392\n",
      "Train: step: 104150, time: 0.218, loss: 2195.704590\n",
      "Train: step: 104160, time: 0.187, loss: 551.554016\n",
      "Train: step: 104170, time: 0.207, loss: 926.603943\n",
      "Train: step: 104180, time: 0.262, loss: 886.097717\n",
      "Train: step: 104190, time: 0.193, loss: 719.380981\n",
      "Train: step: 104200, time: 0.195, loss: 2141.489746\n",
      "Train: step: 104210, time: 0.200, loss: 4041.843750\n",
      "Train: step: 104220, time: 0.190, loss: 2328.504639\n",
      "Train: step: 104230, time: 0.196, loss: 2183.650635\n",
      "Train: step: 104240, time: 0.265, loss: 1877.468018\n",
      "Train: step: 104250, time: 0.192, loss: 1427.421509\n",
      "Train: step: 104260, time: 0.235, loss: 1876.878662\n",
      "Train: step: 104270, time: 0.235, loss: 3254.134033\n",
      "Train: step: 104280, time: 0.189, loss: 2302.167480\n",
      "Train: step: 104290, time: 0.257, loss: 2607.376953\n",
      "Train: step: 104300, time: 0.197, loss: 523.950928\n",
      "Train: step: 104310, time: 0.196, loss: 760.366455\n",
      "Train: step: 104320, time: 0.188, loss: 3146.473145\n",
      "Train: step: 104330, time: 0.200, loss: 4422.451172\n",
      "Train: step: 104340, time: 0.197, loss: 2416.635010\n",
      "Train: step: 104350, time: 0.210, loss: 1346.743408\n",
      "Train: step: 104360, time: 0.191, loss: 3015.700928\n",
      "Train: step: 104370, time: 0.219, loss: 514.002197\n",
      "Train: step: 104380, time: 0.227, loss: 1023.794128\n",
      "Train: step: 104390, time: 0.228, loss: 4212.774414\n",
      "Train: step: 104400, time: 0.192, loss: 2496.358154\n",
      "Train: step: 104410, time: 0.231, loss: 2201.446777\n",
      "Train: step: 104420, time: 0.195, loss: 2155.861816\n",
      "Train: step: 104430, time: 0.210, loss: 1748.815918\n",
      "Train: step: 104440, time: 0.199, loss: 1265.658081\n",
      "Train: step: 104450, time: 0.187, loss: 3488.986084\n",
      "Train: step: 104460, time: 0.244, loss: 2956.282227\n",
      "Train: step: 104470, time: 0.220, loss: 913.476685\n",
      "Train: step: 104480, time: 0.219, loss: 1769.249512\n",
      "Train: step: 104490, time: 0.220, loss: 1398.692993\n",
      "Train: step: 104500, time: 0.195, loss: 1222.823486\n",
      "Train: step: 104510, time: 0.192, loss: 1876.402832\n",
      "Train: step: 104520, time: 0.212, loss: 1982.773926\n",
      "Train: step: 104530, time: 0.194, loss: 1676.966431\n",
      "Train: step: 104540, time: 0.198, loss: 1576.597412\n",
      "Train: step: 104550, time: 0.189, loss: 630.870850\n",
      "Train: step: 104560, time: 0.191, loss: 470.445923\n",
      "Train: step: 104570, time: 0.225, loss: 2389.539062\n",
      "Train: step: 104580, time: 0.190, loss: 1453.184448\n",
      "Train: step: 104590, time: 0.186, loss: 1262.843506\n",
      "Train: step: 104600, time: 0.193, loss: 1586.556152\n",
      "Train: step: 104610, time: 0.218, loss: 2057.909668\n",
      "Train: step: 104620, time: 0.197, loss: 1647.144775\n",
      "Train: step: 104630, time: 0.194, loss: 569.743958\n",
      "Train: step: 104640, time: 0.192, loss: 1135.490356\n",
      "Train: step: 104650, time: 0.203, loss: 3161.469482\n",
      "Train: step: 104660, time: 0.249, loss: 2629.001709\n",
      "Train: step: 104670, time: 0.193, loss: 2069.550049\n",
      "Train: step: 104680, time: 0.193, loss: 1557.880249\n",
      "Train: step: 104690, time: 0.189, loss: 1731.744263\n",
      "Train: step: 104700, time: 0.190, loss: 1046.871216\n",
      "Train: step: 104710, time: 0.216, loss: 2150.975098\n",
      "Train: step: 104720, time: 0.243, loss: 2887.359619\n",
      "Train: step: 104730, time: 0.198, loss: 1522.928101\n",
      "Train: step: 104740, time: 0.193, loss: 1392.405884\n",
      "Train: step: 104750, time: 0.211, loss: 2206.677246\n",
      "Train: step: 104760, time: 0.216, loss: 2544.945068\n",
      "Train: step: 104770, time: 0.203, loss: 888.837585\n",
      "Train: step: 104780, time: 0.245, loss: 2660.578125\n",
      "Train: step: 104790, time: 0.203, loss: 761.724121\n",
      "Train: step: 104800, time: 0.204, loss: 2206.400391\n",
      "Train: step: 104810, time: 0.233, loss: 422.572815\n",
      "Train: step: 104820, time: 0.192, loss: 737.257568\n",
      "Train: step: 104830, time: 0.195, loss: 567.101562\n",
      "Train: step: 104840, time: 0.220, loss: 2488.215088\n",
      "Train: step: 104850, time: 0.197, loss: 2831.571777\n",
      "Train: step: 104860, time: 0.189, loss: 2596.956787\n",
      "Train: step: 104870, time: 0.195, loss: 821.411621\n",
      "Train: step: 104880, time: 0.220, loss: 2070.391602\n",
      "Train: step: 104890, time: 0.192, loss: 1818.965088\n",
      "Train: step: 104900, time: 0.230, loss: 1567.470215\n",
      "Train: step: 104910, time: 0.200, loss: 1955.207031\n",
      "Train: step: 104920, time: 0.196, loss: 2116.553955\n",
      "Train: step: 104930, time: 0.191, loss: 1798.616333\n",
      "Train: step: 104940, time: 0.194, loss: 2424.554932\n",
      "Train: step: 104950, time: 0.198, loss: 1025.435913\n",
      "Train: step: 104960, time: 0.192, loss: 1791.656372\n",
      "Train: step: 104970, time: 0.197, loss: 1862.795898\n",
      "Train: step: 104980, time: 0.195, loss: 2783.465088\n",
      "Train: step: 104990, time: 0.199, loss: 3401.864502\n",
      "Train: step: 105000, time: 0.217, loss: 1332.803101\n",
      "Train: step: 105010, time: 0.222, loss: 3528.614014\n",
      "Train: step: 105020, time: 0.191, loss: 2176.792969\n",
      "Train: step: 105030, time: 0.193, loss: 2307.039062\n",
      "Train: step: 105040, time: 0.191, loss: 1219.586304\n",
      "Train: step: 105050, time: 0.225, loss: 718.043213\n",
      "Train: step: 105060, time: 0.199, loss: 2582.625488\n",
      "Train: step: 105070, time: 0.196, loss: 1978.492798\n",
      "Train: step: 105080, time: 0.200, loss: 2487.044678\n",
      "Train: step: 105090, time: 0.193, loss: 1357.683594\n",
      "Train: step: 105100, time: 0.222, loss: 1838.600342\n",
      "Train: step: 105110, time: 0.191, loss: 636.272888\n",
      "Train: step: 105120, time: 0.195, loss: 2010.354614\n",
      "Train: step: 105130, time: 0.186, loss: 229.189774\n",
      "Train: step: 105140, time: 0.190, loss: 1713.389771\n",
      "Train: step: 105150, time: 0.220, loss: 1287.855713\n",
      "Train: step: 105160, time: 0.194, loss: 315.102234\n",
      "Train: step: 105170, time: 0.217, loss: 1965.988037\n",
      "Train: step: 105180, time: 0.195, loss: 2034.489136\n",
      "Train: step: 105190, time: 0.218, loss: 895.633057\n",
      "Train: step: 105200, time: 0.194, loss: 722.476807\n",
      "Train: step: 105210, time: 0.195, loss: 3125.023682\n",
      "Train: step: 105220, time: 0.213, loss: 879.967834\n",
      "Train: step: 105230, time: 0.189, loss: 1397.759277\n",
      "Train: step: 105240, time: 0.198, loss: 1815.358521\n",
      "Train: step: 105250, time: 0.216, loss: 1696.211914\n",
      "Train: step: 105260, time: 0.190, loss: 1624.776367\n",
      "Train: step: 105270, time: 0.198, loss: 2278.241211\n",
      "Train: step: 105280, time: 0.195, loss: 1286.468262\n",
      "Train: step: 105290, time: 0.190, loss: 739.817505\n",
      "Train: step: 105300, time: 0.192, loss: 1304.525391\n",
      "Train: step: 105310, time: 0.207, loss: 1568.166748\n",
      "Train: step: 105320, time: 0.205, loss: 2442.124268\n",
      "Train: step: 105330, time: 0.228, loss: 2325.418457\n",
      "Train: step: 105340, time: 0.226, loss: 2465.367188\n",
      "Train: step: 105350, time: 0.185, loss: 2532.173096\n",
      "Train: step: 105360, time: 0.204, loss: 1098.873535\n",
      "Train: step: 105370, time: 0.210, loss: 3322.643555\n",
      "Train: step: 105380, time: 0.195, loss: 670.987366\n",
      "Train: step: 105390, time: 0.219, loss: 2338.867432\n",
      "Train: step: 105400, time: 0.190, loss: 2686.168213\n",
      "Train: step: 105410, time: 0.191, loss: 511.255768\n",
      "Train: step: 105420, time: 0.192, loss: 1235.105957\n",
      "Train: step: 105430, time: 0.246, loss: 1873.601929\n",
      "Train: step: 105440, time: 0.226, loss: 2387.279785\n",
      "Train: step: 105450, time: 0.228, loss: 924.116150\n",
      "Train: step: 105460, time: 0.194, loss: 2003.210083\n",
      "Train: step: 105470, time: 0.197, loss: 2274.027588\n",
      "Train: step: 105480, time: 0.201, loss: 2010.861084\n",
      "Train: step: 105490, time: 0.239, loss: 314.310913\n",
      "Train: step: 105500, time: 0.192, loss: 1900.081055\n",
      "Train: step: 105510, time: 0.189, loss: 1885.246704\n",
      "Train: step: 105520, time: 0.193, loss: 1301.672485\n",
      "Train: step: 105530, time: 0.205, loss: 2192.650879\n",
      "Train: step: 105540, time: 0.196, loss: 1086.825806\n",
      "Train: step: 105550, time: 0.192, loss: 303.222443\n",
      "Train: step: 105560, time: 0.200, loss: 768.410217\n",
      "Train: step: 105570, time: 0.196, loss: 489.731567\n",
      "Train: step: 105580, time: 0.193, loss: 709.509216\n",
      "Train: step: 105590, time: 0.192, loss: 1717.794067\n",
      "Train: step: 105600, time: 0.219, loss: 2717.350342\n",
      "Train: step: 105610, time: 0.194, loss: 2889.430176\n",
      "Train: step: 105620, time: 0.192, loss: 391.868530\n",
      "Train: step: 105630, time: 0.198, loss: 1021.655090\n",
      "Train: step: 105640, time: 0.196, loss: 1574.583740\n",
      "Train: step: 105650, time: 0.196, loss: 1277.043457\n",
      "Train: step: 105660, time: 0.204, loss: 1843.133545\n",
      "Train: step: 105670, time: 0.215, loss: 2747.059570\n",
      "Train: step: 105680, time: 0.198, loss: 2261.115479\n",
      "Train: step: 105690, time: 0.194, loss: 3271.542725\n",
      "Train: step: 105700, time: 0.198, loss: 663.202698\n",
      "Train: step: 105710, time: 0.187, loss: 2121.094482\n",
      "Train: step: 105720, time: 0.218, loss: 2607.677979\n",
      "Train: step: 105730, time: 0.247, loss: 3014.854248\n",
      "Train: step: 105740, time: 0.187, loss: 2587.864014\n",
      "Train: step: 105750, time: 0.193, loss: 2142.230469\n",
      "Train: step: 105760, time: 0.213, loss: 3044.950195\n",
      "Train: step: 105770, time: 0.200, loss: 4866.146973\n",
      "Train: step: 105780, time: 0.192, loss: 1354.138916\n",
      "Train: step: 105790, time: 0.194, loss: 1694.665649\n",
      "Train: step: 105800, time: 0.201, loss: 1626.010010\n",
      "Train: step: 105810, time: 0.216, loss: 1133.766846\n",
      "Train: step: 105820, time: 0.193, loss: 433.309662\n",
      "Train: step: 105830, time: 0.188, loss: 834.344910\n",
      "Train: step: 105840, time: 0.195, loss: 3721.932861\n",
      "Train: step: 105850, time: 0.262, loss: 1035.378784\n",
      "Train: step: 105860, time: 0.234, loss: 1577.353149\n",
      "Train: step: 105870, time: 0.193, loss: 1144.576294\n",
      "Train: step: 105880, time: 0.244, loss: 616.659668\n",
      "Train: step: 105890, time: 0.242, loss: 1936.549316\n",
      "Train: step: 105900, time: 0.189, loss: 1352.461792\n",
      "Train: step: 105910, time: 0.218, loss: 1890.429321\n",
      "Train: step: 105920, time: 0.198, loss: 1130.462769\n",
      "Train: step: 105930, time: 0.252, loss: 758.567322\n",
      "Train: step: 105940, time: 0.219, loss: 3256.660400\n",
      "Train: step: 105950, time: 0.188, loss: 3856.780273\n",
      "Train: step: 105960, time: 0.195, loss: 1612.909546\n",
      "Train: step: 105970, time: 0.217, loss: 1982.672485\n",
      "Train: step: 105980, time: 0.192, loss: 3426.323242\n",
      "Train: step: 105990, time: 0.189, loss: 2884.073242\n",
      "Train: step: 106000, time: 0.194, loss: 954.084167\n",
      "Train: step: 106010, time: 0.194, loss: 1037.290405\n",
      "Train: step: 106020, time: 0.199, loss: 731.029358\n",
      "Train: step: 106030, time: 0.191, loss: 3492.122314\n",
      "Train: step: 106040, time: 0.192, loss: 2069.903320\n",
      "Train: step: 106050, time: 0.242, loss: 3217.726318\n",
      "Train: step: 106060, time: 0.194, loss: 2043.510620\n",
      "Train: step: 106070, time: 0.194, loss: 1537.141846\n",
      "Train: step: 106080, time: 0.234, loss: 3572.617676\n",
      "Train: step: 106090, time: 0.218, loss: 2750.654785\n",
      "Train: step: 106100, time: 0.225, loss: 1655.682129\n",
      "Train: step: 106110, time: 0.189, loss: 2571.284668\n",
      "Train: step: 106120, time: 0.218, loss: 607.423645\n",
      "Train: step: 106130, time: 0.228, loss: 1122.822388\n",
      "Train: step: 106140, time: 0.184, loss: 2468.753906\n",
      "Train: step: 106150, time: 0.234, loss: 913.837463\n",
      "Train: step: 106160, time: 0.211, loss: 1770.598999\n",
      "Train: step: 106170, time: 0.192, loss: 977.378784\n",
      "Train: step: 106180, time: 0.215, loss: 1054.815430\n",
      "Train: step: 106190, time: 0.219, loss: 1840.153931\n",
      "Train: step: 106200, time: 0.188, loss: 1580.453125\n",
      "Train: step: 106210, time: 0.210, loss: 1111.880615\n",
      "Train: step: 106220, time: 0.239, loss: 729.803589\n",
      "Train: step: 106230, time: 0.190, loss: 545.766113\n",
      "Train: step: 106240, time: 0.200, loss: 2637.658203\n",
      "Train: step: 106250, time: 0.254, loss: 1297.116577\n",
      "Train: step: 106260, time: 0.231, loss: 1131.517822\n",
      "Train: step: 106270, time: 0.189, loss: 1632.054810\n",
      "Train: step: 106280, time: 0.244, loss: 999.168274\n",
      "Train: step: 106290, time: 0.191, loss: 2150.701904\n",
      "Train: step: 106300, time: 0.192, loss: 1901.602539\n",
      "Train: step: 106310, time: 0.190, loss: 3176.186279\n",
      "Train: step: 106320, time: 0.216, loss: 1351.195068\n",
      "Train: step: 106330, time: 0.195, loss: 2382.969971\n",
      "Train: step: 106340, time: 0.190, loss: 2570.952393\n",
      "Train: step: 106350, time: 0.187, loss: 765.862183\n",
      "Train: step: 106360, time: 0.194, loss: 1219.012695\n",
      "Train: step: 106370, time: 0.191, loss: 851.737427\n",
      "Train: step: 106380, time: 0.200, loss: 1807.238647\n",
      "Train: step: 106390, time: 0.189, loss: 2546.894043\n",
      "Train: step: 106400, time: 0.230, loss: 2040.975708\n",
      "Train: step: 106410, time: 0.245, loss: 1853.630249\n",
      "Train: step: 106420, time: 0.197, loss: 1898.361084\n",
      "Train: step: 106430, time: 0.198, loss: 532.887268\n",
      "Train: step: 106440, time: 0.192, loss: 1685.206665\n",
      "Train: step: 106450, time: 0.188, loss: 1581.989014\n",
      "Train: step: 106460, time: 0.191, loss: 1066.593994\n",
      "Train: step: 106470, time: 0.197, loss: 1879.492188\n",
      "Train: step: 106480, time: 0.194, loss: 1555.370850\n",
      "Train: step: 106490, time: 0.195, loss: 895.039795\n",
      "Train: step: 106500, time: 0.197, loss: 3417.236084\n",
      "Train: step: 106510, time: 0.201, loss: 3925.608643\n",
      "Train: step: 106520, time: 0.199, loss: 2741.004639\n",
      "Train: step: 106530, time: 0.196, loss: 833.111877\n",
      "Train: step: 106540, time: 0.223, loss: 2408.203369\n",
      "Train: step: 106550, time: 0.213, loss: 3251.087158\n",
      "Train: step: 106560, time: 0.238, loss: 2006.692871\n",
      "Train: step: 106570, time: 0.193, loss: 1349.049316\n",
      "Train: step: 106580, time: 0.198, loss: 2330.174316\n",
      "Train: step: 106590, time: 0.206, loss: 2208.972412\n",
      "Train: step: 106600, time: 0.192, loss: 2156.830811\n",
      "Train: step: 106610, time: 0.191, loss: 1299.197754\n",
      "Train: step: 106620, time: 0.193, loss: 375.490875\n",
      "Train: step: 106630, time: 0.191, loss: 919.789062\n",
      "Train: step: 106640, time: 0.190, loss: 2106.724365\n",
      "Train: step: 106650, time: 0.192, loss: 1449.918823\n",
      "Train: step: 106660, time: 0.195, loss: 1622.022339\n",
      "Train: step: 106670, time: 0.193, loss: 2759.082764\n",
      "Train: step: 106680, time: 0.196, loss: 871.542053\n",
      "Train: step: 106690, time: 0.218, loss: 1765.351318\n",
      "Train: step: 106700, time: 0.194, loss: 3366.191650\n",
      "Train: step: 106710, time: 0.198, loss: 1397.133911\n",
      "Train: step: 106720, time: 0.186, loss: 2121.459961\n",
      "Train: step: 106730, time: 0.188, loss: 3265.498535\n",
      "Train: step: 106740, time: 0.193, loss: 681.443298\n",
      "Train: step: 106750, time: 0.195, loss: 2154.990479\n",
      "Train: step: 106760, time: 0.226, loss: 2775.487793\n",
      "Train: step: 106770, time: 0.218, loss: 1750.738892\n",
      "Train: step: 106780, time: 0.233, loss: 1260.208618\n",
      "Train: step: 106790, time: 0.218, loss: 2744.725098\n",
      "Train: step: 106800, time: 0.236, loss: 1409.255737\n",
      "Train: step: 106810, time: 0.195, loss: 1500.304321\n",
      "Train: step: 106820, time: 0.227, loss: 2881.230957\n",
      "Train: step: 106830, time: 0.207, loss: 2869.629639\n",
      "Train: step: 106840, time: 0.193, loss: 431.113159\n",
      "Train: step: 106850, time: 0.230, loss: 282.769592\n",
      "Train: step: 106860, time: 0.194, loss: 1037.174072\n",
      "Train: step: 106870, time: 0.217, loss: 1673.615967\n",
      "Train: step: 106880, time: 0.218, loss: 3632.973633\n",
      "Train: step: 106890, time: 0.186, loss: 2547.992188\n",
      "Train: step: 106900, time: 0.195, loss: 974.914978\n",
      "Train: step: 106910, time: 0.221, loss: 1308.367554\n",
      "Train: step: 106920, time: 0.239, loss: 578.438660\n",
      "Train: step: 106930, time: 0.196, loss: 2504.479248\n",
      "Train: step: 106940, time: 0.193, loss: 3334.250977\n",
      "Train: step: 106950, time: 0.194, loss: 2537.513672\n",
      "Train: step: 106960, time: 0.195, loss: 2702.726562\n",
      "Train: step: 106970, time: 0.198, loss: 2228.179932\n",
      "Train: step: 106980, time: 0.196, loss: 789.613953\n",
      "Train: step: 106990, time: 0.231, loss: 233.083588\n",
      "Train: step: 107000, time: 0.217, loss: 957.994385\n",
      "Train: step: 107010, time: 0.190, loss: 874.124146\n",
      "Train: step: 107020, time: 0.196, loss: 483.872406\n",
      "Train: step: 107030, time: 0.235, loss: 2755.801025\n",
      "Train: step: 107040, time: 0.195, loss: 327.607605\n",
      "Train: step: 107050, time: 0.201, loss: 1247.406128\n",
      "Train: step: 107060, time: 0.197, loss: 1668.677246\n",
      "Train: step: 107070, time: 0.189, loss: 923.108154\n",
      "Train: step: 107080, time: 0.206, loss: 2861.330078\n",
      "Train: step: 107090, time: 0.192, loss: 2318.920654\n",
      "Train: step: 107100, time: 0.195, loss: 1211.108398\n",
      "Train: step: 107110, time: 0.195, loss: 345.338043\n",
      "Train: step: 107120, time: 0.196, loss: 273.431793\n",
      "Train: step: 107130, time: 0.199, loss: 2427.114746\n",
      "Train: step: 107140, time: 0.196, loss: 2331.425537\n",
      "Train: step: 107150, time: 0.210, loss: 2966.443848\n",
      "Train: step: 107160, time: 0.194, loss: 2131.773438\n",
      "Train: step: 107170, time: 0.206, loss: 1787.333252\n",
      "Train: step: 107180, time: 0.217, loss: 3380.766846\n",
      "Train: step: 107190, time: 0.190, loss: 2570.260986\n",
      "Train: step: 107200, time: 0.223, loss: 1032.529785\n",
      "Train: step: 107210, time: 0.218, loss: 2587.035889\n",
      "Train: step: 107220, time: 0.220, loss: 1768.437744\n",
      "Train: step: 107230, time: 0.201, loss: 2182.739502\n",
      "Train: step: 107240, time: 0.219, loss: 1318.431763\n",
      "Train: step: 107250, time: 0.221, loss: 2016.109497\n",
      "Train: step: 107260, time: 0.194, loss: 2331.938721\n",
      "Train: step: 107270, time: 0.234, loss: 2755.722412\n",
      "Train: step: 107280, time: 0.192, loss: 1370.062378\n",
      "Train: step: 107290, time: 0.195, loss: 2311.628662\n",
      "Train: step: 107300, time: 0.216, loss: 1143.335327\n",
      "Train: step: 107310, time: 0.192, loss: 1105.569458\n",
      "Train: step: 107320, time: 0.217, loss: 2718.081787\n",
      "Train: step: 107330, time: 0.200, loss: 2288.672852\n",
      "Train: step: 107340, time: 0.228, loss: 2862.062500\n",
      "Train: step: 107350, time: 0.192, loss: 1051.663940\n",
      "Train: step: 107360, time: 0.200, loss: 1709.949097\n",
      "Train: step: 107370, time: 0.196, loss: 1766.558838\n",
      "Train: step: 107380, time: 0.202, loss: 1632.981079\n",
      "Train: step: 107390, time: 0.209, loss: 1936.347168\n",
      "Train: step: 107400, time: 0.199, loss: 1110.085449\n",
      "Train: step: 107410, time: 0.193, loss: 868.100708\n",
      "Train: step: 107420, time: 0.189, loss: 658.402039\n",
      "Train: step: 107430, time: 0.236, loss: 1199.430908\n",
      "Train: step: 107440, time: 0.202, loss: 2625.071289\n",
      "Train: step: 107450, time: 0.219, loss: 1960.503174\n",
      "Train: step: 107460, time: 0.195, loss: 2375.419922\n",
      "Train: step: 107470, time: 0.220, loss: 2200.708496\n",
      "Train: step: 107480, time: 0.195, loss: 938.548035\n",
      "Train: step: 107490, time: 0.197, loss: 709.280151\n",
      "Train: step: 107500, time: 0.218, loss: 1111.489136\n",
      "Train: step: 107510, time: 0.196, loss: 2309.498291\n",
      "Train: step: 107520, time: 0.225, loss: 1689.067261\n",
      "Train: step: 107530, time: 0.219, loss: 2021.453857\n",
      "Train: step: 107540, time: 0.198, loss: 2014.436890\n",
      "Train: step: 107550, time: 0.228, loss: 266.634766\n",
      "Train: step: 107560, time: 0.196, loss: 4126.256348\n",
      "Train: step: 107570, time: 0.194, loss: 1640.184570\n",
      "Train: step: 107580, time: 0.224, loss: 2001.796509\n",
      "Train: step: 107590, time: 0.219, loss: 1851.547974\n",
      "Train: step: 107600, time: 0.234, loss: 360.693604\n",
      "Train: step: 107610, time: 0.193, loss: 2364.790771\n",
      "Train: step: 107620, time: 0.192, loss: 1240.362915\n",
      "Train: step: 107630, time: 0.188, loss: 3269.737793\n",
      "Train: step: 107640, time: 0.197, loss: 2376.385010\n",
      "Train: step: 107650, time: 0.228, loss: 834.156616\n",
      "Train: step: 107660, time: 0.201, loss: 3536.657715\n",
      "Train: step: 107670, time: 0.190, loss: 1005.358337\n",
      "Train: step: 107680, time: 0.192, loss: 422.463379\n",
      "Train: step: 107690, time: 0.222, loss: 2847.056885\n",
      "Train: step: 107700, time: 0.196, loss: 2930.871582\n",
      "Train: step: 107710, time: 0.195, loss: 1456.413452\n",
      "Train: step: 107720, time: 0.199, loss: 1013.337463\n",
      "Train: step: 107730, time: 0.192, loss: 2072.642334\n",
      "Train: step: 107740, time: 0.219, loss: 3245.510010\n",
      "Train: step: 107750, time: 0.236, loss: 676.503662\n",
      "Train: step: 107760, time: 0.200, loss: 560.257385\n",
      "Train: step: 107770, time: 0.239, loss: 705.740051\n",
      "Train: step: 107780, time: 0.226, loss: 567.856079\n",
      "Train: step: 107790, time: 0.187, loss: 2026.741455\n",
      "Train: step: 107800, time: 0.218, loss: 963.735107\n",
      "Train: step: 107810, time: 0.217, loss: 2941.901855\n",
      "Train: step: 107820, time: 0.230, loss: 1055.951904\n",
      "Train: step: 107830, time: 0.234, loss: 495.123932\n",
      "Train: step: 107840, time: 0.213, loss: 972.553345\n",
      "Train: step: 107850, time: 0.198, loss: 2275.572754\n",
      "Train: step: 107860, time: 0.191, loss: 733.192566\n",
      "Train: step: 107870, time: 0.195, loss: 1179.819580\n",
      "Train: step: 107880, time: 0.190, loss: 2764.918701\n",
      "Train: step: 107890, time: 0.194, loss: 2644.758789\n",
      "Train: step: 107900, time: 0.191, loss: 1633.826904\n",
      "Train: step: 107910, time: 0.191, loss: 1502.118652\n",
      "Train: step: 107920, time: 0.235, loss: 2413.499756\n",
      "Train: step: 107930, time: 0.193, loss: 2193.428711\n",
      "Train: step: 107940, time: 0.236, loss: 1489.495605\n",
      "Train: step: 107950, time: 0.241, loss: 3163.516357\n",
      "Train: step: 107960, time: 0.190, loss: 953.004761\n",
      "Train: step: 107970, time: 0.193, loss: 738.138428\n",
      "Train: step: 107980, time: 0.203, loss: 1133.382324\n",
      "Train: step: 107990, time: 0.254, loss: 3050.129150\n",
      "Train: step: 108000, time: 0.230, loss: 1104.267334\n",
      "Train: step: 108010, time: 0.210, loss: 2568.793701\n",
      "Train: step: 108020, time: 0.193, loss: 839.322510\n",
      "Train: step: 108030, time: 0.259, loss: 892.505737\n",
      "Train: step: 108040, time: 0.241, loss: 1705.102539\n",
      "Train: step: 108050, time: 0.218, loss: 1102.075684\n",
      "Train: step: 108060, time: 0.236, loss: 995.035828\n",
      "Train: step: 108070, time: 0.218, loss: 1746.954346\n",
      "Train: step: 108080, time: 0.226, loss: 1726.760742\n",
      "Train: step: 108090, time: 0.216, loss: 1116.664551\n",
      "Train: step: 108100, time: 0.232, loss: 503.253021\n",
      "Train: step: 108110, time: 0.246, loss: 2606.284424\n",
      "Train: step: 108120, time: 0.191, loss: 2440.726318\n",
      "Train: step: 108130, time: 0.218, loss: 1526.895752\n",
      "Train: step: 108140, time: 0.195, loss: 2715.694092\n",
      "Train: step: 108150, time: 0.200, loss: 2755.677246\n",
      "Train: step: 108160, time: 0.228, loss: 780.128418\n",
      "Train: step: 108170, time: 0.223, loss: 283.266693\n",
      "Train: step: 108180, time: 0.223, loss: 3138.278076\n",
      "Train: step: 108190, time: 0.195, loss: 1319.350830\n",
      "Train: step: 108200, time: 0.190, loss: 1722.908203\n",
      "Train: step: 108210, time: 0.199, loss: 1882.152954\n",
      "Train: step: 108220, time: 0.237, loss: 856.973389\n",
      "Train: step: 108230, time: 0.229, loss: 1388.606445\n",
      "Train: step: 108240, time: 0.217, loss: 2225.793457\n",
      "Train: step: 108250, time: 0.232, loss: 2029.083862\n",
      "Train: step: 108260, time: 0.224, loss: 1215.612305\n",
      "Train: step: 108270, time: 0.192, loss: 2295.702393\n",
      "Train: step: 108280, time: 0.200, loss: 3698.801758\n",
      "Train: step: 108290, time: 0.214, loss: 1465.291138\n",
      "Train: step: 108300, time: 0.191, loss: 3363.177002\n",
      "Train: step: 108310, time: 0.218, loss: 2299.947998\n",
      "Train: step: 108320, time: 0.200, loss: 1739.653076\n",
      "Train: step: 108330, time: 0.219, loss: 1126.797485\n",
      "Train: step: 108340, time: 0.186, loss: 1339.772583\n",
      "Train: step: 108350, time: 0.210, loss: 892.338074\n",
      "Train: step: 108360, time: 0.193, loss: 4175.650879\n",
      "Train: step: 108370, time: 0.193, loss: 1734.937988\n",
      "Train: step: 108380, time: 0.255, loss: 2072.108398\n",
      "Train: step: 108390, time: 0.193, loss: 2698.960938\n",
      "Train: step: 108400, time: 0.192, loss: 2873.522217\n",
      "Train: step: 108410, time: 0.198, loss: 1647.768311\n",
      "Train: step: 108420, time: 0.196, loss: 476.741058\n",
      "Train: step: 108430, time: 0.230, loss: 4021.257812\n",
      "Train: step: 108440, time: 0.193, loss: 2150.502686\n",
      "Train: step: 108450, time: 0.207, loss: 798.670898\n",
      "Train: step: 108460, time: 0.192, loss: 1239.128296\n",
      "Train: step: 108470, time: 0.187, loss: 583.165955\n",
      "Train: step: 108480, time: 0.197, loss: 3022.755371\n",
      "Train: step: 108490, time: 0.205, loss: 1852.146362\n",
      "Train: step: 108500, time: 0.193, loss: 3958.706055\n",
      "Train: step: 108510, time: 0.222, loss: 2022.606079\n",
      "Train: step: 108520, time: 0.202, loss: 591.792603\n",
      "Train: step: 108530, time: 0.203, loss: 1636.411865\n",
      "Train: step: 108540, time: 0.190, loss: 1674.075195\n",
      "Train: step: 108550, time: 0.215, loss: 2150.924561\n",
      "Train: step: 108560, time: 0.187, loss: 1102.792236\n",
      "Train: step: 108570, time: 0.197, loss: 1869.924438\n",
      "Train: step: 108580, time: 0.191, loss: 2658.167969\n",
      "Train: step: 108590, time: 0.192, loss: 173.499069\n",
      "Train: step: 108600, time: 0.189, loss: 704.708618\n",
      "Train: step: 108610, time: 0.213, loss: 2454.035889\n",
      "Train: step: 108620, time: 0.217, loss: 877.127563\n",
      "Train: step: 108630, time: 0.229, loss: 1092.585449\n",
      "Train: step: 108640, time: 0.191, loss: 1574.905396\n",
      "Train: step: 108650, time: 0.196, loss: 1574.250366\n",
      "Train: step: 108660, time: 0.252, loss: 2643.547852\n",
      "Train: step: 108670, time: 0.225, loss: 939.982971\n",
      "Train: step: 108680, time: 0.229, loss: 3416.664551\n",
      "Train: step: 108690, time: 0.198, loss: 2214.040283\n",
      "Train: step: 108700, time: 0.227, loss: 1292.689209\n",
      "Train: step: 108710, time: 0.197, loss: 3578.976562\n",
      "Train: step: 108720, time: 0.217, loss: 770.918213\n",
      "Train: step: 108730, time: 0.190, loss: 2151.054199\n",
      "Train: step: 108740, time: 0.197, loss: 1734.999634\n",
      "Train: step: 108750, time: 0.216, loss: 437.496674\n",
      "Train: step: 108760, time: 0.193, loss: 298.231812\n",
      "Train: step: 108770, time: 0.270, loss: 962.924683\n",
      "Train: step: 108780, time: 0.195, loss: 2410.249756\n",
      "Train: step: 108790, time: 0.186, loss: 1831.364136\n",
      "Train: step: 108800, time: 0.231, loss: 3558.926270\n",
      "Train: step: 108810, time: 0.236, loss: 2800.923584\n",
      "Train: step: 108820, time: 0.215, loss: 3167.717529\n",
      "Train: step: 108830, time: 0.192, loss: 972.446106\n",
      "Train: step: 108840, time: 0.227, loss: 973.111023\n",
      "Train: step: 108850, time: 0.237, loss: 1539.141724\n",
      "Train: step: 108860, time: 0.233, loss: 2815.486328\n",
      "Train: step: 108870, time: 0.192, loss: 1534.243286\n",
      "Train: step: 108880, time: 0.198, loss: 2597.038574\n",
      "Train: step: 108890, time: 0.198, loss: 558.063599\n",
      "Train: step: 108900, time: 0.195, loss: 2139.061035\n",
      "Train: step: 108910, time: 0.221, loss: 1559.926392\n",
      "Train: step: 108920, time: 0.221, loss: 1563.259033\n",
      "Train: step: 108930, time: 0.217, loss: 1554.344849\n",
      "Train: step: 108940, time: 0.224, loss: 2124.814697\n",
      "Train: step: 108950, time: 0.225, loss: 1841.800781\n",
      "Train: step: 108960, time: 0.193, loss: 717.671448\n",
      "Train: step: 108970, time: 0.220, loss: 1380.217773\n",
      "Train: step: 108980, time: 0.202, loss: 3037.253174\n",
      "Train: step: 108990, time: 0.192, loss: 3331.913818\n",
      "Train: step: 109000, time: 0.219, loss: 1220.035767\n",
      "Train: step: 109010, time: 0.314, loss: 1945.154419\n",
      "Train: step: 109020, time: 0.206, loss: 1276.344482\n",
      "Train: step: 109030, time: 0.209, loss: 1216.215820\n",
      "Train: step: 109040, time: 0.178, loss: 1794.768555\n",
      "Train: step: 109050, time: 0.202, loss: 1778.429443\n",
      "Train: step: 109060, time: 0.251, loss: 2218.431885\n",
      "Train: step: 109070, time: 0.218, loss: 1119.052368\n",
      "Train: step: 109080, time: 0.220, loss: 1525.207397\n",
      "Train: step: 109090, time: 0.224, loss: 1085.226685\n",
      "Train: step: 109100, time: 0.253, loss: 1825.257812\n",
      "Train: step: 109110, time: 0.232, loss: 3569.624512\n",
      "Train: step: 109120, time: 0.226, loss: 899.083374\n",
      "Train: step: 109130, time: 0.221, loss: 1138.449341\n",
      "Train: step: 109140, time: 0.201, loss: 1509.659302\n",
      "Train: step: 109150, time: 0.225, loss: 1043.561890\n",
      "Train: step: 109160, time: 0.201, loss: 2310.218262\n",
      "Train: step: 109170, time: 0.224, loss: 852.650879\n",
      "Train: step: 109180, time: 0.237, loss: 2966.819336\n",
      "Train: step: 109190, time: 0.197, loss: 3346.264893\n",
      "Train: step: 109200, time: 0.233, loss: 1836.294556\n",
      "Train: step: 109210, time: 0.189, loss: 1009.190918\n",
      "Train: step: 109220, time: 0.224, loss: 3631.746338\n",
      "Train: step: 109230, time: 0.234, loss: 2120.925049\n",
      "Train: step: 109240, time: 0.213, loss: 1468.774292\n",
      "Train: step: 109250, time: 0.207, loss: 899.625977\n",
      "Train: step: 109260, time: 0.207, loss: 1444.095703\n",
      "Train: step: 109270, time: 0.218, loss: 1401.702759\n",
      "Train: step: 109280, time: 0.190, loss: 1792.226807\n",
      "Train: step: 109290, time: 0.230, loss: 526.101562\n",
      "Train: step: 109300, time: 0.227, loss: 2770.710205\n",
      "Train: step: 109310, time: 0.206, loss: 1667.001099\n",
      "Train: step: 109320, time: 0.197, loss: 1892.560547\n",
      "Train: step: 109330, time: 0.189, loss: 3153.974854\n",
      "Train: step: 109340, time: 0.201, loss: 858.087769\n",
      "Train: step: 109350, time: 0.190, loss: 1112.319702\n",
      "Train: step: 109360, time: 0.224, loss: 544.908813\n",
      "Train: step: 109370, time: 0.193, loss: 1044.074097\n",
      "Train: step: 109380, time: 0.228, loss: 2368.398926\n",
      "Train: step: 109390, time: 0.193, loss: 2997.012695\n",
      "Train: step: 109400, time: 0.194, loss: 2896.077637\n",
      "Train: step: 109410, time: 0.198, loss: 2506.626953\n",
      "Train: step: 109420, time: 0.190, loss: 1849.978516\n",
      "Train: step: 109430, time: 0.192, loss: 1814.332031\n",
      "Train: step: 109440, time: 0.194, loss: 1242.611694\n",
      "Train: step: 109450, time: 0.230, loss: 2065.632080\n",
      "Train: step: 109460, time: 0.217, loss: 1691.872803\n",
      "Train: step: 109470, time: 0.199, loss: 2131.115723\n",
      "Train: step: 109480, time: 0.190, loss: 2332.792236\n",
      "Train: step: 109490, time: 0.194, loss: 1544.760498\n",
      "Train: step: 109500, time: 0.214, loss: 525.530579\n",
      "Train: step: 109510, time: 0.191, loss: 704.437500\n",
      "Train: step: 109520, time: 0.225, loss: 1218.025513\n",
      "Train: step: 109530, time: 0.203, loss: 571.013306\n",
      "Train: step: 109540, time: 0.215, loss: 1059.460815\n",
      "Train: step: 109550, time: 0.199, loss: 1115.839600\n",
      "Train: step: 109560, time: 0.219, loss: 1095.794189\n",
      "Train: step: 109570, time: 0.191, loss: 914.755798\n",
      "Train: step: 109580, time: 0.228, loss: 1160.398560\n",
      "Train: step: 109590, time: 0.244, loss: 2251.900879\n",
      "Train: step: 109600, time: 0.218, loss: 1014.512451\n",
      "Train: step: 109610, time: 0.230, loss: 1226.623291\n",
      "Train: step: 109620, time: 0.193, loss: 1810.636719\n",
      "Train: step: 109630, time: 0.194, loss: 2997.638428\n",
      "Train: step: 109640, time: 0.232, loss: 1431.695312\n",
      "Train: step: 109650, time: 0.233, loss: 1512.959351\n",
      "Train: step: 109660, time: 0.198, loss: 1606.102783\n",
      "Train: step: 109670, time: 0.188, loss: 720.026245\n",
      "Train: step: 109680, time: 0.196, loss: 2922.792236\n",
      "Train: step: 109690, time: 0.218, loss: 729.380005\n",
      "Train: step: 109700, time: 0.198, loss: 1062.247681\n",
      "Train: step: 109710, time: 0.217, loss: 1643.051270\n",
      "Train: step: 109720, time: 0.221, loss: 1693.732178\n",
      "Train: step: 109730, time: 0.192, loss: 398.194885\n",
      "Train: step: 109740, time: 0.219, loss: 1116.218750\n",
      "Train: step: 109750, time: 0.187, loss: 2675.383301\n",
      "Train: step: 109760, time: 0.221, loss: 565.363403\n",
      "Train: step: 109770, time: 0.230, loss: 2990.543457\n",
      "Train: step: 109780, time: 0.217, loss: 777.617859\n",
      "Train: step: 109790, time: 0.197, loss: 2416.850830\n",
      "Train: step: 109800, time: 0.195, loss: 1193.235229\n",
      "Train: step: 109810, time: 0.189, loss: 2280.972900\n",
      "Train: step: 109820, time: 0.196, loss: 2298.021973\n",
      "Train: step: 109830, time: 0.239, loss: 4615.304688\n",
      "Train: step: 109840, time: 0.218, loss: 2751.101318\n",
      "Train: step: 109850, time: 0.201, loss: 2071.593994\n",
      "Train: step: 109860, time: 0.204, loss: 1806.033325\n",
      "Train: step: 109870, time: 0.263, loss: 817.276611\n",
      "Train: step: 109880, time: 0.191, loss: 764.211121\n",
      "Train: step: 109890, time: 0.191, loss: 514.997742\n",
      "Train: step: 109900, time: 0.195, loss: 2637.560303\n",
      "Train: step: 109910, time: 0.194, loss: 2553.170898\n",
      "Train: step: 109920, time: 0.230, loss: 2822.649414\n",
      "Train: step: 109930, time: 0.194, loss: 1171.662109\n",
      "Train: step: 109940, time: 0.198, loss: 1054.304321\n",
      "Train: step: 109950, time: 0.220, loss: 583.930298\n",
      "Train: step: 109960, time: 0.211, loss: 3268.113525\n",
      "Train: step: 109970, time: 0.234, loss: 632.203430\n",
      "Train: step: 109980, time: 0.190, loss: 3166.033936\n",
      "Train: step: 109990, time: 0.219, loss: 966.465271\n",
      "Train: step: 110000, time: 0.229, loss: 2560.193115\n",
      "Train: step: 110010, time: 0.217, loss: 2212.591553\n",
      "Train: step: 110020, time: 0.196, loss: 3971.562744\n",
      "Train: step: 110030, time: 0.194, loss: 385.795959\n",
      "Train: step: 110040, time: 0.193, loss: 1773.134399\n",
      "Train: step: 110050, time: 0.191, loss: 3966.969482\n",
      "Train: step: 110060, time: 0.229, loss: 2775.238037\n",
      "Train: step: 110070, time: 0.228, loss: 2751.459473\n",
      "Train: step: 110080, time: 0.190, loss: 2998.734131\n",
      "Train: step: 110090, time: 0.228, loss: 3428.340576\n",
      "Train: step: 110100, time: 0.189, loss: 812.592712\n",
      "Train: step: 110110, time: 0.195, loss: 969.245789\n",
      "Train: step: 110120, time: 0.190, loss: 2036.609131\n",
      "Train: step: 110130, time: 0.198, loss: 1101.043457\n",
      "Train: step: 110140, time: 0.223, loss: 972.848633\n",
      "Train: step: 110150, time: 0.241, loss: 2541.035645\n",
      "Train: step: 110160, time: 0.242, loss: 851.692688\n",
      "Train: step: 110170, time: 0.197, loss: 1519.540649\n",
      "Train: step: 110180, time: 0.197, loss: 864.589355\n",
      "Train: step: 110190, time: 0.193, loss: 3466.712402\n",
      "Train: step: 110200, time: 0.232, loss: 1199.098877\n",
      "Train: step: 110210, time: 0.193, loss: 1556.047363\n",
      "Train: step: 110220, time: 0.194, loss: 697.474365\n",
      "Train: step: 110230, time: 0.216, loss: 1224.796143\n",
      "Train: step: 110240, time: 0.224, loss: 1507.164673\n",
      "Train: step: 110250, time: 0.194, loss: 914.780273\n",
      "Train: step: 110260, time: 0.220, loss: 2719.654541\n",
      "Train: step: 110270, time: 0.191, loss: 558.219666\n",
      "Train: step: 110280, time: 0.216, loss: 1595.575317\n",
      "Train: step: 110290, time: 0.194, loss: 592.397217\n",
      "Train: step: 110300, time: 0.189, loss: 2831.503662\n",
      "Train: step: 110310, time: 0.194, loss: 6470.073242\n",
      "Train: step: 110320, time: 0.198, loss: 598.043823\n",
      "Train: step: 110330, time: 0.251, loss: 917.239929\n",
      "Train: step: 110340, time: 0.219, loss: 1163.107910\n",
      "Train: step: 110350, time: 0.190, loss: 2755.103516\n",
      "Train: step: 110360, time: 0.206, loss: 3382.188721\n",
      "Train: step: 110370, time: 0.192, loss: 3206.481201\n",
      "Train: step: 110380, time: 0.192, loss: 780.457031\n",
      "Train: step: 110390, time: 0.194, loss: 2011.585205\n",
      "Train: step: 110400, time: 0.248, loss: 1305.260132\n",
      "Train: step: 110410, time: 0.192, loss: 3103.991211\n",
      "Train: step: 110420, time: 0.197, loss: 1700.363770\n",
      "Train: step: 110430, time: 0.227, loss: 986.903503\n",
      "Train: step: 110440, time: 0.192, loss: 3084.972412\n",
      "Train: step: 110450, time: 0.190, loss: 1413.645752\n",
      "Train: step: 110460, time: 0.217, loss: 3724.449219\n",
      "Train: step: 110470, time: 0.217, loss: 3154.843262\n",
      "Train: step: 110480, time: 0.216, loss: 1575.516968\n",
      "Train: step: 110490, time: 0.203, loss: 1539.680054\n",
      "Train: step: 110500, time: 0.217, loss: 2353.206299\n",
      "Train: step: 110510, time: 0.190, loss: 1317.846558\n",
      "Train: step: 110520, time: 0.188, loss: 2450.716797\n",
      "Train: step: 110530, time: 0.233, loss: 266.828003\n",
      "Train: step: 110540, time: 0.192, loss: 1981.144165\n",
      "Train: step: 110550, time: 0.195, loss: 2939.496582\n",
      "Train: step: 110560, time: 0.214, loss: 1434.904297\n",
      "Train: step: 110570, time: 0.204, loss: 589.722717\n",
      "Train: step: 110580, time: 0.219, loss: 2353.044434\n",
      "Train: step: 110590, time: 0.227, loss: 1469.260986\n",
      "Train: step: 110600, time: 0.190, loss: 2310.813232\n",
      "Train: step: 110610, time: 0.219, loss: 1856.823853\n",
      "Train: step: 110620, time: 0.199, loss: 791.934692\n",
      "Train: step: 110630, time: 0.193, loss: 2029.585327\n",
      "Train: step: 110640, time: 0.193, loss: 739.245300\n",
      "Train: step: 110650, time: 0.230, loss: 1910.470947\n",
      "Train: step: 110660, time: 0.193, loss: 175.836151\n",
      "Train: step: 110670, time: 0.192, loss: 2269.229492\n",
      "Train: step: 110680, time: 0.195, loss: 210.447922\n",
      "Train: step: 110690, time: 0.194, loss: 2636.944580\n",
      "Train: step: 110700, time: 0.240, loss: 2450.064697\n",
      "Train: step: 110710, time: 0.195, loss: 915.160522\n",
      "Train: step: 110720, time: 0.199, loss: 966.437134\n",
      "Train: step: 110730, time: 0.193, loss: 2001.536377\n",
      "Train: step: 110740, time: 0.183, loss: 1036.327026\n",
      "Train: step: 110750, time: 0.232, loss: 4261.222168\n",
      "Train: step: 110760, time: 0.202, loss: 3320.774902\n",
      "Train: step: 110770, time: 0.243, loss: 1142.746338\n",
      "Train: step: 110780, time: 0.218, loss: 1876.462524\n",
      "Train: step: 110790, time: 0.196, loss: 2114.443359\n",
      "Train: step: 110800, time: 0.218, loss: 2495.573730\n",
      "Train: step: 110810, time: 0.194, loss: 1891.460815\n",
      "Train: step: 110820, time: 0.231, loss: 2262.833252\n",
      "Train: step: 110830, time: 0.232, loss: 3381.359619\n",
      "Train: step: 110840, time: 0.219, loss: 2290.532715\n",
      "Train: step: 110850, time: 0.213, loss: 551.289307\n",
      "Train: step: 110860, time: 0.201, loss: 3367.133301\n",
      "Train: step: 110870, time: 0.193, loss: 2464.422852\n",
      "Train: step: 110880, time: 0.190, loss: 938.656982\n",
      "Train: step: 110890, time: 0.236, loss: 1934.407959\n",
      "Train: step: 110900, time: 0.202, loss: 1106.411743\n",
      "Train: step: 110910, time: 0.198, loss: 448.802704\n",
      "Train: step: 110920, time: 0.211, loss: 1825.798950\n",
      "Train: step: 110930, time: 0.197, loss: 3226.856445\n",
      "Train: step: 110940, time: 0.216, loss: 1840.397583\n",
      "Train: step: 110950, time: 0.191, loss: 2414.616943\n",
      "Train: step: 110960, time: 0.205, loss: 2606.713379\n",
      "Train: step: 110970, time: 0.245, loss: 3974.483887\n",
      "Train: step: 110980, time: 0.207, loss: 3267.914551\n",
      "Train: step: 110990, time: 0.219, loss: 3829.697266\n",
      "Train: step: 111000, time: 0.192, loss: 4435.543457\n",
      "Train: step: 111010, time: 0.189, loss: 1629.045044\n",
      "Train: step: 111020, time: 0.242, loss: 2822.093018\n",
      "Train: step: 111030, time: 0.197, loss: 2458.998047\n",
      "Train: step: 111040, time: 0.186, loss: 1134.285889\n",
      "Train: step: 111050, time: 0.187, loss: 857.449951\n",
      "Train: step: 111060, time: 0.188, loss: 339.444885\n",
      "Train: step: 111070, time: 0.239, loss: 584.890503\n",
      "Train: step: 111080, time: 0.259, loss: 1771.300415\n",
      "Train: step: 111090, time: 0.229, loss: 2228.530273\n",
      "Train: step: 111100, time: 0.219, loss: 3761.468750\n",
      "Train: step: 111110, time: 0.191, loss: 1676.083252\n",
      "Train: step: 111120, time: 0.227, loss: 2679.107178\n",
      "Train: step: 111130, time: 0.194, loss: 2434.945068\n",
      "Train: step: 111140, time: 0.201, loss: 1133.039673\n",
      "Train: step: 111150, time: 0.260, loss: 1211.349609\n",
      "Train: step: 111160, time: 0.220, loss: 569.019287\n",
      "Train: step: 111170, time: 0.188, loss: 1122.303467\n",
      "Train: step: 111180, time: 0.216, loss: 2887.780273\n",
      "Train: step: 111190, time: 0.192, loss: 2025.995605\n",
      "Train: step: 111200, time: 0.242, loss: 1978.478760\n",
      "Train: step: 111210, time: 0.230, loss: 3129.153564\n",
      "Train: step: 111220, time: 0.190, loss: 1723.084473\n",
      "Train: step: 111230, time: 0.190, loss: 258.382599\n",
      "Train: step: 111240, time: 0.235, loss: 2139.928467\n",
      "Train: step: 111250, time: 0.192, loss: 2247.442627\n",
      "Train: step: 111260, time: 0.202, loss: 463.643524\n",
      "Train: step: 111270, time: 0.190, loss: 1559.012451\n",
      "Train: step: 111280, time: 0.221, loss: 337.206940\n",
      "Train: step: 111290, time: 0.221, loss: 389.188507\n",
      "Train: step: 111300, time: 0.195, loss: 2178.455811\n",
      "Train: step: 111310, time: 0.226, loss: 1796.588013\n",
      "Train: step: 111320, time: 0.221, loss: 2167.704346\n",
      "Train: step: 111330, time: 0.238, loss: 1590.764404\n",
      "Train: step: 111340, time: 0.188, loss: 2084.501221\n",
      "Train: step: 111350, time: 0.217, loss: 1313.411133\n",
      "Train: step: 111360, time: 0.224, loss: 1629.723145\n",
      "Train: step: 111370, time: 0.191, loss: 673.695740\n",
      "Train: step: 111380, time: 0.201, loss: 1450.828613\n",
      "Train: step: 111390, time: 0.201, loss: 1600.609619\n",
      "Train: step: 111400, time: 0.188, loss: 1345.032227\n",
      "Train: step: 111410, time: 0.250, loss: 2482.684326\n",
      "Train: step: 111420, time: 0.225, loss: 3514.149658\n",
      "Train: step: 111430, time: 0.192, loss: 2127.704346\n",
      "Train: step: 111440, time: 0.236, loss: 733.859680\n",
      "Train: step: 111450, time: 0.225, loss: 3125.965576\n",
      "Train: step: 111460, time: 0.216, loss: 1620.970581\n",
      "Train: step: 111470, time: 0.227, loss: 2691.588867\n",
      "Train: step: 111480, time: 0.219, loss: 2028.877441\n",
      "Train: step: 111490, time: 0.191, loss: 2429.997314\n",
      "Train: step: 111500, time: 0.193, loss: 795.141479\n",
      "Train: step: 111510, time: 0.188, loss: 3141.109131\n",
      "Train: step: 111520, time: 0.198, loss: 1035.469727\n",
      "Train: step: 111530, time: 0.192, loss: 1071.868774\n",
      "Train: step: 111540, time: 0.211, loss: 805.689758\n",
      "Train: step: 111550, time: 0.189, loss: 1291.940796\n",
      "Train: step: 111560, time: 0.219, loss: 1496.462524\n",
      "Train: step: 111570, time: 0.188, loss: 3980.418213\n",
      "Train: step: 111580, time: 0.234, loss: 3281.324951\n",
      "Train: step: 111590, time: 0.190, loss: 1631.566895\n",
      "Train: step: 111600, time: 0.223, loss: 1682.961426\n",
      "Train: step: 111610, time: 0.194, loss: 3007.422607\n",
      "Train: step: 111620, time: 0.233, loss: 1023.678711\n",
      "Train: step: 111630, time: 0.232, loss: 577.062439\n",
      "Train: step: 111640, time: 0.228, loss: 1488.616577\n",
      "Train: step: 111650, time: 0.209, loss: 1870.777954\n",
      "Train: step: 111660, time: 0.246, loss: 805.249207\n",
      "Train: step: 111670, time: 0.189, loss: 1329.462769\n",
      "Train: step: 111680, time: 0.191, loss: 2786.175781\n",
      "Train: step: 111690, time: 0.211, loss: 3623.967773\n",
      "Train: step: 111700, time: 0.190, loss: 1922.562988\n",
      "Train: step: 111710, time: 0.220, loss: 4243.440430\n",
      "Train: step: 111720, time: 0.222, loss: 2214.202148\n",
      "Train: step: 111730, time: 0.210, loss: 1919.891602\n",
      "Train: step: 111740, time: 0.239, loss: 1440.110107\n",
      "Train: step: 111750, time: 0.192, loss: 3236.576660\n",
      "Train: step: 111760, time: 0.183, loss: 4137.930664\n",
      "Train: step: 111770, time: 0.195, loss: 1878.505981\n",
      "Train: step: 111780, time: 0.233, loss: 1855.475342\n",
      "Train: step: 111790, time: 0.193, loss: 3133.825195\n",
      "Train: step: 111800, time: 0.218, loss: 1466.669556\n",
      "Train: step: 111810, time: 0.222, loss: 649.571594\n",
      "Train: step: 111820, time: 0.182, loss: 1417.948120\n",
      "Train: step: 111830, time: 0.220, loss: 2722.663818\n",
      "Train: step: 111840, time: 0.194, loss: 1643.726318\n",
      "Train: step: 111850, time: 0.233, loss: 992.808228\n",
      "Train: step: 111860, time: 0.189, loss: 2717.206055\n",
      "Train: step: 111870, time: 0.192, loss: 1451.736694\n",
      "Train: step: 111880, time: 0.227, loss: 682.085632\n",
      "Train: step: 111890, time: 0.216, loss: 1952.599976\n",
      "Train: step: 111900, time: 0.209, loss: 1843.658691\n",
      "Train: step: 111910, time: 0.213, loss: 634.484619\n",
      "Train: step: 111920, time: 0.235, loss: 2146.695557\n",
      "Train: step: 111930, time: 0.220, loss: 1579.537354\n",
      "Train: step: 111940, time: 0.184, loss: 2083.286133\n",
      "Train: step: 111950, time: 0.187, loss: 1838.171997\n",
      "Train: step: 111960, time: 0.184, loss: 827.122253\n",
      "Train: step: 111970, time: 0.180, loss: 2143.710938\n",
      "Train: step: 111980, time: 0.223, loss: 2529.912842\n",
      "Train: step: 111990, time: 0.219, loss: 2094.188721\n",
      "Train: step: 112000, time: 0.224, loss: 3079.099854\n",
      "Train: step: 112010, time: 0.183, loss: 2160.854492\n",
      "Train: step: 112020, time: 0.221, loss: 2955.215576\n",
      "Train: step: 112030, time: 0.218, loss: 2980.945312\n",
      "Train: step: 112040, time: 0.218, loss: 2733.370361\n",
      "Train: step: 112050, time: 0.186, loss: 2508.169678\n",
      "Train: step: 112060, time: 0.216, loss: 852.943604\n",
      "Train: step: 112070, time: 0.194, loss: 527.343018\n",
      "Train: step: 112080, time: 0.216, loss: 3178.107910\n",
      "Train: step: 112090, time: 0.190, loss: 2585.606445\n",
      "Train: step: 112100, time: 0.194, loss: 2504.108887\n",
      "Train: step: 112110, time: 0.212, loss: 826.514709\n",
      "Train: step: 112120, time: 0.187, loss: 895.179321\n",
      "Train: step: 112130, time: 0.226, loss: 2717.464844\n",
      "Train: step: 112140, time: 0.221, loss: 1612.346436\n",
      "Train: step: 112150, time: 0.195, loss: 3057.610352\n",
      "Train: step: 112160, time: 0.192, loss: 2289.072021\n",
      "Train: step: 112170, time: 0.191, loss: 1364.542114\n",
      "Train: step: 112180, time: 0.197, loss: 1148.976318\n",
      "Train: step: 112190, time: 0.200, loss: 838.474854\n",
      "Train: step: 112200, time: 0.197, loss: 2492.690918\n",
      "Train: step: 112210, time: 0.226, loss: 246.543930\n",
      "Train: step: 112220, time: 0.181, loss: 779.315369\n",
      "Train: step: 112230, time: 0.181, loss: 1780.660522\n",
      "Train: step: 112240, time: 0.192, loss: 2437.331299\n",
      "Train: step: 112250, time: 0.191, loss: 1867.279175\n",
      "Train: step: 112260, time: 0.196, loss: 2509.721680\n",
      "Train: step: 112270, time: 0.230, loss: 2625.421387\n",
      "Train: step: 112280, time: 0.234, loss: 1365.935181\n",
      "Train: step: 112290, time: 0.231, loss: 3612.603271\n",
      "Train: step: 112300, time: 0.239, loss: 3945.956543\n",
      "Train: step: 112310, time: 0.198, loss: 700.931458\n",
      "Train: step: 112320, time: 0.188, loss: 3913.913818\n",
      "Train: step: 112330, time: 0.218, loss: 2769.425049\n",
      "Train: step: 112340, time: 0.215, loss: 2987.746338\n",
      "Train: step: 112350, time: 0.194, loss: 2303.140869\n",
      "Train: step: 112360, time: 0.219, loss: 1310.043213\n",
      "Train: step: 112370, time: 0.218, loss: 2032.625366\n",
      "Train: step: 112380, time: 0.188, loss: 427.468353\n",
      "Train: step: 112390, time: 0.226, loss: 919.152161\n",
      "Train: step: 112400, time: 0.189, loss: 2976.713623\n",
      "Train: step: 112410, time: 0.220, loss: 2176.129395\n",
      "Train: step: 112420, time: 0.187, loss: 3087.325195\n",
      "Train: step: 112430, time: 0.256, loss: 2341.800781\n",
      "Train: step: 112440, time: 0.229, loss: 1511.417603\n",
      "Train: step: 112450, time: 0.219, loss: 2211.036377\n",
      "Train: step: 112460, time: 0.244, loss: 3047.430908\n",
      "Train: step: 112470, time: 0.218, loss: 452.761902\n",
      "Train: step: 112480, time: 0.217, loss: 1761.637451\n",
      "Train: step: 112490, time: 0.189, loss: 2430.145752\n",
      "Train: step: 112500, time: 0.194, loss: 628.580078\n",
      "Train: step: 112510, time: 0.190, loss: 1459.743774\n",
      "Train: step: 112520, time: 0.193, loss: 2190.502197\n",
      "Train: step: 112530, time: 0.196, loss: 3252.502441\n",
      "Train: step: 112540, time: 0.187, loss: 3105.354248\n",
      "Train: step: 112550, time: 0.229, loss: 2664.692139\n",
      "Train: step: 112560, time: 0.192, loss: 1110.176514\n",
      "Train: step: 112570, time: 0.229, loss: 1329.026489\n",
      "Train: step: 112580, time: 0.217, loss: 3042.653320\n",
      "Train: step: 112590, time: 0.195, loss: 4394.753418\n",
      "Train: step: 112600, time: 0.194, loss: 1971.704468\n",
      "Train: step: 112610, time: 0.214, loss: 1381.026001\n",
      "Train: step: 112620, time: 0.218, loss: 840.724243\n",
      "Train: step: 112630, time: 0.200, loss: 191.951508\n",
      "Train: step: 112640, time: 0.188, loss: 2383.764160\n",
      "Train: step: 112650, time: 0.187, loss: 1105.191895\n",
      "Train: step: 112660, time: 0.199, loss: 447.558594\n",
      "Train: step: 112670, time: 0.225, loss: 1852.246582\n",
      "Train: step: 112680, time: 0.217, loss: 657.828308\n",
      "Train: step: 112690, time: 0.192, loss: 1272.309570\n",
      "Train: step: 112700, time: 0.193, loss: 2548.968994\n",
      "Train: step: 112710, time: 0.199, loss: 1428.440063\n",
      "Train: step: 112720, time: 0.194, loss: 1926.015381\n",
      "Train: step: 112730, time: 0.189, loss: 1901.647461\n",
      "Train: step: 112740, time: 0.191, loss: 1793.729370\n",
      "Train: step: 112750, time: 0.227, loss: 760.461121\n",
      "Train: step: 112760, time: 0.235, loss: 2270.226074\n",
      "Train: step: 112770, time: 0.244, loss: 2035.286865\n",
      "Train: step: 112780, time: 0.189, loss: 6113.140137\n",
      "Train: step: 112790, time: 0.199, loss: 2772.027832\n",
      "Train: step: 112800, time: 0.191, loss: 1082.565674\n",
      "Train: step: 112810, time: 0.194, loss: 197.032806\n",
      "Train: step: 112820, time: 0.226, loss: 456.248474\n",
      "Train: step: 112830, time: 0.230, loss: 2076.748047\n",
      "Train: step: 112840, time: 0.204, loss: 1502.116089\n",
      "Train: step: 112850, time: 0.213, loss: 1500.772705\n",
      "Train: step: 112860, time: 0.229, loss: 1854.901123\n",
      "Train: step: 112870, time: 0.194, loss: 2901.815430\n",
      "Train: step: 112880, time: 0.221, loss: 1613.330322\n",
      "Train: step: 112890, time: 0.196, loss: 1565.166626\n",
      "Train: step: 112900, time: 0.195, loss: 2910.259033\n",
      "Train: step: 112910, time: 0.243, loss: 1632.473389\n",
      "Train: step: 112920, time: 0.217, loss: 2132.239502\n",
      "Train: step: 112930, time: 0.186, loss: 2769.081787\n",
      "Train: step: 112940, time: 0.220, loss: 2190.551514\n",
      "Train: step: 112950, time: 0.217, loss: 2528.854736\n",
      "Train: step: 112960, time: 0.222, loss: 2072.065918\n",
      "Train: step: 112970, time: 0.191, loss: 1084.778076\n",
      "Train: step: 112980, time: 0.196, loss: 1877.111450\n",
      "Train: step: 112990, time: 0.232, loss: 1550.685059\n",
      "Train: step: 113000, time: 0.229, loss: 4401.279297\n",
      "Train: step: 113010, time: 0.229, loss: 2276.235840\n",
      "Train: step: 113020, time: 0.229, loss: 721.177063\n",
      "Train: step: 113030, time: 0.190, loss: 1172.449463\n",
      "Train: step: 113040, time: 0.232, loss: 2042.420044\n",
      "Train: step: 113050, time: 0.227, loss: 2606.284912\n",
      "Train: step: 113060, time: 0.190, loss: 735.102051\n",
      "Train: step: 113070, time: 0.185, loss: 3083.483154\n",
      "Train: step: 113080, time: 0.196, loss: 1225.543335\n",
      "Train: step: 113090, time: 0.236, loss: 712.470154\n",
      "Train: step: 113100, time: 0.192, loss: 1393.803711\n",
      "Train: step: 113110, time: 0.211, loss: 554.070007\n",
      "Train: step: 113120, time: 0.232, loss: 1501.636108\n",
      "Train: step: 113130, time: 0.234, loss: 255.949753\n",
      "Train: step: 113140, time: 0.225, loss: 1007.927429\n",
      "Train: step: 113150, time: 0.190, loss: 800.346924\n",
      "Train: step: 113160, time: 0.189, loss: 2000.799316\n",
      "Train: step: 113170, time: 0.190, loss: 2872.616211\n",
      "Train: step: 113180, time: 0.215, loss: 997.544678\n",
      "Train: step: 113190, time: 0.204, loss: 2459.410645\n",
      "Train: step: 113200, time: 0.246, loss: 1080.632690\n",
      "Train: step: 113210, time: 0.196, loss: 871.878906\n",
      "Train: step: 113220, time: 0.249, loss: 2284.181641\n",
      "Train: step: 113230, time: 0.234, loss: 1348.452759\n",
      "Train: step: 113240, time: 0.185, loss: 1677.773438\n",
      "Train: step: 113250, time: 0.231, loss: 1132.400024\n",
      "Train: step: 113260, time: 0.196, loss: 3029.790527\n",
      "Train: step: 113270, time: 0.192, loss: 236.170013\n",
      "Train: step: 113280, time: 0.220, loss: 2727.321777\n",
      "Train: step: 113290, time: 0.230, loss: 2173.626709\n",
      "Train: step: 113300, time: 0.237, loss: 3796.068848\n",
      "Train: step: 113310, time: 0.220, loss: 1051.115479\n",
      "Train: step: 113320, time: 0.193, loss: 1276.447266\n",
      "Train: step: 113330, time: 0.189, loss: 1234.093628\n",
      "Train: step: 113340, time: 0.216, loss: 371.899139\n",
      "Train: step: 113350, time: 0.187, loss: 3209.580566\n",
      "Train: step: 113360, time: 0.235, loss: 841.662354\n",
      "Train: step: 113370, time: 0.240, loss: 2365.080078\n",
      "Train: step: 113380, time: 0.207, loss: 740.109985\n",
      "Train: step: 113390, time: 0.222, loss: 1872.138062\n",
      "Train: step: 113400, time: 0.254, loss: 2967.022949\n",
      "Train: step: 113410, time: 0.196, loss: 2002.692505\n",
      "Train: step: 113420, time: 0.183, loss: 1987.473022\n",
      "Train: step: 113430, time: 0.217, loss: 217.819443\n",
      "Train: step: 113440, time: 0.215, loss: 1092.073975\n",
      "Train: step: 113450, time: 0.242, loss: 1172.632935\n",
      "Train: step: 113460, time: 0.188, loss: 1393.902344\n",
      "Train: step: 113470, time: 0.254, loss: 2511.561035\n",
      "Train: step: 113480, time: 0.225, loss: 633.840759\n",
      "Train: step: 113490, time: 0.216, loss: 3344.825928\n",
      "Train: step: 113500, time: 0.192, loss: 1857.930298\n",
      "Train: step: 113510, time: 0.193, loss: 932.191895\n",
      "Train: step: 113520, time: 0.216, loss: 2204.271729\n",
      "Train: step: 113530, time: 0.187, loss: 1011.360535\n",
      "Train: step: 113540, time: 0.192, loss: 5130.876953\n",
      "Train: step: 113550, time: 0.197, loss: 169.633438\n",
      "Train: step: 113560, time: 0.245, loss: 1327.422607\n",
      "Train: step: 113570, time: 0.191, loss: 676.436340\n",
      "Train: step: 113580, time: 0.192, loss: 2684.240723\n",
      "Train: step: 113590, time: 0.191, loss: 1742.322876\n",
      "Train: step: 113600, time: 0.188, loss: 7281.049316\n",
      "Train: step: 113610, time: 0.195, loss: 1505.940552\n",
      "Train: step: 113620, time: 0.234, loss: 2471.815674\n",
      "Train: step: 113630, time: 0.230, loss: 1932.712402\n",
      "Train: step: 113640, time: 0.216, loss: 2974.232178\n",
      "Train: step: 113650, time: 0.185, loss: 1249.506226\n",
      "Train: step: 113660, time: 0.194, loss: 2538.317383\n",
      "Train: step: 113670, time: 0.192, loss: 3219.220947\n",
      "Train: step: 113680, time: 0.191, loss: 2339.463623\n",
      "Train: step: 113690, time: 0.189, loss: 1937.429199\n",
      "Train: step: 113700, time: 0.201, loss: 2246.983154\n",
      "Train: step: 113710, time: 0.195, loss: 3658.979248\n",
      "Train: step: 113720, time: 0.187, loss: 1658.397095\n",
      "Train: step: 113730, time: 0.187, loss: 3233.278809\n",
      "Train: step: 113740, time: 0.203, loss: 1156.047363\n",
      "Train: step: 113750, time: 0.228, loss: 202.430954\n",
      "Train: step: 113760, time: 0.231, loss: 1474.559448\n",
      "Train: step: 113770, time: 0.216, loss: 2450.791504\n",
      "Train: step: 113780, time: 0.217, loss: 1699.631714\n",
      "Train: step: 113790, time: 0.193, loss: 1651.884277\n",
      "Train: step: 113800, time: 0.205, loss: 1470.497925\n",
      "Train: step: 113810, time: 0.196, loss: 1302.091675\n",
      "Train: step: 113820, time: 0.197, loss: 1934.265869\n",
      "Train: step: 113830, time: 0.235, loss: 2854.367432\n",
      "Train: step: 113840, time: 0.194, loss: 422.151794\n",
      "Train: step: 113850, time: 0.193, loss: 3231.119629\n",
      "Train: step: 113860, time: 0.239, loss: 784.837524\n",
      "Train: step: 113870, time: 0.196, loss: 2318.468018\n",
      "Train: step: 113880, time: 0.189, loss: 1632.516602\n",
      "Train: step: 113890, time: 0.216, loss: 1478.768799\n",
      "Train: step: 113900, time: 0.191, loss: 1420.755127\n",
      "Train: step: 113910, time: 0.194, loss: 2465.912109\n",
      "Train: step: 113920, time: 0.189, loss: 3590.125977\n",
      "Train: step: 113930, time: 0.202, loss: 2960.623535\n",
      "Train: step: 113940, time: 0.217, loss: 643.581177\n",
      "Train: step: 113950, time: 0.218, loss: 670.245911\n",
      "Train: step: 113960, time: 0.192, loss: 1504.393921\n",
      "Train: step: 113970, time: 0.187, loss: 555.984070\n",
      "Train: step: 113980, time: 0.193, loss: 1452.429321\n",
      "Train: step: 113990, time: 0.193, loss: 2554.526367\n",
      "Train: step: 114000, time: 0.228, loss: 1201.919800\n",
      "Train: step: 114010, time: 0.216, loss: 1043.642578\n",
      "Train: step: 114020, time: 0.217, loss: 1334.986694\n",
      "Train: step: 114030, time: 0.233, loss: 2414.599121\n",
      "Train: step: 114040, time: 0.231, loss: 2475.049805\n",
      "Train: step: 114050, time: 0.225, loss: 1881.582275\n",
      "Train: step: 114060, time: 0.192, loss: 2475.833252\n",
      "Train: step: 114070, time: 0.185, loss: 2874.621338\n",
      "Train: step: 114080, time: 0.214, loss: 3520.044434\n",
      "Train: step: 114090, time: 0.192, loss: 1718.882812\n",
      "Train: step: 114100, time: 0.191, loss: 1603.792725\n",
      "Train: step: 114110, time: 0.190, loss: 1708.277954\n",
      "Train: step: 114120, time: 0.218, loss: 1651.830078\n",
      "Train: step: 114130, time: 0.190, loss: 1409.396484\n",
      "Train: step: 114140, time: 0.191, loss: 2131.862793\n",
      "Train: step: 114150, time: 0.223, loss: 843.125549\n",
      "Train: step: 114160, time: 0.205, loss: 1690.018311\n",
      "Train: step: 114170, time: 0.217, loss: 2625.770264\n",
      "Train: step: 114180, time: 0.211, loss: 2121.331055\n",
      "Train: step: 114190, time: 0.216, loss: 1247.380005\n",
      "Train: step: 114200, time: 0.235, loss: 895.182556\n",
      "Train: step: 114210, time: 0.192, loss: 1454.976074\n",
      "Train: step: 114220, time: 0.191, loss: 1812.593872\n",
      "Train: step: 114230, time: 0.208, loss: 975.943909\n",
      "Train: step: 114240, time: 0.211, loss: 3129.642578\n",
      "Train: step: 114250, time: 0.197, loss: 843.404236\n",
      "Train: step: 114260, time: 0.194, loss: 426.367310\n",
      "Train: step: 114270, time: 0.226, loss: 357.773041\n",
      "Train: step: 114280, time: 0.224, loss: 646.291016\n",
      "Train: step: 114290, time: 0.236, loss: 904.652466\n",
      "Train: step: 114300, time: 0.230, loss: 1574.398926\n",
      "Train: step: 114310, time: 0.229, loss: 3066.097412\n",
      "Train: step: 114320, time: 0.217, loss: 1686.185547\n",
      "Train: step: 114330, time: 0.190, loss: 2643.980713\n",
      "Train: step: 114340, time: 0.218, loss: 4432.123535\n",
      "Train: step: 114350, time: 0.219, loss: 1720.880737\n",
      "Train: step: 114360, time: 0.192, loss: 2331.689209\n",
      "Train: step: 114370, time: 0.193, loss: 1484.821533\n",
      "Train: step: 114380, time: 0.196, loss: 1230.874268\n",
      "Train: step: 114390, time: 0.217, loss: 1421.542114\n",
      "Train: step: 114400, time: 0.186, loss: 925.521851\n",
      "Train: step: 114410, time: 0.196, loss: 2339.215088\n",
      "Train: step: 114420, time: 0.183, loss: 470.457428\n",
      "Train: step: 114430, time: 0.190, loss: 2240.538330\n",
      "Train: step: 114440, time: 0.219, loss: 1609.324463\n",
      "Train: step: 114450, time: 0.197, loss: 876.008057\n",
      "Train: step: 114460, time: 0.196, loss: 1445.827637\n",
      "Train: step: 114470, time: 0.224, loss: 1364.424072\n",
      "Train: step: 114480, time: 0.217, loss: 2391.337891\n",
      "Train: step: 114490, time: 0.212, loss: 604.514893\n",
      "Train: step: 114500, time: 0.194, loss: 898.608093\n",
      "Train: step: 114510, time: 0.209, loss: 1644.929565\n",
      "Train: step: 114520, time: 0.191, loss: 3055.671875\n",
      "Train: step: 114530, time: 0.197, loss: 1749.770874\n",
      "Train: step: 114540, time: 0.193, loss: 2210.668213\n",
      "Train: step: 114550, time: 0.194, loss: 3714.568359\n",
      "Train: step: 114560, time: 0.199, loss: 1267.476318\n",
      "Train: step: 114570, time: 0.190, loss: 668.954407\n",
      "Train: step: 114580, time: 0.219, loss: 810.739014\n",
      "Train: step: 114590, time: 0.188, loss: 254.813583\n",
      "Train: step: 114600, time: 0.186, loss: 4265.918945\n",
      "Train: step: 114610, time: 0.192, loss: 2401.838623\n",
      "Train: step: 114620, time: 0.196, loss: 1590.113403\n",
      "Train: step: 114630, time: 0.232, loss: 1310.928345\n",
      "Train: step: 114640, time: 0.189, loss: 1930.813843\n",
      "Train: step: 114650, time: 0.217, loss: 3017.308594\n",
      "Train: step: 114660, time: 0.191, loss: 1825.624390\n",
      "Train: step: 114670, time: 0.219, loss: 1778.936523\n",
      "Train: step: 114680, time: 0.239, loss: 2081.751953\n",
      "Train: step: 114690, time: 0.200, loss: 835.747559\n",
      "Train: step: 114700, time: 0.193, loss: 3348.988281\n",
      "Train: step: 114710, time: 0.229, loss: 625.853577\n",
      "Train: step: 114720, time: 0.217, loss: 2114.847168\n",
      "Train: step: 114730, time: 0.187, loss: 1096.005493\n",
      "Train: step: 114740, time: 0.188, loss: 427.082703\n",
      "Train: step: 114750, time: 0.190, loss: 1239.307861\n",
      "Train: step: 114760, time: 0.191, loss: 2556.004639\n",
      "Train: step: 114770, time: 0.197, loss: 1700.120483\n",
      "Train: step: 114780, time: 0.202, loss: 2253.567139\n",
      "Train: step: 114790, time: 0.187, loss: 1040.876831\n",
      "Train: step: 114800, time: 0.191, loss: 608.974670\n",
      "Train: step: 114810, time: 0.188, loss: 670.948486\n",
      "Train: step: 114820, time: 0.189, loss: 2230.818359\n",
      "Train: step: 114830, time: 0.218, loss: 1693.915161\n",
      "Train: step: 114840, time: 0.198, loss: 1731.719360\n",
      "Train: step: 114850, time: 0.198, loss: 990.838684\n",
      "Train: step: 114860, time: 0.227, loss: 2599.857910\n",
      "Train: step: 114870, time: 0.214, loss: 1499.733765\n",
      "Train: step: 114880, time: 0.187, loss: 967.697754\n",
      "Train: step: 114890, time: 0.187, loss: 1072.367310\n",
      "Train: step: 114900, time: 0.186, loss: 2177.167725\n",
      "Train: step: 114910, time: 0.191, loss: 1372.431885\n",
      "Train: step: 114920, time: 0.196, loss: 762.279419\n",
      "Train: step: 114930, time: 0.218, loss: 545.944214\n",
      "Train: step: 114940, time: 0.186, loss: 1370.846313\n",
      "Train: step: 114950, time: 0.228, loss: 2283.061523\n",
      "Train: step: 114960, time: 0.246, loss: 1261.565674\n",
      "Train: step: 114970, time: 0.233, loss: 1400.712646\n",
      "Train: step: 114980, time: 0.193, loss: 2476.180664\n",
      "Train: step: 114990, time: 0.218, loss: 3745.361572\n",
      "Train: step: 115000, time: 0.193, loss: 3716.931641\n",
      "Train: step: 115010, time: 0.195, loss: 3203.437500\n",
      "Train: step: 115020, time: 0.216, loss: 1293.208008\n",
      "Train: step: 115030, time: 0.212, loss: 1578.646606\n",
      "Train: step: 115040, time: 0.187, loss: 2699.660645\n",
      "Train: step: 115050, time: 0.199, loss: 1685.121460\n",
      "Train: step: 115060, time: 0.218, loss: 2454.149414\n",
      "Train: step: 115070, time: 0.196, loss: 1254.549561\n",
      "Train: step: 115080, time: 0.217, loss: 6165.996094\n",
      "Train: step: 115090, time: 0.191, loss: 2845.578857\n",
      "Train: step: 115100, time: 0.219, loss: 308.614471\n",
      "Train: step: 115110, time: 0.203, loss: 548.945801\n",
      "Train: step: 115120, time: 0.200, loss: 3217.481445\n",
      "Train: step: 115130, time: 0.193, loss: 2730.135010\n",
      "Train: step: 115140, time: 0.230, loss: 2541.207520\n",
      "Train: step: 115150, time: 0.189, loss: 3217.119141\n",
      "Train: step: 115160, time: 0.198, loss: 1694.296143\n",
      "Train: step: 115170, time: 0.195, loss: 2318.502441\n",
      "Train: step: 115180, time: 0.206, loss: 1796.393677\n",
      "Train: step: 115190, time: 0.186, loss: 2455.768799\n",
      "Train: step: 115200, time: 0.220, loss: 658.029541\n",
      "Train: step: 115210, time: 0.191, loss: 1983.647827\n",
      "Train: step: 115220, time: 0.194, loss: 2199.462891\n",
      "Train: step: 115230, time: 0.223, loss: 2905.637451\n",
      "Train: step: 115240, time: 0.190, loss: 1452.629028\n",
      "Train: step: 115250, time: 0.224, loss: 1078.572998\n",
      "Train: step: 115260, time: 0.191, loss: 2314.104736\n",
      "Train: step: 115270, time: 0.219, loss: 1422.016602\n",
      "Train: step: 115280, time: 0.204, loss: 2439.727783\n",
      "Train: step: 115290, time: 0.218, loss: 2508.128906\n",
      "Train: step: 115300, time: 0.201, loss: 1702.613037\n",
      "Train: step: 115310, time: 0.204, loss: 1584.702881\n",
      "Train: step: 115320, time: 0.225, loss: 1307.847778\n",
      "Train: step: 115330, time: 0.204, loss: 3188.465088\n",
      "Train: step: 115340, time: 0.198, loss: 1951.260498\n",
      "Train: step: 115350, time: 0.194, loss: 1411.877319\n",
      "Train: step: 115360, time: 0.192, loss: 742.431702\n",
      "Train: step: 115370, time: 0.189, loss: 266.474518\n",
      "Train: step: 115380, time: 0.215, loss: 2018.799438\n",
      "Train: step: 115390, time: 0.208, loss: 968.800232\n",
      "Train: step: 115400, time: 0.266, loss: 2166.168457\n",
      "Train: step: 115410, time: 0.189, loss: 272.351715\n",
      "Train: step: 115420, time: 0.218, loss: 344.655304\n",
      "Train: step: 115430, time: 0.210, loss: 1064.837402\n",
      "Train: step: 115440, time: 0.198, loss: 1854.267822\n",
      "Train: step: 115450, time: 0.227, loss: 2428.632324\n",
      "Train: step: 115460, time: 0.220, loss: 2332.688477\n",
      "Train: step: 115470, time: 0.245, loss: 1133.456421\n",
      "Train: step: 115480, time: 0.232, loss: 1523.762573\n",
      "Train: step: 115490, time: 0.200, loss: 506.555786\n",
      "Train: step: 115500, time: 0.219, loss: 1681.745483\n",
      "Train: step: 115510, time: 0.192, loss: 398.224274\n",
      "Train: step: 115520, time: 0.238, loss: 388.210815\n",
      "Train: step: 115530, time: 0.328, loss: 1299.606812\n",
      "Train: step: 115540, time: 0.236, loss: 1595.545532\n",
      "Train: step: 115550, time: 0.228, loss: 938.256958\n",
      "Train: step: 115560, time: 0.205, loss: 2149.722656\n",
      "Train: step: 115570, time: 0.202, loss: 1390.301025\n",
      "Train: step: 115580, time: 0.232, loss: 705.603394\n",
      "Train: step: 115590, time: 0.218, loss: 1839.305542\n",
      "Train: step: 115600, time: 0.195, loss: 2106.638672\n",
      "Train: step: 115610, time: 0.197, loss: 352.628479\n",
      "Train: step: 115620, time: 0.197, loss: 1560.268799\n",
      "Train: step: 115630, time: 0.227, loss: 3244.473877\n",
      "Train: step: 115640, time: 0.225, loss: 3297.090088\n",
      "Train: step: 115650, time: 0.260, loss: 1915.438110\n",
      "Train: step: 115660, time: 0.236, loss: 1376.799561\n",
      "Train: step: 115670, time: 0.203, loss: 1498.583496\n",
      "Train: step: 115680, time: 0.194, loss: 2074.105469\n",
      "Train: step: 115690, time: 0.227, loss: 2996.983398\n",
      "Train: step: 115700, time: 0.196, loss: 1148.880249\n",
      "Train: step: 115710, time: 0.318, loss: 2205.373047\n",
      "Train: step: 115720, time: 0.207, loss: 555.800171\n",
      "Train: step: 115730, time: 0.227, loss: 2050.998047\n",
      "Train: step: 115740, time: 0.229, loss: 1315.616089\n",
      "Train: step: 115750, time: 0.190, loss: 2101.454590\n",
      "Train: step: 115760, time: 0.209, loss: 1139.833496\n",
      "Train: step: 115770, time: 0.202, loss: 404.857391\n",
      "Train: step: 115780, time: 0.318, loss: 1141.406494\n",
      "Train: step: 115790, time: 0.248, loss: 1517.779541\n",
      "Train: step: 115800, time: 0.233, loss: 1631.407104\n",
      "Train: step: 115810, time: 0.186, loss: 3472.682373\n",
      "Train: step: 115820, time: 0.200, loss: 1532.409790\n",
      "Train: step: 115830, time: 0.201, loss: 2630.379883\n",
      "Train: step: 115840, time: 0.195, loss: 947.353882\n",
      "Train: step: 115850, time: 0.195, loss: 3054.636963\n",
      "Train: step: 115860, time: 0.242, loss: 2860.437988\n",
      "Train: step: 115870, time: 0.230, loss: 2257.113281\n",
      "Train: step: 115880, time: 0.225, loss: 1018.761353\n",
      "Train: step: 115890, time: 0.232, loss: 1353.926025\n",
      "Train: step: 115900, time: 0.207, loss: 691.600586\n",
      "Train: step: 115910, time: 0.224, loss: 3677.269287\n",
      "Train: step: 115920, time: 0.229, loss: 3821.273926\n",
      "Train: step: 115930, time: 0.218, loss: 2342.248779\n",
      "Train: step: 115940, time: 0.200, loss: 2272.977539\n",
      "Train: step: 115950, time: 0.221, loss: 1633.295776\n",
      "Train: step: 115960, time: 0.195, loss: 2131.930420\n",
      "Train: step: 115970, time: 0.202, loss: 2482.422607\n",
      "Train: step: 115980, time: 0.203, loss: 1867.448853\n",
      "Train: step: 115990, time: 0.218, loss: 3186.553711\n",
      "Train: step: 116000, time: 0.285, loss: 2250.701172\n",
      "Train: step: 116010, time: 0.212, loss: 555.885437\n",
      "Train: step: 116020, time: 0.196, loss: 1232.336670\n",
      "Train: step: 116030, time: 0.201, loss: 791.583862\n",
      "Train: step: 116040, time: 0.200, loss: 2631.220703\n",
      "Train: step: 116050, time: 0.343, loss: 1135.994751\n",
      "Train: step: 116060, time: 0.306, loss: 958.416077\n",
      "Train: step: 116070, time: 0.222, loss: 1454.824829\n",
      "Train: step: 116080, time: 0.227, loss: 1645.822998\n",
      "Train: step: 116090, time: 0.220, loss: 1863.165039\n",
      "Train: step: 116100, time: 0.221, loss: 1226.509399\n",
      "Train: step: 116110, time: 0.204, loss: 268.299805\n",
      "Train: step: 116120, time: 0.225, loss: 3446.168945\n",
      "Train: step: 116130, time: 0.231, loss: 3333.167969\n",
      "Train: step: 116140, time: 0.230, loss: 2880.268311\n",
      "Train: step: 116150, time: 0.203, loss: 2500.822998\n",
      "Train: step: 116160, time: 0.204, loss: 523.595398\n",
      "Train: step: 116170, time: 0.243, loss: 898.582092\n",
      "Train: step: 116180, time: 0.204, loss: 896.479553\n",
      "Train: step: 116190, time: 0.187, loss: 3555.967285\n",
      "Train: step: 116200, time: 0.228, loss: 2157.731445\n",
      "Train: step: 116210, time: 0.205, loss: 3364.599854\n",
      "Train: step: 116220, time: 0.238, loss: 3862.502930\n",
      "Train: step: 116230, time: 0.218, loss: 1598.172974\n",
      "Train: step: 116240, time: 0.195, loss: 3639.232666\n",
      "Train: step: 116250, time: 0.224, loss: 1498.751343\n",
      "Train: step: 116260, time: 0.242, loss: 1080.689209\n",
      "Train: step: 116270, time: 0.232, loss: 1438.550659\n",
      "Train: step: 116280, time: 0.219, loss: 1889.469727\n",
      "Train: step: 116290, time: 0.225, loss: 850.958191\n",
      "Train: step: 116300, time: 0.192, loss: 1878.720215\n",
      "Train: step: 116310, time: 0.194, loss: 1967.094604\n",
      "Train: step: 116320, time: 0.221, loss: 2831.513184\n",
      "Train: step: 116330, time: 0.228, loss: 2649.612549\n",
      "Train: step: 116340, time: 0.192, loss: 1552.493896\n",
      "Train: step: 116350, time: 0.217, loss: 2204.544434\n",
      "Train: step: 116360, time: 0.192, loss: 2121.616455\n",
      "Train: step: 116370, time: 0.202, loss: 1873.624390\n",
      "Train: step: 116380, time: 0.198, loss: 555.993835\n",
      "Train: step: 116390, time: 0.298, loss: 2337.694824\n",
      "Train: step: 116400, time: 0.222, loss: 2524.297119\n",
      "Train: step: 116410, time: 0.230, loss: 1840.680664\n",
      "Train: step: 116420, time: 0.209, loss: 1586.115967\n",
      "Train: step: 116430, time: 0.217, loss: 3399.069092\n",
      "Train: step: 116440, time: 0.194, loss: 2547.689697\n",
      "Train: step: 116450, time: 0.193, loss: 1012.052185\n",
      "Train: step: 116460, time: 0.208, loss: 2881.284912\n",
      "Train: step: 116470, time: 0.210, loss: 2975.355957\n",
      "Train: step: 116480, time: 0.188, loss: 2483.286621\n",
      "Train: step: 116490, time: 0.189, loss: 241.755157\n",
      "Train: step: 116500, time: 0.187, loss: 1021.843079\n",
      "Train: step: 116510, time: 0.191, loss: 346.001587\n",
      "Train: step: 116520, time: 0.202, loss: 1391.200684\n",
      "Train: step: 116530, time: 0.187, loss: 1072.943970\n",
      "Train: step: 116540, time: 0.227, loss: 2122.667969\n",
      "Train: step: 116550, time: 0.234, loss: 3404.897217\n",
      "Train: step: 116560, time: 0.217, loss: 2793.530762\n",
      "Train: step: 116570, time: 0.218, loss: 729.819275\n",
      "Train: step: 116580, time: 0.190, loss: 1898.142822\n",
      "Train: step: 116590, time: 0.270, loss: 2687.189453\n",
      "Train: step: 116600, time: 0.219, loss: 1551.813477\n",
      "Train: step: 116610, time: 0.192, loss: 933.774780\n",
      "Train: step: 116620, time: 0.217, loss: 2335.993652\n",
      "Train: step: 116630, time: 0.192, loss: 1118.396484\n",
      "Train: step: 116640, time: 0.194, loss: 1066.886597\n",
      "Train: step: 116650, time: 0.201, loss: 3176.708740\n",
      "Train: step: 116660, time: 0.218, loss: 1686.443237\n",
      "Train: step: 116670, time: 0.226, loss: 2040.173218\n",
      "Train: step: 116680, time: 0.209, loss: 476.380585\n",
      "Train: step: 116690, time: 0.194, loss: 2033.024536\n",
      "Train: step: 116700, time: 0.193, loss: 1148.444580\n",
      "Train: step: 116710, time: 0.211, loss: 479.200317\n",
      "Train: step: 116720, time: 0.201, loss: 1529.840332\n",
      "Train: step: 116730, time: 0.186, loss: 1260.731079\n",
      "Train: step: 116740, time: 0.187, loss: 2581.496094\n",
      "Train: step: 116750, time: 0.202, loss: 2872.832520\n",
      "Train: step: 116760, time: 0.206, loss: 404.243256\n",
      "Train: step: 116770, time: 0.244, loss: 3035.369141\n",
      "Train: step: 116780, time: 0.186, loss: 1758.203979\n",
      "Train: step: 116790, time: 0.228, loss: 2104.639648\n",
      "Train: step: 116800, time: 0.191, loss: 777.283569\n",
      "Train: step: 116810, time: 0.219, loss: 2093.529541\n",
      "Train: step: 116820, time: 0.222, loss: 1587.220581\n",
      "Train: step: 116830, time: 0.232, loss: 1305.414307\n",
      "Train: step: 116840, time: 0.207, loss: 1494.489624\n",
      "Train: step: 116850, time: 0.185, loss: 2398.730225\n",
      "Train: step: 116860, time: 0.202, loss: 700.701721\n",
      "Train: step: 116870, time: 0.181, loss: 1121.372559\n",
      "Train: step: 116880, time: 0.180, loss: 2150.941895\n",
      "Train: step: 116890, time: 0.219, loss: 1294.887329\n",
      "Train: step: 116900, time: 0.199, loss: 513.870117\n",
      "Train: step: 116910, time: 0.204, loss: 1136.249268\n",
      "Train: step: 116920, time: 0.231, loss: 2137.002930\n",
      "Train: step: 116930, time: 0.196, loss: 761.500305\n",
      "Train: step: 116940, time: 0.191, loss: 1277.415771\n",
      "Train: step: 116950, time: 0.207, loss: 1230.773315\n",
      "Train: step: 116960, time: 0.266, loss: 1905.681152\n",
      "Train: step: 116970, time: 0.194, loss: 171.452164\n",
      "Train: step: 116980, time: 0.201, loss: 1799.780029\n",
      "Train: step: 116990, time: 0.217, loss: 1364.275269\n",
      "Train: step: 117000, time: 0.194, loss: 1685.369629\n",
      "Train: step: 117010, time: 0.190, loss: 322.682953\n",
      "Train: step: 117020, time: 0.187, loss: 2445.674072\n",
      "Train: step: 117030, time: 0.188, loss: 2370.171631\n",
      "Train: step: 117040, time: 0.250, loss: 3377.283203\n",
      "Train: step: 117050, time: 0.227, loss: 2134.270752\n",
      "Train: step: 117060, time: 0.217, loss: 2770.001709\n",
      "Train: step: 117070, time: 0.221, loss: 2248.667480\n",
      "Train: step: 117080, time: 0.237, loss: 2666.951172\n",
      "Train: step: 117090, time: 0.198, loss: 2334.260498\n",
      "Train: step: 117100, time: 0.219, loss: 1030.597168\n",
      "Train: step: 117110, time: 0.218, loss: 2900.987793\n",
      "Train: step: 117120, time: 0.216, loss: 2138.289795\n",
      "Train: step: 117130, time: 0.190, loss: 1603.745850\n",
      "Train: step: 117140, time: 0.220, loss: 1137.431274\n",
      "Train: step: 117150, time: 0.210, loss: 1414.005493\n",
      "Train: step: 117160, time: 0.222, loss: 2705.446533\n",
      "Train: step: 117170, time: 0.208, loss: 2125.100342\n",
      "Train: step: 117180, time: 0.201, loss: 1520.417480\n",
      "Train: step: 117190, time: 0.282, loss: 1173.529663\n",
      "Train: step: 117200, time: 0.207, loss: 1641.041626\n",
      "Train: step: 117210, time: 0.247, loss: 2894.460693\n",
      "Train: step: 117220, time: 0.314, loss: 4200.725098\n",
      "Train: step: 117230, time: 0.280, loss: 2865.390137\n",
      "Train: step: 117240, time: 0.225, loss: 2692.028809\n",
      "Train: step: 117250, time: 0.202, loss: 2974.141113\n",
      "Train: step: 117260, time: 0.196, loss: 1779.417358\n",
      "Train: step: 117270, time: 0.214, loss: 1773.887939\n",
      "Train: step: 117280, time: 0.216, loss: 2897.932617\n",
      "Train: step: 117290, time: 0.200, loss: 1823.500610\n",
      "Train: step: 117300, time: 0.202, loss: 1024.635864\n",
      "Train: step: 117310, time: 0.215, loss: 3114.995361\n",
      "Train: step: 117320, time: 0.198, loss: 376.533447\n",
      "Train: step: 117330, time: 0.234, loss: 1682.240112\n",
      "Train: step: 117340, time: 0.218, loss: 1469.142212\n",
      "Train: step: 117350, time: 0.227, loss: 3431.323975\n",
      "Train: step: 117360, time: 0.245, loss: 2350.141846\n",
      "Train: step: 117370, time: 0.227, loss: 2360.262939\n",
      "Train: step: 117380, time: 0.201, loss: 1804.307983\n",
      "Train: step: 117390, time: 0.207, loss: 1218.064087\n",
      "Train: step: 117400, time: 0.194, loss: 4034.467529\n",
      "Train: step: 117410, time: 0.227, loss: 800.322571\n",
      "Train: step: 117420, time: 0.198, loss: 2216.448486\n",
      "Train: step: 117430, time: 0.192, loss: 1414.159058\n",
      "Train: step: 117440, time: 0.240, loss: 2511.334473\n",
      "Train: step: 117450, time: 0.191, loss: 792.108704\n",
      "Train: step: 117460, time: 0.235, loss: 3013.815430\n",
      "Train: step: 117470, time: 0.203, loss: 3197.548584\n",
      "Train: step: 117480, time: 0.207, loss: 3675.481689\n",
      "Train: step: 117490, time: 0.200, loss: 2186.122803\n",
      "Train: step: 117500, time: 0.190, loss: 1629.408081\n",
      "Train: step: 117510, time: 0.218, loss: 2319.738037\n",
      "Train: step: 117520, time: 0.190, loss: 1316.092407\n",
      "Train: step: 117530, time: 0.191, loss: 2820.979492\n",
      "Train: step: 117540, time: 0.227, loss: 1663.690308\n",
      "Train: step: 117550, time: 0.216, loss: 727.269104\n",
      "Train: step: 117560, time: 0.225, loss: 2221.133789\n",
      "Train: step: 117570, time: 0.229, loss: 2142.094727\n",
      "Train: step: 117580, time: 0.192, loss: 3329.753174\n",
      "Train: step: 117590, time: 0.191, loss: 2203.389404\n",
      "Train: step: 117600, time: 0.223, loss: 3256.545898\n",
      "Train: step: 117610, time: 0.221, loss: 1037.784180\n",
      "Train: step: 117620, time: 0.230, loss: 2973.951660\n",
      "Train: step: 117630, time: 0.203, loss: 742.941650\n",
      "Train: step: 117640, time: 0.221, loss: 1607.215454\n",
      "Train: step: 117650, time: 0.201, loss: 2099.951660\n",
      "Train: step: 117660, time: 0.230, loss: 1403.579102\n",
      "Train: step: 117670, time: 0.217, loss: 1314.753296\n",
      "Train: step: 117680, time: 0.218, loss: 4679.838867\n",
      "Train: step: 117690, time: 0.197, loss: 683.121643\n",
      "Train: step: 117700, time: 0.194, loss: 1786.257080\n",
      "Train: step: 117710, time: 0.209, loss: 3633.691650\n",
      "Train: step: 117720, time: 0.200, loss: 699.575256\n",
      "Train: step: 117730, time: 0.215, loss: 2427.560059\n",
      "Train: step: 117740, time: 0.188, loss: 2012.350464\n",
      "Train: step: 117750, time: 0.220, loss: 176.514740\n",
      "Train: step: 117760, time: 0.209, loss: 1139.608398\n",
      "Train: step: 117770, time: 0.193, loss: 1430.099731\n",
      "Train: step: 117780, time: 0.205, loss: 2407.359619\n",
      "Train: step: 117790, time: 0.252, loss: 3472.088135\n",
      "Train: step: 117800, time: 0.198, loss: 3055.603271\n",
      "Train: step: 117810, time: 0.196, loss: 2614.310059\n",
      "Train: step: 117820, time: 0.195, loss: 411.601257\n",
      "Train: step: 117830, time: 0.203, loss: 2966.762939\n",
      "Train: step: 117840, time: 0.204, loss: 2287.868164\n",
      "Train: step: 117850, time: 0.200, loss: 2023.114014\n",
      "Train: step: 117860, time: 0.220, loss: 1805.016479\n",
      "Train: step: 117870, time: 0.235, loss: 1596.846313\n",
      "Train: step: 117880, time: 0.215, loss: 2267.043701\n",
      "Train: step: 117890, time: 0.217, loss: 1707.864380\n",
      "Train: step: 117900, time: 0.200, loss: 694.183044\n",
      "Train: step: 117910, time: 0.228, loss: 1431.681885\n",
      "Train: step: 117920, time: 0.192, loss: 3008.582275\n",
      "Train: step: 117930, time: 0.200, loss: 2438.129150\n",
      "Train: step: 117940, time: 0.223, loss: 963.501160\n",
      "Train: step: 117950, time: 0.188, loss: 539.808472\n",
      "Train: step: 117960, time: 0.204, loss: 2900.191406\n",
      "Train: step: 117970, time: 0.190, loss: 3623.560791\n",
      "Train: step: 117980, time: 0.221, loss: 494.426483\n",
      "Train: step: 117990, time: 0.227, loss: 857.166260\n",
      "Train: step: 118000, time: 0.215, loss: 1887.051270\n",
      "Train: step: 118010, time: 0.230, loss: 1362.214233\n",
      "Train: step: 118020, time: 0.228, loss: 2252.727539\n",
      "Train: step: 118030, time: 0.206, loss: 2950.940918\n",
      "Train: step: 118040, time: 0.202, loss: 727.924866\n",
      "Train: step: 118050, time: 0.204, loss: 1788.999268\n",
      "Train: step: 118060, time: 0.236, loss: 1452.468140\n",
      "Train: step: 118070, time: 0.199, loss: 2746.929443\n",
      "Train: step: 118080, time: 0.232, loss: 1724.197388\n",
      "Train: step: 118090, time: 0.187, loss: 610.742676\n",
      "Train: step: 118100, time: 0.234, loss: 401.732697\n",
      "Train: step: 118110, time: 0.192, loss: 2751.116455\n",
      "Train: step: 118120, time: 0.217, loss: 1126.479980\n",
      "Train: step: 118130, time: 0.230, loss: 3266.794434\n",
      "Train: step: 118140, time: 0.214, loss: 1082.849854\n",
      "Train: step: 118150, time: 0.224, loss: 1071.754150\n",
      "Train: step: 118160, time: 0.232, loss: 2622.028076\n",
      "Train: step: 118170, time: 0.218, loss: 1407.119629\n",
      "Train: step: 118180, time: 0.193, loss: 2259.610596\n",
      "Train: step: 118190, time: 0.201, loss: 2562.830322\n",
      "Train: step: 118200, time: 0.191, loss: 997.097351\n",
      "Train: step: 118210, time: 0.198, loss: 2207.820557\n",
      "Train: step: 118220, time: 0.201, loss: 1809.750366\n",
      "Train: step: 118230, time: 0.242, loss: 1510.314331\n",
      "Train: step: 118240, time: 0.190, loss: 2510.374756\n",
      "Train: step: 118250, time: 0.224, loss: 2273.821533\n",
      "Train: step: 118260, time: 0.192, loss: 676.216736\n",
      "Train: step: 118270, time: 0.187, loss: 3248.772949\n",
      "Train: step: 118280, time: 0.237, loss: 3294.735840\n",
      "Train: step:     10, time: 0.322, loss: 1610.887451\n",
      "Train: step:     20, time: 0.230, loss: 1372.308960\n",
      "Train: step:     30, time: 0.258, loss: 946.162659\n",
      "Train: step:     40, time: 0.230, loss: 2731.377686\n",
      "Train: step:     50, time: 0.233, loss: 2425.208984\n",
      "Train: step:     60, time: 0.199, loss: 2598.638184\n",
      "Train: step:     70, time: 0.225, loss: 2063.573730\n",
      "Train: step:     80, time: 0.227, loss: 841.567810\n",
      "Train: step:     90, time: 0.189, loss: 3324.427246\n",
      "Train: step:    100, time: 0.218, loss: 2942.961182\n",
      "Train: step:    110, time: 0.219, loss: 1249.534668\n",
      "Train: step:    120, time: 0.198, loss: 687.663940\n",
      "Train: step:    130, time: 0.193, loss: 1775.936768\n",
      "Train: step:    140, time: 0.215, loss: 894.216431\n",
      "Train: step:    150, time: 0.204, loss: 2353.744385\n",
      "Train: step:    160, time: 0.248, loss: 537.829651\n",
      "Train: step:    170, time: 0.199, loss: 1212.555054\n",
      "Train: step:    180, time: 0.195, loss: 2650.937744\n",
      "Train: step:    190, time: 0.201, loss: 605.199524\n",
      "Train: step:    200, time: 0.202, loss: 2992.082031\n",
      "Train: step:    210, time: 0.218, loss: 883.312744\n",
      "Train: step:    220, time: 0.189, loss: 1483.330811\n",
      "Train: step:    230, time: 0.199, loss: 3942.397705\n",
      "Train: step:    240, time: 0.198, loss: 1906.621216\n",
      "Train: step:    250, time: 0.210, loss: 981.957520\n",
      "Train: step:    260, time: 0.195, loss: 1087.909912\n",
      "Train: step:    270, time: 0.250, loss: 330.438080\n",
      "Train: step:    280, time: 0.218, loss: 3468.858643\n",
      "Train: step:    290, time: 0.250, loss: 5583.403809\n",
      "Train: step:    300, time: 0.343, loss: 1627.292358\n",
      "Train: step:    310, time: 0.226, loss: 3302.600586\n",
      "Train: step:    320, time: 0.217, loss: 2090.603760\n",
      "Train: step:    330, time: 0.228, loss: 2081.022949\n",
      "Train: step:    340, time: 0.193, loss: 2194.240234\n",
      "Train: step:    350, time: 0.202, loss: 3257.126953\n",
      "Train: step:    360, time: 0.292, loss: 520.515991\n",
      "Train: step:    370, time: 0.305, loss: 1401.718628\n",
      "Train: step:    380, time: 0.209, loss: 458.800110\n",
      "Train: step:    390, time: 0.204, loss: 2669.989014\n",
      "Train: step:    400, time: 0.202, loss: 1843.146484\n",
      "Train: step:    410, time: 0.194, loss: 1563.075928\n",
      "Train: step:    420, time: 0.212, loss: 2016.759766\n",
      "Train: step:    430, time: 0.233, loss: 1199.343140\n",
      "Train: step:    440, time: 0.245, loss: 3016.896973\n",
      "Train: step:    450, time: 0.225, loss: 1255.611206\n",
      "Train: step:    460, time: 0.197, loss: 385.929993\n",
      "Train: step:    470, time: 0.220, loss: 4227.849609\n",
      "Train: step:    480, time: 0.233, loss: 2672.385254\n",
      "Train: step:    490, time: 0.199, loss: 1573.737915\n",
      "Train: step:    500, time: 0.318, loss: 2030.687988\n",
      "Train: step:    510, time: 0.233, loss: 989.892090\n",
      "Train: step:    520, time: 0.190, loss: 1205.498901\n",
      "Train: step:    530, time: 0.189, loss: 1730.722412\n",
      "Train: step:    540, time: 0.237, loss: 2000.337769\n",
      "Train: step:    550, time: 0.255, loss: 1650.681152\n",
      "Train: step:    560, time: 0.224, loss: 1458.108154\n",
      "Train: step:    570, time: 0.229, loss: 1517.650024\n",
      "Train: step:    580, time: 0.250, loss: 1588.459351\n",
      "Train: step:    590, time: 0.187, loss: 2709.935303\n",
      "Train: step:    600, time: 0.236, loss: 381.507965\n",
      "Train: step:    610, time: 0.220, loss: 2205.107666\n",
      "Train: step:    620, time: 0.201, loss: 1986.558838\n",
      "Train: step:    630, time: 0.190, loss: 699.058228\n",
      "Train: step:    640, time: 0.230, loss: 3374.173828\n",
      "Train: step:    650, time: 0.203, loss: 1526.047485\n",
      "Train: step:    660, time: 0.209, loss: 1371.777344\n",
      "Train: step:    670, time: 0.197, loss: 1122.930786\n",
      "Train: step:    680, time: 0.215, loss: 2215.094238\n",
      "Train: step:    690, time: 0.268, loss: 1448.782104\n",
      "Train: step:    700, time: 0.288, loss: 3270.702637\n",
      "Train: step:    710, time: 0.210, loss: 2878.269043\n",
      "Train: step:    720, time: 0.217, loss: 695.932861\n",
      "Train: step:    730, time: 0.218, loss: 2864.958008\n",
      "Train: step:    740, time: 0.276, loss: 2429.442383\n",
      "Train: step:    750, time: 0.194, loss: 2943.423340\n",
      "Train: step:    760, time: 0.207, loss: 1701.560669\n",
      "Train: step:    770, time: 0.218, loss: 2235.505127\n",
      "Train: step:    780, time: 0.216, loss: 2505.687988\n",
      "Train: step:    790, time: 0.248, loss: 3061.232910\n",
      "Train: step:    800, time: 0.229, loss: 1387.627930\n",
      "Train: step:    810, time: 0.217, loss: 1912.862915\n",
      "Train: step:    820, time: 0.231, loss: 2551.345947\n",
      "Train: step:    830, time: 0.231, loss: 1117.816895\n",
      "Train: step:    840, time: 0.217, loss: 1676.310059\n",
      "Train: step:    850, time: 0.204, loss: 1989.802612\n",
      "Train: step:    860, time: 0.206, loss: 2378.183594\n",
      "Train: step:    870, time: 0.237, loss: 1406.082764\n",
      "Train: step:    880, time: 0.187, loss: 2210.432129\n",
      "Train: step:    890, time: 0.191, loss: 2034.730103\n",
      "Train: step:    900, time: 0.220, loss: 3852.788818\n",
      "Train: step:    910, time: 0.205, loss: 2393.206299\n",
      "Train: step:    920, time: 0.199, loss: 2021.581787\n",
      "Train: step:    930, time: 0.230, loss: 2902.706055\n",
      "Train: step:    940, time: 0.199, loss: 2253.985352\n",
      "Train: step:    950, time: 0.197, loss: 767.672363\n",
      "Train: step:    960, time: 0.201, loss: 2515.955811\n",
      "Train: step:    970, time: 0.188, loss: 1893.803589\n",
      "Train: step:    980, time: 0.220, loss: 1787.684570\n",
      "Train: step:    990, time: 0.217, loss: 879.568726\n",
      "Train: step:   1000, time: 0.225, loss: 1893.388306\n",
      "Train: step:   1010, time: 0.193, loss: 1266.069092\n",
      "Train: step:   1020, time: 0.217, loss: 2603.707764\n",
      "Train: step:   1030, time: 0.219, loss: 827.916138\n",
      "Train: step:   1040, time: 0.235, loss: 2067.485840\n",
      "Train: step:   1050, time: 0.226, loss: 1865.781250\n",
      "Train: step:   1060, time: 0.219, loss: 1640.791382\n",
      "Train: step:   1070, time: 0.239, loss: 381.463806\n",
      "Train: step:   1080, time: 0.188, loss: 2787.487549\n",
      "Train: step:   1090, time: 0.230, loss: 2446.241211\n",
      "Train: step:   1100, time: 0.197, loss: 1076.551270\n",
      "Train: step:   1110, time: 0.188, loss: 1343.797974\n",
      "Train: step:   1120, time: 0.191, loss: 2540.116699\n",
      "Train: step:   1130, time: 0.231, loss: 648.625488\n",
      "Train: step:   1140, time: 0.333, loss: 2789.126709\n",
      "Train: step:   1150, time: 0.197, loss: 3412.598145\n",
      "Train: step:   1160, time: 0.203, loss: 1628.803101\n",
      "Train: step:   1170, time: 0.197, loss: 1683.287109\n",
      "Train: step:   1180, time: 0.201, loss: 2282.957031\n",
      "Train: step:   1190, time: 0.234, loss: 190.561798\n",
      "Train: step:   1200, time: 0.219, loss: 2338.225098\n",
      "Train: step:   1210, time: 0.191, loss: 3597.892090\n",
      "Train: step:   1220, time: 0.211, loss: 2421.287109\n",
      "Train: step:   1230, time: 0.192, loss: 1585.547485\n",
      "Train: step:   1240, time: 0.217, loss: 1746.691406\n",
      "Train: step:   1250, time: 0.215, loss: 1414.481689\n",
      "Train: step:   1260, time: 0.196, loss: 2882.138184\n",
      "Train: step:   1270, time: 0.192, loss: 1527.558105\n",
      "Train: step:   1280, time: 0.195, loss: 1232.266357\n",
      "Train: step:   1290, time: 0.329, loss: 798.230286\n",
      "Train: step:   1300, time: 0.191, loss: 2047.284546\n",
      "Train: step:   1310, time: 0.229, loss: 1254.339722\n",
      "Train: step:   1320, time: 0.194, loss: 2132.411133\n",
      "Train: step:   1330, time: 0.229, loss: 1994.626343\n",
      "Train: step:   1340, time: 0.195, loss: 2773.287598\n",
      "Train: step:   1350, time: 0.191, loss: 1375.565186\n",
      "Train: step:   1360, time: 0.234, loss: 1334.666992\n",
      "Train: step:   1370, time: 0.217, loss: 1288.958130\n",
      "Train: step:   1380, time: 0.201, loss: 2562.836182\n",
      "Train: step:   1390, time: 0.295, loss: 2894.893311\n",
      "Train: step:   1400, time: 0.307, loss: 1702.412842\n",
      "Train: step:   1410, time: 0.229, loss: 2505.460205\n",
      "Train: step:   1420, time: 0.201, loss: 1940.205444\n",
      "Train: step:   1430, time: 0.224, loss: 2786.125000\n",
      "Train: step:   1440, time: 0.215, loss: 525.846680\n",
      "Train: step:   1450, time: 0.217, loss: 2009.257080\n",
      "Train: step:   1460, time: 0.225, loss: 2677.240967\n",
      "Train: step:   1470, time: 0.201, loss: 1858.152832\n",
      "Train: step:   1480, time: 0.198, loss: 1166.530151\n",
      "Train: step:   1490, time: 0.189, loss: 187.291367\n",
      "Train: step:   1500, time: 0.243, loss: 305.669220\n",
      "Train: step:   1510, time: 0.192, loss: 2728.973633\n",
      "Train: step:   1520, time: 0.231, loss: 1534.204956\n",
      "Train: step:   1530, time: 0.188, loss: 2458.140869\n",
      "Train: step:   1540, time: 0.194, loss: 958.009033\n",
      "Train: step:   1550, time: 0.208, loss: 1094.765625\n",
      "Train: step:   1560, time: 0.208, loss: 1778.666260\n",
      "Train: step:   1570, time: 0.190, loss: 2696.704834\n",
      "Train: step:   1580, time: 0.192, loss: 2903.410156\n",
      "Train: step:   1590, time: 0.202, loss: 3241.642578\n",
      "Train: step:   1600, time: 0.192, loss: 901.034668\n",
      "Train: step:   1610, time: 0.188, loss: 2560.917969\n",
      "Train: step:   1620, time: 0.192, loss: 4566.107910\n",
      "Train: step:   1630, time: 0.193, loss: 3805.063477\n",
      "Train: step:   1640, time: 0.191, loss: 1166.851685\n",
      "Train: step:   1650, time: 0.198, loss: 2519.520020\n",
      "Train: step:   1660, time: 0.207, loss: 611.241699\n",
      "Train: step:   1670, time: 0.221, loss: 913.762390\n",
      "Train: step:   1680, time: 0.227, loss: 1513.345093\n",
      "Train: step:   1690, time: 0.190, loss: 543.732239\n",
      "Train: step:   1700, time: 0.222, loss: 2470.679199\n",
      "Train: step:   1710, time: 0.217, loss: 1702.080200\n",
      "Train: step:   1720, time: 0.193, loss: 2836.156738\n",
      "Train: step:   1730, time: 0.216, loss: 946.268005\n",
      "Train: step:   1740, time: 0.196, loss: 3302.162109\n",
      "Train: step:   1750, time: 0.201, loss: 1531.841064\n",
      "Train: step:   1760, time: 0.193, loss: 1115.244385\n",
      "Train: step:   1770, time: 0.231, loss: 786.308472\n",
      "Train: step:   1780, time: 0.197, loss: 1536.576660\n",
      "Train: step:   1790, time: 0.206, loss: 2546.912598\n",
      "Train: step:   1800, time: 0.207, loss: 856.721191\n",
      "Train: step:   1810, time: 0.199, loss: 1571.896240\n",
      "Train: step:   1820, time: 0.199, loss: 977.786926\n",
      "Train: step:   1830, time: 0.199, loss: 1913.260132\n",
      "Train: step:   1840, time: 0.207, loss: 1045.394043\n",
      "Train: step:   1850, time: 0.218, loss: 3680.825684\n",
      "Train: step:   1860, time: 0.189, loss: 1414.384033\n",
      "Train: step:   1870, time: 0.217, loss: 1206.086426\n",
      "Train: step:   1880, time: 0.190, loss: 640.758362\n",
      "Train: step:   1890, time: 0.244, loss: 208.397125\n",
      "Train: step:   1900, time: 0.218, loss: 891.033386\n",
      "Train: step:   1910, time: 0.189, loss: 1866.180664\n",
      "Train: step:   1920, time: 0.195, loss: 2672.222656\n",
      "Train: step:   1930, time: 0.188, loss: 3459.700195\n",
      "Train: step:   1940, time: 0.237, loss: 665.745178\n",
      "Train: step:   1950, time: 0.193, loss: 286.955261\n",
      "Train: step:   1960, time: 0.200, loss: 1676.541016\n",
      "Train: step:   1970, time: 0.214, loss: 1401.729370\n",
      "Train: step:   1980, time: 0.190, loss: 3315.170410\n",
      "Train: step:   1990, time: 0.234, loss: 2562.074951\n",
      "Train: step:   2000, time: 0.228, loss: 2589.427490\n",
      "Train: step:   2010, time: 0.242, loss: 2527.584473\n",
      "Train: step:   2020, time: 0.251, loss: 2073.099854\n",
      "Train: step:   2030, time: 0.192, loss: 712.184082\n",
      "Train: step:   2040, time: 0.215, loss: 772.939636\n",
      "Train: step:   2050, time: 0.231, loss: 2458.352051\n",
      "Train: step:   2060, time: 0.216, loss: 2522.594727\n",
      "Train: step:   2070, time: 0.213, loss: 2932.643799\n",
      "Train: step:   2080, time: 0.205, loss: 1099.996216\n",
      "Train: step:   2090, time: 0.206, loss: 2613.433350\n",
      "Train: step:   2100, time: 0.235, loss: 1722.672852\n",
      "Train: step:   2110, time: 0.259, loss: 2788.060547\n",
      "Train: step:   2120, time: 0.184, loss: 768.259338\n",
      "Train: step:   2130, time: 0.225, loss: 2311.122559\n",
      "Train: step:   2140, time: 0.211, loss: 2448.054932\n",
      "Train: step:   2150, time: 0.270, loss: 1522.582886\n",
      "Train: step:   2160, time: 0.218, loss: 2662.945312\n",
      "Train: step:   2170, time: 0.218, loss: 402.469330\n",
      "Train: step:   2180, time: 0.217, loss: 1849.087891\n",
      "Train: step:   2190, time: 0.256, loss: 2017.470581\n",
      "Train: step:   2200, time: 0.230, loss: 1893.915649\n",
      "Train: step:   2210, time: 0.225, loss: 2823.233643\n",
      "Train: step:   2220, time: 0.221, loss: 1022.981689\n",
      "Train: step:   2230, time: 0.223, loss: 215.077438\n",
      "Train: step:   2240, time: 0.234, loss: 1111.912964\n",
      "Train: step:   2250, time: 0.190, loss: 1571.023804\n",
      "Train: step:   2260, time: 0.201, loss: 1584.688599\n",
      "Train: step:   2270, time: 0.223, loss: 1051.821899\n",
      "Train: step:   2280, time: 0.187, loss: 2468.276855\n",
      "Train: step:   2290, time: 0.234, loss: 722.566528\n",
      "Train: step:   2300, time: 0.236, loss: 761.300354\n",
      "Train: step:   2310, time: 0.253, loss: 1233.790161\n",
      "Train: step:   2320, time: 0.244, loss: 2450.406494\n",
      "Train: step:   2330, time: 0.206, loss: 403.932465\n",
      "Train: step:   2340, time: 0.234, loss: 1304.515137\n",
      "Train: step:   2350, time: 0.227, loss: 2021.267944\n",
      "Train: step:   2360, time: 0.201, loss: 1943.033936\n",
      "Train: step:   2370, time: 0.245, loss: 2948.057373\n",
      "Train: step:   2380, time: 0.208, loss: 3282.622559\n",
      "Train: step:   2390, time: 0.212, loss: 4007.157715\n",
      "Train: step:   2400, time: 0.231, loss: 3689.535645\n",
      "Train: step:   2410, time: 0.182, loss: 1519.777710\n",
      "Train: step:   2420, time: 0.194, loss: 3239.299072\n",
      "Train: step:   2430, time: 0.206, loss: 2168.295410\n",
      "Train: step:   2440, time: 0.195, loss: 1898.973633\n",
      "Train: step:   2450, time: 0.198, loss: 1429.694702\n",
      "Train: step:   2460, time: 0.226, loss: 469.599213\n",
      "Train: step:   2470, time: 0.212, loss: 3053.959717\n",
      "Train: step:   2480, time: 0.230, loss: 2964.702393\n",
      "Train: step:   2490, time: 0.226, loss: 2933.923340\n",
      "Train: step:   2500, time: 0.208, loss: 1734.697632\n",
      "Train: step:   2510, time: 0.193, loss: 2096.507568\n",
      "Train: step:   2520, time: 0.191, loss: 2894.515625\n",
      "Train: step:   2530, time: 0.186, loss: 2804.600586\n",
      "Train: step:   2540, time: 0.201, loss: 1354.952026\n",
      "Train: step:   2550, time: 0.227, loss: 2167.624756\n",
      "Train: step:   2560, time: 0.185, loss: 2985.502197\n",
      "Train: step:   2570, time: 0.198, loss: 1050.647705\n",
      "Train: step:   2580, time: 0.227, loss: 1742.664673\n",
      "Train: step:   2590, time: 0.191, loss: 3023.622314\n",
      "Train: step:   2600, time: 0.193, loss: 952.996094\n",
      "Train: step:   2610, time: 0.188, loss: 2588.920654\n",
      "Train: step:   2620, time: 0.206, loss: 798.238708\n",
      "Train: step:   2630, time: 0.193, loss: 1189.490356\n",
      "Train: step:   2640, time: 0.202, loss: 2112.057373\n",
      "Train: step:   2650, time: 0.195, loss: 1034.113525\n",
      "Train: step:   2660, time: 0.192, loss: 1923.334351\n",
      "Train: step:   2670, time: 0.194, loss: 719.556091\n",
      "Train: step:   2680, time: 0.215, loss: 1807.655762\n",
      "Train: step:   2690, time: 0.198, loss: 2051.628418\n",
      "Train: step:   2700, time: 0.205, loss: 382.626343\n",
      "Train: step:   2710, time: 0.202, loss: 3951.776367\n",
      "Train: step:   2720, time: 0.189, loss: 1810.447510\n",
      "Train: step:   2730, time: 0.220, loss: 2160.405762\n",
      "Train: step:   2740, time: 0.190, loss: 794.724243\n",
      "Train: step:   2750, time: 0.285, loss: 4715.927246\n",
      "Train: step:   2760, time: 0.225, loss: 2743.064697\n",
      "Train: step:   2770, time: 0.227, loss: 1191.081177\n",
      "Train: step:   2780, time: 0.239, loss: 918.047607\n",
      "Train: step:   2790, time: 0.197, loss: 600.117554\n",
      "Train: step:   2800, time: 0.276, loss: 2389.114258\n",
      "Train: step:   2810, time: 0.201, loss: 3162.633789\n",
      "Train: step:   2820, time: 0.194, loss: 636.422180\n",
      "Train: step:   2830, time: 0.245, loss: 2751.611328\n",
      "Train: step:   2840, time: 0.191, loss: 2689.840332\n",
      "Train: step:   2850, time: 0.217, loss: 1090.295532\n",
      "Train: step:   2860, time: 0.231, loss: 454.721283\n",
      "Train: step:   2870, time: 0.207, loss: 904.190002\n",
      "Train: step:   2880, time: 0.238, loss: 1890.854858\n",
      "Train: step:   2890, time: 0.191, loss: 1888.818359\n",
      "Train: step:   2900, time: 0.226, loss: 2797.511475\n",
      "Train: step:   2910, time: 0.220, loss: 1391.426392\n",
      "Train: step:   2920, time: 0.199, loss: 3923.603760\n",
      "Train: step:   2930, time: 0.208, loss: 601.514099\n",
      "Train: step:   2940, time: 0.200, loss: 1721.539185\n",
      "Train: step:   2950, time: 0.223, loss: 2120.204346\n",
      "Train: step:   2960, time: 0.236, loss: 1443.917725\n",
      "Train: step:   2970, time: 0.217, loss: 3138.624023\n",
      "Train: step:   2980, time: 0.202, loss: 586.359558\n",
      "Train: step:   2990, time: 0.218, loss: 1641.822876\n",
      "Train: step:   3000, time: 0.199, loss: 1056.336182\n",
      "Train: step:   3010, time: 0.200, loss: 1290.987793\n",
      "Train: step:   3020, time: 0.232, loss: 2375.470215\n",
      "Train: step:   3030, time: 0.216, loss: 1196.188721\n",
      "Train: step:   3040, time: 0.219, loss: 1657.994141\n",
      "Train: step:   3050, time: 0.209, loss: 1878.802124\n",
      "Train: step:   3060, time: 0.211, loss: 3312.039795\n",
      "Train: step:   3070, time: 0.202, loss: 318.717316\n",
      "Train: step:   3080, time: 0.218, loss: 1615.125854\n",
      "Train: step:   3090, time: 0.193, loss: 378.393738\n",
      "Train: step:   3100, time: 0.202, loss: 1595.135010\n",
      "Train: step:   3110, time: 0.247, loss: 1594.405151\n",
      "Train: step:   3120, time: 0.203, loss: 1706.958252\n",
      "Train: step:   3130, time: 0.228, loss: 1252.973145\n",
      "Train: step:   3140, time: 0.200, loss: 3386.867188\n",
      "Train: step:   3150, time: 0.208, loss: 941.712830\n",
      "Train: step:   3160, time: 0.192, loss: 3109.408691\n",
      "Train: step:   3170, time: 0.194, loss: 1186.363647\n",
      "Train: step:   3180, time: 0.203, loss: 1619.257935\n",
      "Train: step:   3190, time: 0.202, loss: 1893.197998\n",
      "Train: step:   3200, time: 0.228, loss: 244.622482\n",
      "Train: step:   3210, time: 0.221, loss: 2196.304443\n",
      "Train: step:   3220, time: 0.233, loss: 754.056396\n",
      "Train: step:   3230, time: 0.251, loss: 2203.286377\n",
      "Train: step:   3240, time: 0.196, loss: 1011.002991\n",
      "Train: step:   3250, time: 0.190, loss: 4168.262695\n",
      "Train: step:   3260, time: 0.223, loss: 2699.752930\n",
      "Train: step:   3270, time: 0.201, loss: 1796.405273\n",
      "Train: step:   3280, time: 0.193, loss: 2113.314453\n",
      "Train: step:   3290, time: 0.198, loss: 2364.479736\n",
      "Train: step:   3300, time: 0.201, loss: 332.517517\n",
      "Train: step:   3310, time: 0.218, loss: 1141.723267\n",
      "Train: step:   3320, time: 0.228, loss: 4517.991699\n",
      "Train: step:   3330, time: 0.281, loss: 1741.852295\n",
      "Train: step:   3340, time: 0.198, loss: 2113.968994\n",
      "Train: step:   3350, time: 0.200, loss: 1115.852905\n",
      "Train: step:   3360, time: 0.197, loss: 2484.142334\n",
      "Train: step:   3370, time: 0.220, loss: 2104.536133\n",
      "Train: step:   3380, time: 0.198, loss: 1679.559326\n",
      "Train: step:   3390, time: 0.192, loss: 2371.669434\n",
      "Train: step:   3400, time: 0.193, loss: 333.296417\n",
      "Train: step:   3410, time: 0.197, loss: 750.405518\n",
      "Train: step:   3420, time: 0.228, loss: 1004.879883\n",
      "Train: step:   3430, time: 0.202, loss: 2064.169434\n",
      "Train: step:   3440, time: 0.201, loss: 1453.824463\n",
      "Train: step:   3450, time: 0.217, loss: 1268.500122\n",
      "Train: step:   3460, time: 0.237, loss: 2665.723877\n",
      "Train: step:   3470, time: 0.211, loss: 2043.171143\n",
      "Train: step:   3480, time: 0.217, loss: 1323.308105\n",
      "Train: step:   3490, time: 0.197, loss: 1365.910278\n",
      "Train: step:   3500, time: 0.221, loss: 2210.861572\n",
      "Train: step:   3510, time: 0.221, loss: 1432.989624\n",
      "Train: step:   3520, time: 0.193, loss: 2962.491455\n",
      "Train: step:   3530, time: 0.212, loss: 3231.815186\n",
      "Train: step:   3540, time: 0.227, loss: 2061.477783\n",
      "Train: step:   3550, time: 0.217, loss: 1328.175903\n",
      "Train: step:   3560, time: 0.223, loss: 895.694153\n",
      "Train: step:   3570, time: 0.220, loss: 888.964172\n",
      "Train: step:   3580, time: 0.192, loss: 1716.718018\n",
      "Train: step:   3590, time: 0.202, loss: 1960.346924\n",
      "Train: step:   3600, time: 0.187, loss: 3333.677246\n",
      "Train: step:   3610, time: 0.244, loss: 3521.709961\n",
      "Train: step:   3620, time: 0.195, loss: 1853.632812\n",
      "Train: step:   3630, time: 0.263, loss: 496.083038\n",
      "Train: step:   3640, time: 0.199, loss: 510.104248\n",
      "Train: step:   3650, time: 0.227, loss: 3503.962646\n",
      "Train: step:   3660, time: 0.225, loss: 2456.445801\n",
      "Train: step:   3670, time: 0.221, loss: 804.203247\n",
      "Train: step:   3680, time: 0.194, loss: 1312.604858\n",
      "Train: step:   3690, time: 0.220, loss: 848.861572\n",
      "Train: step:   3700, time: 0.198, loss: 1016.106323\n",
      "Train: step:   3710, time: 0.203, loss: 3072.700195\n",
      "Train: step:   3720, time: 0.186, loss: 1860.606934\n",
      "Train: step:   3730, time: 0.201, loss: 1438.238892\n",
      "Train: step:   3740, time: 0.206, loss: 1595.023682\n",
      "Train: step:   3750, time: 0.224, loss: 2473.424805\n",
      "Train: step:   3760, time: 0.227, loss: 1045.948242\n",
      "Train: step:   3770, time: 0.217, loss: 2203.441650\n",
      "Train: step:   3780, time: 0.200, loss: 3019.881836\n",
      "Train: step:   3790, time: 0.191, loss: 1475.364380\n",
      "Train: step:   3800, time: 0.184, loss: 2125.689453\n",
      "Train: step:   3810, time: 0.210, loss: 3282.653809\n",
      "Train: step:   3820, time: 0.233, loss: 3497.386475\n",
      "Train: step:   3830, time: 0.220, loss: 777.416321\n",
      "Train: step:   3840, time: 0.189, loss: 1147.290405\n",
      "Train: step:   3850, time: 0.198, loss: 3828.948730\n",
      "Train: step:   3860, time: 0.218, loss: 1884.833252\n",
      "Train: step:   3870, time: 0.193, loss: 1963.718018\n",
      "Train: step:   3880, time: 0.200, loss: 1785.426514\n",
      "Train: step:   3890, time: 0.198, loss: 1295.741577\n",
      "Train: step:   3900, time: 0.203, loss: 605.237732\n",
      "Train: step:   3910, time: 0.189, loss: 800.732239\n",
      "Train: step:   3920, time: 0.250, loss: 2428.491455\n",
      "Train: step:   3930, time: 0.232, loss: 650.227966\n",
      "Train: step:   3940, time: 0.220, loss: 1107.641357\n",
      "Train: step:   3950, time: 0.193, loss: 2157.724121\n",
      "Train: step:   3960, time: 0.192, loss: 779.428101\n",
      "Train: step:   3970, time: 0.257, loss: 2362.162354\n",
      "Train: step:   3980, time: 0.219, loss: 1438.182251\n",
      "Train: step:   3990, time: 0.230, loss: 592.325562\n",
      "Train: step:   4000, time: 0.205, loss: 2629.188721\n",
      "Train: step:   4010, time: 0.189, loss: 1516.068237\n",
      "Train: step:   4020, time: 0.230, loss: 2216.320312\n",
      "Train: step:   4030, time: 0.200, loss: 331.851257\n",
      "Train: step:   4040, time: 0.194, loss: 2246.717285\n",
      "Train: step:   4050, time: 0.206, loss: 974.953674\n",
      "Train: step:   4060, time: 0.191, loss: 1402.475464\n",
      "Train: step:   4070, time: 0.225, loss: 1888.686157\n",
      "Train: step:   4080, time: 0.233, loss: 1990.052246\n",
      "Train: step:   4090, time: 0.207, loss: 2379.575195\n",
      "Train: step:   4100, time: 0.194, loss: 455.187286\n",
      "Train: step:   4110, time: 0.215, loss: 1162.351562\n",
      "Train: step:   4120, time: 0.213, loss: 1643.487549\n",
      "Train: step:   4130, time: 0.241, loss: 244.033020\n",
      "Train: step:   4140, time: 0.231, loss: 361.323578\n",
      "Train: step:   4150, time: 0.218, loss: 3477.767578\n",
      "Train: step:   4160, time: 0.190, loss: 1680.158813\n",
      "Train: step:   4170, time: 0.219, loss: 2566.535400\n",
      "Train: step:   4180, time: 0.199, loss: 308.566528\n",
      "Train: step:   4190, time: 0.195, loss: 2001.406372\n",
      "Train: step:   4200, time: 0.199, loss: 2722.145508\n",
      "Train: step:   4210, time: 0.193, loss: 1291.717896\n",
      "Train: step:   4220, time: 0.198, loss: 2143.306152\n",
      "Train: step:   4230, time: 0.226, loss: 502.327240\n",
      "Train: step:   4240, time: 0.203, loss: 847.416077\n",
      "Train: step:   4250, time: 0.224, loss: 355.056641\n",
      "Train: step:   4260, time: 0.199, loss: 619.549072\n",
      "Train: step:   4270, time: 0.195, loss: 2033.847778\n",
      "Train: step:   4280, time: 0.216, loss: 1768.444946\n",
      "Train: step:   4290, time: 0.195, loss: 1913.397583\n",
      "Train: step:   4300, time: 0.252, loss: 2469.870117\n",
      "Train: step:   4310, time: 0.237, loss: 1994.828247\n",
      "Train: step:   4320, time: 0.222, loss: 1308.963257\n",
      "Train: step:   4330, time: 0.209, loss: 824.574768\n",
      "Train: step:   4340, time: 0.224, loss: 1224.482422\n",
      "Train: step:   4350, time: 0.212, loss: 361.794617\n",
      "Train: step:   4360, time: 0.202, loss: 1993.077515\n",
      "Train: step:   4370, time: 0.194, loss: 1248.537964\n",
      "Train: step:   4380, time: 0.216, loss: 464.018433\n",
      "Train: step:   4390, time: 0.189, loss: 1369.381958\n",
      "Train: step:   4400, time: 0.218, loss: 1968.074463\n",
      "Train: step:   4410, time: 0.226, loss: 1537.019531\n",
      "Train: step:   4420, time: 0.187, loss: 1619.815918\n",
      "Train: step:   4430, time: 0.188, loss: 3166.988281\n",
      "Train: step:   4440, time: 0.199, loss: 2313.284912\n",
      "Train: step:   4450, time: 0.190, loss: 571.095459\n",
      "Train: step:   4460, time: 0.330, loss: 1546.331909\n",
      "Train: step:   4470, time: 0.231, loss: 1785.550293\n",
      "Train: step:   4480, time: 0.189, loss: 976.048645\n",
      "Train: step:   4490, time: 0.188, loss: 2844.099365\n",
      "Train: step:   4500, time: 0.193, loss: 1202.995850\n",
      "Train: step:   4510, time: 0.190, loss: 1488.508423\n",
      "Train: step:   4520, time: 0.224, loss: 2943.930420\n",
      "Train: step:   4530, time: 0.195, loss: 1581.348389\n",
      "Train: step:   4540, time: 0.192, loss: 705.377197\n",
      "Train: step:   4550, time: 0.203, loss: 657.124878\n",
      "Train: step:   4560, time: 0.215, loss: 1682.199951\n",
      "Train: step:   4570, time: 0.185, loss: 1666.427490\n",
      "Train: step:   4580, time: 0.230, loss: 1382.979004\n",
      "Train: step:   4590, time: 0.188, loss: 1121.911499\n",
      "Train: step:   4600, time: 0.196, loss: 1243.712280\n",
      "Train: step:   4610, time: 0.343, loss: 2087.278809\n",
      "Train: step:   4620, time: 0.201, loss: 2264.748047\n",
      "Train: step:   4630, time: 0.234, loss: 982.156494\n",
      "Train: step:   4640, time: 0.191, loss: 509.050659\n",
      "Train: step:   4650, time: 0.257, loss: 4068.230957\n",
      "Train: step:   4660, time: 0.199, loss: 3167.786621\n",
      "Train: step:   4670, time: 0.230, loss: 2209.015137\n",
      "Train: step:   4680, time: 0.194, loss: 1718.466797\n",
      "Train: step:   4690, time: 0.209, loss: 2440.926514\n",
      "Train: step:   4700, time: 0.201, loss: 821.599976\n",
      "Train: step:   4710, time: 0.225, loss: 2246.333008\n",
      "Train: step:   4720, time: 0.216, loss: 2931.334717\n",
      "Train: step:   4730, time: 0.218, loss: 753.517151\n",
      "Train: step:   4740, time: 0.205, loss: 3113.918945\n",
      "Train: step:   4750, time: 0.250, loss: 846.703918\n",
      "Train: step:   4760, time: 0.250, loss: 1881.158691\n",
      "Train: step:   4770, time: 0.200, loss: 2533.070312\n",
      "Train: step:   4780, time: 0.204, loss: 772.225586\n",
      "Train: step:   4790, time: 0.224, loss: 1941.888916\n",
      "Train: step:   4800, time: 0.214, loss: 1941.200073\n",
      "Train: step:   4810, time: 0.201, loss: 1180.917114\n",
      "Train: step:   4820, time: 0.198, loss: 1463.789551\n",
      "Train: step:   4830, time: 0.191, loss: 912.434631\n",
      "Train: step:   4840, time: 0.198, loss: 1025.805176\n",
      "Train: step:   4850, time: 0.245, loss: 770.540222\n",
      "Train: step:   4860, time: 0.198, loss: 803.783997\n",
      "Train: step:   4870, time: 0.230, loss: 1451.293457\n",
      "Train: step:   4880, time: 0.197, loss: 1699.686523\n",
      "Train: step:   4890, time: 0.201, loss: 1698.307495\n",
      "Train: step:   4900, time: 0.215, loss: 3395.876709\n",
      "Train: step:   4910, time: 0.186, loss: 1671.663574\n",
      "Train: step:   4920, time: 0.191, loss: 1795.837280\n",
      "Train: step:   4930, time: 0.192, loss: 1880.434448\n",
      "Train: step:   4940, time: 0.193, loss: 2001.482056\n",
      "Train: step:   4950, time: 0.219, loss: 2219.591797\n",
      "Train: step:   4960, time: 0.212, loss: 2272.424316\n",
      "Train: step:   4970, time: 0.236, loss: 2570.207764\n",
      "Train: step:   4980, time: 0.314, loss: 1625.193970\n",
      "Train: step:   4990, time: 0.225, loss: 373.803986\n",
      "Train: step:   5000, time: 0.193, loss: 2070.099854\n",
      "Train: step:   5010, time: 0.204, loss: 886.508850\n",
      "Train: step:   5020, time: 0.188, loss: 1618.686523\n",
      "Train: step:   5030, time: 0.185, loss: 1547.155029\n",
      "Train: step:   5040, time: 0.216, loss: 1328.704590\n",
      "Train: step:   5050, time: 0.186, loss: 1892.021362\n",
      "Train: step:   5060, time: 0.313, loss: 191.222305\n",
      "Train: step:   5070, time: 0.229, loss: 474.982117\n",
      "Train: step:   5080, time: 0.195, loss: 647.618591\n",
      "Train: step:   5090, time: 0.241, loss: 1038.292725\n",
      "Train: step:   5100, time: 0.193, loss: 2742.240234\n",
      "Train: step:   5110, time: 0.196, loss: 1285.448608\n",
      "Train: step:   5120, time: 0.235, loss: 2406.044922\n",
      "Train: step:   5130, time: 0.218, loss: 1816.726318\n",
      "Train: step:   5140, time: 0.215, loss: 2552.581787\n",
      "Train: step:   5150, time: 0.227, loss: 1638.066528\n",
      "Train: step:   5160, time: 0.230, loss: 1113.708862\n",
      "Train: step:   5170, time: 0.204, loss: 885.352661\n",
      "Train: step:   5180, time: 0.204, loss: 839.184753\n",
      "Train: step:   5190, time: 0.195, loss: 2299.759766\n",
      "Train: step:   5200, time: 0.200, loss: 2199.129395\n",
      "Train: step:   5210, time: 0.230, loss: 1242.249146\n",
      "Train: step:   5220, time: 0.195, loss: 2477.455078\n",
      "Train: step:   5230, time: 0.195, loss: 2029.351685\n",
      "Train: step:   5240, time: 0.190, loss: 3751.175049\n",
      "Train: step:   5250, time: 0.217, loss: 1753.387939\n",
      "Train: step:   5260, time: 0.224, loss: 694.870422\n",
      "Train: step:   5270, time: 0.198, loss: 885.245972\n",
      "Train: step:   5280, time: 0.194, loss: 419.244720\n",
      "Train: step:   5290, time: 0.191, loss: 1850.027710\n",
      "Train: step:   5300, time: 0.237, loss: 2914.148682\n",
      "Train: step:   5310, time: 0.197, loss: 1746.271851\n",
      "Train: step:   5320, time: 0.231, loss: 1384.851074\n",
      "Train: step:   5330, time: 0.224, loss: 1946.123779\n",
      "Train: step:   5340, time: 0.217, loss: 1629.994751\n",
      "Train: step:   5350, time: 0.190, loss: 1825.124023\n",
      "Train: step:   5360, time: 0.198, loss: 1507.276245\n",
      "Train: step:   5370, time: 0.196, loss: 2649.885986\n",
      "Train: step:   5380, time: 0.219, loss: 1135.794800\n",
      "Train: step:   5390, time: 0.192, loss: 1351.770630\n",
      "Train: step:   5400, time: 0.197, loss: 592.872070\n",
      "Train: step:   5410, time: 0.194, loss: 746.234192\n",
      "Train: step:   5420, time: 0.191, loss: 1367.234863\n",
      "Train: step:   5430, time: 0.224, loss: 1970.013428\n",
      "Train: step:   5440, time: 0.196, loss: 2134.502441\n",
      "Train: step:   5450, time: 0.187, loss: 377.053406\n",
      "Train: step:   5460, time: 0.192, loss: 1814.249023\n",
      "Train: step:   5470, time: 0.190, loss: 2722.185303\n",
      "Train: step:   5480, time: 0.228, loss: 499.257965\n",
      "Train: step:   5490, time: 0.192, loss: 937.064026\n",
      "Train: step:   5500, time: 0.216, loss: 2279.786865\n",
      "Train: step:   5510, time: 0.187, loss: 327.907227\n",
      "Train: step:   5520, time: 0.197, loss: 562.451660\n",
      "Train: step:   5530, time: 0.198, loss: 3476.136719\n",
      "Train: step:   5540, time: 0.192, loss: 3426.563477\n",
      "Train: step:   5550, time: 0.194, loss: 3474.735352\n",
      "Train: step:   5560, time: 0.195, loss: 4801.875488\n",
      "Train: step:   5570, time: 0.194, loss: 2678.898682\n",
      "Train: step:   5580, time: 0.192, loss: 2126.732422\n",
      "Train: step:   5590, time: 0.186, loss: 1951.439575\n",
      "Train: step:   5600, time: 0.187, loss: 176.027237\n",
      "Train: step:   5610, time: 0.239, loss: 1551.140015\n",
      "Train: step:   5620, time: 0.216, loss: 954.954041\n",
      "Train: step:   5630, time: 0.189, loss: 2489.066895\n",
      "Train: step:   5640, time: 0.194, loss: 1597.898438\n",
      "Train: step:   5650, time: 0.226, loss: 3241.113037\n",
      "Train: step:   5660, time: 0.228, loss: 2544.287109\n",
      "Train: step:   5670, time: 0.201, loss: 2183.615479\n",
      "Train: step:   5680, time: 0.202, loss: 2565.807373\n",
      "Train: step:   5690, time: 0.231, loss: 3123.774414\n",
      "Train: step:   5700, time: 0.219, loss: 1109.721802\n",
      "Train: step:   5710, time: 0.189, loss: 1308.417847\n",
      "Train: step:   5720, time: 0.196, loss: 3271.650391\n",
      "Train: step:   5730, time: 0.192, loss: 1871.775879\n",
      "Train: step:   5740, time: 0.218, loss: 1520.913940\n",
      "Train: step:   5750, time: 0.187, loss: 3243.012939\n",
      "Train: step:   5760, time: 0.188, loss: 1809.468628\n",
      "Train: step:   5770, time: 0.197, loss: 423.022552\n",
      "Train: step:   5780, time: 0.245, loss: 2944.299316\n",
      "Train: step:   5790, time: 0.194, loss: 508.388062\n",
      "Train: step:   5800, time: 0.199, loss: 735.499695\n",
      "Train: step:   5810, time: 0.218, loss: 1351.544189\n",
      "Train: step:   5820, time: 0.206, loss: 3313.246338\n",
      "Train: step:   5830, time: 0.232, loss: 598.749329\n",
      "Train: step:   5840, time: 0.223, loss: 1995.432129\n",
      "Train: step:   5850, time: 0.222, loss: 1040.135986\n",
      "Train: step:   5860, time: 0.196, loss: 342.410370\n",
      "Train: step:   5870, time: 0.227, loss: 850.774963\n",
      "Train: step:   5880, time: 0.189, loss: 1682.175537\n",
      "Train: step:   5890, time: 0.199, loss: 2279.841797\n",
      "Train: step:   5900, time: 0.226, loss: 827.231934\n",
      "Train: step:   5910, time: 0.195, loss: 1102.035400\n",
      "Train: step:   5920, time: 0.216, loss: 2505.559570\n",
      "Train: step:   5930, time: 0.217, loss: 3055.089111\n",
      "Train: step:   5940, time: 0.197, loss: 2781.507812\n",
      "Train: step:   5950, time: 0.196, loss: 2441.013428\n",
      "Train: step:   5960, time: 0.204, loss: 844.654419\n",
      "Train: step:   5970, time: 0.195, loss: 1690.913696\n",
      "Train: step:   5980, time: 0.217, loss: 2464.067139\n",
      "Train: step:   5990, time: 0.201, loss: 2151.428223\n",
      "Train: step:   6000, time: 0.195, loss: 1759.185303\n",
      "Train: step:   6010, time: 0.191, loss: 2274.182373\n",
      "Train: step:   6020, time: 0.198, loss: 2280.707764\n",
      "Train: step:   6030, time: 0.194, loss: 441.367310\n",
      "Train: step:   6040, time: 0.209, loss: 2434.761963\n",
      "Train: step:   6050, time: 0.225, loss: 498.091980\n",
      "Train: step:   6060, time: 0.216, loss: 1710.827881\n",
      "Train: step:   6070, time: 0.216, loss: 1954.983521\n",
      "Train: step:   6080, time: 0.195, loss: 1969.955200\n",
      "Train: step:   6090, time: 0.188, loss: 1552.356445\n",
      "Train: step:   6100, time: 0.219, loss: 1607.406128\n",
      "Train: step:   6110, time: 0.213, loss: 3428.203857\n",
      "Train: step:   6120, time: 0.217, loss: 2091.855713\n",
      "Train: step:   6130, time: 0.217, loss: 1403.865601\n",
      "Train: step:   6140, time: 0.188, loss: 2113.480957\n",
      "Train: step:   6150, time: 0.212, loss: 3739.520020\n",
      "Train: step:   6160, time: 0.220, loss: 3430.188477\n",
      "Train: step:   6170, time: 0.187, loss: 2850.484863\n",
      "Train: step:   6180, time: 0.217, loss: 3093.770508\n",
      "Train: step:   6190, time: 0.218, loss: 2938.433838\n",
      "Train: step:   6200, time: 0.234, loss: 4010.829346\n",
      "Train: step:   6210, time: 0.208, loss: 3877.346191\n",
      "Train: step:   6220, time: 0.190, loss: 455.253967\n",
      "Train: step:   6230, time: 0.196, loss: 765.223389\n",
      "Train: step:   6240, time: 0.218, loss: 2401.543457\n",
      "Train: step:   6250, time: 0.196, loss: 2462.937256\n",
      "Train: step:   6260, time: 0.186, loss: 2014.706177\n",
      "Train: step:   6270, time: 0.195, loss: 1507.723755\n",
      "Train: step:   6280, time: 0.237, loss: 1121.344360\n",
      "Train: step:   6290, time: 0.219, loss: 512.645447\n",
      "Train: step:   6300, time: 0.199, loss: 2239.951660\n",
      "Train: step:   6310, time: 0.194, loss: 2195.178467\n",
      "Train: step:   6320, time: 0.191, loss: 1016.726257\n",
      "Train: step:   6330, time: 0.195, loss: 2508.566406\n",
      "Train: step:   6340, time: 0.223, loss: 3431.475342\n",
      "Train: step:   6350, time: 0.208, loss: 1669.068726\n",
      "Train: step:   6360, time: 0.222, loss: 2872.104980\n",
      "Train: step:   6370, time: 0.221, loss: 3207.698486\n",
      "Train: step:   6380, time: 0.192, loss: 2217.270264\n",
      "Train: step:   6390, time: 0.190, loss: 1766.141357\n",
      "Train: step:   6400, time: 0.187, loss: 2083.514893\n",
      "Train: step:   6410, time: 0.191, loss: 4031.918457\n",
      "Train: step:   6420, time: 0.200, loss: 2125.035156\n",
      "Train: step:   6430, time: 0.196, loss: 3219.053467\n",
      "Train: step:   6440, time: 0.201, loss: 1804.887939\n",
      "Train: step:   6450, time: 0.192, loss: 1574.040039\n",
      "Train: step:   6460, time: 0.196, loss: 2007.764038\n",
      "Train: step:   6470, time: 0.210, loss: 2294.678223\n",
      "Train: step:   6480, time: 0.239, loss: 494.334015\n",
      "Train: step:   6490, time: 0.190, loss: 1899.938232\n",
      "Train: step:   6500, time: 0.190, loss: 3175.683594\n",
      "Train: step:   6510, time: 0.215, loss: 1546.898560\n",
      "Train: step:   6520, time: 0.223, loss: 3389.829590\n",
      "Train: step:   6530, time: 0.230, loss: 1700.714111\n",
      "Train: step:   6540, time: 0.230, loss: 3505.104492\n",
      "Train: step:   6550, time: 0.229, loss: 441.938934\n",
      "Train: step:   6560, time: 0.185, loss: 2866.580566\n",
      "Train: step:   6570, time: 0.198, loss: 2663.953613\n",
      "Train: step:   6580, time: 0.231, loss: 1675.808838\n",
      "Train: step:   6590, time: 0.194, loss: 2233.400635\n",
      "Train: step:   6600, time: 0.203, loss: 1837.374146\n",
      "Train: step:   6610, time: 0.223, loss: 1087.450073\n",
      "Train: step:   6620, time: 0.194, loss: 2415.869629\n",
      "Train: step:   6630, time: 0.224, loss: 3536.369873\n",
      "Train: step:   6640, time: 0.221, loss: 2193.987549\n",
      "Train: step:   6650, time: 0.234, loss: 2996.110352\n",
      "Train: step:   6660, time: 0.241, loss: 919.052856\n",
      "Train: step:   6670, time: 0.197, loss: 1258.541504\n",
      "Train: step:   6680, time: 0.195, loss: 1167.074219\n",
      "Train: step:   6690, time: 0.218, loss: 3832.025391\n",
      "Train: step:   6700, time: 0.200, loss: 3619.607422\n",
      "Train: step:   6710, time: 0.232, loss: 1494.022583\n",
      "Train: step:   6720, time: 0.216, loss: 2249.882324\n",
      "Train: step:   6730, time: 0.217, loss: 2599.545410\n",
      "Train: step:   6740, time: 0.207, loss: 2376.027100\n",
      "Train: step:   6750, time: 0.191, loss: 1166.565552\n",
      "Train: step:   6760, time: 0.246, loss: 646.640747\n",
      "Train: step:   6770, time: 0.207, loss: 732.573975\n",
      "Train: step:   6780, time: 0.219, loss: 1716.450073\n",
      "Train: step:   6790, time: 0.232, loss: 2440.260986\n",
      "Train: step:   6800, time: 0.216, loss: 2040.157471\n",
      "Train: step:   6810, time: 0.192, loss: 1277.971191\n",
      "Train: step:   6820, time: 0.196, loss: 2944.598389\n",
      "Train: step:   6830, time: 0.219, loss: 2605.756592\n",
      "Train: step:   6840, time: 0.228, loss: 4343.401367\n",
      "Train: step:   6850, time: 0.215, loss: 1235.278198\n",
      "Train: step:   6860, time: 0.197, loss: 2225.169678\n",
      "Train: step:   6870, time: 0.250, loss: 1416.188721\n",
      "Train: step:   6880, time: 0.208, loss: 1270.521362\n",
      "Train: step:   6890, time: 0.228, loss: 2105.293945\n",
      "Train: step:   6900, time: 0.216, loss: 887.137146\n",
      "Train: step:   6910, time: 0.200, loss: 2905.209961\n",
      "Train: step:   6920, time: 0.194, loss: 2204.654053\n",
      "Train: step:   6930, time: 0.208, loss: 835.533997\n",
      "Train: step:   6940, time: 0.219, loss: 1921.275024\n",
      "Train: step:   6950, time: 0.227, loss: 1133.715332\n",
      "Train: step:   6960, time: 0.200, loss: 2516.037598\n",
      "Train: step:   6970, time: 0.197, loss: 1196.529175\n",
      "Train: step:   6980, time: 0.231, loss: 774.016602\n",
      "Train: step:   6990, time: 0.213, loss: 554.039246\n",
      "Train: step:   7000, time: 0.202, loss: 1919.535645\n",
      "Train: step:   7010, time: 0.231, loss: 296.792114\n",
      "Train: step:   7020, time: 0.193, loss: 585.042297\n",
      "Train: step:   7030, time: 0.208, loss: 1760.440308\n",
      "Train: step:   7040, time: 0.195, loss: 495.788300\n",
      "Train: step:   7050, time: 0.221, loss: 980.531311\n",
      "Train: step:   7060, time: 0.200, loss: 1186.775269\n",
      "Train: step:   7070, time: 0.197, loss: 2027.798584\n",
      "Train: step:   7080, time: 0.229, loss: 3250.331787\n",
      "Train: step:   7090, time: 0.234, loss: 2135.534912\n",
      "Train: step:   7100, time: 0.229, loss: 2859.291016\n",
      "Train: step:   7110, time: 0.224, loss: 2785.989258\n",
      "Train: step:   7120, time: 0.202, loss: 2207.961426\n",
      "Train: step:   7130, time: 0.202, loss: 1217.012085\n",
      "Train: step:   7140, time: 0.218, loss: 4438.397461\n",
      "Train: step:   7150, time: 0.195, loss: 2211.403076\n",
      "Train: step:   7160, time: 0.195, loss: 2642.466797\n",
      "Train: step:   7170, time: 0.198, loss: 1543.685059\n",
      "Train: step:   7180, time: 0.196, loss: 2112.095459\n",
      "Train: step:   7190, time: 0.198, loss: 377.157104\n",
      "Train: step:   7200, time: 0.213, loss: 929.246521\n",
      "Train: step:   7210, time: 0.190, loss: 1510.235840\n",
      "Train: step:   7220, time: 0.195, loss: 979.269409\n",
      "Train: step:   7230, time: 0.215, loss: 1223.440552\n",
      "Train: step:   7240, time: 0.193, loss: 2266.348389\n",
      "Train: step:   7250, time: 0.221, loss: 3117.600342\n",
      "Train: step:   7260, time: 0.192, loss: 1966.518555\n",
      "Train: step:   7270, time: 0.195, loss: 633.582214\n",
      "Train: step:   7280, time: 0.194, loss: 1354.208618\n",
      "Train: step:   7290, time: 0.222, loss: 1780.163086\n",
      "Train: step:   7300, time: 0.194, loss: 1845.299072\n",
      "Train: step:   7310, time: 0.199, loss: 2867.021240\n",
      "Train: step:   7320, time: 0.219, loss: 1560.483032\n",
      "Train: step:   7330, time: 0.222, loss: 1281.931396\n",
      "Train: step:   7340, time: 0.201, loss: 2457.804688\n",
      "Train: step:   7350, time: 0.191, loss: 242.004456\n",
      "Train: step:   7360, time: 0.195, loss: 1973.482788\n",
      "Train: step:   7370, time: 0.201, loss: 1416.239258\n",
      "Train: step:   7380, time: 0.217, loss: 447.499115\n",
      "Train: step:   7390, time: 0.193, loss: 518.294250\n",
      "Train: step:   7400, time: 0.219, loss: 2959.324463\n",
      "Train: step:   7410, time: 0.228, loss: 1529.946533\n",
      "Train: step:   7420, time: 0.207, loss: 1655.218994\n",
      "Train: step:   7430, time: 0.201, loss: 1034.117310\n",
      "Train: step:   7440, time: 0.200, loss: 1150.988770\n",
      "Train: step:   7450, time: 0.230, loss: 420.629242\n",
      "Train: step:   7460, time: 0.222, loss: 782.515869\n",
      "Train: step:   7470, time: 0.224, loss: 2453.307861\n",
      "Train: step:   7480, time: 0.219, loss: 1573.980469\n",
      "Train: step:   7490, time: 0.192, loss: 2199.209229\n",
      "Train: step:   7500, time: 0.228, loss: 1041.286499\n",
      "Train: step:   7510, time: 0.220, loss: 2891.040771\n",
      "Train: step:   7520, time: 0.201, loss: 1617.970703\n",
      "Train: step:   7530, time: 0.227, loss: 991.833374\n",
      "Train: step:   7540, time: 0.196, loss: 2463.213623\n",
      "Train: step:   7550, time: 0.188, loss: 1789.365601\n",
      "Train: step:   7560, time: 0.229, loss: 3118.871338\n",
      "Train: step:   7570, time: 0.216, loss: 1057.336548\n",
      "Train: step:   7580, time: 0.200, loss: 1147.798950\n",
      "Train: step:   7590, time: 0.193, loss: 1995.764282\n",
      "Train: step:   7600, time: 0.193, loss: 3004.647461\n",
      "Train: step:   7610, time: 0.214, loss: 2574.048096\n",
      "Train: step:   7620, time: 0.202, loss: 2128.179688\n",
      "Train: step:   7630, time: 0.190, loss: 1849.836304\n",
      "Train: step:   7640, time: 0.196, loss: 5136.024902\n",
      "Train: step:   7650, time: 0.191, loss: 1763.801880\n",
      "Train: step:   7660, time: 0.217, loss: 2500.622070\n",
      "Train: step:   7670, time: 0.200, loss: 2866.542480\n",
      "Train: step:   7680, time: 0.218, loss: 1716.866333\n",
      "Train: step:   7690, time: 0.209, loss: 831.071777\n",
      "Train: step:   7700, time: 0.217, loss: 1754.619141\n",
      "Train: step:   7710, time: 0.203, loss: 2478.944580\n",
      "Train: step:   7720, time: 0.193, loss: 304.557037\n",
      "Train: step:   7730, time: 0.222, loss: 1588.705444\n",
      "Train: step:   7740, time: 0.193, loss: 2145.301758\n",
      "Train: step:   7750, time: 0.217, loss: 2645.403320\n",
      "Train: step:   7760, time: 0.217, loss: 2019.195679\n",
      "Train: step:   7770, time: 0.216, loss: 1502.443726\n",
      "Train: step:   7780, time: 0.200, loss: 1761.982910\n",
      "Train: step:   7790, time: 0.189, loss: 923.170898\n",
      "Train: step:   7800, time: 0.189, loss: 734.274841\n",
      "Train: step:   7810, time: 0.196, loss: 2975.993652\n",
      "Train: step:   7820, time: 0.218, loss: 2265.527588\n",
      "Train: step:   7830, time: 0.205, loss: 1454.001343\n",
      "Train: step:   7840, time: 0.199, loss: 1506.719971\n",
      "Train: step:   7850, time: 0.228, loss: 1985.382690\n",
      "Train: step:   7860, time: 0.196, loss: 2359.665771\n",
      "Train: step:   7870, time: 0.184, loss: 981.614380\n",
      "Train: step:   7880, time: 0.190, loss: 1197.847534\n",
      "Train: step:   7890, time: 0.202, loss: 1978.437134\n",
      "Train: step:   7900, time: 0.187, loss: 926.498962\n",
      "Train: step:   7910, time: 0.231, loss: 2531.554688\n",
      "Train: step:   7920, time: 0.194, loss: 2897.031494\n",
      "Train: step:   7930, time: 0.194, loss: 2282.045898\n",
      "Train: step:   7940, time: 0.232, loss: 296.138489\n",
      "Train: step:   7950, time: 0.205, loss: 2526.020752\n",
      "Train: step:   7960, time: 0.226, loss: 1146.273804\n",
      "Train: step:   7970, time: 0.190, loss: 2036.319946\n",
      "Train: step:   7980, time: 0.187, loss: 3774.993652\n",
      "Train: step:   7990, time: 0.234, loss: 2846.430908\n",
      "Train: step:   8000, time: 0.204, loss: 1272.909912\n",
      "Train: step:   8010, time: 0.212, loss: 3949.203125\n",
      "Train: step:   8020, time: 0.190, loss: 1203.150635\n",
      "Train: step:   8030, time: 0.198, loss: 194.781723\n",
      "Train: step:   8040, time: 0.193, loss: 803.020508\n",
      "Train: step:   8050, time: 0.217, loss: 812.256165\n",
      "Train: step:   8060, time: 0.247, loss: 486.262177\n",
      "Train: step:   8070, time: 0.212, loss: 374.352753\n",
      "Train: step:   8080, time: 0.223, loss: 4045.910156\n",
      "Train: step:   8090, time: 0.193, loss: 2359.676025\n",
      "Train: step:   8100, time: 0.188, loss: 1051.785156\n",
      "Train: step:   8110, time: 0.223, loss: 2467.129395\n",
      "Train: step:   8120, time: 0.262, loss: 4067.094238\n",
      "Train: step:   8130, time: 0.191, loss: 920.682983\n",
      "Train: step:   8140, time: 0.215, loss: 1868.985718\n",
      "Train: step:   8150, time: 0.225, loss: 2594.727783\n",
      "Train: step:   8160, time: 0.217, loss: 1928.167603\n",
      "Train: step:   8170, time: 0.187, loss: 694.409363\n",
      "Train: step:   8180, time: 0.228, loss: 1896.433350\n",
      "Train: step:   8190, time: 0.204, loss: 475.177124\n",
      "Train: step:   8200, time: 0.231, loss: 1009.008362\n",
      "Train: step:   8210, time: 0.219, loss: 427.083557\n",
      "Train: step:   8220, time: 0.190, loss: 2246.444336\n",
      "Train: step:   8230, time: 0.194, loss: 720.284729\n",
      "Train: step:   8240, time: 0.237, loss: 1599.088135\n",
      "Train: step:   8250, time: 0.201, loss: 489.405365\n",
      "Train: step:   8260, time: 0.212, loss: 1829.399414\n",
      "Train: step:   8270, time: 0.217, loss: 1797.977173\n",
      "Train: step:   8280, time: 0.188, loss: 1753.220215\n",
      "Train: step:   8290, time: 0.216, loss: 1918.258301\n",
      "Train: step:   8300, time: 0.194, loss: 2494.431396\n",
      "Train: step:   8310, time: 0.241, loss: 1517.265503\n",
      "Train: step:   8320, time: 0.219, loss: 3054.422607\n",
      "Train: step:   8330, time: 0.190, loss: 369.747742\n",
      "Train: step:   8340, time: 0.188, loss: 2346.254883\n",
      "Train: step:   8350, time: 0.203, loss: 3383.297607\n",
      "Train: step:   8360, time: 0.189, loss: 1999.603149\n",
      "Train: step:   8370, time: 0.202, loss: 1064.153198\n",
      "Train: step:   8380, time: 0.226, loss: 1426.579956\n",
      "Train: step:   8390, time: 0.217, loss: 3817.673828\n",
      "Train: step:   8400, time: 0.247, loss: 2573.113525\n",
      "Train: step:   8410, time: 0.184, loss: 2990.017334\n",
      "Train: step:   8420, time: 0.188, loss: 788.047241\n",
      "Train: step:   8430, time: 0.200, loss: 1606.779541\n",
      "Train: step:   8440, time: 0.189, loss: 3582.026367\n",
      "Train: step:   8450, time: 0.219, loss: 1320.614990\n",
      "Train: step:   8460, time: 0.238, loss: 2136.916748\n",
      "Train: step:   8470, time: 0.195, loss: 2983.398926\n",
      "Train: step:   8480, time: 0.190, loss: 3230.822510\n",
      "Train: step:   8490, time: 0.189, loss: 1170.879883\n",
      "Train: step:   8500, time: 0.186, loss: 953.158081\n",
      "Train: step:   8510, time: 0.232, loss: 766.063843\n",
      "Train: step:   8520, time: 0.193, loss: 838.882446\n",
      "Train: step:   8530, time: 0.189, loss: 2313.540771\n",
      "Train: step:   8540, time: 0.197, loss: 611.556091\n",
      "Train: step:   8550, time: 0.206, loss: 2089.679688\n",
      "Train: step:   8560, time: 0.228, loss: 2021.727661\n",
      "Train: step:   8570, time: 0.196, loss: 680.709045\n",
      "Train: step:   8580, time: 0.227, loss: 2746.691650\n",
      "Train: step:   8590, time: 0.228, loss: 3399.855957\n",
      "Train: step:   8600, time: 0.232, loss: 863.689331\n",
      "Train: step:   8610, time: 0.198, loss: 2887.242432\n",
      "Train: step:   8620, time: 0.207, loss: 2735.156738\n",
      "Train: step:   8630, time: 0.198, loss: 2382.427979\n",
      "Train: step:   8640, time: 0.197, loss: 1567.061523\n",
      "Train: step:   8650, time: 0.215, loss: 1725.475220\n",
      "Train: step:   8660, time: 0.195, loss: 2786.689697\n",
      "Train: step:   8670, time: 0.220, loss: 419.632782\n",
      "Train: step:   8680, time: 0.188, loss: 584.851135\n",
      "Train: step:   8690, time: 0.187, loss: 1078.254028\n",
      "Train: step:   8700, time: 0.227, loss: 660.864075\n",
      "Train: step:   8710, time: 0.290, loss: 2262.962402\n",
      "Train: step:   8720, time: 0.226, loss: 722.343933\n",
      "Train: step:   8730, time: 0.191, loss: 1993.687378\n",
      "Train: step:   8740, time: 0.236, loss: 715.926941\n",
      "Train: step:   8750, time: 0.244, loss: 1302.758179\n",
      "Train: step:   8760, time: 0.230, loss: 1818.719604\n",
      "Train: step:   8770, time: 0.215, loss: 1019.122559\n",
      "Train: step:   8780, time: 0.201, loss: 1484.965820\n",
      "Train: step:   8790, time: 0.191, loss: 1763.296021\n",
      "Train: step:   8800, time: 0.188, loss: 2226.235107\n",
      "Train: step:   8810, time: 0.194, loss: 1786.710205\n",
      "Train: step:   8820, time: 0.204, loss: 1708.846924\n",
      "Train: step:   8830, time: 0.218, loss: 1991.457031\n",
      "Train: step:   8840, time: 0.205, loss: 805.852478\n",
      "Train: step:   8850, time: 0.198, loss: 1621.017578\n",
      "Train: step:   8860, time: 0.217, loss: 1065.532104\n",
      "Train: step:   8870, time: 0.195, loss: 327.427917\n",
      "Train: step:   8880, time: 0.209, loss: 1616.773804\n",
      "Train: step:   8890, time: 0.195, loss: 1666.953003\n",
      "Train: step:   8900, time: 0.192, loss: 785.787109\n",
      "Train: step:   8910, time: 0.189, loss: 3039.677734\n",
      "Train: step:   8920, time: 0.228, loss: 2713.345947\n",
      "Train: step:   8930, time: 0.184, loss: 3487.649902\n",
      "Train: step:   8940, time: 0.216, loss: 642.836365\n",
      "Train: step:   8950, time: 0.186, loss: 671.128967\n",
      "Train: step:   8960, time: 0.192, loss: 1943.260620\n",
      "Train: step:   8970, time: 0.192, loss: 3186.377930\n",
      "Train: step:   8980, time: 0.232, loss: 1054.494751\n",
      "Train: step:   8990, time: 0.232, loss: 257.387054\n",
      "Train: step:   9000, time: 0.185, loss: 794.128235\n",
      "Train: step:   9010, time: 0.188, loss: 964.828003\n",
      "Train: step:   9020, time: 0.190, loss: 2759.798340\n",
      "Train: step:   9030, time: 0.202, loss: 614.456055\n",
      "Train: step:   9040, time: 0.227, loss: 819.453125\n",
      "Train: step:   9050, time: 0.217, loss: 2033.417114\n",
      "Train: step:   9060, time: 0.234, loss: 1495.288086\n",
      "Train: step:   9070, time: 0.253, loss: 2158.127197\n",
      "Train: step:   9080, time: 0.187, loss: 1830.521973\n",
      "Train: step:   9090, time: 0.234, loss: 1433.303223\n",
      "Train: step:   9100, time: 0.214, loss: 2974.566650\n",
      "Train: step:   9110, time: 0.191, loss: 892.627625\n",
      "Train: step:   9120, time: 0.188, loss: 3031.806641\n",
      "Train: step:   9130, time: 0.189, loss: 527.330994\n",
      "Train: step:   9140, time: 0.195, loss: 2782.000732\n",
      "Train: step:   9150, time: 0.204, loss: 1869.787720\n",
      "Train: step:   9160, time: 0.193, loss: 891.742798\n",
      "Train: step:   9170, time: 0.196, loss: 2621.782959\n",
      "Train: step:   9180, time: 0.182, loss: 909.879211\n",
      "Train: step:   9190, time: 0.193, loss: 1179.775024\n",
      "Train: step:   9200, time: 0.192, loss: 2644.269287\n",
      "Train: step:   9210, time: 0.233, loss: 669.468689\n",
      "Train: step:   9220, time: 0.230, loss: 1795.015015\n",
      "Train: step:   9230, time: 0.187, loss: 975.030029\n",
      "Train: step:   9240, time: 0.185, loss: 2320.113770\n",
      "Train: step:   9250, time: 0.190, loss: 2343.445068\n",
      "Train: step:   9260, time: 0.199, loss: 3050.348877\n",
      "Train: step:   9270, time: 0.191, loss: 898.155090\n",
      "Train: step:   9280, time: 0.192, loss: 796.823486\n",
      "Train: step:   9290, time: 0.186, loss: 2293.050537\n",
      "Train: step:   9300, time: 0.193, loss: 1901.942993\n",
      "Train: step:   9310, time: 0.216, loss: 1125.177124\n",
      "Train: step:   9320, time: 0.183, loss: 699.295532\n",
      "Train: step:   9330, time: 0.184, loss: 2698.341797\n",
      "Train: step:   9340, time: 0.187, loss: 1622.444458\n",
      "Train: step:   9350, time: 0.188, loss: 2777.973145\n",
      "Train: step:   9360, time: 0.190, loss: 2595.286621\n",
      "Train: step:   9370, time: 0.196, loss: 1658.382324\n",
      "Train: step:   9380, time: 0.226, loss: 283.852722\n",
      "Train: step:   9390, time: 0.215, loss: 2936.852051\n",
      "Train: step:   9400, time: 0.186, loss: 2304.906250\n",
      "Train: step:   9410, time: 0.260, loss: 3182.293701\n",
      "Train: step:   9420, time: 0.187, loss: 698.558716\n",
      "Train: step:   9430, time: 0.185, loss: 2379.291504\n",
      "Train: step:   9440, time: 0.190, loss: 796.587158\n",
      "Train: step:   9450, time: 0.190, loss: 809.134216\n",
      "Train: step:   9460, time: 0.183, loss: 1612.031982\n",
      "Train: step:   9470, time: 0.216, loss: 891.292297\n",
      "Train: step:   9480, time: 0.184, loss: 2326.772217\n",
      "Train: step:   9490, time: 0.185, loss: 3509.875977\n",
      "Train: step:   9500, time: 0.211, loss: 1938.443726\n",
      "Train: step:   9510, time: 0.197, loss: 1894.808838\n",
      "Train: step:   9520, time: 0.229, loss: 1525.054688\n",
      "Train: step:   9530, time: 0.216, loss: 2613.333984\n",
      "Train: step:   9540, time: 0.192, loss: 2751.399902\n",
      "Train: step:   9550, time: 0.196, loss: 2267.550781\n",
      "Train: step:   9560, time: 0.193, loss: 2128.547607\n",
      "Train: step:   9570, time: 0.219, loss: 2123.496094\n",
      "Train: step:   9580, time: 0.217, loss: 1924.571167\n",
      "Train: step:   9590, time: 0.211, loss: 1781.219360\n",
      "Train: step:   9600, time: 0.182, loss: 1898.494263\n",
      "Train: step:   9610, time: 0.201, loss: 905.375000\n",
      "Train: step:   9620, time: 0.188, loss: 653.270569\n",
      "Train: step:   9630, time: 0.187, loss: 3295.076416\n",
      "Train: step:   9640, time: 0.216, loss: 1737.319092\n",
      "Train: step:   9650, time: 0.250, loss: 1593.982544\n",
      "Train: step:   9660, time: 0.218, loss: 3452.055664\n",
      "Train: step:   9670, time: 0.244, loss: 2636.197266\n",
      "Train: step:   9680, time: 0.184, loss: 1055.942749\n",
      "Train: step:   9690, time: 0.218, loss: 1344.873901\n",
      "Train: step:   9700, time: 0.192, loss: 2447.837158\n",
      "Train: step:   9710, time: 0.234, loss: 2118.484131\n",
      "Train: step:   9720, time: 0.190, loss: 1027.660767\n",
      "Train: step:   9730, time: 0.188, loss: 993.506409\n",
      "Train: step:   9740, time: 0.192, loss: 2738.941895\n",
      "Train: step:   9750, time: 0.189, loss: 2149.159180\n",
      "Train: step:   9760, time: 0.191, loss: 1711.508423\n",
      "Train: step:   9770, time: 0.190, loss: 3302.745117\n",
      "Train: step:   9780, time: 0.187, loss: 725.260193\n",
      "Train: step:   9790, time: 0.186, loss: 1494.190918\n",
      "Train: step:   9800, time: 0.218, loss: 2499.975098\n",
      "Train: step:   9810, time: 0.215, loss: 2333.297119\n",
      "Train: step:   9820, time: 0.185, loss: 1435.202637\n",
      "Train: step:   9830, time: 0.197, loss: 1374.069092\n",
      "Train: step:   9840, time: 0.216, loss: 1878.227417\n",
      "Train: step:   9850, time: 0.182, loss: 903.928955\n",
      "Train: step:   9860, time: 0.189, loss: 862.117493\n",
      "Train: step:   9870, time: 0.187, loss: 1809.781616\n",
      "Train: step:   9880, time: 0.187, loss: 399.606567\n",
      "Train: step:   9890, time: 0.216, loss: 579.970337\n",
      "Train: step:   9900, time: 0.186, loss: 964.308777\n",
      "Train: step:   9910, time: 0.195, loss: 2661.458740\n",
      "Train: step:   9920, time: 0.185, loss: 2556.049316\n",
      "Train: step:   9930, time: 0.220, loss: 323.784882\n",
      "Train: step:   9940, time: 0.199, loss: 2206.095459\n",
      "Train: step:   9950, time: 0.233, loss: 2388.906494\n",
      "Train: step:   9960, time: 0.209, loss: 1235.891846\n",
      "Train: step:   9970, time: 0.199, loss: 2843.366455\n",
      "Train: step:   9980, time: 0.187, loss: 463.732361\n",
      "Train: step:   9990, time: 0.229, loss: 4630.877441\n",
      "Train: step:  10000, time: 0.217, loss: 2077.498779\n",
      "Train: step:  10010, time: 0.195, loss: 1012.691467\n",
      "Train: step:  10020, time: 0.189, loss: 2093.137207\n",
      "Train: step:  10030, time: 0.226, loss: 1639.117188\n",
      "Train: step:  10040, time: 0.215, loss: 1678.896973\n",
      "Train: step:  10050, time: 0.206, loss: 2532.674316\n",
      "Train: step:  10060, time: 0.193, loss: 1522.292480\n",
      "Train: step:  10070, time: 0.245, loss: 1602.042480\n",
      "Train: step:  10080, time: 0.226, loss: 1931.344116\n",
      "Train: step:  10090, time: 0.253, loss: 2916.473145\n",
      "Train: step:  10100, time: 0.231, loss: 2426.552490\n",
      "Train: step:  10110, time: 0.197, loss: 1202.752930\n",
      "Train: step:  10120, time: 0.216, loss: 2234.131104\n",
      "Train: step:  10130, time: 0.194, loss: 1666.359863\n",
      "Train: step:  10140, time: 0.219, loss: 335.484833\n",
      "Train: step:  10150, time: 0.197, loss: 540.380737\n",
      "Train: step:  10160, time: 0.185, loss: 2145.248291\n",
      "Train: step:  10170, time: 0.185, loss: 624.501038\n",
      "Train: step:  10180, time: 0.214, loss: 3047.551514\n",
      "Train: step:  10190, time: 0.185, loss: 1038.938232\n",
      "Train: step:  10200, time: 0.217, loss: 3018.455566\n",
      "Train: step:  10210, time: 0.213, loss: 1062.595093\n",
      "Train: step:  10220, time: 0.184, loss: 2456.546631\n",
      "Train: step:  10230, time: 0.227, loss: 1521.075195\n",
      "Train: step:  10240, time: 0.231, loss: 1729.207764\n",
      "Train: step:  10250, time: 0.217, loss: 1327.683350\n",
      "Train: step:  10260, time: 0.184, loss: 2397.412598\n",
      "Train: step:  10270, time: 0.198, loss: 2810.934326\n",
      "Train: step:  10280, time: 0.191, loss: 3932.181885\n",
      "Train: step:  10290, time: 0.180, loss: 2789.253906\n",
      "Train: step:  10300, time: 0.217, loss: 1857.959229\n",
      "Train: step:  10310, time: 0.228, loss: 1567.411377\n",
      "Train: step:  10320, time: 0.195, loss: 3597.376709\n",
      "Train: step:  10330, time: 0.180, loss: 1122.593628\n",
      "Train: step:  10340, time: 0.213, loss: 2347.117920\n",
      "Train: step:  10350, time: 0.187, loss: 434.669159\n",
      "Train: step:  10360, time: 0.221, loss: 1490.956787\n",
      "Train: step:  10370, time: 0.197, loss: 2872.082520\n",
      "Train: step:  10380, time: 0.183, loss: 1316.529541\n",
      "Train: step:  10390, time: 0.187, loss: 1999.905029\n",
      "Train: step:  10400, time: 0.187, loss: 1565.640259\n",
      "Train: step:  10410, time: 0.182, loss: 2099.192383\n",
      "Train: step:  10420, time: 0.247, loss: 724.992554\n",
      "Train: step:  10430, time: 0.224, loss: 684.996521\n",
      "Train: step:  10440, time: 0.179, loss: 1348.687622\n",
      "Train: step:  10450, time: 0.197, loss: 2247.106445\n",
      "Train: step:  10460, time: 0.190, loss: 866.541077\n",
      "Train: step:  10470, time: 0.185, loss: 3272.250244\n",
      "Train: step:  10480, time: 0.185, loss: 361.523743\n",
      "Train: step:  10490, time: 0.191, loss: 544.027771\n",
      "Train: step:  10500, time: 0.198, loss: 1330.166748\n",
      "Train: step:  10510, time: 0.187, loss: 4008.684082\n",
      "Train: step:  10520, time: 0.189, loss: 2566.510254\n",
      "Train: step:  10530, time: 0.181, loss: 1526.574219\n",
      "Train: step:  10540, time: 0.213, loss: 350.769897\n",
      "Train: step:  10550, time: 0.213, loss: 1294.964844\n",
      "Train: step:  10560, time: 0.183, loss: 2121.459961\n",
      "Train: step:  10570, time: 0.187, loss: 2175.884766\n",
      "Train: step:  10580, time: 0.189, loss: 1286.928101\n",
      "Train: step:  10590, time: 0.215, loss: 1015.688721\n",
      "Train: step:  10600, time: 0.234, loss: 1389.429199\n",
      "Train: step:  10610, time: 0.202, loss: 1541.128662\n",
      "Train: step:  10620, time: 0.195, loss: 1323.745972\n",
      "Train: step:  10630, time: 0.195, loss: 2568.452393\n",
      "Train: step:  10640, time: 0.238, loss: 1519.881958\n",
      "Train: step:  10650, time: 0.192, loss: 3970.377686\n",
      "Train: step:  10660, time: 0.186, loss: 1604.986206\n",
      "Train: step:  10670, time: 0.189, loss: 2702.165771\n",
      "Train: step:  10680, time: 0.213, loss: 747.239929\n",
      "Train: step:  10690, time: 0.264, loss: 3278.342041\n",
      "Train: step:  10700, time: 0.232, loss: 395.912109\n",
      "Train: step:  10710, time: 0.215, loss: 1428.358765\n",
      "Train: step:  10720, time: 0.190, loss: 2464.903076\n",
      "Train: step:  10730, time: 0.187, loss: 1568.810547\n",
      "Train: step:  10740, time: 0.193, loss: 3396.537354\n",
      "Train: step:  10750, time: 0.183, loss: 1563.840698\n",
      "Train: step:  10760, time: 0.223, loss: 1953.664673\n",
      "Train: step:  10770, time: 0.183, loss: 2415.782959\n",
      "Train: step:  10780, time: 0.213, loss: 2272.374756\n",
      "Train: step:  10790, time: 0.189, loss: 1390.655396\n",
      "Train: step:  10800, time: 0.226, loss: 2117.269043\n",
      "Train: step:  10810, time: 0.215, loss: 899.634338\n",
      "Train: step:  10820, time: 0.215, loss: 2926.537109\n",
      "Train: step:  10830, time: 0.186, loss: 449.341949\n",
      "Train: step:  10840, time: 0.209, loss: 1749.332642\n",
      "Train: step:  10850, time: 0.216, loss: 882.630188\n",
      "Train: step:  10860, time: 0.182, loss: 2990.199951\n",
      "Train: step:  10870, time: 0.216, loss: 2740.478516\n",
      "Train: step:  10880, time: 0.188, loss: 1862.066895\n",
      "Train: step:  10890, time: 0.207, loss: 1035.408325\n",
      "Train: step:  10900, time: 0.186, loss: 2244.055420\n",
      "Train: step:  10910, time: 0.186, loss: 2709.099609\n",
      "Train: step:  10920, time: 0.189, loss: 2384.724365\n",
      "Train: step:  10930, time: 0.255, loss: 3092.843750\n",
      "Train: step:  10940, time: 0.186, loss: 855.920837\n",
      "Train: step:  10950, time: 0.217, loss: 1138.563110\n",
      "Train: step:  10960, time: 0.216, loss: 787.339355\n",
      "Train: step:  10970, time: 0.214, loss: 2343.334961\n",
      "Train: step:  10980, time: 0.225, loss: 877.552612\n",
      "Train: step:  10990, time: 0.188, loss: 1383.415649\n",
      "Train: step:  11000, time: 0.190, loss: 1895.524414\n",
      "Train: step:  11010, time: 0.192, loss: 1766.946289\n",
      "Train: step:  11020, time: 0.188, loss: 1936.325317\n",
      "Train: step:  11030, time: 0.216, loss: 250.649170\n",
      "Train: step:  11040, time: 0.183, loss: 3681.035645\n",
      "Train: step:  11050, time: 0.192, loss: 2083.781982\n",
      "Train: step:  11060, time: 0.189, loss: 1134.938354\n",
      "Train: step:  11070, time: 0.217, loss: 1574.555420\n",
      "Train: step:  11080, time: 0.230, loss: 888.925049\n",
      "Train: step:  11090, time: 0.182, loss: 2675.102051\n",
      "Train: step:  11100, time: 0.216, loss: 1707.256470\n",
      "Train: step:  11110, time: 0.227, loss: 4109.513184\n",
      "Train: step:  11120, time: 0.194, loss: 417.376862\n",
      "Train: step:  11130, time: 0.198, loss: 2994.006104\n",
      "Train: step:  11140, time: 0.217, loss: 1363.339966\n",
      "Train: step:  11150, time: 0.187, loss: 1771.385498\n",
      "Train: step:  11160, time: 0.189, loss: 2374.567139\n",
      "Train: step:  11170, time: 0.232, loss: 1310.977539\n",
      "Train: step:  11180, time: 0.217, loss: 2117.179199\n",
      "Train: step:  11190, time: 0.220, loss: 3553.495361\n",
      "Train: step:  11200, time: 0.193, loss: 424.076721\n",
      "Train: step:  11210, time: 0.229, loss: 2172.218994\n",
      "Train: step:  11220, time: 0.191, loss: 935.401428\n",
      "Train: step:  11230, time: 0.197, loss: 2805.988770\n",
      "Train: step:  11240, time: 0.184, loss: 1874.087280\n",
      "Train: step:  11250, time: 0.208, loss: 1431.001953\n",
      "Train: step:  11260, time: 0.224, loss: 1384.856201\n",
      "Train: step:  11270, time: 0.188, loss: 2584.785400\n",
      "Train: step:  11280, time: 0.346, loss: 2061.237793\n",
      "Train: step:  11290, time: 0.184, loss: 2890.553467\n",
      "Train: step:  11300, time: 0.186, loss: 1727.235107\n",
      "Train: step:  11310, time: 0.204, loss: 1108.265015\n",
      "Train: step:  11320, time: 0.210, loss: 857.870544\n",
      "Train: step:  11330, time: 0.197, loss: 2479.914795\n",
      "Train: step:  11340, time: 0.188, loss: 1041.009644\n",
      "Train: step:  11350, time: 0.224, loss: 2035.168091\n",
      "Train: step:  11360, time: 0.197, loss: 1263.443604\n",
      "Train: step:  11370, time: 0.219, loss: 3250.219238\n",
      "Train: step:  11380, time: 0.207, loss: 1320.029175\n",
      "Train: step:  11390, time: 0.230, loss: 1674.183105\n",
      "Train: step:  11400, time: 0.193, loss: 1654.875122\n",
      "Train: step:  11410, time: 0.228, loss: 1803.348389\n",
      "Train: step:  11420, time: 0.188, loss: 2960.260254\n",
      "Train: step:  11430, time: 0.181, loss: 463.327179\n",
      "Train: step:  11440, time: 0.218, loss: 1737.722778\n",
      "Train: step:  11450, time: 0.190, loss: 917.021667\n",
      "Train: step:  11460, time: 0.186, loss: 3568.452881\n",
      "Train: step:  11470, time: 0.194, loss: 503.184540\n",
      "Train: step:  11480, time: 0.196, loss: 2155.213623\n",
      "Train: step:  11490, time: 0.188, loss: 1378.134521\n",
      "Train: step:  11500, time: 0.196, loss: 557.983215\n",
      "Train: step:  11510, time: 0.196, loss: 699.264587\n",
      "Train: step:  11520, time: 0.184, loss: 2837.868408\n",
      "Train: step:  11530, time: 0.186, loss: 371.413147\n",
      "Train: step:  11540, time: 0.217, loss: 2248.787109\n",
      "Train: step:  11550, time: 0.186, loss: 1408.082764\n",
      "Train: step:  11560, time: 0.180, loss: 1755.039795\n",
      "Train: step:  11570, time: 0.187, loss: 1788.986084\n",
      "Train: step:  11580, time: 0.179, loss: 1208.223755\n",
      "Train: step:  11590, time: 0.193, loss: 1538.140747\n",
      "Train: step:  11600, time: 0.190, loss: 828.063538\n",
      "Train: step:  11610, time: 0.229, loss: 1414.744873\n",
      "Train: step:  11620, time: 0.193, loss: 332.290894\n",
      "Train: step:  11630, time: 0.193, loss: 2733.020752\n",
      "Train: step:  11640, time: 0.188, loss: 4109.241699\n",
      "Train: step:  11650, time: 0.228, loss: 617.229309\n",
      "Train: step:  11660, time: 0.217, loss: 1763.186523\n",
      "Train: step:  11670, time: 0.187, loss: 1200.592896\n",
      "Train: step:  11680, time: 0.189, loss: 1149.877441\n",
      "Train: step:  11690, time: 0.191, loss: 3280.307129\n",
      "Train: step:  11700, time: 0.216, loss: 281.826874\n",
      "Train: step:  11710, time: 0.186, loss: 2410.689209\n",
      "Train: step:  11720, time: 0.245, loss: 2092.736572\n",
      "Train: step:  11730, time: 0.188, loss: 2056.725830\n",
      "Train: step:  11740, time: 0.247, loss: 1983.482910\n",
      "Train: step:  11750, time: 0.219, loss: 931.432861\n",
      "Train: step:  11760, time: 0.190, loss: 3442.595947\n",
      "Train: step:  11770, time: 0.196, loss: 1408.620117\n",
      "Train: step:  11780, time: 0.190, loss: 1474.747192\n",
      "Train: step:  11790, time: 0.190, loss: 605.798035\n",
      "Train: step:  11800, time: 0.207, loss: 2298.020508\n",
      "Train: step:  11810, time: 0.188, loss: 3147.714600\n",
      "Train: step:  11820, time: 0.229, loss: 2008.264893\n",
      "Train: step:  11830, time: 0.229, loss: 398.120422\n",
      "Train: step:  11840, time: 0.244, loss: 2734.007568\n",
      "Train: step:  11850, time: 0.176, loss: 1193.588501\n",
      "Train: step:  11860, time: 0.216, loss: 1448.420044\n",
      "Train: step:  11870, time: 0.216, loss: 759.151184\n",
      "Train: step:  11880, time: 0.235, loss: 1268.290771\n",
      "Train: step:  11890, time: 0.229, loss: 1080.005371\n",
      "Train: step:  11900, time: 0.217, loss: 1435.861572\n",
      "Train: step:  11910, time: 0.189, loss: 2800.092529\n",
      "Train: step:  11920, time: 0.182, loss: 2992.138916\n",
      "Train: step:  11930, time: 0.185, loss: 1184.780518\n",
      "Train: step:  11940, time: 0.230, loss: 1995.918457\n",
      "Train: step:  11950, time: 0.174, loss: 1558.819214\n",
      "Train: step:  11960, time: 0.204, loss: 1588.104126\n",
      "Train: step:  11970, time: 0.185, loss: 2350.808594\n",
      "Train: step:  11980, time: 0.184, loss: 1409.911621\n",
      "Train: step:  11990, time: 0.183, loss: 2273.643066\n",
      "Train: step:  12000, time: 0.238, loss: 2974.475586\n",
      "Train: step:  12010, time: 0.184, loss: 1456.346680\n",
      "Train: step:  12020, time: 0.199, loss: 2003.031250\n",
      "Train: step:  12030, time: 0.186, loss: 1327.607788\n",
      "Train: step:  12040, time: 0.195, loss: 3266.723145\n",
      "Train: step:  12050, time: 0.213, loss: 820.401367\n",
      "Train: step:  12060, time: 0.185, loss: 1312.555664\n",
      "Train: step:  12070, time: 0.226, loss: 2555.824463\n",
      "Train: step:  12080, time: 0.217, loss: 783.844238\n",
      "Train: step:  12090, time: 0.246, loss: 3124.186523\n",
      "Train: step:  12100, time: 0.229, loss: 2203.498291\n",
      "Train: step:  12110, time: 0.181, loss: 2980.426514\n",
      "Train: step:  12120, time: 0.184, loss: 2241.176025\n",
      "Train: step:  12130, time: 0.237, loss: 1849.811401\n",
      "Train: step:  12140, time: 0.228, loss: 893.229492\n",
      "Train: step:  12150, time: 0.197, loss: 1927.307983\n",
      "Train: step:  12160, time: 0.224, loss: 840.381287\n",
      "Train: step:  12170, time: 0.215, loss: 2083.885986\n",
      "Train: step:  12180, time: 0.179, loss: 2071.437256\n",
      "Train: step:  12190, time: 0.227, loss: 2638.635986\n",
      "Train: step:  12200, time: 0.192, loss: 3676.958496\n",
      "Train: step:  12210, time: 0.185, loss: 3371.031494\n",
      "Train: step:  12220, time: 0.189, loss: 738.197815\n",
      "Train: step:  12230, time: 0.236, loss: 2939.382568\n",
      "Train: step:  12240, time: 0.266, loss: 1825.396973\n",
      "Train: step:  12250, time: 0.188, loss: 1316.437622\n",
      "Train: step:  12260, time: 0.217, loss: 2129.300537\n",
      "Train: step:  12270, time: 0.190, loss: 2025.387451\n",
      "Train: step:  12280, time: 0.194, loss: 1459.394287\n",
      "Train: step:  12290, time: 0.189, loss: 2735.213867\n",
      "Train: step:  12300, time: 0.188, loss: 1826.015503\n",
      "Train: step:  12310, time: 0.202, loss: 1644.664429\n",
      "Train: step:  12320, time: 0.231, loss: 1444.837280\n",
      "Train: step:  12330, time: 0.214, loss: 2764.401367\n",
      "Train: step:  12340, time: 0.189, loss: 2003.905884\n",
      "Train: step:  12350, time: 0.219, loss: 2810.780762\n",
      "Train: step:  12360, time: 0.191, loss: 749.657532\n",
      "Train: step:  12370, time: 0.219, loss: 1221.651001\n",
      "Train: step:  12380, time: 0.191, loss: 659.417847\n",
      "Train: step:  12390, time: 0.190, loss: 1221.542236\n",
      "Train: step:  12400, time: 0.186, loss: 2561.831299\n",
      "Train: step:  12410, time: 0.198, loss: 2108.057373\n",
      "Train: step:  12420, time: 0.197, loss: 5181.117676\n",
      "Train: step:  12430, time: 0.252, loss: 813.569702\n",
      "Train: step:  12440, time: 0.217, loss: 2839.683838\n",
      "Train: step:  12450, time: 0.186, loss: 3078.985596\n",
      "Train: step:  12460, time: 0.192, loss: 3026.839111\n",
      "Train: step:  12470, time: 0.185, loss: 2123.976807\n",
      "Train: step:  12480, time: 0.187, loss: 1971.081177\n",
      "Train: step:  12490, time: 0.240, loss: 800.745850\n",
      "Train: step:  12500, time: 0.192, loss: 1167.337769\n",
      "Train: step:  12510, time: 0.179, loss: 1039.918945\n",
      "Train: step:  12520, time: 0.222, loss: 1960.998413\n",
      "Train: step:  12530, time: 0.233, loss: 1024.816040\n",
      "Train: step:  12540, time: 0.228, loss: 2511.511719\n",
      "Train: step:  12550, time: 0.228, loss: 1864.727783\n",
      "Train: step:  12560, time: 0.192, loss: 3856.743408\n",
      "Train: step:  12570, time: 0.217, loss: 1668.328735\n",
      "Train: step:  12580, time: 0.189, loss: 1999.188110\n",
      "Train: step:  12590, time: 0.199, loss: 711.200989\n",
      "Train: step:  12600, time: 0.197, loss: 1367.327148\n",
      "Train: step:  12610, time: 0.182, loss: 2689.749023\n",
      "Train: step:  12620, time: 0.190, loss: 1563.364502\n",
      "Train: step:  12630, time: 0.199, loss: 1323.341675\n",
      "Train: step:  12640, time: 0.190, loss: 2569.518799\n",
      "Train: step:  12650, time: 0.219, loss: 1032.812622\n",
      "Train: step:  12660, time: 0.229, loss: 201.268021\n",
      "Train: step:  12670, time: 0.193, loss: 2365.053955\n",
      "Train: step:  12680, time: 0.224, loss: 2447.713135\n",
      "Train: step:  12690, time: 0.189, loss: 1546.440796\n",
      "Train: step:  12700, time: 0.208, loss: 3491.048584\n",
      "Train: step:  12710, time: 0.240, loss: 759.161011\n",
      "Train: step:  12720, time: 0.217, loss: 598.039307\n",
      "Train: step:  12730, time: 0.187, loss: 756.595520\n",
      "Train: step:  12740, time: 0.216, loss: 989.902222\n",
      "Train: step:  12750, time: 0.178, loss: 2289.370605\n",
      "Train: step:  12760, time: 0.185, loss: 1565.849854\n",
      "Train: step:  12770, time: 0.181, loss: 1950.886841\n",
      "Train: step:  12780, time: 0.194, loss: 2119.466797\n",
      "Train: step:  12790, time: 0.188, loss: 2656.384033\n",
      "Train: step:  12800, time: 0.185, loss: 1873.735107\n",
      "Train: step:  12810, time: 0.192, loss: 3456.659912\n",
      "Train: step:  12820, time: 0.203, loss: 633.769165\n",
      "Train: step:  12830, time: 0.206, loss: 2558.506836\n",
      "Train: step:  12840, time: 0.232, loss: 779.090942\n",
      "Train: step:  12850, time: 0.226, loss: 2864.748047\n",
      "Train: step:  12860, time: 0.216, loss: 677.177246\n",
      "Train: step:  12870, time: 0.217, loss: 1098.731445\n",
      "Train: step:  12880, time: 0.197, loss: 2010.774536\n",
      "Train: step:  12890, time: 0.186, loss: 3303.266846\n",
      "Train: step:  12900, time: 0.195, loss: 1827.185303\n",
      "Train: step:  12910, time: 0.191, loss: 185.507690\n",
      "Train: step:  12920, time: 0.184, loss: 1515.387207\n",
      "Train: step:  12930, time: 0.232, loss: 3486.695068\n",
      "Train: step:  12940, time: 0.217, loss: 3124.141846\n",
      "Train: step:  12950, time: 0.236, loss: 3232.033203\n",
      "Train: step:  12960, time: 0.217, loss: 787.452759\n",
      "Train: step:  12970, time: 0.234, loss: 625.023376\n",
      "Train: step:  12980, time: 0.193, loss: 1155.947632\n",
      "Train: step:  12990, time: 0.217, loss: 1744.710815\n",
      "Train: step:  13000, time: 0.220, loss: 1954.111328\n",
      "Train: step:  13010, time: 0.227, loss: 1153.230225\n",
      "Train: step:  13020, time: 0.191, loss: 1343.543335\n",
      "Train: step:  13030, time: 0.196, loss: 829.610168\n",
      "Train: step:  13040, time: 0.216, loss: 2888.373291\n",
      "Train: step:  13050, time: 0.189, loss: 3410.438232\n",
      "Train: step:  13060, time: 0.203, loss: 2471.121338\n",
      "Train: step:  13070, time: 0.189, loss: 3046.298584\n",
      "Train: step:  13080, time: 0.198, loss: 2566.181885\n",
      "Train: step:  13090, time: 0.218, loss: 691.294922\n",
      "Train: step:  13100, time: 0.194, loss: 2144.783447\n",
      "Train: step:  13110, time: 0.229, loss: 1065.713501\n",
      "Train: step:  13120, time: 0.192, loss: 2359.288086\n",
      "Train: step:  13130, time: 0.191, loss: 1903.652710\n",
      "Train: step:  13140, time: 0.195, loss: 1763.685181\n",
      "Train: step:  13150, time: 0.194, loss: 1028.733032\n",
      "Train: step:  13160, time: 0.187, loss: 1399.375122\n",
      "Train: step:  13170, time: 0.212, loss: 552.307373\n",
      "Train: step:  13180, time: 0.187, loss: 2325.044189\n",
      "Train: step:  13190, time: 0.227, loss: 897.562622\n",
      "Train: step:  13200, time: 0.189, loss: 954.503906\n",
      "Train: step:  13210, time: 0.230, loss: 1380.613892\n",
      "Train: step:  13220, time: 0.217, loss: 1868.906860\n",
      "Train: step:  13230, time: 0.179, loss: 2567.428955\n",
      "Train: step:  13240, time: 0.187, loss: 3963.195068\n",
      "Train: step:  13250, time: 0.230, loss: 351.051941\n",
      "Train: step:  13260, time: 0.217, loss: 2345.692871\n",
      "Train: step:  13270, time: 0.215, loss: 2494.881348\n",
      "Train: step:  13280, time: 0.185, loss: 1985.021240\n",
      "Train: step:  13290, time: 0.192, loss: 1853.536987\n",
      "Train: step:  13300, time: 0.204, loss: 3194.108643\n",
      "Train: step:  13310, time: 0.204, loss: 252.721161\n",
      "Train: step:  13320, time: 0.190, loss: 1414.119385\n",
      "Train: step:  13330, time: 0.189, loss: 1687.271973\n",
      "Train: step:  13340, time: 0.186, loss: 1869.447388\n",
      "Train: step:  13350, time: 0.195, loss: 765.444153\n",
      "Train: step:  13360, time: 0.215, loss: 1493.439819\n",
      "Train: step:  13370, time: 0.223, loss: 1237.553711\n",
      "Train: step:  13380, time: 0.231, loss: 1527.113281\n",
      "Train: step:  13390, time: 0.188, loss: 681.491211\n",
      "Train: step:  13400, time: 0.211, loss: 1660.096313\n",
      "Train: step:  13410, time: 0.186, loss: 1456.584229\n",
      "Train: step:  13420, time: 0.184, loss: 1856.090332\n",
      "Train: step:  13430, time: 0.197, loss: 998.331909\n",
      "Train: step:  13440, time: 0.232, loss: 1728.987061\n",
      "Train: step:  13450, time: 0.217, loss: 2042.172119\n",
      "Train: step:  13460, time: 0.184, loss: 2886.306396\n",
      "Train: step:  13470, time: 0.194, loss: 2907.700684\n",
      "Train: step:  13480, time: 0.189, loss: 2587.650635\n",
      "Train: step:  13490, time: 0.193, loss: 466.294250\n",
      "Train: step:  13500, time: 0.187, loss: 1723.302856\n",
      "Train: step:  13510, time: 0.215, loss: 1890.501831\n",
      "Train: step:  13520, time: 0.199, loss: 637.223511\n",
      "Train: step:  13530, time: 0.195, loss: 2698.818359\n",
      "Train: step:  13540, time: 0.188, loss: 3606.027344\n",
      "Train: step:  13550, time: 0.239, loss: 2332.027832\n",
      "Train: step:  13560, time: 0.251, loss: 1268.124023\n",
      "Train: step:  13570, time: 0.217, loss: 1961.091309\n",
      "Train: step:  13580, time: 0.186, loss: 2150.900391\n",
      "Train: step:  13590, time: 0.216, loss: 1203.371460\n",
      "Train: step:  13600, time: 0.217, loss: 1722.897705\n",
      "Train: step:  13610, time: 0.206, loss: 3099.723389\n",
      "Train: step:  13620, time: 0.248, loss: 1324.122192\n",
      "Train: step:  13630, time: 0.241, loss: 1555.439819\n",
      "Train: step:  13640, time: 0.223, loss: 3060.248779\n",
      "Train: step:  13650, time: 0.238, loss: 2797.369141\n",
      "Train: step:  13660, time: 0.192, loss: 2597.864014\n",
      "Train: step:  13670, time: 0.236, loss: 1726.385986\n",
      "Train: step:  13680, time: 0.190, loss: 2390.296875\n",
      "Train: step:  13690, time: 0.195, loss: 2415.897949\n",
      "Train: step:  13700, time: 0.227, loss: 2202.759277\n",
      "Train: step:  13710, time: 0.190, loss: 1038.010864\n",
      "Train: step:  13720, time: 0.244, loss: 417.519928\n",
      "Train: step:  13730, time: 0.231, loss: 2341.163818\n",
      "Train: step:  13740, time: 0.189, loss: 1840.053833\n",
      "Train: step:  13750, time: 0.184, loss: 2426.760742\n",
      "Train: step:  13760, time: 0.229, loss: 2373.954590\n",
      "Train: step:  13770, time: 0.182, loss: 1271.413086\n",
      "Train: step:  13780, time: 0.182, loss: 1953.211304\n",
      "Train: step:  13790, time: 0.201, loss: 609.080994\n",
      "Train: step:  13800, time: 0.229, loss: 2508.306152\n",
      "Train: step:  13810, time: 0.231, loss: 958.006165\n",
      "Train: step:  13820, time: 0.217, loss: 1381.860474\n",
      "Train: step:  13830, time: 0.209, loss: 561.743713\n",
      "Train: step:  13840, time: 0.189, loss: 244.954300\n",
      "Train: step:  13850, time: 0.218, loss: 3011.731934\n",
      "Train: step:  13860, time: 0.182, loss: 2201.955322\n",
      "Train: step:  13870, time: 0.231, loss: 1634.120361\n",
      "Train: step:  13880, time: 0.192, loss: 1696.219971\n",
      "Train: step:  13890, time: 0.191, loss: 2457.414062\n",
      "Train: step:  13900, time: 0.188, loss: 3089.252197\n",
      "Train: step:  13910, time: 0.189, loss: 2511.505859\n",
      "Train: step:  13920, time: 0.231, loss: 1579.711182\n",
      "Train: step:  13930, time: 0.203, loss: 1177.613647\n",
      "Train: step:  13940, time: 0.190, loss: 2575.171875\n",
      "Train: step:  13950, time: 0.193, loss: 2711.949219\n",
      "Train: step:  13960, time: 0.183, loss: 1112.235229\n",
      "Train: step:  13970, time: 0.257, loss: 684.828552\n",
      "Train: step:  13980, time: 0.240, loss: 1985.360718\n",
      "Train: step:  13990, time: 0.240, loss: 1721.967041\n",
      "Train: step:  14000, time: 0.194, loss: 3717.395508\n",
      "Train: step:  14010, time: 0.185, loss: 2083.959961\n",
      "Train: step:  14020, time: 0.187, loss: 2197.753906\n",
      "Train: step:  14030, time: 0.188, loss: 2216.077637\n",
      "Train: step:  14040, time: 0.186, loss: 1423.715454\n",
      "Train: step:  14050, time: 0.185, loss: 965.463135\n",
      "Train: step:  14060, time: 0.192, loss: 2600.832764\n",
      "Train: step:  14070, time: 0.179, loss: 2511.206787\n",
      "Train: step:  14080, time: 0.184, loss: 3323.805664\n",
      "Train: step:  14090, time: 0.187, loss: 3235.161865\n",
      "Train: step:  14100, time: 0.182, loss: 1970.834473\n",
      "Train: step:  14110, time: 0.203, loss: 347.307587\n",
      "Train: step:  14120, time: 0.186, loss: 3700.202881\n",
      "Train: step:  14130, time: 0.181, loss: 2341.774170\n",
      "Train: step:  14140, time: 0.191, loss: 1407.366455\n",
      "Train: step:  14150, time: 0.226, loss: 3253.329102\n",
      "Train: step:  14160, time: 0.226, loss: 2672.717041\n",
      "Train: step:  14170, time: 0.196, loss: 2122.184570\n",
      "Train: step:  14180, time: 0.226, loss: 1571.776367\n",
      "Train: step:  14190, time: 0.182, loss: 377.512482\n",
      "Train: step:  14200, time: 0.200, loss: 1157.322388\n",
      "Train: step:  14210, time: 0.217, loss: 1345.494507\n",
      "Train: step:  14220, time: 0.184, loss: 835.159363\n",
      "Train: step:  14230, time: 0.218, loss: 944.882568\n",
      "Train: step:  14240, time: 0.187, loss: 2166.983154\n",
      "Train: step:  14250, time: 0.217, loss: 1293.205933\n",
      "Train: step:  14260, time: 0.192, loss: 1351.234741\n",
      "Train: step:  14270, time: 0.191, loss: 3525.739258\n",
      "Train: step:  14280, time: 0.186, loss: 2109.595947\n",
      "Train: step:  14290, time: 0.187, loss: 1438.318726\n",
      "Train: step:  14300, time: 0.234, loss: 2155.154053\n",
      "Train: step:  14310, time: 0.227, loss: 972.049866\n",
      "Train: step:  14320, time: 0.184, loss: 2096.870117\n",
      "Train: step:  14330, time: 0.217, loss: 1138.238892\n",
      "Train: step:  14340, time: 0.220, loss: 2646.319092\n",
      "Train: step:  14350, time: 0.201, loss: 2800.673828\n",
      "Train: step:  14360, time: 0.211, loss: 1788.504272\n",
      "Train: step:  14370, time: 0.285, loss: 839.919006\n",
      "Train: step:  14380, time: 0.232, loss: 1356.241333\n",
      "Train: step:  14390, time: 0.264, loss: 2327.563965\n",
      "Train: step:  14400, time: 0.202, loss: 2910.649902\n",
      "Train: step:  14410, time: 0.191, loss: 1645.217041\n",
      "Train: step:  14420, time: 0.211, loss: 1770.992676\n",
      "Train: step:  14430, time: 0.200, loss: 995.924133\n",
      "Train: step:  14440, time: 0.216, loss: 3000.432861\n",
      "Train: step:  14450, time: 0.231, loss: 1387.838135\n",
      "Train: step:  14460, time: 0.230, loss: 406.371399\n",
      "Train: step:  14470, time: 0.184, loss: 350.420807\n",
      "Train: step:  14480, time: 0.182, loss: 1556.676147\n",
      "Train: step:  14490, time: 0.230, loss: 1013.722900\n",
      "Train: step:  14500, time: 0.212, loss: 2461.140381\n",
      "Train: step:  14510, time: 0.191, loss: 2398.123535\n",
      "Train: step:  14520, time: 0.220, loss: 1887.573730\n",
      "Train: step:  14530, time: 0.217, loss: 1663.391724\n",
      "Train: step:  14540, time: 0.198, loss: 2045.563599\n",
      "Train: step:  14550, time: 0.217, loss: 3145.292480\n",
      "Train: step:  14560, time: 0.190, loss: 1513.743896\n",
      "Train: step:  14570, time: 0.231, loss: 2378.107178\n",
      "Train: step:  14580, time: 0.231, loss: 2423.333984\n",
      "Train: step:  14590, time: 0.247, loss: 786.857605\n",
      "Train: step:  14600, time: 0.188, loss: 1303.686768\n",
      "Train: step:  14610, time: 0.258, loss: 3089.333740\n",
      "Train: step:  14620, time: 0.182, loss: 2741.999756\n",
      "Train: step:  14630, time: 0.183, loss: 1864.365967\n",
      "Train: step:  14640, time: 0.231, loss: 1141.398682\n",
      "Train: step:  14650, time: 0.189, loss: 1484.521362\n",
      "Train: step:  14660, time: 0.215, loss: 3136.076904\n",
      "Train: step:  14670, time: 0.182, loss: 1036.286133\n",
      "Train: step:  14680, time: 0.182, loss: 3682.816162\n",
      "Train: step:  14690, time: 0.189, loss: 3077.274902\n",
      "Train: step:  14700, time: 0.184, loss: 3831.207764\n",
      "Train: step:  14710, time: 0.183, loss: 3213.121338\n",
      "Train: step:  14720, time: 0.231, loss: 3330.549805\n",
      "Train: step:  14730, time: 0.194, loss: 598.820984\n",
      "Train: step:  14740, time: 0.193, loss: 1492.919800\n",
      "Train: step:  14750, time: 0.216, loss: 969.714905\n",
      "Train: step:  14760, time: 0.213, loss: 2574.262939\n",
      "Train: step:  14770, time: 0.192, loss: 1397.223145\n",
      "Train: step:  14780, time: 0.217, loss: 1062.579468\n",
      "Train: step:  14790, time: 0.185, loss: 1297.360352\n",
      "Train: step:  14800, time: 0.210, loss: 1603.571411\n",
      "Train: step:  14810, time: 0.227, loss: 1926.912231\n",
      "Train: step:  14820, time: 0.187, loss: 2455.540527\n",
      "Train: step:  14830, time: 0.188, loss: 1635.458130\n",
      "Train: step:  14840, time: 0.189, loss: 2678.588623\n",
      "Train: step:  14850, time: 0.199, loss: 3707.380859\n",
      "Train: step:  14860, time: 0.190, loss: 3395.014404\n",
      "Train: step:  14870, time: 0.186, loss: 591.174500\n",
      "Train: step:  14880, time: 0.188, loss: 2042.570068\n",
      "Train: step:  14890, time: 0.233, loss: 833.400635\n",
      "Train: step:  14900, time: 0.215, loss: 988.167542\n",
      "Train: step:  14910, time: 0.187, loss: 2923.858887\n",
      "Train: step:  14920, time: 0.217, loss: 370.276031\n",
      "Train: step:  14930, time: 0.213, loss: 2491.652344\n",
      "Train: step:  14940, time: 0.184, loss: 3268.251709\n",
      "Train: step:  14950, time: 0.182, loss: 1096.710938\n",
      "Train: step:  14960, time: 0.246, loss: 2855.010742\n",
      "Train: step:  14970, time: 0.179, loss: 2899.200195\n",
      "Train: step:  14980, time: 0.185, loss: 1295.847778\n",
      "Train: step:  14990, time: 0.184, loss: 253.036316\n",
      "Train: step:  15000, time: 0.185, loss: 2993.955078\n",
      "Train: step:  15010, time: 0.233, loss: 3114.871826\n",
      "Train: step:  15020, time: 0.195, loss: 665.047913\n",
      "Train: step:  15030, time: 0.185, loss: 772.261536\n",
      "Train: step:  15040, time: 0.192, loss: 1683.143555\n",
      "Train: step:  15050, time: 0.227, loss: 1368.151001\n",
      "Train: step:  15060, time: 0.197, loss: 1408.932861\n",
      "Train: step:  15070, time: 0.231, loss: 3591.683594\n",
      "Train: step:  15080, time: 0.206, loss: 1956.425049\n",
      "Train: step:  15090, time: 0.236, loss: 1256.825684\n",
      "Train: step:  15100, time: 0.193, loss: 457.620178\n",
      "Train: step:  15110, time: 0.191, loss: 4077.516357\n",
      "Train: step:  15120, time: 0.185, loss: 414.774261\n",
      "Train: step:  15130, time: 0.197, loss: 3329.701172\n",
      "Train: step:  15140, time: 0.216, loss: 1573.661133\n",
      "Train: step:  15150, time: 0.240, loss: 2034.662720\n",
      "Train: step:  15160, time: 0.186, loss: 976.266846\n",
      "Train: step:  15170, time: 0.185, loss: 2273.433594\n",
      "Train: step:  15180, time: 0.191, loss: 2792.309814\n",
      "Train: step:  15190, time: 0.180, loss: 2735.482666\n",
      "Train: step:  15200, time: 0.217, loss: 1864.062622\n",
      "Train: step:  15210, time: 0.192, loss: 1662.054688\n",
      "Train: step:  15220, time: 0.216, loss: 2238.208496\n",
      "Train: step:  15230, time: 0.191, loss: 1736.498779\n",
      "Train: step:  15240, time: 0.195, loss: 1986.240479\n",
      "Train: step:  15250, time: 0.180, loss: 3467.847412\n",
      "Train: step:  15260, time: 0.185, loss: 1312.640991\n",
      "Train: step:  15270, time: 0.227, loss: 1978.679565\n",
      "Train: step:  15280, time: 0.192, loss: 1162.837158\n",
      "Train: step:  15290, time: 0.260, loss: 1835.576538\n",
      "Train: step:  15300, time: 0.187, loss: 584.945618\n",
      "Train: step:  15310, time: 0.187, loss: 2264.692383\n",
      "Train: step:  15320, time: 0.214, loss: 553.225281\n",
      "Train: step:  15330, time: 0.217, loss: 217.331329\n",
      "Train: step:  15340, time: 0.234, loss: 2297.553711\n",
      "Train: step:  15350, time: 0.191, loss: 1621.942505\n",
      "Train: step:  15360, time: 0.191, loss: 2944.781006\n",
      "Train: step:  15370, time: 0.229, loss: 3052.322266\n",
      "Train: step:  15380, time: 0.194, loss: 2617.525879\n",
      "Train: step:  15390, time: 0.216, loss: 471.905090\n",
      "Train: step:  15400, time: 0.231, loss: 1253.366455\n",
      "Train: step:  15410, time: 0.193, loss: 3006.440186\n",
      "Train: step:  15420, time: 0.189, loss: 1501.505737\n",
      "Train: step:  15430, time: 0.217, loss: 3032.531250\n",
      "Train: step:  15440, time: 0.232, loss: 5324.774414\n",
      "Train: step:  15450, time: 0.200, loss: 2417.791992\n",
      "Train: step:  15460, time: 0.216, loss: 664.732422\n",
      "Train: step:  15470, time: 0.192, loss: 2657.067383\n",
      "Train: step:  15480, time: 0.211, loss: 1734.392700\n",
      "Train: step:  15490, time: 0.186, loss: 1135.409546\n",
      "Train: step:  15500, time: 0.190, loss: 393.058624\n",
      "Train: step:  15510, time: 0.225, loss: 3297.405762\n",
      "Train: step:  15520, time: 0.220, loss: 458.961548\n",
      "Train: step:  15530, time: 0.216, loss: 611.314209\n",
      "Train: step:  15540, time: 0.187, loss: 878.620300\n",
      "Train: step:  15550, time: 0.191, loss: 599.164734\n",
      "Train: step:  15560, time: 0.187, loss: 1444.093384\n",
      "Train: step:  15570, time: 0.251, loss: 1639.389282\n",
      "Train: step:  15580, time: 0.187, loss: 2892.082520\n",
      "Train: step:  15590, time: 0.198, loss: 4044.806396\n",
      "Train: step:  15600, time: 0.185, loss: 478.684387\n",
      "Train: step:  15610, time: 0.195, loss: 686.918701\n",
      "Train: step:  15620, time: 0.217, loss: 1043.006958\n",
      "Train: step:  15630, time: 0.196, loss: 684.993286\n",
      "Train: step:  15640, time: 0.189, loss: 1588.612671\n",
      "Train: step:  15650, time: 0.188, loss: 2138.206787\n",
      "Train: step:  15660, time: 0.231, loss: 1166.225708\n",
      "Train: step:  15670, time: 0.246, loss: 605.079712\n",
      "Train: step:  15680, time: 0.261, loss: 2777.367920\n",
      "Train: step:  15690, time: 0.190, loss: 1455.772339\n",
      "Train: step:  15700, time: 0.190, loss: 2692.614014\n",
      "Train: step:  15710, time: 0.225, loss: 1506.522583\n",
      "Train: step:  15720, time: 0.186, loss: 360.256226\n",
      "Train: step:  15730, time: 0.216, loss: 1355.399170\n",
      "Train: step:  15740, time: 0.190, loss: 384.237457\n",
      "Train: step:  15750, time: 0.248, loss: 480.980438\n",
      "Train: step:  15760, time: 0.230, loss: 886.491028\n",
      "Train: step:  15770, time: 0.194, loss: 2781.929443\n",
      "Train: step:  15780, time: 0.217, loss: 3067.471680\n",
      "Train: step:  15790, time: 0.188, loss: 615.049255\n",
      "Train: step:  15800, time: 0.189, loss: 1853.322510\n",
      "Train: step:  15810, time: 0.218, loss: 2356.436279\n",
      "Train: step:  15820, time: 0.201, loss: 1379.372681\n",
      "Train: step:  15830, time: 0.229, loss: 1793.488403\n",
      "Train: step:  15840, time: 0.189, loss: 2546.215820\n",
      "Train: step:  15850, time: 0.219, loss: 1838.055298\n",
      "Train: step:  15860, time: 0.195, loss: 421.771576\n",
      "Train: step:  15870, time: 0.186, loss: 1367.716919\n",
      "Train: step:  15880, time: 0.194, loss: 829.212708\n",
      "Train: step:  15890, time: 0.229, loss: 1203.023071\n",
      "Train: step:  15900, time: 0.228, loss: 2565.467285\n",
      "Train: step:  15910, time: 0.191, loss: 3065.320801\n",
      "Train: step:  15920, time: 0.193, loss: 950.412415\n",
      "Train: step:  15930, time: 0.226, loss: 2811.518555\n",
      "Train: step:  15940, time: 0.217, loss: 595.378296\n",
      "Train: step:  15950, time: 0.195, loss: 1371.057251\n",
      "Train: step:  15960, time: 0.219, loss: 1363.429321\n",
      "Train: step:  15970, time: 0.189, loss: 1830.348389\n",
      "Train: step:  15980, time: 0.193, loss: 545.308594\n",
      "Train: step:  15990, time: 0.188, loss: 262.585327\n",
      "Train: step:  16000, time: 0.225, loss: 3502.853760\n",
      "Train: step:  16010, time: 0.217, loss: 2132.984131\n",
      "Train: step:  16020, time: 0.217, loss: 1093.787476\n",
      "Train: step:  16030, time: 0.228, loss: 938.412109\n",
      "Train: step:  16040, time: 0.196, loss: 1145.909912\n",
      "Train: step:  16050, time: 0.186, loss: 1187.283936\n",
      "Train: step:  16060, time: 0.278, loss: 2734.067383\n",
      "Train: step:  16070, time: 0.191, loss: 1006.875305\n",
      "Train: step:  16080, time: 0.183, loss: 1200.665405\n",
      "Train: step:  16090, time: 0.216, loss: 1363.684937\n",
      "Train: step:  16100, time: 0.185, loss: 990.791321\n",
      "Train: step:  16110, time: 0.216, loss: 782.599060\n",
      "Train: step:  16120, time: 0.184, loss: 1139.579956\n",
      "Train: step:  16130, time: 0.216, loss: 1856.561890\n",
      "Train: step:  16140, time: 0.225, loss: 2103.593262\n",
      "Train: step:  16150, time: 0.215, loss: 1322.410522\n",
      "Train: step:  16160, time: 0.193, loss: 2370.267822\n",
      "Train: step:  16170, time: 0.224, loss: 1228.381104\n",
      "Train: step:  16180, time: 0.216, loss: 2446.945068\n",
      "Train: step:  16190, time: 0.188, loss: 2434.279053\n",
      "Train: step:  16200, time: 0.192, loss: 1577.838501\n",
      "Train: step:  16210, time: 0.214, loss: 1665.674194\n",
      "Train: step:  16220, time: 0.188, loss: 2046.510864\n",
      "Train: step:  16230, time: 0.185, loss: 1897.879395\n",
      "Train: step:  16240, time: 0.208, loss: 1890.177856\n",
      "Train: step:  16250, time: 0.183, loss: 568.888977\n",
      "Train: step:  16260, time: 0.197, loss: 2298.502686\n",
      "Train: step:  16270, time: 0.196, loss: 2033.967163\n",
      "Train: step:  16280, time: 0.200, loss: 911.401917\n",
      "Train: step:  16290, time: 0.189, loss: 601.861267\n",
      "Train: step:  16300, time: 0.193, loss: 2382.056396\n",
      "Train: step:  16310, time: 0.193, loss: 990.769104\n",
      "Train: step:  16320, time: 0.204, loss: 1246.699707\n",
      "Train: step:  16330, time: 0.187, loss: 1766.535889\n",
      "Train: step:  16340, time: 0.219, loss: 545.335083\n",
      "Train: step:  16350, time: 0.216, loss: 2775.338135\n",
      "Train: step:  16360, time: 0.183, loss: 2665.602539\n",
      "Train: step:  16370, time: 0.231, loss: 3022.977295\n",
      "Train: step:  16380, time: 0.181, loss: 646.848999\n",
      "Train: step:  16390, time: 0.190, loss: 1036.398193\n",
      "Train: step:  16400, time: 0.190, loss: 1626.241577\n",
      "Train: step:  16410, time: 0.234, loss: 1690.352783\n",
      "Train: step:  16420, time: 0.218, loss: 685.541931\n",
      "Train: step:  16430, time: 0.192, loss: 2378.748291\n",
      "Train: step:  16440, time: 0.189, loss: 3310.414551\n",
      "Train: step:  16450, time: 0.249, loss: 2020.869629\n",
      "Train: step:  16460, time: 0.250, loss: 3701.406494\n",
      "Train: step:  16470, time: 0.194, loss: 1593.893188\n",
      "Train: step:  16480, time: 0.182, loss: 1946.187622\n",
      "Train: step:  16490, time: 0.182, loss: 1607.400391\n",
      "Train: step:  16500, time: 0.192, loss: 1600.115723\n",
      "Train: step:  16510, time: 0.187, loss: 3614.577148\n",
      "Train: step:  16520, time: 0.184, loss: 2040.530640\n",
      "Train: step:  16530, time: 0.186, loss: 2250.380615\n",
      "Train: step:  16540, time: 0.193, loss: 1486.584473\n",
      "Train: step:  16550, time: 0.220, loss: 1599.061035\n",
      "Train: step:  16560, time: 0.181, loss: 1593.984009\n",
      "Train: step:  16570, time: 0.187, loss: 851.362488\n",
      "Train: step:  16580, time: 0.216, loss: 1833.972290\n",
      "Train: step:  16590, time: 0.191, loss: 1622.162598\n",
      "Train: step:  16600, time: 0.190, loss: 1098.001343\n",
      "Train: step:  16610, time: 0.185, loss: 1118.549194\n",
      "Train: step:  16620, time: 0.229, loss: 1465.144287\n",
      "Train: step:  16630, time: 0.184, loss: 668.036316\n",
      "Train: step:  16640, time: 0.189, loss: 1320.372559\n",
      "Train: step:  16650, time: 0.195, loss: 922.933289\n",
      "Train: step:  16660, time: 0.222, loss: 1032.283081\n",
      "Train: step:  16670, time: 0.215, loss: 1022.099487\n",
      "Train: step:  16680, time: 0.232, loss: 2374.950439\n",
      "Train: step:  16690, time: 0.195, loss: 2317.551025\n",
      "Train: step:  16700, time: 0.206, loss: 1922.295654\n",
      "Train: step:  16710, time: 0.237, loss: 1167.913818\n",
      "Train: step:  16720, time: 0.185, loss: 1466.336670\n",
      "Train: step:  16730, time: 0.189, loss: 2965.042969\n",
      "Train: step:  16740, time: 0.187, loss: 2061.325928\n",
      "Train: step:  16750, time: 0.198, loss: 2889.857178\n",
      "Train: step:  16760, time: 0.238, loss: 3470.979492\n",
      "Train: step:  16770, time: 0.228, loss: 628.332947\n",
      "Train: step:  16780, time: 0.189, loss: 1883.148071\n",
      "Train: step:  16790, time: 0.189, loss: 3706.568604\n",
      "Train: step:  16800, time: 0.218, loss: 2251.974609\n",
      "Train: step:  16810, time: 0.223, loss: 1044.856323\n",
      "Train: step:  16820, time: 0.217, loss: 972.781555\n",
      "Train: step:  16830, time: 0.181, loss: 2819.607910\n",
      "Train: step:  16840, time: 0.183, loss: 1629.946045\n",
      "Train: step:  16850, time: 0.190, loss: 2902.369873\n",
      "Train: step:  16860, time: 0.191, loss: 768.620728\n",
      "Train: step:  16870, time: 0.189, loss: 2597.094238\n",
      "Train: step:  16880, time: 0.178, loss: 2497.207031\n",
      "Train: step:  16890, time: 0.236, loss: 1029.734985\n",
      "Train: step:  16900, time: 0.183, loss: 470.315308\n",
      "Train: step:  16910, time: 0.216, loss: 1827.475098\n",
      "Train: step:  16920, time: 0.231, loss: 2554.486084\n",
      "Train: step:  16930, time: 0.188, loss: 2249.940186\n",
      "Train: step:  16940, time: 0.188, loss: 1428.736206\n",
      "Train: step:  16950, time: 0.185, loss: 578.432678\n",
      "Train: step:  16960, time: 0.224, loss: 712.862366\n",
      "Train: step:  16970, time: 0.187, loss: 3121.511230\n",
      "Train: step:  16980, time: 0.187, loss: 563.685791\n",
      "Train: step:  16990, time: 0.197, loss: 2757.504883\n",
      "Train: step:  17000, time: 0.216, loss: 1513.781250\n",
      "Train: step:  17010, time: 0.187, loss: 1213.635498\n",
      "Train: step:  17020, time: 0.197, loss: 2526.427002\n",
      "Train: step:  17030, time: 0.227, loss: 2422.300537\n",
      "Train: step:  17040, time: 0.215, loss: 1344.221680\n",
      "Train: step:  17050, time: 0.199, loss: 1236.627441\n",
      "Train: step:  17060, time: 0.188, loss: 3039.236328\n",
      "Train: step:  17070, time: 0.183, loss: 2745.050293\n",
      "Train: step:  17080, time: 0.191, loss: 2229.085938\n",
      "Train: step:  17090, time: 0.202, loss: 2106.040527\n",
      "Train: step:  17100, time: 0.259, loss: 1393.450562\n",
      "Train: step:  17110, time: 0.252, loss: 656.301086\n",
      "Train: step:  17120, time: 0.235, loss: 3081.167480\n",
      "Train: step:  17130, time: 0.218, loss: 1159.336792\n",
      "Train: step:  17140, time: 0.231, loss: 2885.883057\n",
      "Train: step:  17150, time: 0.226, loss: 7798.805664\n",
      "Train: step:  17160, time: 0.215, loss: 2658.134277\n",
      "Train: step:  17170, time: 0.200, loss: 612.105408\n",
      "Train: step:  17180, time: 0.196, loss: 3394.219727\n",
      "Train: step:  17190, time: 0.229, loss: 2563.678223\n",
      "Train: step:  17200, time: 0.243, loss: 1137.667358\n",
      "Train: step:  17210, time: 0.217, loss: 372.068146\n",
      "Train: step:  17220, time: 0.221, loss: 1858.854248\n",
      "Train: step:  17230, time: 0.216, loss: 2102.917480\n",
      "Train: step:  17240, time: 0.214, loss: 1784.536377\n",
      "Train: step:  17250, time: 0.194, loss: 948.720520\n",
      "Train: step:  17260, time: 0.229, loss: 2035.556274\n",
      "Train: step:  17270, time: 0.217, loss: 1307.260864\n",
      "Train: step:  17280, time: 0.222, loss: 1471.940796\n",
      "Train: step:  17290, time: 0.217, loss: 1562.335327\n",
      "Train: step:  17300, time: 0.213, loss: 566.192749\n",
      "Train: step:  17310, time: 0.196, loss: 2565.589600\n",
      "Train: step:  17320, time: 0.186, loss: 2315.395264\n",
      "Train: step:  17330, time: 0.239, loss: 2217.325439\n",
      "Train: step:  17340, time: 0.203, loss: 1904.894287\n",
      "Train: step:  17350, time: 0.193, loss: 1608.273926\n",
      "Train: step:  17360, time: 0.184, loss: 1715.188965\n",
      "Train: step:  17370, time: 0.191, loss: 2246.254639\n",
      "Train: step:  17380, time: 0.193, loss: 1931.824707\n",
      "Train: step:  17390, time: 0.200, loss: 1501.661499\n",
      "Train: step:  17400, time: 0.182, loss: 1176.610962\n",
      "Train: step:  17410, time: 0.217, loss: 3003.265625\n",
      "Train: step:  17420, time: 0.186, loss: 1698.115112\n",
      "Train: step:  17430, time: 0.218, loss: 2092.277832\n",
      "Train: step:  17440, time: 0.189, loss: 744.603394\n",
      "Train: step:  17450, time: 0.185, loss: 3326.505371\n",
      "Train: step:  17460, time: 0.193, loss: 1422.226440\n",
      "Train: step:  17470, time: 0.187, loss: 2542.257812\n",
      "Train: step:  17480, time: 0.185, loss: 2808.953857\n",
      "Train: step:  17490, time: 0.187, loss: 2088.893066\n",
      "Train: step:  17500, time: 0.227, loss: 2869.622803\n",
      "Train: step:  17510, time: 0.189, loss: 1800.578125\n",
      "Train: step:  17520, time: 0.218, loss: 1698.356201\n",
      "Train: step:  17530, time: 0.229, loss: 444.733429\n",
      "Train: step:  17540, time: 0.189, loss: 1614.662476\n",
      "Train: step:  17550, time: 0.234, loss: 1637.803711\n",
      "Train: step:  17560, time: 0.185, loss: 1637.880127\n",
      "Train: step:  17570, time: 0.241, loss: 1974.066040\n",
      "Train: step:  17580, time: 0.219, loss: 2515.850586\n",
      "Train: step:  17590, time: 0.189, loss: 3217.567383\n",
      "Train: step:  17600, time: 0.191, loss: 1207.454712\n",
      "Train: step:  17610, time: 0.189, loss: 1933.603516\n",
      "Train: step:  17620, time: 0.227, loss: 2272.993652\n",
      "Train: step:  17630, time: 0.223, loss: 2560.248535\n",
      "Train: step:  17640, time: 0.224, loss: 2351.317871\n",
      "Train: step:  17650, time: 0.213, loss: 308.421326\n",
      "Train: step:  17660, time: 0.188, loss: 1372.333618\n",
      "Train: step:  17670, time: 0.220, loss: 1238.019775\n",
      "Train: step:  17680, time: 0.183, loss: 1941.154785\n",
      "Train: step:  17690, time: 0.229, loss: 1848.538086\n",
      "Train: step:  17700, time: 0.194, loss: 1964.113525\n",
      "Train: step:  17710, time: 0.217, loss: 2557.382812\n",
      "Train: step:  17720, time: 0.192, loss: 1764.681030\n",
      "Train: step:  17730, time: 0.186, loss: 2225.572266\n",
      "Train: step:  17740, time: 0.185, loss: 1671.219482\n",
      "Train: step:  17750, time: 0.193, loss: 309.531555\n",
      "Train: step:  17760, time: 0.198, loss: 1825.213013\n",
      "Train: step:  17770, time: 0.187, loss: 2685.418701\n",
      "Train: step:  17780, time: 0.188, loss: 2982.471191\n",
      "Train: step:  17790, time: 0.232, loss: 2499.835938\n",
      "Train: step:  17800, time: 0.216, loss: 2026.351562\n",
      "Train: step:  17810, time: 0.216, loss: 2096.390381\n",
      "Train: step:  17820, time: 0.192, loss: 2082.286377\n",
      "Train: step:  17830, time: 0.189, loss: 3328.733643\n",
      "Train: step:  17840, time: 0.230, loss: 1692.551758\n",
      "Train: step:  17850, time: 0.215, loss: 2317.424561\n",
      "Train: step:  17860, time: 0.237, loss: 2416.590088\n",
      "Train: step:  17870, time: 0.223, loss: 1350.893555\n",
      "Train: step:  17880, time: 0.216, loss: 342.465698\n",
      "Train: step:  17890, time: 0.189, loss: 3143.081787\n",
      "Train: step:  17900, time: 0.201, loss: 371.909668\n",
      "Train: step:  17910, time: 0.189, loss: 2064.010986\n",
      "Train: step:  17920, time: 0.229, loss: 2198.865967\n",
      "Train: step:  17930, time: 0.220, loss: 3242.683350\n",
      "Train: step:  17940, time: 0.197, loss: 1873.152710\n",
      "Train: step:  17950, time: 0.227, loss: 2401.478760\n",
      "Train: step:  17960, time: 0.227, loss: 2022.737183\n",
      "Train: step:  17970, time: 0.185, loss: 1217.295166\n",
      "Train: step:  17980, time: 0.182, loss: 2339.060059\n",
      "Train: step:  17990, time: 0.228, loss: 2210.564941\n",
      "Train: step:  18000, time: 0.231, loss: 1182.771118\n",
      "Train: step:  18010, time: 0.224, loss: 1531.344238\n",
      "Train: step:  18020, time: 0.189, loss: 1366.882324\n",
      "Train: step:  18030, time: 0.242, loss: 2272.577881\n",
      "Train: step:  18040, time: 0.191, loss: 649.594910\n",
      "Train: step:  18050, time: 0.253, loss: 1638.304932\n",
      "Train: step:  18060, time: 0.189, loss: 1295.860474\n",
      "Train: step:  18070, time: 0.192, loss: 1298.971436\n",
      "Train: step:  18080, time: 0.229, loss: 815.148621\n",
      "Train: step:  18090, time: 0.184, loss: 3597.179688\n",
      "Train: step:  18100, time: 0.201, loss: 1714.373413\n",
      "Train: step:  18110, time: 0.201, loss: 2635.235840\n",
      "Train: step:  18120, time: 0.233, loss: 2931.088867\n",
      "Train: step:  18130, time: 0.188, loss: 1621.779785\n",
      "Train: step:  18140, time: 0.211, loss: 3897.457764\n",
      "Train: step:  18150, time: 0.191, loss: 318.711182\n",
      "Train: step:  18160, time: 0.195, loss: 2281.906250\n",
      "Train: step:  18170, time: 0.229, loss: 316.364044\n",
      "Train: step:  18180, time: 0.238, loss: 1470.762573\n",
      "Train: step:  18190, time: 0.220, loss: 1942.637085\n",
      "Train: step:  18200, time: 0.217, loss: 1567.239380\n",
      "Train: step:  18210, time: 0.197, loss: 1921.581665\n",
      "Train: step:  18220, time: 0.217, loss: 1519.914917\n",
      "Train: step:  18230, time: 0.191, loss: 2016.475220\n",
      "Train: step:  18240, time: 0.215, loss: 1916.770752\n",
      "Train: step:  18250, time: 0.188, loss: 1635.714111\n",
      "Train: step:  18260, time: 0.227, loss: 1824.093262\n",
      "Train: step:  18270, time: 0.183, loss: 1439.091309\n",
      "Train: step:  18280, time: 0.216, loss: 1768.012695\n",
      "Train: step:  18290, time: 0.188, loss: 2374.197266\n",
      "Train: step:  18300, time: 0.197, loss: 1803.448364\n",
      "Train: step:  18310, time: 0.215, loss: 1721.154419\n",
      "Train: step:  18320, time: 0.223, loss: 1292.767090\n",
      "Train: step:  18330, time: 0.195, loss: 1234.542725\n",
      "Train: step:  18340, time: 0.188, loss: 3691.322021\n",
      "Train: step:  18350, time: 0.183, loss: 495.574280\n",
      "Train: step:  18360, time: 0.227, loss: 2318.289551\n",
      "Train: step:  18370, time: 0.191, loss: 2697.604248\n",
      "Train: step:  18380, time: 0.208, loss: 2779.352539\n",
      "Train: step:  18390, time: 0.190, loss: 1323.525024\n",
      "Train: step:  18400, time: 0.211, loss: 1423.826294\n",
      "Train: step:  18410, time: 0.220, loss: 3164.871582\n",
      "Train: step:  18420, time: 0.186, loss: 206.177399\n",
      "Train: step:  18430, time: 0.185, loss: 2384.180664\n",
      "Train: step:  18440, time: 0.188, loss: 2101.744141\n",
      "Train: step:  18450, time: 0.217, loss: 1690.548706\n",
      "Train: step:  18460, time: 0.222, loss: 2237.218018\n",
      "Train: step:  18470, time: 0.198, loss: 2553.538818\n",
      "Train: step:  18480, time: 0.216, loss: 2298.718750\n",
      "Train: step:  18490, time: 0.186, loss: 1132.292603\n",
      "Train: step:  18500, time: 0.222, loss: 966.157288\n",
      "Train: step:  18510, time: 0.235, loss: 2618.680176\n",
      "Train: step:  18520, time: 0.217, loss: 2005.985962\n",
      "Train: step:  18530, time: 0.194, loss: 3350.881104\n",
      "Train: step:  18540, time: 0.210, loss: 1607.263794\n",
      "Train: step:  18550, time: 0.252, loss: 652.997009\n",
      "Train: step:  18560, time: 0.216, loss: 902.326782\n",
      "Train: step:  18570, time: 0.218, loss: 2501.158203\n",
      "Train: step:  18580, time: 0.216, loss: 1942.965088\n",
      "Train: step:  18590, time: 0.189, loss: 2206.678955\n",
      "Train: step:  18600, time: 0.181, loss: 2087.034668\n",
      "Train: step:  18610, time: 0.183, loss: 1603.344604\n",
      "Train: step:  18620, time: 0.283, loss: 2547.413330\n",
      "Train: step:  18630, time: 0.215, loss: 3134.208252\n",
      "Train: step:  18640, time: 0.201, loss: 1350.120239\n",
      "Train: step:  18650, time: 0.229, loss: 3091.850098\n",
      "Train: step:  18660, time: 0.198, loss: 1662.715332\n",
      "Train: step:  18670, time: 0.229, loss: 2869.938232\n",
      "Train: step:  18680, time: 0.229, loss: 2646.416748\n",
      "Train: step:  18690, time: 0.227, loss: 704.559753\n",
      "Train: step:  18700, time: 0.252, loss: 1819.911621\n",
      "Train: step:  18710, time: 0.183, loss: 1688.581543\n",
      "Train: step:  18720, time: 0.251, loss: 997.728394\n",
      "Train: step:  18730, time: 0.186, loss: 2841.897217\n",
      "Train: step:  18740, time: 0.190, loss: 1445.464722\n",
      "Train: step:  18750, time: 0.216, loss: 938.834106\n",
      "Train: step:  18760, time: 0.195, loss: 1456.023315\n",
      "Train: step:  18770, time: 0.186, loss: 1173.077026\n",
      "Train: step:  18780, time: 0.184, loss: 3299.578369\n",
      "Train: step:  18790, time: 0.216, loss: 2640.421387\n",
      "Train: step:  18800, time: 0.187, loss: 1424.393188\n",
      "Train: step:  18810, time: 0.199, loss: 2634.250000\n",
      "Train: step:  18820, time: 0.194, loss: 1877.644043\n",
      "Train: step:  18830, time: 0.195, loss: 2200.150391\n",
      "Train: step:  18840, time: 0.218, loss: 278.096344\n",
      "Train: step:  18850, time: 0.187, loss: 1802.415161\n",
      "Train: step:  18860, time: 0.214, loss: 1118.812744\n",
      "Train: step:  18870, time: 0.194, loss: 811.753784\n",
      "Train: step:  18880, time: 0.217, loss: 1591.272095\n",
      "Train: step:  18890, time: 0.189, loss: 1951.206543\n",
      "Train: step:  18900, time: 0.186, loss: 2787.808838\n",
      "Train: step:  18910, time: 0.183, loss: 542.081177\n",
      "Train: step:  18920, time: 0.193, loss: 2071.598145\n",
      "Train: step:  18930, time: 0.192, loss: 3571.189209\n",
      "Train: step:  18940, time: 0.191, loss: 903.392700\n",
      "Train: step:  18950, time: 0.216, loss: 1157.356812\n",
      "Train: step:  18960, time: 0.228, loss: 2816.639648\n",
      "Train: step:  18970, time: 0.212, loss: 3536.062012\n",
      "Train: step:  18980, time: 0.221, loss: 323.998840\n",
      "Train: step:  18990, time: 0.186, loss: 4261.026855\n",
      "Train: step:  19000, time: 0.219, loss: 2709.992188\n",
      "Train: step:  19010, time: 0.212, loss: 1644.496826\n",
      "Train: step:  19020, time: 0.230, loss: 2087.823242\n",
      "Train: step:  19030, time: 0.240, loss: 1500.115967\n",
      "Train: step:  19040, time: 0.261, loss: 1306.535156\n",
      "Train: step:  19050, time: 0.220, loss: 3951.056396\n",
      "Train: step:  19060, time: 0.229, loss: 2865.305176\n",
      "Train: step:  19070, time: 0.223, loss: 1133.331787\n",
      "Train: step:  19080, time: 0.185, loss: 1747.993652\n",
      "Train: step:  19090, time: 0.196, loss: 232.028214\n",
      "Train: step:  19100, time: 0.235, loss: 2469.676758\n",
      "Train: step:  19110, time: 0.221, loss: 2123.133301\n",
      "Train: step:  19120, time: 0.186, loss: 835.677795\n",
      "Train: step:  19130, time: 0.215, loss: 2750.434814\n",
      "Train: step:  19140, time: 0.192, loss: 787.365234\n",
      "Train: step:  19150, time: 0.197, loss: 2274.911377\n",
      "Train: step:  19160, time: 0.213, loss: 1085.234131\n",
      "Train: step:  19170, time: 0.213, loss: 2116.256592\n",
      "Train: step:  19180, time: 0.186, loss: 1375.345093\n",
      "Train: step:  19190, time: 0.216, loss: 1960.564941\n",
      "Train: step:  19200, time: 0.216, loss: 965.473694\n",
      "Train: step:  19210, time: 0.234, loss: 324.295258\n",
      "Train: step:  19220, time: 0.226, loss: 1250.641846\n",
      "Train: step:  19230, time: 0.229, loss: 308.122711\n",
      "Train: step:  19240, time: 0.191, loss: 2592.360840\n",
      "Train: step:  19250, time: 0.180, loss: 5876.067383\n",
      "Train: step:  19260, time: 0.224, loss: 1789.042725\n",
      "Train: step:  19270, time: 0.247, loss: 2193.670166\n",
      "Train: step:  19280, time: 0.238, loss: 3570.975830\n",
      "Train: step:  19290, time: 0.178, loss: 2647.823975\n",
      "Train: step:  19300, time: 0.190, loss: 1583.360474\n",
      "Train: step:  19310, time: 0.190, loss: 1139.417236\n",
      "Train: step:  19320, time: 0.193, loss: 1295.782227\n",
      "Train: step:  19330, time: 0.216, loss: 2250.005127\n",
      "Train: step:  19340, time: 0.240, loss: 1419.571777\n",
      "Train: step:  19350, time: 0.188, loss: 1385.718994\n",
      "Train: step:  19360, time: 0.230, loss: 369.455200\n",
      "Train: step:  19370, time: 0.215, loss: 1961.150635\n",
      "Train: step:  19380, time: 0.189, loss: 1973.683350\n",
      "Train: step:  19390, time: 0.194, loss: 1418.183350\n",
      "Train: step:  19400, time: 0.186, loss: 2353.658936\n",
      "Train: step:  19410, time: 0.182, loss: 1860.587402\n",
      "Train: step:  19420, time: 0.217, loss: 2071.759277\n",
      "Train: step:  19430, time: 0.230, loss: 1838.765503\n",
      "Train: step:  19440, time: 0.218, loss: 2308.050781\n",
      "Train: step:  19450, time: 0.184, loss: 2774.007568\n",
      "Train: step:  19460, time: 0.191, loss: 907.920898\n",
      "Train: step:  19470, time: 0.215, loss: 1356.261597\n",
      "Train: step:  19480, time: 0.186, loss: 602.700195\n",
      "Train: step:  19490, time: 0.188, loss: 1497.508179\n",
      "Train: step:  19500, time: 0.230, loss: 2612.273438\n",
      "Train: step:  19510, time: 0.228, loss: 1608.518555\n",
      "Train: step:  19520, time: 0.217, loss: 1234.166260\n",
      "Train: step:  19530, time: 0.189, loss: 1125.273560\n",
      "Train: step:  19540, time: 0.227, loss: 2818.477051\n",
      "Train: step:  19550, time: 0.203, loss: 2345.175293\n",
      "Train: step:  19560, time: 0.186, loss: 1677.803467\n",
      "Train: step:  19570, time: 0.186, loss: 2190.624756\n",
      "Train: step:  19580, time: 0.226, loss: 1160.789429\n",
      "Train: step:  19590, time: 0.227, loss: 1206.764771\n",
      "Train: step:  19600, time: 0.216, loss: 1539.645142\n",
      "Train: step:  19610, time: 0.184, loss: 2490.801514\n",
      "Train: step:  19620, time: 0.216, loss: 2806.292969\n",
      "Train: step:  19630, time: 0.192, loss: 2805.710938\n",
      "Train: step:  19640, time: 0.185, loss: 1386.316528\n",
      "Train: step:  19650, time: 0.218, loss: 852.562378\n",
      "Train: step:  19660, time: 0.198, loss: 1811.901855\n",
      "Train: step:  19670, time: 0.181, loss: 476.217102\n",
      "Train: step:  19680, time: 0.184, loss: 3751.800049\n",
      "Train: step:  19690, time: 0.201, loss: 1310.987061\n",
      "Train: step:  19700, time: 0.192, loss: 1312.930054\n",
      "Train: step:  19710, time: 0.184, loss: 2471.183350\n",
      "Train: step:  19720, time: 0.236, loss: 1982.646484\n",
      "Train: step:  19730, time: 0.191, loss: 1005.754639\n",
      "Train: step:  19740, time: 0.221, loss: 1021.696777\n",
      "Train: step:  19750, time: 0.228, loss: 1483.863525\n",
      "Train: step:  19760, time: 0.192, loss: 5575.473633\n",
      "Train: step:  19770, time: 0.192, loss: 1779.368774\n",
      "Train: step:  19780, time: 0.185, loss: 540.230591\n",
      "Train: step:  19790, time: 0.189, loss: 1896.734375\n",
      "Train: step:  19800, time: 0.217, loss: 923.854736\n",
      "Train: step:  19810, time: 0.196, loss: 1731.229858\n",
      "Train: step:  19820, time: 0.215, loss: 1308.904297\n",
      "Train: step:  19830, time: 0.183, loss: 1878.535034\n",
      "Train: step:  19840, time: 0.307, loss: 2061.109131\n",
      "Train: step:  19850, time: 0.229, loss: 938.552124\n",
      "Train: step:  19860, time: 0.217, loss: 1950.392700\n",
      "Train: step:  19870, time: 0.215, loss: 3470.780029\n",
      "Train: step:  19880, time: 0.229, loss: 2556.258301\n",
      "Train: step:  19890, time: 0.218, loss: 493.994446\n",
      "Train: step:  19900, time: 0.230, loss: 2417.854004\n",
      "Train: step:  19910, time: 0.222, loss: 654.688049\n",
      "Train: step:  19920, time: 0.226, loss: 1695.398315\n",
      "Train: step:  19930, time: 0.217, loss: 856.781494\n",
      "Train: step:  19940, time: 0.192, loss: 843.642273\n",
      "Train: step:  19950, time: 0.235, loss: 1728.423462\n",
      "Train: step:  19960, time: 0.219, loss: 3720.314453\n",
      "Train: step:  19970, time: 0.228, loss: 262.184875\n",
      "Train: step:  19980, time: 0.190, loss: 1375.124512\n",
      "Train: step:  19990, time: 0.214, loss: 1380.437622\n",
      "Train: step:  20000, time: 0.226, loss: 2878.664551\n",
      "Train: step:  20010, time: 0.189, loss: 1253.347046\n",
      "Train: step:  20020, time: 0.217, loss: 795.651794\n",
      "Train: step:  20030, time: 0.226, loss: 1876.680664\n",
      "Train: step:  20040, time: 0.207, loss: 1391.968628\n",
      "Train: step:  20050, time: 0.223, loss: 670.615112\n",
      "Train: step:  20060, time: 0.236, loss: 2214.980225\n",
      "Train: step:  20070, time: 0.243, loss: 2656.981934\n",
      "Train: step:  20080, time: 0.186, loss: 2851.307861\n",
      "Train: step:  20090, time: 0.195, loss: 3062.856445\n",
      "Train: step:  20100, time: 0.240, loss: 2796.239746\n",
      "Train: step:  20110, time: 0.185, loss: 2025.506226\n",
      "Train: step:  20120, time: 0.188, loss: 262.184692\n",
      "Train: step:  20130, time: 0.185, loss: 1341.407104\n",
      "Train: step:  20140, time: 0.223, loss: 206.674011\n",
      "Train: step:  20150, time: 0.193, loss: 1131.331299\n",
      "Train: step:  20160, time: 0.187, loss: 1165.638916\n",
      "Train: step:  20170, time: 0.219, loss: 1497.879395\n",
      "Train: step:  20180, time: 0.211, loss: 2881.229248\n",
      "Train: step:  20190, time: 0.234, loss: 1555.960083\n",
      "Train: step:  20200, time: 0.219, loss: 1908.462402\n",
      "Train: step:  20210, time: 0.218, loss: 768.070679\n",
      "Train: step:  20220, time: 0.190, loss: 1786.129395\n",
      "Train: step:  20230, time: 0.220, loss: 1251.575439\n",
      "Train: step:  20240, time: 0.183, loss: 1557.046143\n",
      "Train: step:  20250, time: 0.216, loss: 2598.569824\n",
      "Train: step:  20260, time: 0.217, loss: 1829.657349\n",
      "Train: step:  20270, time: 0.230, loss: 1094.012085\n",
      "Train: step:  20280, time: 0.201, loss: 1198.779541\n",
      "Train: step:  20290, time: 0.227, loss: 1996.777710\n",
      "Train: step:  20300, time: 0.251, loss: 2074.978516\n",
      "Train: step:  20310, time: 0.187, loss: 854.695190\n",
      "Train: step:  20320, time: 0.200, loss: 368.362030\n",
      "Train: step:  20330, time: 0.188, loss: 1421.116699\n",
      "Train: step:  20340, time: 0.225, loss: 3339.584717\n",
      "Train: step:  20350, time: 0.218, loss: 1705.480957\n",
      "Train: step:  20360, time: 0.218, loss: 3010.666504\n",
      "Train: step:  20370, time: 0.191, loss: 2209.163574\n",
      "Train: step:  20380, time: 0.235, loss: 1997.113770\n",
      "Train: step:  20390, time: 0.188, loss: 2875.626709\n",
      "Train: step:  20400, time: 0.184, loss: 1642.085205\n",
      "Train: step:  20410, time: 0.214, loss: 489.117828\n",
      "Train: step:  20420, time: 0.192, loss: 1982.550293\n",
      "Train: step:  20430, time: 0.216, loss: 1254.306030\n",
      "Train: step:  20440, time: 0.192, loss: 661.354858\n",
      "Train: step:  20450, time: 0.224, loss: 1163.461304\n",
      "Train: step:  20460, time: 0.216, loss: 1461.967041\n",
      "Train: step:  20470, time: 0.184, loss: 2831.512695\n",
      "Train: step:  20480, time: 0.240, loss: 1755.934448\n",
      "Train: step:  20490, time: 0.221, loss: 923.643921\n",
      "Train: step:  20500, time: 0.199, loss: 2559.377441\n",
      "Train: step:  20510, time: 0.224, loss: 1005.145691\n",
      "Train: step:  20520, time: 0.183, loss: 3108.209961\n",
      "Train: step:  20530, time: 0.187, loss: 1404.564209\n",
      "Train: step:  20540, time: 0.235, loss: 3120.912354\n",
      "Train: step:  20550, time: 0.194, loss: 3244.031982\n",
      "Train: step:  20560, time: 0.187, loss: 1289.040039\n",
      "Train: step:  20570, time: 0.201, loss: 2556.855469\n",
      "Train: step:  20580, time: 0.191, loss: 2133.643066\n",
      "Train: step:  20590, time: 0.195, loss: 1185.882080\n",
      "Train: step:  20600, time: 0.218, loss: 939.215759\n",
      "Train: step:  20610, time: 0.227, loss: 719.580872\n",
      "Train: step:  20620, time: 0.215, loss: 2551.613525\n",
      "Train: step:  20630, time: 0.186, loss: 748.785461\n",
      "Train: step:  20640, time: 0.185, loss: 3484.686279\n",
      "Train: step:  20650, time: 0.197, loss: 1252.247437\n",
      "Train: step:  20660, time: 0.191, loss: 1126.868042\n",
      "Train: step:  20670, time: 0.191, loss: 1917.348145\n",
      "Train: step:  20680, time: 0.232, loss: 1446.935913\n",
      "Train: step:  20690, time: 0.229, loss: 1246.058472\n",
      "Train: step:  20700, time: 0.187, loss: 3510.849365\n",
      "Train: step:  20710, time: 0.219, loss: 1188.556885\n",
      "Train: step:  20720, time: 0.181, loss: 4107.939453\n",
      "Train: step:  20730, time: 0.226, loss: 2426.877930\n",
      "Train: step:  20740, time: 0.195, loss: 1561.067383\n",
      "Train: step:  20750, time: 0.198, loss: 572.032227\n",
      "Train: step:  20760, time: 0.195, loss: 665.677856\n",
      "Train: step:  20770, time: 0.187, loss: 867.640137\n",
      "Train: step:  20780, time: 0.200, loss: 1098.974243\n",
      "Train: step:  20790, time: 0.218, loss: 2942.371582\n",
      "Train: step:  20800, time: 0.196, loss: 776.707031\n",
      "Train: step:  20810, time: 0.193, loss: 1990.339233\n",
      "Train: step:  20820, time: 0.189, loss: 1255.811890\n",
      "Train: step:  20830, time: 0.199, loss: 2286.151611\n",
      "Train: step:  20840, time: 0.188, loss: 1769.038574\n",
      "Train: step:  20850, time: 0.198, loss: 1552.683228\n",
      "Train: step:  20860, time: 0.185, loss: 2668.122070\n",
      "Train: step:  20870, time: 0.220, loss: 3098.034424\n",
      "Train: step:  20880, time: 0.190, loss: 1765.105469\n",
      "Train: step:  20890, time: 0.187, loss: 1900.612793\n",
      "Train: step:  20900, time: 0.220, loss: 947.054077\n",
      "Train: step:  20910, time: 0.189, loss: 2652.742676\n",
      "Train: step:  20920, time: 0.269, loss: 2575.030029\n",
      "Train: step:  20930, time: 0.185, loss: 3271.882568\n",
      "Train: step:  20940, time: 0.227, loss: 1462.275269\n",
      "Train: step:  20950, time: 0.188, loss: 3453.466064\n",
      "Train: step:  20960, time: 0.187, loss: 4086.139893\n",
      "Train: step:  20970, time: 0.228, loss: 2407.749268\n",
      "Train: step:  20980, time: 0.245, loss: 1781.214600\n",
      "Train: step:  20990, time: 0.217, loss: 1972.716675\n",
      "Train: step:  21000, time: 0.229, loss: 2259.948730\n",
      "Train: step:  21010, time: 0.185, loss: 2058.633057\n",
      "Train: step:  21020, time: 0.194, loss: 2546.290039\n",
      "Train: step:  21030, time: 0.193, loss: 1921.112915\n",
      "Train: step:  21040, time: 0.225, loss: 1678.993286\n",
      "Train: step:  21050, time: 0.193, loss: 2033.398804\n",
      "Train: step:  21060, time: 0.187, loss: 3552.186768\n",
      "Train: step:  21070, time: 0.187, loss: 1866.923950\n",
      "Train: step:  21080, time: 0.226, loss: 2890.525635\n",
      "Train: step:  21090, time: 0.233, loss: 1234.194580\n",
      "Train: step:  21100, time: 0.188, loss: 1170.078613\n",
      "Train: step:  21110, time: 0.187, loss: 3109.448730\n",
      "Train: step:  21120, time: 0.191, loss: 405.695953\n",
      "Train: step:  21130, time: 0.185, loss: 2816.368896\n",
      "Train: step:  21140, time: 0.195, loss: 2337.287842\n",
      "Train: step:  21150, time: 0.198, loss: 2827.983154\n",
      "Train: step:  21160, time: 0.235, loss: 1808.770508\n",
      "Train: step:  21170, time: 0.190, loss: 1931.275024\n",
      "Train: step:  21180, time: 0.191, loss: 2324.946045\n",
      "Train: step:  21190, time: 0.217, loss: 2989.149170\n",
      "Train: step:  21200, time: 0.200, loss: 1603.086426\n",
      "Train: step:  21210, time: 0.223, loss: 1484.915894\n",
      "Train: step:  21220, time: 0.216, loss: 894.110229\n",
      "Train: step:  21230, time: 0.200, loss: 1236.947632\n",
      "Train: step:  21240, time: 0.197, loss: 1740.323242\n",
      "Train: step:  21250, time: 0.214, loss: 2357.689941\n",
      "Train: step:  21260, time: 0.264, loss: 1369.619629\n",
      "Train: step:  21270, time: 0.218, loss: 1092.921631\n",
      "Train: step:  21280, time: 0.254, loss: 1046.312256\n",
      "Train: step:  21290, time: 0.183, loss: 1626.781250\n",
      "Train: step:  21300, time: 0.194, loss: 876.601257\n",
      "Train: step:  21310, time: 0.195, loss: 2464.317383\n",
      "Train: step:  21320, time: 0.219, loss: 1816.316650\n",
      "Train: step:  21330, time: 0.214, loss: 1071.034058\n",
      "Train: step:  21340, time: 0.189, loss: 1222.915771\n",
      "Train: step:  21350, time: 0.227, loss: 1423.582520\n",
      "Train: step:  21360, time: 0.186, loss: 1887.962158\n",
      "Train: step:  21370, time: 0.192, loss: 1254.936279\n",
      "Train: step:  21380, time: 0.215, loss: 1571.268433\n",
      "Train: step:  21390, time: 0.204, loss: 2727.616699\n",
      "Train: step:  21400, time: 0.204, loss: 1819.838501\n",
      "Train: step:  21410, time: 0.220, loss: 1517.158447\n",
      "Train: step:  21420, time: 0.232, loss: 3121.555420\n",
      "Train: step:  21430, time: 0.194, loss: 963.890564\n",
      "Train: step:  21440, time: 0.244, loss: 2086.072266\n",
      "Train: step:  21450, time: 0.183, loss: 2456.928711\n",
      "Train: step:  21460, time: 0.204, loss: 1775.361572\n",
      "Train: step:  21470, time: 0.238, loss: 830.158447\n",
      "Train: step:  21480, time: 0.209, loss: 388.245148\n",
      "Train: step:  21490, time: 0.207, loss: 308.296844\n",
      "Train: step:  21500, time: 0.216, loss: 1835.052856\n",
      "Train: step:  21510, time: 0.218, loss: 1488.539917\n",
      "Train: step:  21520, time: 0.231, loss: 2837.818115\n",
      "Train: step:  21530, time: 0.288, loss: 985.413147\n",
      "Train: step:  21540, time: 0.223, loss: 1512.307617\n",
      "Train: step:  21550, time: 0.187, loss: 424.108490\n",
      "Train: step:  21560, time: 0.193, loss: 1358.947998\n",
      "Train: step:  21570, time: 0.204, loss: 1323.390747\n",
      "Train: step:  21580, time: 0.186, loss: 1901.893555\n",
      "Train: step:  21590, time: 0.194, loss: 409.581848\n",
      "Train: step:  21600, time: 0.190, loss: 1171.672729\n",
      "Train: step:  21610, time: 0.223, loss: 1021.407410\n",
      "Train: step:  21620, time: 0.193, loss: 1322.531372\n",
      "Train: step:  21630, time: 0.185, loss: 2646.642822\n",
      "Train: step:  21640, time: 0.218, loss: 1162.445801\n",
      "Train: step:  21650, time: 0.218, loss: 456.256012\n",
      "Train: step:  21660, time: 0.226, loss: 1018.376648\n",
      "Train: step:  21670, time: 0.192, loss: 2216.788574\n",
      "Train: step:  21680, time: 0.187, loss: 158.771133\n",
      "Train: step:  21690, time: 0.214, loss: 1102.803833\n",
      "Train: step:  21700, time: 0.188, loss: 1244.083130\n",
      "Train: step:  21710, time: 0.189, loss: 238.135727\n",
      "Train: step:  21720, time: 0.214, loss: 2115.279785\n",
      "Train: step:  21730, time: 0.182, loss: 3038.955811\n",
      "Train: step:  21740, time: 0.193, loss: 2164.975098\n",
      "Train: step:  21750, time: 0.187, loss: 2084.480957\n",
      "Train: step:  21760, time: 0.193, loss: 3265.371826\n",
      "Train: step:  21770, time: 0.189, loss: 2617.273193\n",
      "Train: step:  21780, time: 0.189, loss: 2951.143066\n",
      "Train: step:  21790, time: 0.214, loss: 4057.653809\n",
      "Train: step:  21800, time: 0.217, loss: 1553.473633\n",
      "Train: step:  21810, time: 0.198, loss: 2753.043457\n",
      "Train: step:  21820, time: 0.187, loss: 724.248413\n",
      "Train: step:  21830, time: 0.223, loss: 2713.767822\n",
      "Train: step:  21840, time: 0.216, loss: 3220.889893\n",
      "Train: step:  21850, time: 0.186, loss: 3363.789795\n",
      "Train: step:  21860, time: 0.234, loss: 1361.835693\n",
      "Train: step:  21870, time: 0.187, loss: 4378.082031\n",
      "Train: step:  21880, time: 0.185, loss: 1781.332764\n",
      "Train: step:  21890, time: 0.189, loss: 3051.176758\n",
      "Train: step:  21900, time: 0.184, loss: 1188.594971\n",
      "Train: step:  21910, time: 0.197, loss: 1809.381714\n",
      "Train: step:  21920, time: 0.216, loss: 3009.041992\n",
      "Train: step:  21930, time: 0.226, loss: 2119.980713\n",
      "Train: step:  21940, time: 0.187, loss: 1972.400513\n",
      "Train: step:  21950, time: 0.228, loss: 1584.669556\n",
      "Train: step:  21960, time: 0.197, loss: 2246.013184\n",
      "Train: step:  21970, time: 0.240, loss: 1448.169556\n",
      "Train: step:  21980, time: 0.218, loss: 1893.563599\n",
      "Train: step:  21990, time: 0.196, loss: 753.635803\n",
      "Train: step:  22000, time: 0.218, loss: 1555.069214\n",
      "Train: step:  22010, time: 0.184, loss: 376.298096\n",
      "Train: step:  22020, time: 0.241, loss: 2172.399170\n",
      "Train: step:  22030, time: 0.212, loss: 680.685486\n",
      "Train: step:  22040, time: 0.227, loss: 2454.961670\n",
      "Train: step:  22050, time: 0.213, loss: 2270.206055\n",
      "Train: step:  22060, time: 0.195, loss: 1231.271973\n",
      "Train: step:  22070, time: 0.192, loss: 4540.651855\n",
      "Train: step:  22080, time: 0.269, loss: 2042.753906\n",
      "Train: step:  22090, time: 0.219, loss: 1117.131470\n",
      "Train: step:  22100, time: 0.232, loss: 1844.807129\n",
      "Train: step:  22110, time: 0.216, loss: 2666.872559\n",
      "Train: step:  22120, time: 0.236, loss: 1918.982788\n",
      "Train: step:  22130, time: 0.184, loss: 2342.961182\n",
      "Train: step:  22140, time: 0.184, loss: 3077.139160\n",
      "Train: step:  22150, time: 0.217, loss: 735.525024\n",
      "Train: step:  22160, time: 0.207, loss: 2087.253418\n",
      "Train: step:  22170, time: 0.220, loss: 2315.691895\n",
      "Train: step:  22180, time: 0.186, loss: 981.839844\n",
      "Train: step:  22190, time: 0.191, loss: 1962.555298\n",
      "Train: step:  22200, time: 0.189, loss: 988.321594\n",
      "Train: step:  22210, time: 0.216, loss: 3105.598389\n",
      "Train: step:  22220, time: 0.208, loss: 2224.604004\n",
      "Train: step:  22230, time: 0.210, loss: 2986.011719\n",
      "Train: step:  22240, time: 0.221, loss: 1517.778076\n",
      "Train: step:  22250, time: 0.217, loss: 484.882599\n",
      "Train: step:  22260, time: 0.188, loss: 2749.740234\n",
      "Train: step:  22270, time: 0.191, loss: 1589.815796\n",
      "Train: step:  22280, time: 0.191, loss: 2318.394043\n",
      "Train: step:  22290, time: 0.194, loss: 2234.254883\n",
      "Train: step:  22300, time: 0.227, loss: 1559.293091\n",
      "Train: step:  22310, time: 0.230, loss: 2334.477783\n",
      "Train: step:  22320, time: 0.220, loss: 2532.299316\n",
      "Train: step:  22330, time: 0.215, loss: 2410.943115\n",
      "Train: step:  22340, time: 0.184, loss: 2488.602539\n",
      "Train: step:  22350, time: 0.224, loss: 3036.799072\n",
      "Train: step:  22360, time: 0.195, loss: 2572.177002\n",
      "Train: step:  22370, time: 0.217, loss: 792.704773\n",
      "Train: step:  22380, time: 0.198, loss: 1221.799194\n",
      "Train: step:  22390, time: 0.198, loss: 1931.027100\n",
      "Train: step:  22400, time: 0.217, loss: 1259.240967\n",
      "Train: step:  22410, time: 0.216, loss: 2674.899902\n",
      "Train: step:  22420, time: 0.188, loss: 1841.451660\n",
      "Train: step:  22430, time: 0.219, loss: 2258.063232\n",
      "Train: step:  22440, time: 0.215, loss: 1849.433350\n",
      "Train: step:  22450, time: 0.186, loss: 1403.291626\n",
      "Train: step:  22460, time: 0.189, loss: 2804.506592\n",
      "Train: step:  22470, time: 0.258, loss: 3171.447266\n",
      "Train: step:  22480, time: 0.188, loss: 851.858521\n",
      "Train: step:  22490, time: 0.219, loss: 2863.025146\n",
      "Train: step:  22500, time: 0.205, loss: 327.105316\n",
      "Train: step:  22510, time: 0.194, loss: 1381.345581\n",
      "Train: step:  22520, time: 0.213, loss: 3232.694092\n",
      "Train: step:  22530, time: 0.192, loss: 2673.458496\n",
      "Train: step:  22540, time: 0.231, loss: 2799.814453\n",
      "Train: step:  22550, time: 0.214, loss: 1224.713989\n",
      "Train: step:  22560, time: 0.219, loss: 532.681458\n",
      "Train: step:  22570, time: 0.215, loss: 3297.744629\n",
      "Train: step:  22580, time: 0.192, loss: 2544.592529\n",
      "Train: step:  22590, time: 0.188, loss: 1885.476196\n",
      "Train: step:  22600, time: 0.230, loss: 1942.952759\n",
      "Train: step:  22610, time: 0.222, loss: 2523.953125\n",
      "Train: step:  22620, time: 0.219, loss: 342.857178\n",
      "Train: step:  22630, time: 0.195, loss: 2253.318359\n",
      "Train: step:  22640, time: 0.188, loss: 457.573151\n",
      "Train: step:  22650, time: 0.188, loss: 1969.175171\n",
      "Train: step:  22660, time: 0.199, loss: 2936.057861\n",
      "Train: step:  22670, time: 0.192, loss: 2583.592285\n",
      "Train: step:  22680, time: 0.193, loss: 1445.785034\n",
      "Train: step:  22690, time: 0.193, loss: 1120.497803\n",
      "Train: step:  22700, time: 0.189, loss: 1510.958008\n",
      "Train: step:  22710, time: 0.217, loss: 959.648926\n",
      "Train: step:  22720, time: 0.200, loss: 1244.186401\n",
      "Train: step:  22730, time: 0.192, loss: 1542.580566\n",
      "Train: step:  22740, time: 0.186, loss: 1642.756104\n",
      "Train: step:  22750, time: 0.183, loss: 1462.814453\n",
      "Train: step:  22760, time: 0.193, loss: 1304.834961\n",
      "Train: step:  22770, time: 0.226, loss: 961.964722\n",
      "Train: step:  22780, time: 0.184, loss: 1735.625366\n",
      "Train: step:  22790, time: 0.220, loss: 1286.000610\n",
      "Train: step:  22800, time: 0.233, loss: 580.388123\n",
      "Train: step:  22810, time: 0.187, loss: 2848.333740\n",
      "Train: step:  22820, time: 0.188, loss: 1388.338989\n",
      "Train: step:  22830, time: 0.186, loss: 2831.870605\n",
      "Train: step:  22840, time: 0.218, loss: 1515.324707\n",
      "Train: step:  22850, time: 0.217, loss: 3166.352539\n",
      "Train: step:  22860, time: 0.222, loss: 3498.396484\n",
      "Train: step:  22870, time: 0.186, loss: 2212.608643\n",
      "Train: step:  22880, time: 0.204, loss: 695.650085\n",
      "Train: step:  22890, time: 0.218, loss: 2016.229980\n",
      "Train: step:  22900, time: 0.218, loss: 1747.846558\n",
      "Train: step:  22910, time: 0.211, loss: 3456.136475\n",
      "Train: step:  22920, time: 0.194, loss: 2626.381592\n",
      "Train: step:  22930, time: 0.218, loss: 3099.143799\n",
      "Train: step:  22940, time: 0.225, loss: 2639.393066\n",
      "Train: step:  22950, time: 0.216, loss: 1394.443481\n",
      "Train: step:  22960, time: 0.182, loss: 1700.618042\n",
      "Train: step:  22970, time: 0.189, loss: 819.187256\n",
      "Train: step:  22980, time: 0.201, loss: 2194.127686\n",
      "Train: step:  22990, time: 0.184, loss: 1881.541504\n",
      "Train: step:  23000, time: 0.231, loss: 2221.258057\n",
      "Train: step:  23010, time: 0.192, loss: 750.622986\n",
      "Train: step:  23020, time: 0.218, loss: 1665.512207\n",
      "Train: step:  23030, time: 0.191, loss: 2059.935059\n",
      "Train: step:  23040, time: 0.194, loss: 3091.959717\n",
      "Train: step:  23050, time: 0.198, loss: 2178.739746\n",
      "Train: step:  23060, time: 0.209, loss: 667.781250\n",
      "Train: step:  23070, time: 0.238, loss: 2433.878418\n",
      "Train: step:  23080, time: 0.232, loss: 1440.577026\n",
      "Train: step:  23090, time: 0.210, loss: 1841.696411\n",
      "Train: step:  23100, time: 0.188, loss: 2692.171875\n",
      "Train: step:  23110, time: 0.233, loss: 879.138977\n",
      "Train: step:  23120, time: 0.191, loss: 1585.779297\n",
      "Train: step:  23130, time: 0.189, loss: 1350.620728\n",
      "Train: step:  23140, time: 0.204, loss: 2682.960693\n",
      "Train: step:  23150, time: 0.203, loss: 3388.993652\n",
      "Train: step:  23160, time: 0.215, loss: 3644.950195\n",
      "Train: step:  23170, time: 0.228, loss: 1235.714355\n",
      "Train: step:  23180, time: 0.188, loss: 944.684082\n",
      "Train: step:  23190, time: 0.188, loss: 1519.046631\n",
      "Train: step:  23200, time: 0.195, loss: 1294.706787\n",
      "Train: step:  23210, time: 0.219, loss: 1384.580566\n",
      "Train: step:  23220, time: 0.193, loss: 456.082703\n",
      "Train: step:  23230, time: 0.240, loss: 1918.463013\n",
      "Train: step:  23240, time: 0.227, loss: 1625.737427\n",
      "Train: step:  23250, time: 0.199, loss: 2015.087769\n",
      "Train: step:  23260, time: 0.186, loss: 1589.437988\n",
      "Train: step:  23270, time: 0.189, loss: 2660.520020\n",
      "Train: step:  23280, time: 0.235, loss: 1315.479248\n",
      "Train: step:  23290, time: 0.245, loss: 1943.820190\n",
      "Train: step:  23300, time: 0.216, loss: 411.860260\n",
      "Train: step:  23310, time: 0.217, loss: 4702.754883\n",
      "Train: step:  23320, time: 0.239, loss: 2022.271118\n",
      "Train: step:  23330, time: 0.229, loss: 846.417297\n",
      "Train: step:  23340, time: 0.192, loss: 1426.225586\n",
      "Train: step:  23350, time: 0.192, loss: 2343.191895\n",
      "Train: step:  23360, time: 0.237, loss: 1814.301514\n",
      "Train: step:  23370, time: 0.213, loss: 1176.363647\n",
      "Train: step:  23380, time: 0.190, loss: 489.854431\n",
      "Train: step:  23390, time: 0.236, loss: 3425.880615\n",
      "Train: step:  23400, time: 0.183, loss: 792.852356\n",
      "Train: step:  23410, time: 0.217, loss: 1328.827881\n",
      "Train: step:  23420, time: 0.219, loss: 898.246460\n",
      "Train: step:  23430, time: 0.185, loss: 878.640198\n",
      "Train: step:  23440, time: 0.217, loss: 1746.121582\n",
      "Train: step:  23450, time: 0.217, loss: 2571.964355\n",
      "Train: step:  23460, time: 0.191, loss: 2537.842773\n",
      "Train: step:  23470, time: 0.233, loss: 2139.888184\n",
      "Train: step:  23480, time: 0.230, loss: 3747.242188\n",
      "Train: step:  23490, time: 0.193, loss: 1484.036133\n",
      "Train: step:  23500, time: 0.229, loss: 3401.246338\n",
      "Train: step:  23510, time: 0.183, loss: 1267.444092\n",
      "Train: step:  23520, time: 0.186, loss: 1930.759521\n",
      "Train: step:  23530, time: 0.227, loss: 3200.310303\n",
      "Train: step:  23540, time: 0.185, loss: 1637.475098\n",
      "Train: step:  23550, time: 0.196, loss: 1152.008057\n",
      "Train: step:  23560, time: 0.191, loss: 3000.489014\n",
      "Train: step:  23570, time: 0.190, loss: 2292.813965\n",
      "Train: step:  23580, time: 0.182, loss: 4390.308594\n",
      "Train: step:  23590, time: 0.208, loss: 2629.988037\n",
      "Train: step:  23600, time: 0.189, loss: 1167.629150\n",
      "Train: step:  23610, time: 0.195, loss: 2395.659424\n",
      "Train: step:  23620, time: 0.217, loss: 3591.776123\n",
      "Train: step:  23630, time: 0.197, loss: 2363.914551\n",
      "Train: step:  23640, time: 0.202, loss: 1761.335938\n",
      "Train: step:  23650, time: 0.220, loss: 988.109497\n",
      "Train: step:  23660, time: 0.195, loss: 2577.100342\n",
      "Train: step:  23670, time: 0.187, loss: 2141.358643\n",
      "Train: step:  23680, time: 0.191, loss: 1142.862183\n",
      "Train: step:  23690, time: 0.232, loss: 3375.213135\n",
      "Train: step:  23700, time: 0.230, loss: 4678.069336\n",
      "Train: step:  23710, time: 0.185, loss: 1036.385986\n",
      "Train: step:  23720, time: 0.224, loss: 3187.083252\n",
      "Train: step:  23730, time: 0.219, loss: 2907.954834\n",
      "Train: step:  23740, time: 0.203, loss: 2619.231201\n",
      "Train: step:  23750, time: 0.192, loss: 2002.713745\n",
      "Train: step:  23760, time: 0.190, loss: 3164.606689\n",
      "Train: step:  23770, time: 0.268, loss: 3518.283691\n",
      "Train: step:  23780, time: 0.227, loss: 2436.935059\n",
      "Train: step:  23790, time: 0.192, loss: 509.872650\n",
      "Train: step:  23800, time: 0.207, loss: 1649.298584\n",
      "Train: step:  23810, time: 0.193, loss: 279.142334\n",
      "Train: step:  23820, time: 0.198, loss: 1217.034424\n",
      "Train: step:  23830, time: 0.220, loss: 1604.551636\n",
      "Train: step:  23840, time: 0.216, loss: 2492.783203\n",
      "Train: step:  23850, time: 0.217, loss: 1796.799805\n",
      "Train: step:  23860, time: 0.232, loss: 831.715149\n",
      "Train: step:  23870, time: 0.234, loss: 801.432129\n",
      "Train: step:  23880, time: 0.195, loss: 3051.247803\n",
      "Train: step:  23890, time: 0.227, loss: 2319.859863\n",
      "Train: step:  23900, time: 0.195, loss: 1767.813721\n",
      "Train: step:  23910, time: 0.182, loss: 1374.175903\n",
      "Train: step:  23920, time: 0.219, loss: 4049.429688\n",
      "Train: step:  23930, time: 0.217, loss: 2113.364502\n",
      "Train: step:  23940, time: 0.237, loss: 2759.791992\n",
      "Train: step:  23950, time: 0.187, loss: 1891.700562\n",
      "Train: step:  23960, time: 0.240, loss: 1852.742920\n",
      "Train: step:  23970, time: 0.192, loss: 3153.359863\n",
      "Train: step:  23980, time: 0.187, loss: 898.902039\n",
      "Train: step:  23990, time: 0.212, loss: 1897.175049\n",
      "Train: step:  24000, time: 0.180, loss: 1706.631958\n",
      "Train: step:  24010, time: 0.198, loss: 752.278198\n",
      "Train: step:  24020, time: 0.182, loss: 1774.967896\n",
      "Train: step:  24030, time: 0.228, loss: 2452.909912\n",
      "Train: step:  24040, time: 0.200, loss: 2120.560547\n",
      "Train: step:  24050, time: 0.186, loss: 2452.380859\n",
      "Train: step:  24060, time: 0.195, loss: 2536.910645\n",
      "Train: step:  24070, time: 0.186, loss: 1438.306763\n",
      "Train: step:  24080, time: 0.188, loss: 2932.705566\n",
      "Train: step:  24090, time: 0.210, loss: 676.097473\n",
      "Train: step:  24100, time: 0.193, loss: 1656.850586\n",
      "Train: step:  24110, time: 0.184, loss: 375.373535\n",
      "Train: step:  24120, time: 0.187, loss: 4123.731934\n",
      "Train: step:  24130, time: 0.188, loss: 2221.051514\n",
      "Train: step:  24140, time: 0.190, loss: 1181.823608\n",
      "Train: step:  24150, time: 0.235, loss: 1994.969971\n",
      "Train: step:  24160, time: 0.188, loss: 1395.741943\n",
      "Train: step:  24170, time: 0.218, loss: 1735.932983\n",
      "Train: step:  24180, time: 0.228, loss: 1895.592041\n",
      "Train: step:  24190, time: 0.218, loss: 4955.445312\n",
      "Train: step:  24200, time: 0.194, loss: 2324.802246\n",
      "Train: step:  24210, time: 0.225, loss: 1994.983521\n",
      "Train: step:  24220, time: 0.286, loss: 2968.608154\n",
      "Train: step:  24230, time: 0.217, loss: 700.053528\n",
      "Train: step:  24240, time: 0.206, loss: 1885.604614\n",
      "Train: step:  24250, time: 0.195, loss: 2482.006836\n",
      "Train: step:  24260, time: 0.220, loss: 1508.369629\n",
      "Train: step:  24270, time: 0.219, loss: 2125.302734\n",
      "Train: step:  24280, time: 0.215, loss: 2534.338135\n",
      "Train: step:  24290, time: 0.190, loss: 1937.328491\n",
      "Train: step:  24300, time: 0.192, loss: 2126.556152\n",
      "Train: step:  24310, time: 0.219, loss: 685.491821\n",
      "Train: step:  24320, time: 0.189, loss: 3029.071533\n",
      "Train: step:  24330, time: 0.190, loss: 1970.180420\n",
      "Train: step:  24340, time: 0.186, loss: 2001.811890\n",
      "Train: step:  24350, time: 0.188, loss: 1141.985107\n",
      "Train: step:  24360, time: 0.183, loss: 1350.129761\n",
      "Train: step:  24370, time: 0.216, loss: 1686.952393\n",
      "Train: step:  24380, time: 0.218, loss: 1217.096558\n",
      "Train: step:  24390, time: 0.191, loss: 259.974365\n",
      "Train: step:  24400, time: 0.199, loss: 3184.772217\n",
      "Train: step:  24410, time: 0.202, loss: 2292.935791\n",
      "Train: step:  24420, time: 0.214, loss: 1328.601440\n",
      "Train: step:  24430, time: 0.226, loss: 778.124512\n",
      "Train: step:  24440, time: 0.190, loss: 3673.651367\n",
      "Train: step:  24450, time: 0.188, loss: 1716.005249\n",
      "Train: step:  24460, time: 0.191, loss: 952.601501\n",
      "Train: step:  24470, time: 0.196, loss: 639.107666\n",
      "Train: step:  24480, time: 0.219, loss: 1870.052856\n",
      "Train: step:  24490, time: 0.206, loss: 3386.571533\n",
      "Train: step:  24500, time: 0.231, loss: 917.517700\n",
      "Train: step:  24510, time: 0.194, loss: 2194.232178\n",
      "Train: step:  24520, time: 0.209, loss: 1027.446899\n",
      "Train: step:  24530, time: 0.191, loss: 817.191956\n",
      "Train: step:  24540, time: 0.218, loss: 2751.824219\n",
      "Train: step:  24550, time: 0.192, loss: 1127.139526\n",
      "Train: step:  24560, time: 0.233, loss: 2785.875732\n",
      "Train: step:  24570, time: 0.227, loss: 2445.532227\n",
      "Train: step:  24580, time: 0.192, loss: 806.264160\n",
      "Train: step:  24590, time: 0.214, loss: 962.881409\n",
      "Train: step:  24600, time: 0.217, loss: 2082.541748\n",
      "Train: step:  24610, time: 0.217, loss: 369.829224\n",
      "Train: step:  24620, time: 0.194, loss: 2381.460205\n",
      "Train: step:  24630, time: 0.211, loss: 2795.130859\n",
      "Train: step:  24640, time: 0.253, loss: 2069.658691\n",
      "Train: step:  24650, time: 0.230, loss: 772.543884\n",
      "Train: step:  24660, time: 0.193, loss: 2293.322266\n",
      "Train: step:  24670, time: 0.217, loss: 2376.070801\n",
      "Train: step:  24680, time: 0.184, loss: 1470.050293\n",
      "Train: step:  24690, time: 0.184, loss: 1187.957275\n",
      "Train: step:  24700, time: 0.219, loss: 1984.421997\n",
      "Train: step:  24710, time: 0.229, loss: 910.633118\n",
      "Train: step:  24720, time: 0.193, loss: 1044.524658\n",
      "Train: step:  24730, time: 0.186, loss: 780.177734\n",
      "Train: step:  24740, time: 0.209, loss: 2090.071777\n",
      "Train: step:  24750, time: 0.189, loss: 1634.002930\n",
      "Train: step:  24760, time: 0.229, loss: 1300.144409\n",
      "Train: step:  24770, time: 0.229, loss: 929.021362\n",
      "Train: step:  24780, time: 0.200, loss: 1894.887329\n",
      "Train: step:  24790, time: 0.190, loss: 1744.705200\n",
      "Train: step:  24800, time: 0.220, loss: 1755.161255\n",
      "Train: step:  24810, time: 0.189, loss: 880.945984\n",
      "Train: step:  24820, time: 0.231, loss: 2007.440796\n",
      "Train: step:  24830, time: 0.189, loss: 954.414490\n",
      "Train: step:  24840, time: 0.241, loss: 2101.636963\n",
      "Train: step:  24850, time: 0.195, loss: 1404.213989\n",
      "Train: step:  24860, time: 0.218, loss: 1752.412964\n",
      "Train: step:  24870, time: 0.202, loss: 2374.599121\n",
      "Train: step:  24880, time: 0.187, loss: 226.348007\n",
      "Train: step:  24890, time: 0.229, loss: 1841.527832\n",
      "Train: step:  24900, time: 0.229, loss: 1083.349487\n",
      "Train: step:  24910, time: 0.183, loss: 1566.701172\n",
      "Train: step:  24920, time: 0.193, loss: 2125.004639\n",
      "Train: step:  24930, time: 0.216, loss: 2293.314941\n",
      "Train: step:  24940, time: 0.194, loss: 1457.804321\n",
      "Train: step:  24950, time: 0.218, loss: 3670.067871\n",
      "Train: step:  24960, time: 0.183, loss: 2280.494141\n",
      "Train: step:  24970, time: 0.227, loss: 2431.904297\n",
      "Train: step:  24980, time: 0.256, loss: 1966.922852\n",
      "Train: step:  24990, time: 0.200, loss: 728.691345\n",
      "Train: step:  25000, time: 0.202, loss: 2509.271240\n",
      "Train: step:  25010, time: 0.214, loss: 2932.219238\n",
      "Train: step:  25020, time: 0.192, loss: 910.275940\n",
      "Train: step:  25030, time: 0.190, loss: 867.312561\n",
      "Train: step:  25040, time: 0.191, loss: 772.958557\n",
      "Train: step:  25050, time: 0.190, loss: 2148.077148\n",
      "Train: step:  25060, time: 0.220, loss: 2819.699463\n",
      "Train: step:  25070, time: 0.217, loss: 2339.932861\n",
      "Train: step:  25080, time: 0.217, loss: 1310.213013\n",
      "Train: step:  25090, time: 0.227, loss: 2308.252686\n",
      "Train: step:  25100, time: 0.194, loss: 1172.082275\n",
      "Train: step:  25110, time: 0.215, loss: 1378.808594\n",
      "Train: step:  25120, time: 0.201, loss: 2144.838135\n",
      "Train: step:  25130, time: 0.204, loss: 772.419128\n",
      "Train: step:  25140, time: 0.214, loss: 1476.357544\n",
      "Train: step:  25150, time: 0.216, loss: 2026.264526\n",
      "Train: step:  25160, time: 0.193, loss: 2748.250488\n",
      "Train: step:  25170, time: 0.189, loss: 3061.421875\n",
      "Train: step:  25180, time: 0.188, loss: 3058.595947\n",
      "Train: step:  25190, time: 0.196, loss: 1633.536133\n",
      "Train: step:  25200, time: 0.256, loss: 2795.234619\n",
      "Train: step:  25210, time: 0.193, loss: 1688.557495\n",
      "Train: step:  25220, time: 0.219, loss: 1122.676514\n",
      "Train: step:  25230, time: 0.228, loss: 2038.933472\n",
      "Train: step:  25240, time: 0.195, loss: 2887.600342\n",
      "Train: step:  25250, time: 0.249, loss: 1687.659790\n",
      "Train: step:  25260, time: 0.231, loss: 1090.404419\n",
      "Train: step:  25270, time: 0.230, loss: 2922.177490\n",
      "Train: step:  25280, time: 0.202, loss: 399.164062\n",
      "Train: step:  25290, time: 0.217, loss: 2247.938232\n",
      "Train: step:  25300, time: 0.201, loss: 1198.909424\n",
      "Train: step:  25310, time: 0.189, loss: 1442.454468\n",
      "Train: step:  25320, time: 0.231, loss: 2498.877441\n",
      "Train: step:  25330, time: 0.216, loss: 1976.831543\n",
      "Train: step:  25340, time: 0.242, loss: 1004.349976\n",
      "Train: step:  25350, time: 0.186, loss: 3038.184326\n",
      "Train: step:  25360, time: 0.218, loss: 1597.009399\n",
      "Train: step:  25370, time: 0.189, loss: 1342.499146\n",
      "Train: step:  25380, time: 0.219, loss: 588.555603\n",
      "Train: step:  25390, time: 0.219, loss: 2280.974609\n",
      "Train: step:  25400, time: 0.217, loss: 1892.713745\n",
      "Train: step:  25410, time: 0.225, loss: 2262.123779\n",
      "Train: step:  25420, time: 0.229, loss: 1524.097046\n",
      "Train: step:  25430, time: 0.191, loss: 1715.410522\n",
      "Train: step:  25440, time: 0.185, loss: 1484.895020\n",
      "Train: step:  25450, time: 0.219, loss: 3009.707764\n",
      "Train: step:  25460, time: 0.195, loss: 1250.112671\n",
      "Train: step:  25470, time: 0.231, loss: 2653.076172\n",
      "Train: step:  25480, time: 0.220, loss: 1997.264038\n",
      "Train: step:  25490, time: 0.187, loss: 1988.903931\n",
      "Train: step:  25500, time: 0.216, loss: 1921.979492\n",
      "Train: step:  25510, time: 0.197, loss: 740.659424\n",
      "Train: step:  25520, time: 0.215, loss: 1566.856201\n",
      "Train: step:  25530, time: 0.233, loss: 1887.448364\n",
      "Train: step:  25540, time: 0.211, loss: 1666.852417\n",
      "Train: step:  25550, time: 0.188, loss: 1476.766113\n",
      "Train: step:  25560, time: 0.205, loss: 687.166138\n",
      "Train: step:  25570, time: 0.231, loss: 1409.057495\n",
      "Train: step:  25580, time: 0.189, loss: 1085.734863\n",
      "Train: step:  25590, time: 0.221, loss: 2261.460205\n",
      "Train: step:  25600, time: 0.223, loss: 1981.160767\n",
      "Train: step:  25610, time: 0.241, loss: 3299.488281\n",
      "Train: step:  25620, time: 0.226, loss: 1773.403320\n",
      "Train: step:  25630, time: 0.217, loss: 1095.839966\n",
      "Train: step:  25640, time: 0.197, loss: 386.368835\n",
      "Train: step:  25650, time: 0.191, loss: 2176.461914\n",
      "Train: step:  25660, time: 0.237, loss: 1328.462036\n",
      "Train: step:  25670, time: 0.237, loss: 1677.892334\n",
      "Train: step:  25680, time: 0.191, loss: 2506.980469\n",
      "Train: step:  25690, time: 0.252, loss: 1252.408325\n",
      "Train: step:  25700, time: 0.194, loss: 1430.772705\n",
      "Train: step:  25710, time: 0.262, loss: 1509.145142\n",
      "Train: step:  25720, time: 0.190, loss: 1672.247314\n",
      "Train: step:  25730, time: 0.203, loss: 2454.061279\n",
      "Train: step:  25740, time: 0.202, loss: 1832.876587\n",
      "Train: step:  25750, time: 0.219, loss: 2327.641357\n",
      "Train: step:  25760, time: 0.199, loss: 807.605286\n",
      "Train: step:  25770, time: 0.220, loss: 2100.084717\n",
      "Train: step:  25780, time: 0.206, loss: 1016.038574\n",
      "Train: step:  25790, time: 0.216, loss: 2385.299072\n",
      "Train: step:  25800, time: 0.226, loss: 209.526703\n",
      "Train: step:  25810, time: 0.220, loss: 1837.807739\n",
      "Train: step:  25820, time: 0.233, loss: 4488.613770\n",
      "Train: step:  25830, time: 0.191, loss: 505.624146\n",
      "Train: step:  25840, time: 0.206, loss: 1691.060303\n",
      "Train: step:  25850, time: 0.222, loss: 1122.001953\n",
      "Train: step:  25860, time: 0.190, loss: 2001.943848\n",
      "Train: step:  25870, time: 0.220, loss: 2115.289551\n",
      "Train: step:  25880, time: 0.218, loss: 1343.114990\n",
      "Train: step:  25890, time: 0.194, loss: 877.947449\n",
      "Train: step:  25900, time: 0.197, loss: 1020.328369\n",
      "Train: step:  25910, time: 0.204, loss: 2053.801025\n",
      "Train: step:  25920, time: 0.183, loss: 622.593567\n",
      "Train: step:  25930, time: 0.186, loss: 3346.216064\n",
      "Train: step:  25940, time: 0.185, loss: 1627.695679\n",
      "Train: step:  25950, time: 0.195, loss: 1604.109009\n",
      "Train: step:  25960, time: 0.182, loss: 1355.122681\n",
      "Train: step:  25970, time: 0.251, loss: 1329.068359\n",
      "Train: step:  25980, time: 0.227, loss: 1560.828857\n",
      "Train: step:  25990, time: 0.190, loss: 928.668884\n",
      "Train: step:  26000, time: 0.186, loss: 553.510803\n",
      "Train: step:  26010, time: 0.186, loss: 2035.199585\n",
      "Train: step:  26020, time: 0.218, loss: 1582.728394\n",
      "Train: step:  26030, time: 0.216, loss: 1221.436279\n",
      "Train: step:  26040, time: 0.225, loss: 1420.347778\n",
      "Train: step:  26050, time: 0.218, loss: 2775.505859\n",
      "Train: step:  26060, time: 0.215, loss: 3322.976807\n",
      "Train: step:  26070, time: 0.215, loss: 873.985474\n",
      "Train: step:  26080, time: 0.188, loss: 797.819702\n",
      "Train: step:  26090, time: 0.187, loss: 1202.155029\n",
      "Train: step:  26100, time: 0.189, loss: 2389.587891\n",
      "Train: step:  26110, time: 0.185, loss: 1888.116455\n",
      "Train: step:  26120, time: 0.193, loss: 1600.180298\n",
      "Train: step:  26130, time: 0.223, loss: 2459.033203\n",
      "Train: step:  26140, time: 0.186, loss: 1928.027222\n",
      "Train: step:  26150, time: 0.192, loss: 1719.561401\n",
      "Train: step:  26160, time: 0.188, loss: 1267.483276\n",
      "Train: step:  26170, time: 0.194, loss: 2362.737061\n",
      "Train: step:  26180, time: 0.253, loss: 659.632446\n",
      "Train: step:  26190, time: 0.225, loss: 2243.390137\n",
      "Train: step:  26200, time: 0.218, loss: 1438.359009\n",
      "Train: step:  26210, time: 0.202, loss: 1015.813599\n",
      "Train: step:  26220, time: 0.185, loss: 1438.554810\n",
      "Train: step:  26230, time: 0.234, loss: 1353.655640\n",
      "Train: step:  26240, time: 0.228, loss: 1935.842407\n",
      "Train: step:  26250, time: 0.214, loss: 224.674789\n",
      "Train: step:  26260, time: 0.199, loss: 2163.548828\n",
      "Train: step:  26270, time: 0.219, loss: 1389.952881\n",
      "Train: step:  26280, time: 0.189, loss: 2565.663086\n",
      "Train: step:  26290, time: 0.192, loss: 2135.268555\n",
      "Train: step:  26300, time: 0.207, loss: 1455.996582\n",
      "Train: step:  26310, time: 0.224, loss: 2961.871582\n",
      "Train: step:  26320, time: 0.230, loss: 561.256409\n",
      "Train: step:  26330, time: 0.189, loss: 1145.276001\n",
      "Train: step:  26340, time: 0.218, loss: 1165.337158\n",
      "Train: step:  26350, time: 0.198, loss: 515.162415\n",
      "Train: step:  26360, time: 0.232, loss: 2434.521240\n",
      "Train: step:  26370, time: 0.208, loss: 3391.948486\n",
      "Train: step:  26380, time: 0.192, loss: 1427.846313\n",
      "Train: step:  26390, time: 0.223, loss: 367.963562\n",
      "Train: step:  26400, time: 0.191, loss: 1518.567871\n",
      "Train: step:  26410, time: 0.197, loss: 2477.020996\n",
      "Train: step:  26420, time: 0.189, loss: 2348.591309\n",
      "Train: step:  26430, time: 0.226, loss: 3071.942139\n",
      "Train: step:  26440, time: 0.226, loss: 424.192169\n",
      "Train: step:  26450, time: 0.228, loss: 2934.010986\n",
      "Train: step:  26460, time: 0.200, loss: 1430.664307\n",
      "Train: step:  26470, time: 0.194, loss: 1578.282227\n",
      "Train: step:  26480, time: 0.217, loss: 836.214050\n",
      "Train: step:  26490, time: 0.190, loss: 989.038330\n",
      "Train: step:  26500, time: 0.239, loss: 1930.428589\n",
      "Train: step:  26510, time: 0.227, loss: 2470.378662\n",
      "Train: step:  26520, time: 0.191, loss: 3137.336426\n",
      "Train: step:  26530, time: 0.194, loss: 1397.427856\n",
      "Train: step:  26540, time: 0.229, loss: 2919.422119\n",
      "Train: step:  26550, time: 0.194, loss: 486.742157\n",
      "Train: step:  26560, time: 0.183, loss: 918.073120\n",
      "Train: step:  26570, time: 0.217, loss: 1560.058105\n",
      "Train: step:  26580, time: 0.247, loss: 2522.123779\n",
      "Train: step:  26590, time: 0.195, loss: 1006.511780\n",
      "Train: step:  26600, time: 0.218, loss: 2269.023926\n",
      "Train: step:  26610, time: 0.218, loss: 1993.833984\n",
      "Train: step:  26620, time: 0.216, loss: 1447.224854\n",
      "Train: step:  26630, time: 0.217, loss: 1391.863770\n",
      "Train: step:  26640, time: 0.195, loss: 1407.816284\n",
      "Train: step:  26650, time: 0.238, loss: 2575.604736\n",
      "Train: step:  26660, time: 0.239, loss: 3240.256592\n",
      "Train: step:  26670, time: 0.189, loss: 637.826904\n",
      "Train: step:  26680, time: 0.191, loss: 648.222351\n",
      "Train: step:  26690, time: 0.182, loss: 2365.531006\n",
      "Train: step:  26700, time: 0.225, loss: 1255.347656\n",
      "Train: step:  26710, time: 0.236, loss: 783.796509\n",
      "Train: step:  26720, time: 0.217, loss: 3247.624756\n",
      "Train: step:  26730, time: 0.232, loss: 2195.948730\n",
      "Train: step:  26740, time: 0.189, loss: 1901.254272\n",
      "Train: step:  26750, time: 0.206, loss: 2650.433594\n",
      "Train: step:  26760, time: 0.188, loss: 409.246704\n",
      "Train: step:  26770, time: 0.185, loss: 1893.704468\n",
      "Train: step:  26780, time: 0.216, loss: 1387.954590\n",
      "Train: step:  26790, time: 0.224, loss: 1948.830322\n",
      "Train: step:  26800, time: 0.191, loss: 1842.743042\n",
      "Train: step:  26810, time: 0.235, loss: 650.356506\n",
      "Train: step:  26820, time: 0.184, loss: 1454.739380\n",
      "Train: step:  26830, time: 0.228, loss: 1769.452759\n",
      "Train: step:  26840, time: 0.226, loss: 1857.796631\n",
      "Train: step:  26850, time: 0.190, loss: 1307.430786\n",
      "Train: step:  26860, time: 0.228, loss: 2778.782227\n",
      "Train: step:  26870, time: 0.220, loss: 3039.094238\n",
      "Train: step:  26880, time: 0.191, loss: 1222.928223\n",
      "Train: step:  26890, time: 0.197, loss: 3768.390381\n",
      "Train: step:  26900, time: 0.217, loss: 988.018555\n",
      "Train: step:  26910, time: 0.186, loss: 228.146988\n",
      "Train: step:  26920, time: 0.217, loss: 216.677979\n",
      "Train: step:  26930, time: 0.228, loss: 1560.109741\n",
      "Train: step:  26940, time: 0.218, loss: 1483.742676\n",
      "Train: step:  26950, time: 0.190, loss: 1565.167236\n",
      "Train: step:  26960, time: 0.230, loss: 2571.408447\n",
      "Train: step:  26970, time: 0.225, loss: 2125.455811\n",
      "Train: step:  26980, time: 0.186, loss: 479.053772\n",
      "Train: step:  26990, time: 0.223, loss: 3497.994141\n",
      "Train: step:  27000, time: 0.201, loss: 1571.620361\n",
      "Train: step:  27010, time: 0.247, loss: 2673.044922\n",
      "Train: step:  27020, time: 0.193, loss: 1466.807739\n",
      "Train: step:  27030, time: 0.195, loss: 4152.198730\n",
      "Train: step:  27040, time: 0.229, loss: 1439.404419\n",
      "Train: step:  27050, time: 0.211, loss: 430.586639\n",
      "Train: step:  27060, time: 0.190, loss: 2086.045898\n",
      "Train: step:  27070, time: 0.216, loss: 1436.196899\n",
      "Train: step:  27080, time: 0.239, loss: 209.437805\n",
      "Train: step:  27090, time: 0.190, loss: 818.737366\n",
      "Train: step:  27100, time: 0.227, loss: 487.215881\n",
      "Train: step:  27110, time: 0.196, loss: 1865.217896\n",
      "Train: step:  27120, time: 0.188, loss: 541.328918\n",
      "Train: step:  27130, time: 0.203, loss: 1973.074219\n",
      "Train: step:  27140, time: 0.193, loss: 1042.060547\n",
      "Train: step:  27150, time: 0.191, loss: 3388.938477\n",
      "Train: step:  27160, time: 0.186, loss: 1729.019531\n",
      "Train: step:  27170, time: 0.185, loss: 3237.127930\n",
      "Train: step:  27180, time: 0.189, loss: 2481.128662\n",
      "Train: step:  27190, time: 0.190, loss: 998.445557\n",
      "Train: step:  27200, time: 0.217, loss: 1832.400513\n",
      "Train: step:  27210, time: 0.218, loss: 899.776917\n",
      "Train: step:  27220, time: 0.217, loss: 2937.760254\n",
      "Train: step:  27230, time: 0.235, loss: 2036.254639\n",
      "Train: step:  27240, time: 0.203, loss: 1102.709717\n",
      "Train: step:  27250, time: 0.193, loss: 1967.783936\n",
      "Train: step:  27260, time: 0.186, loss: 1674.255737\n",
      "Train: step:  27270, time: 0.192, loss: 2745.841309\n",
      "Train: step:  27280, time: 0.185, loss: 2544.869141\n",
      "Train: step:  27290, time: 0.189, loss: 1974.949585\n",
      "Train: step:  27300, time: 0.196, loss: 1391.242188\n",
      "Train: step:  27310, time: 0.189, loss: 1508.608521\n",
      "Train: step:  27320, time: 0.188, loss: 2730.771240\n",
      "Train: step:  27330, time: 0.191, loss: 1339.175293\n",
      "Train: step:  27340, time: 0.190, loss: 3005.983643\n",
      "Train: step:  27350, time: 0.191, loss: 1180.217529\n",
      "Train: step:  27360, time: 0.188, loss: 3439.643799\n",
      "Train: step:  27370, time: 0.217, loss: 1452.298828\n",
      "Train: step:  27380, time: 0.205, loss: 1802.532837\n",
      "Train: step:  27390, time: 0.189, loss: 2118.756104\n",
      "Train: step:  27400, time: 0.229, loss: 2246.584229\n",
      "Train: step:  27410, time: 0.221, loss: 1477.360962\n",
      "Train: step:  27420, time: 0.240, loss: 4026.173584\n",
      "Train: step:  27430, time: 0.184, loss: 1004.915039\n",
      "Train: step:  27440, time: 0.219, loss: 410.485626\n",
      "Train: step:  27450, time: 0.235, loss: 1591.869263\n",
      "Train: step:  27460, time: 0.203, loss: 3692.609375\n",
      "Train: step:  27470, time: 0.187, loss: 1077.782959\n",
      "Train: step:  27480, time: 0.213, loss: 1127.017334\n",
      "Train: step:  27490, time: 0.229, loss: 2420.822021\n",
      "Train: step:  27500, time: 0.186, loss: 3103.073242\n",
      "Train: step:  27510, time: 0.213, loss: 2176.335938\n",
      "Train: step:  27520, time: 0.203, loss: 1333.887573\n",
      "Train: step:  27530, time: 0.188, loss: 1036.427979\n",
      "Train: step:  27540, time: 0.231, loss: 1452.097412\n",
      "Train: step:  27550, time: 0.217, loss: 2582.019043\n",
      "Train: step:  27560, time: 0.182, loss: 383.398682\n",
      "Train: step:  27570, time: 0.194, loss: 1446.981812\n",
      "Train: step:  27580, time: 0.230, loss: 501.282654\n",
      "Train: step:  27590, time: 0.191, loss: 1310.017700\n",
      "Train: step:  27600, time: 0.222, loss: 653.063110\n",
      "Train: step:  27610, time: 0.209, loss: 1745.264893\n",
      "Train: step:  27620, time: 0.186, loss: 1541.228394\n",
      "Train: step:  27630, time: 0.189, loss: 3850.570801\n",
      "Train: step:  27640, time: 0.217, loss: 4350.167480\n",
      "Train: step:  27650, time: 0.229, loss: 1515.102417\n",
      "Train: step:  27660, time: 0.192, loss: 1213.645630\n",
      "Train: step:  27670, time: 0.261, loss: 2719.829590\n",
      "Train: step:  27680, time: 0.184, loss: 1077.253662\n",
      "Train: step:  27690, time: 0.217, loss: 2010.535400\n",
      "Train: step:  27700, time: 0.210, loss: 2508.208740\n",
      "Train: step:  27710, time: 0.201, loss: 2518.126709\n",
      "Train: step:  27720, time: 0.219, loss: 2601.434082\n",
      "Train: step:  27730, time: 0.193, loss: 2251.829346\n",
      "Train: step:  27740, time: 0.189, loss: 3509.208252\n",
      "Train: step:  27750, time: 0.237, loss: 1597.688354\n",
      "Train: step:  27760, time: 0.189, loss: 535.434753\n",
      "Train: step:  27770, time: 0.190, loss: 1426.980225\n",
      "Train: step:  27780, time: 0.194, loss: 1715.240601\n",
      "Train: step:  27790, time: 0.226, loss: 1972.552124\n",
      "Train: step:  27800, time: 0.248, loss: 1960.650024\n",
      "Train: step:  27810, time: 0.197, loss: 3446.314209\n",
      "Train: step:  27820, time: 0.206, loss: 3206.198730\n",
      "Train: step:  27830, time: 0.195, loss: 2678.885498\n",
      "Train: step:  27840, time: 0.189, loss: 2109.681152\n",
      "Train: step:  27850, time: 0.240, loss: 3312.631592\n",
      "Train: step:  27860, time: 0.229, loss: 2023.000610\n",
      "Train: step:  27870, time: 0.238, loss: 1263.714600\n",
      "Train: step:  27880, time: 0.195, loss: 1273.072144\n",
      "Train: step:  27890, time: 0.216, loss: 2572.395264\n",
      "Train: step:  27900, time: 0.227, loss: 1169.802124\n",
      "Train: step:  27910, time: 0.213, loss: 3622.450684\n",
      "Train: step:  27920, time: 0.217, loss: 1224.980103\n",
      "Train: step:  27930, time: 0.242, loss: 2142.094482\n",
      "Train: step:  27940, time: 0.220, loss: 1003.212280\n",
      "Train: step:  27950, time: 0.184, loss: 1038.170654\n",
      "Train: step:  27960, time: 0.242, loss: 2599.758789\n",
      "Train: step:  27970, time: 0.217, loss: 3244.886719\n",
      "Train: step:  27980, time: 0.224, loss: 1169.500854\n",
      "Train: step:  27990, time: 0.217, loss: 1802.090698\n",
      "Train: step:  28000, time: 0.191, loss: 2312.775879\n",
      "Train: step:  28010, time: 0.189, loss: 391.756714\n",
      "Train: step:  28020, time: 0.187, loss: 2326.799805\n",
      "Train: step:  28030, time: 0.217, loss: 864.809875\n",
      "Train: step:  28040, time: 0.234, loss: 2071.131348\n",
      "Train: step:  28050, time: 0.184, loss: 2242.959473\n",
      "Train: step:  28060, time: 0.217, loss: 756.648865\n",
      "Train: step:  28070, time: 0.219, loss: 2982.870361\n",
      "Train: step:  28080, time: 0.240, loss: 2832.331055\n",
      "Train: step:  28090, time: 0.184, loss: 2369.581299\n",
      "Train: step:  28100, time: 0.198, loss: 1446.614624\n",
      "Train: step:  28110, time: 0.182, loss: 638.159973\n",
      "Train: step:  28120, time: 0.219, loss: 1716.638794\n",
      "Train: step:  28130, time: 0.193, loss: 484.401276\n",
      "Train: step:  28140, time: 0.217, loss: 1457.915161\n",
      "Train: step:  28150, time: 0.191, loss: 2490.985352\n",
      "Train: step:  28160, time: 0.195, loss: 2534.152344\n",
      "Train: step:  28170, time: 0.188, loss: 2606.285156\n",
      "Train: step:  28180, time: 0.186, loss: 710.687012\n",
      "Train: step:  28190, time: 0.186, loss: 1293.214722\n",
      "Train: step:  28200, time: 0.231, loss: 661.929688\n",
      "Train: step:  28210, time: 0.188, loss: 2168.322021\n",
      "Train: step:  28220, time: 0.186, loss: 2247.998535\n",
      "Train: step:  28230, time: 0.219, loss: 2235.639404\n",
      "Train: step:  28240, time: 0.197, loss: 852.748169\n",
      "Train: step:  28250, time: 0.232, loss: 996.262329\n",
      "Train: step:  28260, time: 0.201, loss: 1556.090210\n",
      "Train: step:  28270, time: 0.195, loss: 2168.063721\n",
      "Train: step:  28280, time: 0.226, loss: 1097.184326\n",
      "Train: step:  28290, time: 0.197, loss: 2506.498535\n",
      "Train: step:  28300, time: 0.230, loss: 2777.061768\n",
      "Train: step:  28310, time: 0.217, loss: 2216.750977\n",
      "Train: step:  28320, time: 0.192, loss: 2259.656250\n",
      "Train: step:  28330, time: 0.185, loss: 2028.831665\n",
      "Train: step:  28340, time: 0.212, loss: 528.982483\n",
      "Train: step:  28350, time: 0.192, loss: 680.453613\n",
      "Train: step:  28360, time: 0.188, loss: 1968.293213\n",
      "Train: step:  28370, time: 0.206, loss: 1130.573608\n",
      "Train: step:  28380, time: 0.194, loss: 670.209045\n",
      "Train: step:  28390, time: 0.184, loss: 431.937469\n",
      "Train: step:  28400, time: 0.220, loss: 1763.025879\n",
      "Train: step:  28410, time: 0.229, loss: 643.164307\n",
      "Train: step:  28420, time: 0.204, loss: 2372.265625\n",
      "Train: step:  28430, time: 0.196, loss: 1764.752930\n",
      "Train: step:  28440, time: 0.220, loss: 1458.293091\n",
      "Train: step:  28450, time: 0.190, loss: 924.111938\n",
      "Train: step:  28460, time: 0.205, loss: 2340.992188\n",
      "Train: step:  28470, time: 0.222, loss: 3486.944092\n",
      "Train: step:  28480, time: 0.202, loss: 3114.189209\n",
      "Train: step:  28490, time: 0.218, loss: 1452.886230\n",
      "Train: step:  28500, time: 0.184, loss: 694.251343\n",
      "Train: step:  28510, time: 0.199, loss: 2460.542969\n",
      "Train: step:  28520, time: 0.200, loss: 1511.318115\n",
      "Train: step:  28530, time: 0.193, loss: 1223.926880\n",
      "Train: step:  28540, time: 0.246, loss: 1754.739380\n",
      "Train: step:  28550, time: 0.217, loss: 812.339844\n",
      "Train: step:  28560, time: 0.227, loss: 5540.642578\n",
      "Train: step:  28570, time: 0.217, loss: 2491.862549\n",
      "Train: step:  28580, time: 0.193, loss: 1050.110352\n",
      "Train: step:  28590, time: 0.207, loss: 667.694397\n",
      "Train: step:  28600, time: 0.217, loss: 648.244934\n",
      "Train: step:  28610, time: 0.211, loss: 434.305847\n",
      "Train: step:  28620, time: 0.234, loss: 1203.151245\n",
      "Train: step:  28630, time: 0.216, loss: 1792.135498\n",
      "Train: step:  28640, time: 0.185, loss: 4337.218262\n",
      "Train: step:  28650, time: 0.183, loss: 1876.471680\n",
      "Train: step:  28660, time: 0.212, loss: 924.507324\n",
      "Train: step:  28670, time: 0.180, loss: 2161.670898\n",
      "Train: step:  28680, time: 0.217, loss: 1965.508057\n",
      "Train: step:  28690, time: 0.188, loss: 734.074829\n",
      "Train: step:  28700, time: 0.222, loss: 760.831909\n",
      "Train: step:  28710, time: 0.206, loss: 1922.569092\n",
      "Train: step:  28720, time: 0.187, loss: 860.878784\n",
      "Train: step:  28730, time: 0.188, loss: 1402.898926\n",
      "Train: step:  28740, time: 0.198, loss: 1752.535889\n",
      "Train: step:  28750, time: 0.214, loss: 1653.387573\n",
      "Train: step:  28760, time: 0.193, loss: 2725.446045\n",
      "Train: step:  28770, time: 0.198, loss: 1997.118652\n",
      "Train: step:  28780, time: 0.218, loss: 2506.784668\n",
      "Train: step:  28790, time: 0.186, loss: 2101.714844\n",
      "Train: step:  28800, time: 0.191, loss: 818.144958\n",
      "Train: step:  28810, time: 0.193, loss: 1222.460693\n",
      "Train: step:  28820, time: 0.196, loss: 1780.692261\n",
      "Train: step:  28830, time: 0.188, loss: 3661.157227\n",
      "Train: step:  28840, time: 0.229, loss: 925.711609\n",
      "Train: step:  28850, time: 0.225, loss: 2094.724854\n",
      "Train: step:  28860, time: 0.229, loss: 1856.337524\n",
      "Train: step:  28870, time: 0.192, loss: 716.541931\n",
      "Train: step:  28880, time: 0.234, loss: 2164.614502\n",
      "Train: step:  28890, time: 0.235, loss: 2671.381348\n",
      "Train: step:  28900, time: 0.204, loss: 1429.016357\n",
      "Train: step:  28910, time: 0.216, loss: 2275.269043\n",
      "Train: step:  28920, time: 0.198, loss: 1011.596313\n",
      "Train: step:  28930, time: 0.187, loss: 720.416443\n",
      "Train: step:  28940, time: 0.200, loss: 1664.732056\n",
      "Train: step:  28950, time: 0.195, loss: 3008.932861\n",
      "Train: step:  28960, time: 0.207, loss: 2769.153320\n",
      "Train: step:  28970, time: 0.188, loss: 837.907410\n",
      "Train: step:  28980, time: 0.224, loss: 2681.291260\n",
      "Train: step:  28990, time: 0.215, loss: 1222.240601\n",
      "Train: step:  29000, time: 0.192, loss: 1414.769043\n",
      "Train: step:  29010, time: 0.196, loss: 2923.013916\n",
      "Train: step:  29020, time: 0.215, loss: 1487.459961\n",
      "Train: step:  29030, time: 0.190, loss: 2064.530518\n",
      "Train: step:  29040, time: 0.201, loss: 2764.596924\n",
      "Train: step:  29050, time: 0.191, loss: 1621.086792\n",
      "Train: step:  29060, time: 0.331, loss: 2079.295166\n",
      "Train: step:  29070, time: 0.193, loss: 2152.319824\n",
      "Train: step:  29080, time: 0.191, loss: 1289.102417\n",
      "Train: step:  29090, time: 0.217, loss: 2992.264648\n",
      "Train: step:  29100, time: 0.242, loss: 1072.387085\n",
      "Train: step:  29110, time: 0.226, loss: 1164.954102\n",
      "Train: step:  29120, time: 0.219, loss: 3809.835693\n",
      "Train: step:  29130, time: 0.217, loss: 1762.581543\n",
      "Train: step:  29140, time: 0.217, loss: 2174.626465\n",
      "Train: step:  29150, time: 0.233, loss: 1876.189331\n",
      "Train: step:  29160, time: 0.202, loss: 751.841492\n",
      "Train: step:  29170, time: 0.216, loss: 1789.519287\n",
      "Train: step:  29180, time: 0.216, loss: 2215.989014\n",
      "Train: step:  29190, time: 0.216, loss: 2803.583252\n",
      "Train: step:  29200, time: 0.260, loss: 1652.948853\n",
      "Train: step:  29210, time: 0.192, loss: 351.019196\n",
      "Train: step:  29220, time: 0.191, loss: 1119.300171\n",
      "Train: step:  29230, time: 0.185, loss: 1951.177490\n",
      "Train: step:  29240, time: 0.214, loss: 535.880249\n",
      "Train: step:  29250, time: 0.186, loss: 3097.721436\n",
      "Train: step:  29260, time: 0.197, loss: 2240.898438\n",
      "Train: step:  29270, time: 0.223, loss: 1680.567627\n",
      "Train: step:  29280, time: 0.202, loss: 2325.216309\n",
      "Train: step:  29290, time: 0.184, loss: 1206.455688\n",
      "Train: step:  29300, time: 0.217, loss: 3505.172363\n",
      "Train: step:  29310, time: 0.191, loss: 1290.418091\n",
      "Train: step:  29320, time: 0.189, loss: 3586.498047\n",
      "Train: step:  29330, time: 0.224, loss: 790.315857\n",
      "Train: step:  29340, time: 0.233, loss: 2490.652832\n",
      "Train: step:  29350, time: 0.210, loss: 906.390198\n",
      "Train: step:  29360, time: 0.189, loss: 2880.447266\n",
      "Train: step:  29370, time: 0.185, loss: 2037.167236\n",
      "Train: step:  29380, time: 0.188, loss: 2586.116943\n",
      "Train: step:  29390, time: 0.217, loss: 3164.307129\n",
      "Train: step:  29400, time: 0.193, loss: 2452.868164\n",
      "Train: step:  29410, time: 0.191, loss: 1813.860962\n",
      "Train: step:  29420, time: 0.218, loss: 2716.834961\n",
      "Train: step:  29430, time: 0.230, loss: 1915.673218\n",
      "Train: step:  29440, time: 0.231, loss: 846.572266\n",
      "Train: step:  29450, time: 0.216, loss: 1922.289307\n",
      "Train: step:  29460, time: 0.226, loss: 1840.798096\n",
      "Train: step:  29470, time: 0.180, loss: 2102.411621\n",
      "Train: step:  29480, time: 0.202, loss: 559.361633\n",
      "Train: step:  29490, time: 0.190, loss: 3249.058105\n",
      "Train: step:  29500, time: 0.238, loss: 2644.152344\n",
      "Train: step:  29510, time: 0.191, loss: 609.124573\n",
      "Train: step:  29520, time: 0.227, loss: 573.354553\n",
      "Train: step:  29530, time: 0.190, loss: 1790.621582\n",
      "Train: step:  29540, time: 0.232, loss: 907.396912\n",
      "Train: step:  29550, time: 0.226, loss: 2847.116943\n",
      "Train: step:  29560, time: 0.215, loss: 758.194397\n",
      "Train: step:  29570, time: 0.336, loss: 2970.526123\n",
      "Train: step:  29580, time: 0.229, loss: 3404.588379\n",
      "Train: step:  29590, time: 0.216, loss: 1108.124268\n",
      "Train: step:  29600, time: 0.229, loss: 1741.784302\n",
      "Train: step:  29610, time: 0.216, loss: 1365.991333\n",
      "Train: step:  29620, time: 0.191, loss: 1885.142334\n",
      "Train: step:  29630, time: 0.194, loss: 1477.335938\n",
      "Train: step:  29640, time: 0.227, loss: 1137.269897\n",
      "Train: step:  29650, time: 0.214, loss: 2580.461670\n",
      "Train: step:  29660, time: 0.193, loss: 1696.811157\n",
      "Train: step:  29670, time: 0.239, loss: 2485.777832\n",
      "Train: step:  29680, time: 0.236, loss: 2185.105713\n",
      "Train: step:  29690, time: 0.213, loss: 1817.811646\n",
      "Train: step:  29700, time: 0.186, loss: 1904.849731\n",
      "Train: step:  29710, time: 0.213, loss: 1009.647278\n",
      "Train: step:  29720, time: 0.193, loss: 2604.901855\n",
      "Train: step:  29730, time: 0.220, loss: 1607.845581\n",
      "Train: step:  29740, time: 0.184, loss: 2102.604004\n",
      "Train: step:  29750, time: 0.228, loss: 2421.256592\n",
      "Train: step:  29760, time: 0.195, loss: 1249.188477\n",
      "Train: step:  29770, time: 0.217, loss: 1424.674316\n",
      "Train: step:  29780, time: 0.219, loss: 1165.006470\n",
      "Train: step:  29790, time: 0.214, loss: 2672.614746\n",
      "Train: step:  29800, time: 0.188, loss: 3600.790771\n",
      "Train: step:  29810, time: 0.191, loss: 1971.760742\n",
      "Train: step:  29820, time: 0.187, loss: 1587.286865\n",
      "Train: step:  29830, time: 0.186, loss: 1541.815430\n",
      "Train: step:  29840, time: 0.191, loss: 741.831116\n",
      "Train: step:  29850, time: 0.219, loss: 1537.114746\n",
      "Train: step:  29860, time: 0.185, loss: 1875.172485\n",
      "Train: step:  29870, time: 0.187, loss: 1881.382202\n",
      "Train: step:  29880, time: 0.206, loss: 2291.533447\n",
      "Train: step:  29890, time: 0.225, loss: 1502.301270\n",
      "Train: step:  29900, time: 0.190, loss: 3223.519287\n",
      "Train: step:  29910, time: 0.229, loss: 2004.780029\n",
      "Train: step:  29920, time: 0.191, loss: 1663.666870\n",
      "Train: step:  29930, time: 0.185, loss: 4803.246582\n",
      "Train: step:  29940, time: 0.203, loss: 998.827942\n",
      "Train: step:  29950, time: 0.211, loss: 2900.824707\n",
      "Train: step:  29960, time: 0.181, loss: 3178.146973\n",
      "Train: step:  29970, time: 0.185, loss: 458.158173\n",
      "Train: step:  29980, time: 0.186, loss: 1441.294189\n",
      "Train: step:  29990, time: 0.192, loss: 1152.654907\n",
      "Train: step:  30000, time: 0.222, loss: 1870.451050\n",
      "Train: step:  30010, time: 0.217, loss: 1100.017334\n",
      "Train: step:  30020, time: 0.222, loss: 2042.307983\n",
      "Train: step:  30030, time: 0.234, loss: 2616.360596\n",
      "Train: step:  30040, time: 0.196, loss: 531.641052\n",
      "Train: step:  30050, time: 0.184, loss: 1062.367065\n",
      "Train: step:  30060, time: 0.234, loss: 2334.815918\n",
      "Train: step:  30070, time: 0.187, loss: 1100.468384\n",
      "Train: step:  30080, time: 0.187, loss: 476.965454\n",
      "Train: step:  30090, time: 0.252, loss: 1824.020142\n",
      "Train: step:  30100, time: 0.226, loss: 4012.620605\n",
      "Train: step:  30110, time: 0.226, loss: 2917.354980\n",
      "Train: step:  30120, time: 0.222, loss: 1518.413818\n",
      "Train: step:  30130, time: 0.218, loss: 1819.570923\n",
      "Train: step:  30140, time: 0.187, loss: 2436.154541\n",
      "Train: step:  30150, time: 0.202, loss: 2466.005127\n",
      "Train: step:  30160, time: 0.193, loss: 1568.395874\n",
      "Train: step:  30170, time: 0.221, loss: 3019.681152\n",
      "Train: step:  30180, time: 0.219, loss: 886.057007\n",
      "Train: step:  30190, time: 0.199, loss: 2141.674072\n",
      "Train: step:  30200, time: 0.186, loss: 1341.085083\n",
      "Train: step:  30210, time: 0.191, loss: 584.382690\n",
      "Train: step:  30220, time: 0.188, loss: 1139.745483\n",
      "Train: step:  30230, time: 0.199, loss: 1318.157715\n",
      "Train: step:  30240, time: 0.203, loss: 3522.933594\n",
      "Train: step:  30250, time: 0.229, loss: 726.601074\n",
      "Train: step:  30260, time: 0.234, loss: 1508.184692\n",
      "Train: step:  30270, time: 0.226, loss: 900.417480\n",
      "Train: step:  30280, time: 0.185, loss: 297.617706\n",
      "Train: step:  30290, time: 0.188, loss: 1936.186035\n",
      "Train: step:  30300, time: 0.201, loss: 568.075745\n",
      "Train: step:  30310, time: 0.244, loss: 356.193848\n",
      "Train: step:  30320, time: 0.252, loss: 561.805115\n",
      "Train: step:  30330, time: 0.239, loss: 1656.160034\n",
      "Train: step:  30340, time: 0.230, loss: 1141.966675\n",
      "Train: step:  30350, time: 0.189, loss: 416.564911\n",
      "Train: step:  30360, time: 0.191, loss: 4734.223145\n",
      "Train: step:  30370, time: 0.225, loss: 1902.451660\n",
      "Train: step:  30380, time: 0.193, loss: 1113.783081\n",
      "Train: step:  30390, time: 0.226, loss: 3648.811768\n",
      "Train: step:  30400, time: 0.209, loss: 1438.648682\n",
      "Train: step:  30410, time: 0.187, loss: 1646.812500\n",
      "Train: step:  30420, time: 0.197, loss: 1546.276001\n",
      "Train: step:  30430, time: 0.226, loss: 1450.740601\n",
      "Train: step:  30440, time: 0.216, loss: 2362.819580\n",
      "Train: step:  30450, time: 0.185, loss: 2499.199951\n",
      "Train: step:  30460, time: 0.227, loss: 1472.901489\n",
      "Train: step:  30470, time: 0.218, loss: 3003.771240\n",
      "Train: step:  30480, time: 0.189, loss: 1955.581543\n",
      "Train: step:  30490, time: 0.187, loss: 1014.365540\n",
      "Train: step:  30500, time: 0.235, loss: 682.679016\n",
      "Train: step:  30510, time: 0.189, loss: 1855.957886\n",
      "Train: step:  30520, time: 0.209, loss: 2315.447510\n",
      "Train: step:  30530, time: 0.194, loss: 3350.298096\n",
      "Train: step:  30540, time: 0.230, loss: 2193.718506\n",
      "Train: step:  30550, time: 0.227, loss: 2036.494141\n",
      "Train: step:  30560, time: 0.218, loss: 1584.244507\n",
      "Train: step:  30570, time: 0.198, loss: 1151.500610\n",
      "Train: step:  30580, time: 0.187, loss: 1376.738525\n",
      "Train: step:  30590, time: 0.217, loss: 1943.578003\n",
      "Train: step:  30600, time: 0.220, loss: 488.915436\n",
      "Train: step:  30610, time: 0.184, loss: 2385.860596\n",
      "Train: step:  30620, time: 0.189, loss: 570.290833\n",
      "Train: step:  30630, time: 0.191, loss: 1573.366821\n",
      "Train: step:  30640, time: 0.181, loss: 1595.986572\n",
      "Train: step:  30650, time: 0.202, loss: 498.236176\n",
      "Train: step:  30660, time: 0.225, loss: 2559.403564\n",
      "Train: step:  30670, time: 0.195, loss: 1843.610474\n",
      "Train: step:  30680, time: 0.227, loss: 2644.956787\n",
      "Train: step:  30690, time: 0.229, loss: 1448.092163\n",
      "Train: step:  30700, time: 0.231, loss: 3151.584229\n",
      "Train: step:  30710, time: 0.190, loss: 1141.655029\n",
      "Train: step:  30720, time: 0.219, loss: 2100.713135\n",
      "Train: step:  30730, time: 0.216, loss: 1941.903076\n",
      "Train: step:  30740, time: 0.191, loss: 2839.581787\n",
      "Train: step:  30750, time: 0.196, loss: 4886.125977\n",
      "Train: step:  30760, time: 0.217, loss: 1381.176636\n",
      "Train: step:  30770, time: 0.186, loss: 1157.746094\n",
      "Train: step:  30780, time: 0.227, loss: 2740.402832\n",
      "Train: step:  30790, time: 0.197, loss: 1630.739258\n",
      "Train: step:  30800, time: 0.216, loss: 1484.268677\n",
      "Train: step:  30810, time: 0.197, loss: 1687.284302\n",
      "Train: step:  30820, time: 0.208, loss: 2150.499512\n",
      "Train: step:  30830, time: 0.192, loss: 1330.101562\n",
      "Train: step:  30840, time: 0.188, loss: 1710.569580\n",
      "Train: step:  30850, time: 0.229, loss: 2546.977295\n",
      "Train: step:  30860, time: 0.195, loss: 1981.078247\n",
      "Train: step:  30870, time: 0.190, loss: 3036.434082\n",
      "Train: step:  30880, time: 0.217, loss: 2997.091553\n",
      "Train: step:  30890, time: 0.216, loss: 2443.963867\n",
      "Train: step:  30900, time: 0.190, loss: 3518.096436\n",
      "Train: step:  30910, time: 0.221, loss: 1722.022827\n",
      "Train: step:  30920, time: 0.188, loss: 4156.458008\n",
      "Train: step:  30930, time: 0.191, loss: 2406.819824\n",
      "Train: step:  30940, time: 0.222, loss: 1299.817505\n",
      "Train: step:  30950, time: 0.195, loss: 2306.858887\n",
      "Train: step:  30960, time: 0.194, loss: 303.203644\n",
      "Train: step:  30970, time: 0.189, loss: 347.311462\n",
      "Train: step:  30980, time: 0.199, loss: 912.896118\n",
      "Train: step:  30990, time: 0.187, loss: 978.896545\n",
      "Train: step:  31000, time: 0.192, loss: 1457.099609\n",
      "Train: step:  31010, time: 0.214, loss: 2498.752930\n",
      "Train: step:  31020, time: 0.186, loss: 1996.270386\n",
      "Train: step:  31030, time: 0.188, loss: 2972.834961\n",
      "Train: step:  31040, time: 0.218, loss: 1693.317383\n",
      "Train: step:  31050, time: 0.195, loss: 1299.891235\n",
      "Train: step:  31060, time: 0.188, loss: 2015.719360\n",
      "Train: step:  31070, time: 0.231, loss: 582.193298\n",
      "Train: step:  31080, time: 0.187, loss: 2122.688721\n",
      "Train: step:  31090, time: 0.216, loss: 997.623352\n",
      "Train: step:  31100, time: 0.217, loss: 2962.726074\n",
      "Train: step:  31110, time: 0.187, loss: 697.291504\n",
      "Train: step:  31120, time: 0.188, loss: 2264.255859\n",
      "Train: step:  31130, time: 0.191, loss: 2352.468506\n",
      "Train: step:  31140, time: 0.230, loss: 826.881714\n",
      "Train: step:  31150, time: 0.213, loss: 1047.368652\n",
      "Train: step:  31160, time: 0.216, loss: 217.395355\n",
      "Train: step:  31170, time: 0.220, loss: 2855.117676\n",
      "Train: step:  31180, time: 0.219, loss: 2157.795166\n",
      "Train: step:  31190, time: 0.209, loss: 2052.621338\n",
      "Train: step:  31200, time: 0.217, loss: 1297.667358\n",
      "Train: step:  31210, time: 0.191, loss: 1630.068237\n",
      "Train: step:  31220, time: 0.212, loss: 2426.215332\n",
      "Train: step:  31230, time: 0.232, loss: 437.816833\n",
      "Train: step:  31240, time: 0.209, loss: 890.692078\n",
      "Train: step:  31250, time: 0.184, loss: 1809.821899\n",
      "Train: step:  31260, time: 0.217, loss: 1897.624390\n",
      "Train: step:  31270, time: 0.190, loss: 985.454163\n",
      "Train: step:  31280, time: 0.217, loss: 3150.895996\n",
      "Train: step:  31290, time: 0.218, loss: 1384.818970\n",
      "Train: step:  31300, time: 0.198, loss: 3037.732422\n",
      "Train: step:  31310, time: 0.190, loss: 2174.035889\n",
      "Train: step:  31320, time: 0.197, loss: 3235.312744\n",
      "Train: step:  31330, time: 0.200, loss: 1236.460327\n",
      "Train: step:  31340, time: 0.229, loss: 980.911133\n",
      "Train: step:  31350, time: 0.217, loss: 2669.133545\n",
      "Train: step:  31360, time: 0.226, loss: 2080.946777\n",
      "Train: step:  31370, time: 0.207, loss: 895.764893\n",
      "Train: step:  31380, time: 0.228, loss: 2437.814697\n",
      "Train: step:  31390, time: 0.215, loss: 2198.533447\n",
      "Train: step:  31400, time: 0.188, loss: 541.210754\n",
      "Train: step:  31410, time: 0.219, loss: 2107.080566\n",
      "Train: step:  31420, time: 0.214, loss: 2219.088379\n",
      "Train: step:  31430, time: 0.192, loss: 430.438019\n",
      "Train: step:  31440, time: 0.200, loss: 1164.660522\n",
      "Train: step:  31450, time: 0.228, loss: 2593.841797\n",
      "Train: step:  31460, time: 0.236, loss: 1089.269043\n",
      "Train: step:  31470, time: 0.225, loss: 2854.615967\n",
      "Train: step:  31480, time: 0.225, loss: 1767.497314\n",
      "Train: step:  31490, time: 0.189, loss: 923.536865\n",
      "Train: step:  31500, time: 0.199, loss: 2727.328125\n",
      "Train: step:  31510, time: 0.201, loss: 1651.308472\n",
      "Train: step:  31520, time: 0.183, loss: 2594.088867\n",
      "Train: step:  31530, time: 0.193, loss: 3025.597412\n",
      "Train: step:  31540, time: 0.184, loss: 2033.515625\n",
      "Train: step:  31550, time: 0.196, loss: 1494.570068\n",
      "Train: step:  31560, time: 0.229, loss: 4027.916748\n",
      "Train: step:  31570, time: 0.188, loss: 983.916260\n",
      "Train: step:  31580, time: 0.216, loss: 906.322693\n",
      "Train: step:  31590, time: 0.207, loss: 2551.652344\n",
      "Train: step:  31600, time: 0.201, loss: 1636.034058\n",
      "Train: step:  31610, time: 0.213, loss: 493.457092\n",
      "Train: step:  31620, time: 0.231, loss: 1043.112793\n",
      "Train: step:  31630, time: 0.192, loss: 3447.709717\n",
      "Train: step:  31640, time: 0.216, loss: 2776.700439\n",
      "Train: step:  31650, time: 0.196, loss: 3008.804443\n",
      "Train: step:  31660, time: 0.187, loss: 2165.210449\n",
      "Train: step:  31670, time: 0.218, loss: 2833.902100\n",
      "Train: step:  31680, time: 0.184, loss: 2265.332764\n",
      "Train: step:  31690, time: 0.286, loss: 2437.079102\n",
      "Train: step:  31700, time: 0.213, loss: 2983.927490\n",
      "Train: step:  31710, time: 0.185, loss: 2544.646240\n",
      "Train: step:  31720, time: 0.233, loss: 1318.386963\n",
      "Train: step:  31730, time: 0.204, loss: 3507.359375\n",
      "Train: step:  31740, time: 0.221, loss: 1844.086792\n",
      "Train: step:  31750, time: 0.211, loss: 3232.298828\n",
      "Train: step:  31760, time: 0.195, loss: 2048.184814\n",
      "Train: step:  31770, time: 0.217, loss: 2512.331299\n",
      "Train: step:  31780, time: 0.212, loss: 1066.782471\n",
      "Train: step:  31790, time: 0.205, loss: 1647.642578\n",
      "Train: step:  31800, time: 0.186, loss: 2301.825195\n",
      "Train: step:  31810, time: 0.186, loss: 2722.418701\n",
      "Train: step:  31820, time: 0.189, loss: 3040.771729\n",
      "Train: step:  31830, time: 0.241, loss: 2477.560303\n",
      "Train: step:  31840, time: 0.189, loss: 524.420715\n",
      "Train: step:  31850, time: 0.233, loss: 728.108032\n",
      "Train: step:  31860, time: 0.229, loss: 2034.259277\n",
      "Train: step:  31870, time: 0.197, loss: 2557.667969\n",
      "Train: step:  31880, time: 0.188, loss: 1986.775513\n",
      "Train: step:  31890, time: 0.197, loss: 4368.267578\n",
      "Train: step:  31900, time: 0.226, loss: 2335.674316\n",
      "Train: step:  31910, time: 0.247, loss: 498.184845\n",
      "Train: step:  31920, time: 0.194, loss: 4159.894043\n",
      "Train: step:  31930, time: 0.186, loss: 2678.965820\n",
      "Train: step:  31940, time: 0.190, loss: 822.105896\n",
      "Train: step:  31950, time: 0.217, loss: 860.419861\n",
      "Train: step:  31960, time: 0.221, loss: 2098.163574\n",
      "Train: step:  31970, time: 0.217, loss: 812.375122\n",
      "Train: step:  31980, time: 0.234, loss: 2455.816406\n",
      "Train: step:  31990, time: 0.236, loss: 2144.879150\n",
      "Train: step:  32000, time: 0.183, loss: 1984.270508\n",
      "Train: step:  32010, time: 0.221, loss: 1272.922607\n",
      "Train: step:  32020, time: 0.219, loss: 831.365112\n",
      "Train: step:  32030, time: 0.189, loss: 2395.975830\n",
      "Train: step:  32040, time: 0.185, loss: 2470.358643\n",
      "Train: step:  32050, time: 0.223, loss: 828.583008\n",
      "Train: step:  32060, time: 0.192, loss: 1843.933472\n",
      "Train: step:  32070, time: 0.206, loss: 1667.873535\n",
      "Train: step:  32080, time: 0.216, loss: 2038.001221\n",
      "Train: step:  32090, time: 0.197, loss: 1576.483154\n",
      "Train: step:  32100, time: 0.202, loss: 922.398376\n",
      "Train: step:  32110, time: 0.189, loss: 2029.506836\n",
      "Train: step:  32120, time: 0.255, loss: 488.474579\n",
      "Train: step:  32130, time: 0.235, loss: 2172.199707\n",
      "Train: step:  32140, time: 0.219, loss: 1148.003662\n",
      "Train: step:  32150, time: 0.192, loss: 756.966858\n",
      "Train: step:  32160, time: 0.216, loss: 554.864990\n",
      "Train: step:  32170, time: 0.242, loss: 795.673767\n",
      "Train: step:  32180, time: 0.190, loss: 1023.434570\n",
      "Train: step:  32190, time: 0.215, loss: 4387.030273\n",
      "Train: step:  32200, time: 0.230, loss: 1815.752441\n",
      "Train: step:  32210, time: 0.225, loss: 3167.360596\n",
      "Train: step:  32220, time: 0.225, loss: 1127.005249\n",
      "Train: step:  32230, time: 0.200, loss: 1642.884399\n",
      "Train: step:  32240, time: 0.222, loss: 3557.634766\n",
      "Train: step:  32250, time: 0.187, loss: 1885.619751\n",
      "Train: step:  32260, time: 0.213, loss: 2746.763916\n",
      "Train: step:  32270, time: 0.185, loss: 1638.693604\n",
      "Train: step:  32280, time: 0.200, loss: 1960.884155\n",
      "Train: step:  32290, time: 0.217, loss: 917.536316\n",
      "Train: step:  32300, time: 0.191, loss: 1080.760742\n",
      "Train: step:  32310, time: 0.225, loss: 701.234924\n",
      "Train: step:  32320, time: 0.244, loss: 1633.930176\n",
      "Train: step:  32330, time: 0.231, loss: 4604.904785\n",
      "Train: step:  32340, time: 0.185, loss: 648.761475\n",
      "Train: step:  32350, time: 0.186, loss: 1581.438721\n",
      "Train: step:  32360, time: 0.213, loss: 2067.431641\n",
      "Train: step:  32370, time: 0.188, loss: 394.467102\n",
      "Train: step:  32380, time: 0.227, loss: 1888.403442\n",
      "Train: step:  32390, time: 0.217, loss: 617.322998\n",
      "Train: step:  32400, time: 0.187, loss: 2141.366211\n",
      "Train: step:  32410, time: 0.194, loss: 455.525146\n",
      "Train: step:  32420, time: 0.217, loss: 4430.700195\n",
      "Train: step:  32430, time: 0.191, loss: 1106.577148\n",
      "Train: step:  32440, time: 0.216, loss: 1345.034912\n",
      "Train: step:  32450, time: 0.231, loss: 2132.810059\n",
      "Train: step:  32460, time: 0.186, loss: 505.049500\n",
      "Train: step:  32470, time: 0.190, loss: 566.694885\n",
      "Train: step:  32480, time: 0.191, loss: 430.626312\n",
      "Train: step:  32490, time: 0.226, loss: 1403.181519\n",
      "Train: step:  32500, time: 0.185, loss: 1363.328979\n",
      "Train: step:  32510, time: 0.213, loss: 1267.368408\n",
      "Train: step:  32520, time: 0.193, loss: 3843.474854\n",
      "Train: step:  32530, time: 0.191, loss: 1178.454468\n",
      "Train: step:  32540, time: 0.195, loss: 1681.100586\n",
      "Train: step:  32550, time: 0.208, loss: 2027.977905\n",
      "Train: step:  32560, time: 0.191, loss: 1464.285034\n",
      "Train: step:  32570, time: 0.215, loss: 4475.288086\n",
      "Train: step:  32580, time: 0.189, loss: 1694.382446\n",
      "Train: step:  32590, time: 0.185, loss: 2315.637939\n",
      "Train: step:  32600, time: 0.186, loss: 2500.594238\n",
      "Train: step:  32610, time: 0.188, loss: 2073.699951\n",
      "Train: step:  32620, time: 0.186, loss: 1462.537476\n",
      "Train: step:  32630, time: 0.230, loss: 2666.030762\n",
      "Train: step:  32640, time: 0.192, loss: 3936.450439\n",
      "Train: step:  32650, time: 0.189, loss: 290.477661\n",
      "Train: step:  32660, time: 0.217, loss: 1652.066895\n",
      "Train: step:  32670, time: 0.194, loss: 2484.152832\n",
      "Train: step:  32680, time: 0.203, loss: 2875.916504\n",
      "Train: step:  32690, time: 0.194, loss: 653.755188\n",
      "Train: step:  32700, time: 0.219, loss: 920.403992\n",
      "Train: step:  32710, time: 0.216, loss: 3905.774414\n",
      "Train: step:  32720, time: 0.235, loss: 625.427551\n",
      "Train: step:  32730, time: 0.222, loss: 2875.113281\n",
      "Train: step:  32740, time: 0.231, loss: 1246.626099\n",
      "Train: step:  32750, time: 0.196, loss: 583.147034\n",
      "Train: step:  32760, time: 0.226, loss: 1068.973145\n",
      "Train: step:  32770, time: 0.219, loss: 1591.952271\n",
      "Train: step:  32780, time: 0.190, loss: 2407.486572\n",
      "Train: step:  32790, time: 0.225, loss: 3217.940430\n",
      "Train: step:  32800, time: 0.189, loss: 278.170380\n",
      "Train: step:  32810, time: 0.194, loss: 332.671722\n",
      "Train: step:  32820, time: 0.192, loss: 2548.576416\n",
      "Train: step:  32830, time: 0.217, loss: 1891.364990\n",
      "Train: step:  32840, time: 0.227, loss: 2585.249756\n",
      "Train: step:  32850, time: 0.217, loss: 4423.271973\n",
      "Train: step:  32860, time: 0.222, loss: 3025.474609\n",
      "Train: step:  32870, time: 0.237, loss: 2108.786133\n",
      "Train: step:  32880, time: 0.226, loss: 2936.483643\n",
      "Train: step:  32890, time: 0.231, loss: 1651.936401\n",
      "Train: step:  32900, time: 0.231, loss: 3371.345459\n",
      "Train: step:  32910, time: 0.228, loss: 992.951111\n",
      "Train: step:  32920, time: 0.231, loss: 813.410400\n",
      "Train: step:  32930, time: 0.215, loss: 1138.408691\n",
      "Train: step:  32940, time: 0.192, loss: 2632.974365\n",
      "Train: step:  32950, time: 0.198, loss: 1931.444458\n",
      "Train: step:  32960, time: 0.217, loss: 2834.605225\n",
      "Train: step:  32970, time: 0.194, loss: 2989.527100\n",
      "Train: step:  32980, time: 0.218, loss: 1042.537720\n",
      "Train: step:  32990, time: 0.188, loss: 2024.166992\n",
      "Train: step:  33000, time: 0.253, loss: 1417.484863\n",
      "Train: step:  33010, time: 0.219, loss: 1432.881348\n",
      "Train: step:  33020, time: 0.196, loss: 2109.928711\n",
      "Train: step:  33030, time: 0.227, loss: 746.912598\n",
      "Train: step:  33040, time: 0.218, loss: 3285.972656\n",
      "Train: step:  33050, time: 0.226, loss: 2006.046753\n",
      "Train: step:  33060, time: 0.215, loss: 521.178772\n",
      "Train: step:  33070, time: 0.213, loss: 2584.144043\n",
      "Train: step:  33080, time: 0.217, loss: 3326.634521\n",
      "Train: step:  33090, time: 0.196, loss: 1879.302246\n",
      "Train: step:  33100, time: 0.191, loss: 2832.558350\n",
      "Train: step:  33110, time: 0.218, loss: 1302.813965\n",
      "Train: step:  33120, time: 0.229, loss: 1476.804199\n",
      "Train: step:  33130, time: 0.191, loss: 1388.910645\n",
      "Train: step:  33140, time: 0.193, loss: 1308.377563\n",
      "Train: step:  33150, time: 0.191, loss: 763.593506\n",
      "Train: step:  33160, time: 0.184, loss: 1797.805664\n",
      "Train: step:  33170, time: 0.259, loss: 1377.234497\n",
      "Train: step:  33180, time: 0.226, loss: 1505.475586\n",
      "Train: step:  33190, time: 0.227, loss: 3159.185791\n",
      "Train: step:  33200, time: 0.224, loss: 1060.883301\n",
      "Train: step:  33210, time: 0.190, loss: 2676.544678\n",
      "Train: step:  33220, time: 0.193, loss: 2210.723633\n",
      "Train: step:  33230, time: 0.193, loss: 2387.850586\n",
      "Train: step:  33240, time: 0.194, loss: 2369.298584\n",
      "Train: step:  33250, time: 0.216, loss: 2725.625977\n",
      "Train: step:  33260, time: 0.195, loss: 1046.167969\n",
      "Train: step:  33270, time: 0.216, loss: 2097.500244\n",
      "Train: step:  33280, time: 0.190, loss: 1197.564941\n",
      "Train: step:  33290, time: 0.218, loss: 1157.424194\n",
      "Train: step:  33300, time: 0.213, loss: 3206.808838\n",
      "Train: step:  33310, time: 0.217, loss: 871.578125\n",
      "Train: step:  33320, time: 0.186, loss: 2480.361572\n",
      "Train: step:  33330, time: 0.217, loss: 2362.411377\n",
      "Train: step:  33340, time: 0.192, loss: 3315.384766\n",
      "Train: step:  33350, time: 0.189, loss: 3013.675781\n",
      "Train: step:  33360, time: 0.217, loss: 824.189148\n",
      "Train: step:  33370, time: 0.232, loss: 2598.744873\n",
      "Train: step:  33380, time: 0.220, loss: 3249.927734\n",
      "Train: step:  33390, time: 0.243, loss: 848.826660\n",
      "Train: step:  33400, time: 0.189, loss: 2284.929443\n",
      "Train: step:  33410, time: 0.189, loss: 2384.465820\n",
      "Train: step:  33420, time: 0.244, loss: 1859.673462\n",
      "Train: step:  33430, time: 0.246, loss: 546.907837\n",
      "Train: step:  33440, time: 0.239, loss: 1869.350464\n",
      "Train: step:  33450, time: 0.238, loss: 783.580261\n",
      "Train: step:  33460, time: 0.183, loss: 2538.402344\n",
      "Train: step:  33470, time: 0.229, loss: 2243.871582\n",
      "Train: step:  33480, time: 0.215, loss: 2668.383057\n",
      "Train: step:  33490, time: 0.256, loss: 1496.140747\n",
      "Train: step:  33500, time: 0.199, loss: 2243.454346\n",
      "Train: step:  33510, time: 0.238, loss: 3717.612793\n",
      "Train: step:  33520, time: 0.192, loss: 878.367493\n",
      "Train: step:  33530, time: 0.222, loss: 451.962555\n",
      "Train: step:  33540, time: 0.190, loss: 911.774963\n",
      "Train: step:  33550, time: 0.231, loss: 2957.483154\n",
      "Train: step:  33560, time: 0.189, loss: 2224.280273\n",
      "Train: step:  33570, time: 0.200, loss: 1988.611816\n",
      "Train: step:  33580, time: 0.224, loss: 3133.339600\n",
      "Train: step:  33590, time: 0.192, loss: 2534.471436\n",
      "Train: step:  33600, time: 0.199, loss: 965.518372\n",
      "Train: step:  33610, time: 0.194, loss: 1722.359253\n",
      "Train: step:  33620, time: 0.210, loss: 3196.292236\n",
      "Train: step:  33630, time: 0.215, loss: 3156.267822\n",
      "Train: step:  33640, time: 0.225, loss: 1871.070801\n",
      "Train: step:  33650, time: 0.247, loss: 1366.879761\n",
      "Train: step:  33660, time: 0.260, loss: 1058.406494\n",
      "Train: step:  33670, time: 0.192, loss: 544.726562\n",
      "Train: step:  33680, time: 0.184, loss: 506.275574\n",
      "Train: step:  33690, time: 0.217, loss: 1925.186646\n",
      "Train: step:  33700, time: 0.217, loss: 2623.393311\n",
      "Train: step:  33710, time: 0.204, loss: 1291.229492\n",
      "Train: step:  33720, time: 0.187, loss: 3313.340088\n",
      "Train: step:  33730, time: 0.233, loss: 880.969910\n",
      "Train: step:  33740, time: 0.194, loss: 2721.216797\n",
      "Train: step:  33750, time: 0.217, loss: 2289.782227\n",
      "Train: step:  33760, time: 0.185, loss: 3039.239014\n",
      "Train: step:  33770, time: 0.225, loss: 3109.498779\n",
      "Train: step:  33780, time: 0.204, loss: 1257.673706\n",
      "Train: step:  33790, time: 0.185, loss: 2763.124023\n",
      "Train: step:  33800, time: 0.231, loss: 1070.872437\n",
      "Train: step:  33810, time: 0.217, loss: 1217.100952\n",
      "Train: step:  33820, time: 0.187, loss: 840.823669\n",
      "Train: step:  33830, time: 0.190, loss: 810.012634\n",
      "Train: step:  33840, time: 0.188, loss: 1955.695801\n",
      "Train: step:  33850, time: 0.197, loss: 2429.803223\n",
      "Train: step:  33860, time: 0.185, loss: 2796.558105\n",
      "Train: step:  33870, time: 0.214, loss: 1232.334839\n",
      "Train: step:  33880, time: 0.188, loss: 849.081665\n",
      "Train: step:  33890, time: 0.189, loss: 2521.624268\n",
      "Train: step:  33900, time: 0.256, loss: 820.387634\n",
      "Train: step:  33910, time: 0.200, loss: 2013.131348\n",
      "Train: step:  33920, time: 0.184, loss: 2489.847168\n",
      "Train: step:  33930, time: 0.214, loss: 1577.694580\n",
      "Train: step:  33940, time: 0.238, loss: 2575.693359\n",
      "Train: step:  33950, time: 0.251, loss: 1581.135010\n",
      "Train: step:  33960, time: 0.231, loss: 1293.662720\n",
      "Train: step:  33970, time: 0.240, loss: 471.190552\n",
      "Train: step:  33980, time: 0.215, loss: 457.624451\n",
      "Train: step:  33990, time: 0.249, loss: 606.140076\n",
      "Train: step:  34000, time: 0.203, loss: 626.373718\n",
      "Train: step:  34010, time: 0.216, loss: 4098.722656\n",
      "Train: step:  34020, time: 0.218, loss: 1482.164185\n",
      "Train: step:  34030, time: 0.191, loss: 2949.005615\n",
      "Train: step:  34040, time: 0.217, loss: 1103.066040\n",
      "Train: step:  34050, time: 0.197, loss: 3115.667725\n",
      "Train: step:  34060, time: 0.192, loss: 1677.085083\n",
      "Train: step:  34070, time: 0.226, loss: 1221.145020\n",
      "Train: step:  34080, time: 0.187, loss: 2021.529175\n",
      "Train: step:  34090, time: 0.194, loss: 2112.546387\n",
      "Train: step:  34100, time: 0.226, loss: 1779.477051\n",
      "Train: step:  34110, time: 0.209, loss: 1317.707764\n",
      "Train: step:  34120, time: 0.216, loss: 4027.534180\n",
      "Train: step:  34130, time: 0.212, loss: 564.781067\n",
      "Train: step:  34140, time: 0.194, loss: 1093.731567\n",
      "Train: step:  34150, time: 0.193, loss: 1574.958252\n",
      "Train: step:  34160, time: 0.192, loss: 1751.780151\n",
      "Train: step:  34170, time: 0.231, loss: 2899.282471\n",
      "Train: step:  34180, time: 0.192, loss: 2215.297363\n",
      "Train: step:  34190, time: 0.187, loss: 2195.950684\n",
      "Train: step:  34200, time: 0.199, loss: 1798.737549\n",
      "Train: step:  34210, time: 0.193, loss: 1074.868896\n",
      "Train: step:  34220, time: 0.217, loss: 2055.908447\n",
      "Train: step:  34230, time: 0.193, loss: 1010.589905\n",
      "Train: step:  34240, time: 0.191, loss: 2059.759766\n",
      "Train: step:  34250, time: 0.229, loss: 1174.754272\n",
      "Train: step:  34260, time: 0.190, loss: 1276.421021\n",
      "Train: step:  34270, time: 0.214, loss: 522.906555\n",
      "Train: step:  34280, time: 0.214, loss: 1059.369263\n",
      "Train: step:  34290, time: 0.190, loss: 1754.241211\n",
      "Train: step:  34300, time: 0.186, loss: 2608.448486\n",
      "Train: step:  34310, time: 0.227, loss: 1110.712036\n",
      "Train: step:  34320, time: 0.193, loss: 1459.206909\n",
      "Train: step:  34330, time: 0.192, loss: 796.310425\n",
      "Train: step:  34340, time: 0.234, loss: 1331.868286\n",
      "Train: step:  34350, time: 0.232, loss: 1334.847656\n",
      "Train: step:  34360, time: 0.227, loss: 1883.869019\n",
      "Train: step:  34370, time: 0.192, loss: 399.863708\n",
      "Train: step:  34380, time: 0.220, loss: 3031.023682\n",
      "Train: step:  34390, time: 0.256, loss: 530.289246\n",
      "Train: step:  34400, time: 0.199, loss: 1651.044312\n",
      "Train: step:  34410, time: 0.245, loss: 982.069580\n",
      "Train: step:  34420, time: 0.216, loss: 2183.480713\n",
      "Train: step:  34430, time: 0.228, loss: 2847.606201\n",
      "Train: step:  34440, time: 0.244, loss: 1807.786743\n",
      "Train: step:  34450, time: 0.190, loss: 2091.115723\n",
      "Train: step:  34460, time: 0.217, loss: 928.757263\n",
      "Train: step:  34470, time: 0.218, loss: 2828.648926\n",
      "Train: step:  34480, time: 0.228, loss: 2838.022705\n",
      "Train: step:  34490, time: 0.218, loss: 2681.540771\n",
      "Train: step:  34500, time: 0.238, loss: 1229.984985\n",
      "Train: step:  34510, time: 0.218, loss: 1623.599609\n",
      "Train: step:  34520, time: 0.212, loss: 649.455200\n",
      "Train: step:  34530, time: 0.191, loss: 1209.668579\n",
      "Train: step:  34540, time: 0.216, loss: 1290.088257\n",
      "Train: step:  34550, time: 0.211, loss: 760.882874\n",
      "Train: step:  34560, time: 0.182, loss: 3031.786377\n",
      "Train: step:  34570, time: 0.229, loss: 1498.793335\n",
      "Train: step:  34580, time: 0.363, loss: 609.590332\n",
      "Train: step:  34590, time: 0.218, loss: 587.771973\n",
      "Train: step:  34600, time: 0.229, loss: 2562.853271\n",
      "Train: step:  34610, time: 0.218, loss: 1065.879272\n",
      "Train: step:  34620, time: 0.227, loss: 2200.874512\n",
      "Train: step:  34630, time: 0.238, loss: 1571.192871\n",
      "Train: step:  34640, time: 0.220, loss: 2531.771240\n",
      "Train: step:  34650, time: 0.202, loss: 2025.967407\n",
      "Train: step:  34660, time: 0.219, loss: 2186.122070\n",
      "Train: step:  34670, time: 0.193, loss: 1760.672241\n",
      "Train: step:  34680, time: 0.190, loss: 2774.302002\n",
      "Train: step:  34690, time: 0.191, loss: 2362.580566\n",
      "Train: step:  34700, time: 0.194, loss: 1596.669434\n",
      "Train: step:  34710, time: 0.188, loss: 1149.514282\n",
      "Train: step:  34720, time: 0.218, loss: 1583.512085\n",
      "Train: step:  34730, time: 0.227, loss: 1594.241577\n",
      "Train: step:  34740, time: 0.215, loss: 1967.301025\n",
      "Train: step:  34750, time: 0.229, loss: 1363.309692\n",
      "Train: step:  34760, time: 0.251, loss: 2139.520020\n",
      "Train: step:  34770, time: 0.196, loss: 1438.994507\n",
      "Train: step:  34780, time: 0.192, loss: 744.152588\n",
      "Train: step:  34790, time: 0.199, loss: 3114.445312\n",
      "Train: step:  34800, time: 0.230, loss: 1089.883179\n",
      "Train: step:  34810, time: 0.184, loss: 1673.862305\n",
      "Train: step:  34820, time: 0.192, loss: 2545.234375\n",
      "Train: step:  34830, time: 0.231, loss: 1796.540771\n",
      "Train: step:  34840, time: 0.197, loss: 714.533386\n",
      "Train: step:  34850, time: 0.217, loss: 1125.752808\n",
      "Train: step:  34860, time: 0.286, loss: 625.814636\n",
      "Train: step:  34870, time: 0.244, loss: 2441.695557\n",
      "Train: step:  34880, time: 0.196, loss: 1937.567627\n",
      "Train: step:  34890, time: 0.202, loss: 1393.521606\n",
      "Train: step:  34900, time: 0.225, loss: 2797.350830\n",
      "Train: step:  34910, time: 0.187, loss: 2261.839600\n",
      "Train: step:  34920, time: 0.229, loss: 1367.694824\n",
      "Train: step:  34930, time: 0.241, loss: 2485.192627\n",
      "Train: step:  34940, time: 0.212, loss: 1962.501953\n",
      "Train: step:  34950, time: 0.237, loss: 1509.155151\n",
      "Train: step:  34960, time: 0.186, loss: 1583.380859\n",
      "Train: step:  34970, time: 0.236, loss: 2271.340576\n",
      "Train: step:  34980, time: 0.195, loss: 3093.301025\n",
      "Train: step:  34990, time: 0.219, loss: 2039.564575\n",
      "Train: step:  35000, time: 0.217, loss: 1080.767212\n",
      "Train: step:  35010, time: 0.225, loss: 397.369690\n",
      "Train: step:  35020, time: 0.193, loss: 2780.737793\n",
      "Train: step:  35030, time: 0.220, loss: 1852.661255\n",
      "Train: step:  35040, time: 0.215, loss: 1094.614136\n",
      "Train: step:  35050, time: 0.217, loss: 3043.575439\n",
      "Train: step:  35060, time: 0.209, loss: 3036.349854\n",
      "Train: step:  35070, time: 0.218, loss: 1710.790771\n",
      "Train: step:  35080, time: 0.232, loss: 1421.228027\n",
      "Train: step:  35090, time: 0.236, loss: 2475.064697\n",
      "Train: step:  35100, time: 0.236, loss: 2302.723389\n",
      "Train: step:  35110, time: 0.217, loss: 1145.553223\n",
      "Train: step:  35120, time: 0.189, loss: 1307.895874\n",
      "Train: step:  35130, time: 0.191, loss: 1505.434692\n",
      "Train: step:  35140, time: 0.187, loss: 1687.228271\n",
      "Train: step:  35150, time: 0.227, loss: 1110.039185\n",
      "Train: step:  35160, time: 0.236, loss: 2062.278076\n",
      "Train: step:  35170, time: 0.228, loss: 1367.368408\n",
      "Train: step:  35180, time: 0.218, loss: 2003.123657\n",
      "Train: step:  35190, time: 0.212, loss: 803.249512\n",
      "Train: step:  35200, time: 0.193, loss: 2588.934570\n",
      "Train: step:  35210, time: 0.217, loss: 1665.716187\n",
      "Train: step:  35220, time: 0.216, loss: 179.902298\n",
      "Train: step:  35230, time: 0.217, loss: 2700.636475\n",
      "Train: step:  35240, time: 0.187, loss: 4149.385742\n",
      "Train: step:  35250, time: 0.229, loss: 2765.140381\n",
      "Train: step:  35260, time: 0.199, loss: 2456.726562\n",
      "Train: step:  35270, time: 0.182, loss: 1532.153320\n",
      "Train: step:  35280, time: 0.184, loss: 1888.644287\n",
      "Train: step:  35290, time: 0.186, loss: 3100.956299\n",
      "Train: step:  35300, time: 0.228, loss: 2597.460449\n",
      "Train: step:  35310, time: 0.217, loss: 3233.403320\n",
      "Train: step:  35320, time: 0.220, loss: 3617.769531\n",
      "Train: step:  35330, time: 0.191, loss: 402.632172\n",
      "Train: step:  35340, time: 0.228, loss: 916.881470\n",
      "Train: step:  35350, time: 0.218, loss: 808.439331\n",
      "Train: step:  35360, time: 0.197, loss: 2805.753174\n",
      "Train: step:  35370, time: 0.243, loss: 839.832153\n",
      "Train: step:  35380, time: 0.218, loss: 1901.700806\n",
      "Train: step:  35390, time: 0.192, loss: 2504.958252\n",
      "Train: step:  35400, time: 0.260, loss: 2875.989258\n",
      "Train: step:  35410, time: 0.188, loss: 4205.029297\n",
      "Train: step:  35420, time: 0.191, loss: 696.893066\n",
      "Train: step:  35430, time: 0.195, loss: 1811.055786\n",
      "Train: step:  35440, time: 0.215, loss: 1712.829468\n",
      "Train: step:  35450, time: 0.220, loss: 634.835754\n",
      "Train: step:  35460, time: 0.189, loss: 1660.154785\n",
      "Train: step:  35470, time: 0.226, loss: 2824.032959\n",
      "Train: step:  35480, time: 0.188, loss: 846.522705\n",
      "Train: step:  35490, time: 0.216, loss: 3258.417725\n",
      "Train: step:  35500, time: 0.217, loss: 3839.978027\n",
      "Train: step:  35510, time: 0.189, loss: 3612.122314\n",
      "Train: step:  35520, time: 0.201, loss: 2316.684326\n",
      "Train: step:  35530, time: 0.193, loss: 1681.642700\n",
      "Train: step:  35540, time: 0.190, loss: 2186.032227\n",
      "Train: step:  35550, time: 0.202, loss: 2318.081055\n",
      "Train: step:  35560, time: 0.189, loss: 3418.052490\n",
      "Train: step:  35570, time: 0.223, loss: 2326.061035\n",
      "Train: step:  35580, time: 0.226, loss: 959.496887\n",
      "Train: step:  35590, time: 0.216, loss: 430.141266\n",
      "Train: step:  35600, time: 0.218, loss: 1532.435059\n",
      "Train: step:  35610, time: 0.230, loss: 2324.526611\n",
      "Train: step:  35620, time: 0.198, loss: 2487.288574\n",
      "Train: step:  35630, time: 0.229, loss: 776.608398\n",
      "Train: step:  35640, time: 0.228, loss: 2654.470947\n",
      "Train: step:  35650, time: 0.190, loss: 1588.051880\n",
      "Train: step:  35660, time: 0.233, loss: 4034.680176\n",
      "Train: step:  35670, time: 0.234, loss: 2647.406006\n",
      "Train: step:  35680, time: 0.190, loss: 2330.099854\n",
      "Train: step:  35690, time: 0.229, loss: 1269.456177\n",
      "Train: step:  35700, time: 0.186, loss: 2340.045166\n",
      "Train: step:  35710, time: 0.191, loss: 677.010498\n",
      "Train: step:  35720, time: 0.192, loss: 2221.907715\n",
      "Train: step:  35730, time: 0.186, loss: 3120.616699\n",
      "Train: step:  35740, time: 0.228, loss: 1516.518921\n",
      "Train: step:  35750, time: 0.217, loss: 733.037903\n",
      "Train: step:  35760, time: 0.189, loss: 1192.954468\n",
      "Train: step:  35770, time: 0.223, loss: 2200.643066\n",
      "Train: step:  35780, time: 0.229, loss: 1299.447266\n",
      "Train: step:  35790, time: 0.186, loss: 1517.534668\n",
      "Train: step:  35800, time: 0.228, loss: 1068.035767\n",
      "Train: step:  35810, time: 0.218, loss: 481.100311\n",
      "Train: step:  35820, time: 0.187, loss: 341.494324\n",
      "Train: step:  35830, time: 0.226, loss: 2524.642090\n",
      "Train: step:  35840, time: 0.217, loss: 3688.745117\n",
      "Train: step:  35850, time: 0.230, loss: 4566.814941\n",
      "Train: step:  35860, time: 0.226, loss: 1517.757324\n",
      "Train: step:  35870, time: 0.189, loss: 2070.697998\n",
      "Train: step:  35880, time: 0.224, loss: 903.083618\n",
      "Train: step:  35890, time: 0.250, loss: 1882.922485\n",
      "Train: step:  35900, time: 0.218, loss: 1681.720093\n",
      "Train: step:  35910, time: 0.232, loss: 1916.414429\n",
      "Train: step:  35920, time: 0.190, loss: 2367.385498\n",
      "Train: step:  35930, time: 0.217, loss: 1932.251099\n",
      "Train: step:  35940, time: 0.192, loss: 2783.824707\n",
      "Train: step:  35950, time: 0.246, loss: 2791.339600\n",
      "Train: step:  35960, time: 0.207, loss: 1329.583496\n",
      "Train: step:  35970, time: 0.190, loss: 1524.332275\n",
      "Train: step:  35980, time: 0.190, loss: 1984.000000\n",
      "Train: step:  35990, time: 0.228, loss: 2112.692139\n",
      "Train: step:  36000, time: 0.199, loss: 924.046997\n",
      "Train: step:  36010, time: 0.227, loss: 919.712280\n",
      "Train: step:  36020, time: 0.217, loss: 1492.001587\n",
      "Train: step:  36030, time: 0.188, loss: 2411.534668\n",
      "Train: step:  36040, time: 0.215, loss: 2663.296631\n",
      "Train: step:  36050, time: 0.251, loss: 939.042297\n",
      "Train: step:  36060, time: 0.192, loss: 1862.361938\n",
      "Train: step:  36070, time: 0.192, loss: 4259.698242\n",
      "Train: step:  36080, time: 0.184, loss: 3236.943115\n",
      "Train: step:  36090, time: 0.194, loss: 2230.107666\n",
      "Train: step:  36100, time: 0.221, loss: 2712.113770\n",
      "Train: step:  36110, time: 0.188, loss: 2821.476562\n",
      "Train: step:  36120, time: 0.195, loss: 516.868530\n",
      "Train: step:  36130, time: 0.184, loss: 2246.503662\n",
      "Train: step:  36140, time: 0.192, loss: 839.843811\n",
      "Train: step:  36150, time: 0.212, loss: 2482.071533\n",
      "Train: step:  36160, time: 0.220, loss: 1867.586914\n",
      "Train: step:  36170, time: 0.192, loss: 1529.389404\n",
      "Train: step:  36180, time: 0.194, loss: 350.474243\n",
      "Train: step:  36190, time: 0.222, loss: 515.147217\n",
      "Train: step:  36200, time: 0.217, loss: 1383.219971\n",
      "Train: step:  36210, time: 0.189, loss: 2094.909424\n",
      "Train: step:  36220, time: 0.217, loss: 1569.615967\n",
      "Train: step:  36230, time: 0.232, loss: 1910.710571\n",
      "Train: step:  36240, time: 0.230, loss: 1659.336182\n",
      "Train: step:  36250, time: 0.184, loss: 2103.435059\n",
      "Train: step:  36260, time: 0.195, loss: 2049.507812\n",
      "Train: step:  36270, time: 0.199, loss: 1012.957092\n",
      "Train: step:  36280, time: 0.206, loss: 2866.316895\n",
      "Train: step:  36290, time: 0.187, loss: 1291.013428\n",
      "Train: step:  36300, time: 0.183, loss: 2262.160889\n",
      "Train: step:  36310, time: 0.191, loss: 2111.608643\n",
      "Train: step:  36320, time: 0.196, loss: 1730.429810\n",
      "Train: step:  36330, time: 0.196, loss: 2435.283447\n",
      "Train: step:  36340, time: 0.195, loss: 2493.248291\n",
      "Train: step:  36350, time: 0.209, loss: 1222.953125\n",
      "Train: step:  36360, time: 0.231, loss: 3861.843750\n",
      "Train: step:  36370, time: 0.183, loss: 1785.123169\n",
      "Train: step:  36380, time: 0.217, loss: 2303.378662\n",
      "Train: step:  36390, time: 0.185, loss: 1008.226990\n",
      "Train: step:  36400, time: 0.217, loss: 2368.095947\n",
      "Train: step:  36410, time: 0.189, loss: 3550.527832\n",
      "Train: step:  36420, time: 0.216, loss: 1755.004395\n",
      "Train: step:  36430, time: 0.237, loss: 2417.072754\n",
      "Train: step:  36440, time: 0.189, loss: 2490.951416\n",
      "Train: step:  36450, time: 0.184, loss: 2894.428955\n",
      "Train: step:  36460, time: 0.241, loss: 2164.803223\n",
      "Train: step:  36470, time: 0.217, loss: 1220.514648\n",
      "Train: step:  36480, time: 0.233, loss: 2587.270020\n",
      "Train: step:  36490, time: 0.241, loss: 377.410492\n",
      "Train: step:  36500, time: 0.215, loss: 3479.317627\n",
      "Train: step:  36510, time: 0.200, loss: 1182.917480\n",
      "Train: step:  36520, time: 0.224, loss: 3324.855225\n",
      "Train: step:  36530, time: 0.188, loss: 1342.966187\n",
      "Train: step:  36540, time: 0.183, loss: 2944.685059\n",
      "Train: step:  36550, time: 0.217, loss: 1828.135132\n",
      "Train: step:  36560, time: 0.215, loss: 703.465637\n",
      "Train: step:  36570, time: 0.182, loss: 1461.767944\n",
      "Train: step:  36580, time: 0.184, loss: 2151.583008\n",
      "Train: step:  36590, time: 0.184, loss: 1468.384155\n",
      "Train: step:  36600, time: 0.213, loss: 2398.395020\n",
      "Train: step:  36610, time: 0.278, loss: 3173.376709\n",
      "Train: step:  36620, time: 0.189, loss: 1995.791748\n",
      "Train: step:  36630, time: 0.235, loss: 2480.750000\n",
      "Train: step:  36640, time: 0.188, loss: 1817.299561\n",
      "Train: step:  36650, time: 0.220, loss: 2522.652100\n",
      "Train: step:  36660, time: 0.198, loss: 1151.911133\n",
      "Train: step:  36670, time: 0.193, loss: 564.730042\n",
      "Train: step:  36680, time: 0.187, loss: 1043.690430\n",
      "Train: step:  36690, time: 0.192, loss: 253.079437\n",
      "Train: step:  36700, time: 0.217, loss: 2538.654297\n",
      "Train: step:  36710, time: 0.233, loss: 1642.246216\n",
      "Train: step:  36720, time: 0.227, loss: 2400.495117\n",
      "Train: step:  36730, time: 0.187, loss: 532.160706\n",
      "Train: step:  36740, time: 0.191, loss: 1541.165527\n",
      "Train: step:  36750, time: 0.202, loss: 1072.144897\n",
      "Train: step:  36760, time: 0.227, loss: 1990.781738\n",
      "Train: step:  36770, time: 0.195, loss: 923.665955\n",
      "Train: step:  36780, time: 0.193, loss: 1875.727173\n",
      "Train: step:  36790, time: 0.194, loss: 1752.286377\n",
      "Train: step:  36800, time: 0.215, loss: 2139.238281\n",
      "Train: step:  36810, time: 0.231, loss: 1559.197876\n",
      "Train: step:  36820, time: 0.194, loss: 693.650146\n",
      "Train: step:  36830, time: 0.202, loss: 853.865540\n",
      "Train: step:  36840, time: 0.233, loss: 2175.027100\n",
      "Train: step:  36850, time: 0.223, loss: 1019.827393\n",
      "Train: step:  36860, time: 0.192, loss: 361.122345\n",
      "Train: step:  36870, time: 0.185, loss: 3856.325928\n",
      "Train: step:  36880, time: 0.185, loss: 1966.499878\n",
      "Train: step:  36890, time: 0.187, loss: 1909.046997\n",
      "Train: step:  36900, time: 0.202, loss: 466.585815\n",
      "Train: step:  36910, time: 0.193, loss: 1613.974121\n",
      "Train: step:  36920, time: 0.200, loss: 3021.571289\n",
      "Train: step:  36930, time: 0.216, loss: 694.674194\n",
      "Train: step:  36940, time: 0.185, loss: 876.461182\n",
      "Train: step:  36950, time: 0.201, loss: 2209.002686\n",
      "Train: step:  36960, time: 0.234, loss: 1707.317749\n",
      "Train: step:  36970, time: 0.189, loss: 1817.742065\n",
      "Train: step:  36980, time: 0.214, loss: 741.769287\n",
      "Train: step:  36990, time: 0.186, loss: 2434.197021\n",
      "Train: step:  37000, time: 0.212, loss: 1417.179565\n",
      "Train: step:  37010, time: 0.196, loss: 1239.191650\n",
      "Train: step:  37020, time: 0.196, loss: 1176.307983\n",
      "Train: step:  37030, time: 0.190, loss: 491.608063\n",
      "Train: step:  37040, time: 0.231, loss: 1857.273438\n",
      "Train: step:  37050, time: 0.220, loss: 514.477417\n",
      "Train: step:  37060, time: 0.193, loss: 2995.553467\n",
      "Train: step:  37070, time: 0.217, loss: 1743.219238\n",
      "Train: step:  37080, time: 0.187, loss: 782.480530\n",
      "Train: step:  37090, time: 0.219, loss: 2622.140137\n",
      "Train: step:  37100, time: 0.261, loss: 1711.634155\n",
      "Train: step:  37110, time: 0.192, loss: 1889.574463\n",
      "Train: step:  37120, time: 0.189, loss: 1937.884888\n",
      "Train: step:  37130, time: 0.192, loss: 3629.608154\n",
      "Train: step:  37140, time: 0.222, loss: 996.230652\n",
      "Train: step:  37150, time: 0.230, loss: 1281.118164\n",
      "Train: step:  37160, time: 0.216, loss: 2016.857788\n",
      "Train: step:  37170, time: 0.230, loss: 1769.516113\n",
      "Train: step:  37180, time: 0.188, loss: 860.206482\n",
      "Train: step:  37190, time: 0.216, loss: 2373.684326\n",
      "Train: step:  37200, time: 0.216, loss: 663.909973\n",
      "Train: step:  37210, time: 0.186, loss: 1852.717041\n",
      "Train: step:  37220, time: 0.191, loss: 1725.030029\n",
      "Train: step:  37230, time: 0.226, loss: 1696.146729\n",
      "Train: step:  37240, time: 0.213, loss: 2146.286621\n",
      "Train: step:  37250, time: 0.227, loss: 2690.402344\n",
      "Train: step:  37260, time: 0.226, loss: 865.832275\n",
      "Train: step:  37270, time: 0.186, loss: 2360.776367\n",
      "Train: step:  37280, time: 0.195, loss: 1869.712891\n",
      "Train: step:  37290, time: 0.186, loss: 3745.048340\n",
      "Train: step:  37300, time: 0.219, loss: 2621.268555\n",
      "Train: step:  37310, time: 0.195, loss: 1807.700684\n",
      "Train: step:  37320, time: 0.190, loss: 1946.007324\n",
      "Train: step:  37330, time: 0.196, loss: 2874.617188\n",
      "Train: step:  37340, time: 0.190, loss: 1625.468018\n",
      "Train: step:  37350, time: 0.227, loss: 2187.247314\n",
      "Train: step:  37360, time: 0.199, loss: 1579.869263\n",
      "Train: step:  37370, time: 0.226, loss: 186.861862\n",
      "Train: step:  37380, time: 0.201, loss: 1193.092651\n",
      "Train: step:  37390, time: 0.219, loss: 1117.804199\n",
      "Train: step:  37400, time: 0.226, loss: 2902.306152\n",
      "Train: step:  37410, time: 0.220, loss: 719.574219\n",
      "Train: step:  37420, time: 0.215, loss: 1602.163940\n",
      "Train: step:  37430, time: 0.195, loss: 919.748291\n",
      "Train: step:  37440, time: 0.191, loss: 2411.735840\n",
      "Train: step:  37450, time: 0.221, loss: 902.395813\n",
      "Train: step:  37460, time: 0.233, loss: 2163.559570\n",
      "Train: step:  37470, time: 0.191, loss: 627.678833\n",
      "Train: step:  37480, time: 0.216, loss: 1109.260620\n",
      "Train: step:  37490, time: 0.191, loss: 881.310303\n",
      "Train: step:  37500, time: 0.218, loss: 1392.488159\n",
      "Train: step:  37510, time: 0.231, loss: 2529.473877\n",
      "Train: step:  37520, time: 0.194, loss: 2212.243896\n",
      "Train: step:  37530, time: 0.187, loss: 1540.886475\n",
      "Train: step:  37540, time: 0.191, loss: 2463.085938\n",
      "Train: step:  37550, time: 0.218, loss: 2215.620605\n",
      "Train: step:  37560, time: 0.222, loss: 4556.739258\n",
      "Train: step:  37570, time: 0.192, loss: 920.840393\n",
      "Train: step:  37580, time: 0.208, loss: 654.465515\n",
      "Train: step:  37590, time: 0.203, loss: 1602.441040\n",
      "Train: step:  37600, time: 0.216, loss: 1642.521484\n",
      "Train: step:  37610, time: 0.193, loss: 1795.302124\n",
      "Train: step:  37620, time: 0.230, loss: 1246.823730\n",
      "Train: step:  37630, time: 0.217, loss: 2098.545654\n",
      "Train: step:  37640, time: 0.193, loss: 2784.608643\n",
      "Train: step:  37650, time: 0.227, loss: 1268.443359\n",
      "Train: step:  37660, time: 0.229, loss: 2383.472656\n",
      "Train: step:  37670, time: 0.226, loss: 1595.413086\n",
      "Train: step:  37680, time: 0.255, loss: 1502.577026\n",
      "Train: step:  37690, time: 0.231, loss: 663.081787\n",
      "Train: step:  37700, time: 0.250, loss: 1699.646606\n",
      "Train: step:  37710, time: 0.193, loss: 2374.666504\n",
      "Train: step:  37720, time: 0.198, loss: 3449.765381\n",
      "Train: step:  37730, time: 0.183, loss: 2005.598022\n",
      "Train: step:  37740, time: 0.182, loss: 1574.195435\n",
      "Train: step:  37750, time: 0.203, loss: 1635.432495\n",
      "Train: step:  37760, time: 0.218, loss: 956.076904\n",
      "Train: step:  37770, time: 0.184, loss: 3184.067383\n",
      "Train: step:  37780, time: 0.240, loss: 371.012451\n",
      "Train: step:  37790, time: 0.229, loss: 2010.302368\n",
      "Train: step:  37800, time: 0.218, loss: 3404.989258\n",
      "Train: step:  37810, time: 0.217, loss: 356.340332\n",
      "Train: step:  37820, time: 0.217, loss: 2988.417236\n",
      "Train: step:  37830, time: 0.194, loss: 3348.206055\n",
      "Train: step:  37840, time: 0.229, loss: 1583.176880\n",
      "Train: step:  37850, time: 0.227, loss: 1466.683228\n",
      "Train: step:  37860, time: 0.188, loss: 2784.123291\n",
      "Train: step:  37870, time: 0.186, loss: 1320.341797\n",
      "Train: step:  37880, time: 0.190, loss: 1052.960571\n",
      "Train: step:  37890, time: 0.238, loss: 1635.910034\n",
      "Train: step:  37900, time: 0.227, loss: 2651.162109\n",
      "Train: step:  37910, time: 0.213, loss: 2886.102051\n",
      "Train: step:  37920, time: 0.191, loss: 2554.044189\n",
      "Train: step:  37930, time: 0.210, loss: 834.646912\n",
      "Train: step:  37940, time: 0.195, loss: 1462.853516\n",
      "Train: step:  37950, time: 0.201, loss: 2471.165039\n",
      "Train: step:  37960, time: 0.217, loss: 1810.241089\n",
      "Train: step:  37970, time: 0.226, loss: 1590.957764\n",
      "Train: step:  37980, time: 0.213, loss: 3364.830811\n",
      "Train: step:  37990, time: 0.190, loss: 2777.911621\n",
      "Train: step:  38000, time: 0.194, loss: 2294.885254\n",
      "Train: step:  38010, time: 0.192, loss: 1832.188965\n",
      "Train: step:  38020, time: 0.235, loss: 654.976685\n",
      "Train: step:  38030, time: 0.229, loss: 897.909851\n",
      "Train: step:  38040, time: 0.188, loss: 2008.692871\n",
      "Train: step:  38050, time: 0.221, loss: 780.502991\n",
      "Train: step:  38060, time: 0.195, loss: 1694.433716\n",
      "Train: step:  38070, time: 0.248, loss: 1636.523804\n",
      "Train: step:  38080, time: 0.192, loss: 1331.024902\n",
      "Train: step:  38090, time: 0.237, loss: 1956.092285\n",
      "Train: step:  38100, time: 0.204, loss: 1661.861328\n",
      "Train: step:  38110, time: 0.227, loss: 1408.855835\n",
      "Train: step:  38120, time: 0.199, loss: 1147.831177\n",
      "Train: step:  38130, time: 0.208, loss: 1315.894043\n",
      "Train: step:  38140, time: 0.231, loss: 2512.169922\n",
      "Train: step:  38150, time: 0.216, loss: 485.558655\n",
      "Train: step:  38160, time: 0.189, loss: 2045.392212\n",
      "Train: step:  38170, time: 0.216, loss: 719.178345\n",
      "Train: step:  38180, time: 0.226, loss: 3270.841309\n",
      "Train: step:  38190, time: 0.230, loss: 1920.386230\n",
      "Train: step:  38200, time: 0.253, loss: 2999.164795\n",
      "Train: step:  38210, time: 0.214, loss: 3134.389648\n",
      "Train: step:  38220, time: 0.231, loss: 811.959534\n",
      "Train: step:  38230, time: 0.197, loss: 1007.974731\n",
      "Train: step:  38240, time: 0.234, loss: 1462.978638\n",
      "Train: step:  38250, time: 0.199, loss: 1015.054138\n",
      "Train: step:  38260, time: 0.193, loss: 1642.572632\n",
      "Train: step:  38270, time: 0.233, loss: 1379.947632\n",
      "Train: step:  38280, time: 0.189, loss: 1309.439453\n",
      "Train: step:  38290, time: 0.191, loss: 1058.011963\n",
      "Train: step:  38300, time: 0.184, loss: 628.446533\n",
      "Train: step:  38310, time: 0.187, loss: 1652.524414\n",
      "Train: step:  38320, time: 0.217, loss: 413.845032\n",
      "Train: step:  38330, time: 0.226, loss: 2343.142822\n",
      "Train: step:  38340, time: 0.233, loss: 1583.094116\n",
      "Train: step:  38350, time: 0.219, loss: 1692.589478\n",
      "Train: step:  38360, time: 0.233, loss: 1658.152344\n",
      "Train: step:  38370, time: 0.225, loss: 2094.684326\n",
      "Train: step:  38380, time: 0.190, loss: 2495.028076\n",
      "Train: step:  38390, time: 0.188, loss: 2397.905273\n",
      "Train: step:  38400, time: 0.186, loss: 1656.770020\n",
      "Train: step:  38410, time: 0.209, loss: 717.808716\n",
      "Train: step:  38420, time: 0.224, loss: 2027.093628\n",
      "Train: step:  38430, time: 0.194, loss: 3358.450195\n",
      "Train: step:  38440, time: 0.235, loss: 546.217224\n",
      "Train: step:  38450, time: 0.228, loss: 291.956146\n",
      "Train: step:  38460, time: 0.229, loss: 2325.677734\n",
      "Train: step:  38470, time: 0.186, loss: 1023.716125\n",
      "Train: step:  38480, time: 0.188, loss: 718.916748\n",
      "Train: step:  38490, time: 0.187, loss: 2562.793457\n",
      "Train: step:  38500, time: 0.226, loss: 1683.869507\n",
      "Train: step:  38510, time: 0.205, loss: 2575.483887\n",
      "Train: step:  38520, time: 0.229, loss: 1048.020264\n",
      "Train: step:  38530, time: 0.206, loss: 652.419067\n",
      "Train: step:  38540, time: 0.234, loss: 1506.938599\n",
      "Train: step:  38550, time: 0.212, loss: 1009.844177\n",
      "Train: step:  38560, time: 0.262, loss: 2199.044189\n",
      "Train: step:  38570, time: 0.191, loss: 939.855591\n",
      "Train: step:  38580, time: 0.233, loss: 1865.310181\n",
      "Train: step:  38590, time: 0.231, loss: 1640.638794\n",
      "Train: step:  38600, time: 0.221, loss: 2959.813721\n",
      "Train: step:  38610, time: 0.188, loss: 2928.431152\n",
      "Train: step:  38620, time: 0.250, loss: 2345.786377\n",
      "Train: step:  38630, time: 0.228, loss: 682.745544\n",
      "Train: step:  38640, time: 0.177, loss: 1023.972595\n",
      "Train: step:  38650, time: 0.203, loss: 302.740570\n",
      "Train: step:  38660, time: 0.241, loss: 5670.946777\n",
      "Train: step:  38670, time: 0.233, loss: 559.329590\n",
      "Train: step:  38680, time: 0.229, loss: 1925.871948\n",
      "Train: step:  38690, time: 0.216, loss: 1030.076294\n",
      "Train: step:  38700, time: 0.188, loss: 3536.563477\n",
      "Train: step:  38710, time: 0.193, loss: 962.056213\n",
      "Train: step:  38720, time: 0.195, loss: 3347.802490\n",
      "Train: step:  38730, time: 0.221, loss: 1744.868164\n",
      "Train: step:  38740, time: 0.195, loss: 2018.575073\n",
      "Train: step:  38750, time: 0.248, loss: 2541.217041\n",
      "Train: step:  38760, time: 0.222, loss: 3629.648193\n",
      "Train: step:  38770, time: 0.193, loss: 2446.490479\n",
      "Train: step:  38780, time: 0.200, loss: 1917.451172\n",
      "Train: step:  38790, time: 0.187, loss: 2400.919678\n",
      "Train: step:  38800, time: 0.229, loss: 1377.564087\n",
      "Train: step:  38810, time: 0.197, loss: 2555.141602\n",
      "Train: step:  38820, time: 0.191, loss: 244.139221\n",
      "Train: step:  38830, time: 0.216, loss: 573.615601\n",
      "Train: step:  38840, time: 0.213, loss: 1824.302002\n",
      "Train: step:  38850, time: 0.237, loss: 1642.217529\n",
      "Train: step:  38860, time: 0.208, loss: 1201.044556\n",
      "Train: step:  38870, time: 0.217, loss: 1210.228638\n",
      "Train: step:  38880, time: 0.216, loss: 2250.860107\n",
      "Train: step:  38890, time: 0.226, loss: 410.197632\n",
      "Train: step:  38900, time: 0.187, loss: 2216.194092\n",
      "Train: step:  38910, time: 0.194, loss: 2094.486084\n",
      "Train: step:  38920, time: 0.192, loss: 1809.110718\n",
      "Train: step:  38930, time: 0.190, loss: 1371.640991\n",
      "Train: step:  38940, time: 0.188, loss: 1531.213867\n",
      "Train: step:  38950, time: 0.220, loss: 1533.676025\n",
      "Train: step:  38960, time: 0.187, loss: 610.687500\n",
      "Train: step:  38970, time: 0.188, loss: 2597.119385\n",
      "Train: step:  38980, time: 0.223, loss: 1421.212769\n",
      "Train: step:  38990, time: 0.189, loss: 835.603821\n",
      "Train: step:  39000, time: 0.195, loss: 555.801086\n",
      "Train: step:  39010, time: 0.202, loss: 1518.528198\n",
      "Train: step:  39020, time: 0.217, loss: 1307.100098\n",
      "Train: step:  39030, time: 0.183, loss: 1968.214233\n",
      "Train: step:  39040, time: 0.187, loss: 3414.115234\n",
      "Train: step:  39050, time: 0.185, loss: 3186.768799\n",
      "Train: step:  39060, time: 0.183, loss: 409.977478\n",
      "Train: step:  39070, time: 0.182, loss: 1224.045166\n",
      "Train: step:  39080, time: 0.190, loss: 2195.799072\n",
      "Train: step:  39090, time: 0.215, loss: 2096.707520\n",
      "Train: step:  39100, time: 0.228, loss: 2207.654297\n",
      "Train: step:  39110, time: 0.217, loss: 915.781982\n",
      "Train: step:  39120, time: 0.258, loss: 2107.049072\n",
      "Train: step:  39130, time: 0.218, loss: 1307.914062\n",
      "Train: step:  39140, time: 0.232, loss: 2241.617188\n",
      "Train: step:  39150, time: 0.219, loss: 2632.076172\n",
      "Train: step:  39160, time: 0.208, loss: 3520.734863\n",
      "Train: step:  39170, time: 0.219, loss: 2264.164307\n",
      "Train: step:  39180, time: 0.227, loss: 841.975159\n",
      "Train: step:  39190, time: 0.189, loss: 1247.243774\n",
      "Train: step:  39200, time: 0.217, loss: 1098.255005\n",
      "Train: step:  39210, time: 0.194, loss: 1145.833374\n",
      "Train: step:  39220, time: 0.194, loss: 1791.072388\n",
      "Train: step:  39230, time: 0.216, loss: 3050.639893\n",
      "Train: step:  39240, time: 0.215, loss: 259.580902\n",
      "Train: step:  39250, time: 0.221, loss: 2286.489990\n",
      "Train: step:  39260, time: 0.193, loss: 1365.679077\n",
      "Train: step:  39270, time: 0.191, loss: 1838.954834\n",
      "Train: step:  39280, time: 0.214, loss: 2671.821533\n",
      "Train: step:  39290, time: 0.217, loss: 3397.520752\n",
      "Train: step:  39300, time: 0.233, loss: 2843.794189\n",
      "Train: step:  39310, time: 0.204, loss: 2079.550293\n",
      "Train: step:  39320, time: 0.189, loss: 2813.256104\n",
      "Train: step:  39330, time: 0.193, loss: 1498.156372\n",
      "Train: step:  39340, time: 0.218, loss: 631.101440\n",
      "Train: step:  39350, time: 0.259, loss: 913.523376\n",
      "Train: step:  39360, time: 0.216, loss: 2767.198730\n",
      "Train: step:  39370, time: 0.216, loss: 3304.324219\n",
      "Train: step:  39380, time: 0.217, loss: 903.474243\n",
      "Train: step:  39390, time: 0.219, loss: 1089.010010\n",
      "Train: step:  39400, time: 0.231, loss: 2702.860840\n",
      "Train: step:  39410, time: 0.220, loss: 2110.761230\n",
      "Train: step:  39420, time: 0.219, loss: 405.054047\n",
      "Train: step:  39430, time: 0.240, loss: 1440.859863\n",
      "Train: step:  39440, time: 0.221, loss: 903.119812\n",
      "Train: step:  39450, time: 0.225, loss: 1343.800659\n",
      "Train: step:  39460, time: 0.189, loss: 996.508423\n",
      "Train: step:  39470, time: 0.185, loss: 1642.286865\n",
      "Train: step:  39480, time: 0.219, loss: 1672.896362\n",
      "Train: step:  39490, time: 0.219, loss: 1122.179932\n",
      "Train: step:  39500, time: 0.197, loss: 556.466187\n",
      "Train: step:  39510, time: 0.185, loss: 670.187561\n",
      "Train: step:  39520, time: 0.229, loss: 1112.194946\n",
      "Train: step:  39530, time: 0.216, loss: 1886.473999\n",
      "Train: step:  39540, time: 0.188, loss: 1116.423950\n",
      "Train: step:  39550, time: 0.186, loss: 243.741714\n",
      "Train: step:  39560, time: 0.195, loss: 3437.026611\n",
      "Train: step:  39570, time: 0.199, loss: 1878.975098\n",
      "Train: step:  39580, time: 0.227, loss: 3177.559570\n",
      "Train: step:  39590, time: 0.213, loss: 379.341644\n",
      "Train: step:  39600, time: 0.199, loss: 1864.496704\n",
      "Train: step:  39610, time: 0.197, loss: 1168.427490\n",
      "Train: step:  39620, time: 0.227, loss: 1254.566040\n",
      "Train: step:  39630, time: 0.188, loss: 1495.826172\n",
      "Train: step:  39640, time: 0.221, loss: 534.694763\n",
      "Train: step:  39650, time: 0.218, loss: 1488.673828\n",
      "Train: step:  39660, time: 0.186, loss: 1438.550049\n",
      "Train: step:  39670, time: 0.182, loss: 1379.478027\n",
      "Train: step:  39680, time: 0.194, loss: 1598.708740\n",
      "Train: step:  39690, time: 0.258, loss: 3033.127197\n",
      "Train: step:  39700, time: 0.218, loss: 1046.076050\n",
      "Train: step:  39710, time: 0.218, loss: 701.247498\n",
      "Train: step:  39720, time: 0.224, loss: 2864.840820\n",
      "Train: step:  39730, time: 0.226, loss: 2162.525635\n",
      "Train: step:  39740, time: 0.273, loss: 1897.695801\n",
      "Train: step:  39750, time: 0.211, loss: 1105.525513\n",
      "Train: step:  39760, time: 0.195, loss: 2214.001465\n",
      "Train: step:  39770, time: 0.218, loss: 853.460144\n",
      "Train: step:  39780, time: 0.229, loss: 1246.084717\n",
      "Train: step:  39790, time: 0.228, loss: 1718.941284\n",
      "Train: step:  39800, time: 0.183, loss: 1367.023804\n",
      "Train: step:  39810, time: 0.216, loss: 2263.035645\n",
      "Train: step:  39820, time: 0.221, loss: 925.955872\n",
      "Train: step:  39830, time: 0.201, loss: 1649.946167\n",
      "Train: step:  39840, time: 0.219, loss: 340.825073\n",
      "Train: step:  39850, time: 0.224, loss: 1689.085815\n",
      "Train: step:  39860, time: 0.188, loss: 2914.683350\n",
      "Train: step:  39870, time: 0.220, loss: 1557.036865\n",
      "Train: step:  39880, time: 0.235, loss: 4201.307617\n",
      "Train: step:  39890, time: 0.236, loss: 991.969238\n",
      "Train: step:  39900, time: 0.204, loss: 1041.153442\n",
      "Train: step:  39910, time: 0.193, loss: 884.375793\n",
      "Train: step:  39920, time: 0.195, loss: 676.234131\n",
      "Train: step:  39930, time: 0.198, loss: 400.447021\n",
      "Train: step:  39940, time: 0.205, loss: 791.755981\n",
      "Train: step:  39950, time: 0.191, loss: 2062.930176\n",
      "Train: step:  39960, time: 0.217, loss: 255.997559\n",
      "Train: step:  39970, time: 0.199, loss: 724.450195\n",
      "Train: step:  39980, time: 0.187, loss: 617.960327\n",
      "Train: step:  39990, time: 0.195, loss: 2681.045898\n",
      "Train: step:  40000, time: 0.190, loss: 3333.295166\n",
      "Train: step:  40010, time: 0.186, loss: 2233.913818\n",
      "Train: step:  40020, time: 0.212, loss: 3264.547363\n",
      "Train: step:  40030, time: 0.216, loss: 1808.004395\n",
      "Train: step:  40040, time: 0.191, loss: 2639.045654\n",
      "Train: step:  40050, time: 0.206, loss: 3066.069092\n",
      "Train: step:  40060, time: 0.230, loss: 1354.222168\n",
      "Train: step:  40070, time: 0.226, loss: 1285.202271\n",
      "Train: step:  40080, time: 0.218, loss: 1439.500732\n",
      "Train: step:  40090, time: 0.183, loss: 3076.951416\n",
      "Train: step:  40100, time: 0.193, loss: 611.324646\n",
      "Train: step:  40110, time: 0.222, loss: 1693.670166\n",
      "Train: step:  40120, time: 0.217, loss: 2388.535400\n",
      "Train: step:  40130, time: 0.218, loss: 725.694946\n",
      "Train: step:  40140, time: 0.191, loss: 1607.512207\n",
      "Train: step:  40150, time: 0.216, loss: 978.973694\n",
      "Train: step:  40160, time: 0.219, loss: 2618.859375\n",
      "Train: step:  40170, time: 0.203, loss: 2707.266846\n",
      "Train: step:  40180, time: 0.206, loss: 2142.784668\n",
      "Train: step:  40190, time: 0.244, loss: 4453.168945\n",
      "Train: step:  40200, time: 0.222, loss: 1741.206177\n",
      "Train: step:  40210, time: 0.229, loss: 1979.847046\n",
      "Train: step:  40220, time: 0.218, loss: 2625.425781\n",
      "Train: step:  40230, time: 0.199, loss: 733.989624\n",
      "Train: step:  40240, time: 0.249, loss: 760.250916\n",
      "Train: step:  40250, time: 0.223, loss: 2032.500000\n",
      "Train: step:  40260, time: 0.197, loss: 861.681946\n",
      "Train: step:  40270, time: 0.190, loss: 5176.229980\n",
      "Train: step:  40280, time: 0.224, loss: 1760.194824\n",
      "Train: step:  40290, time: 0.239, loss: 3047.366943\n",
      "Train: step:  40300, time: 0.263, loss: 1822.500854\n",
      "Train: step:  40310, time: 0.228, loss: 645.387268\n",
      "Train: step:  40320, time: 0.241, loss: 1616.380859\n",
      "Train: step:  40330, time: 0.228, loss: 2618.866699\n",
      "Train: step:  40340, time: 0.250, loss: 1237.028198\n",
      "Train: step:  40350, time: 0.193, loss: 948.356506\n",
      "Train: step:  40360, time: 0.239, loss: 894.656555\n",
      "Train: step:  40370, time: 0.240, loss: 2799.756348\n",
      "Train: step:  40380, time: 0.226, loss: 2938.681396\n",
      "Train: step:  40390, time: 0.216, loss: 740.331177\n",
      "Train: step:  40400, time: 0.193, loss: 1208.095459\n",
      "Train: step:  40410, time: 0.221, loss: 5960.410156\n",
      "Train: step:  40420, time: 0.203, loss: 2537.615479\n",
      "Train: step:  40430, time: 0.199, loss: 2147.396729\n",
      "Train: step:  40440, time: 0.201, loss: 3134.319824\n",
      "Train: step:  40450, time: 0.201, loss: 1463.247559\n",
      "Train: step:  40460, time: 0.221, loss: 2161.010498\n",
      "Train: step:  40470, time: 0.240, loss: 3052.030518\n",
      "Train: step:  40480, time: 0.213, loss: 1177.657349\n",
      "Train: step:  40490, time: 0.222, loss: 1660.349609\n",
      "Train: step:  40500, time: 0.216, loss: 1333.691406\n",
      "Train: step:  40510, time: 0.218, loss: 1655.169678\n",
      "Train: step:  40520, time: 0.232, loss: 2254.280029\n",
      "Train: step:  40530, time: 0.202, loss: 1648.853027\n",
      "Train: step:  40540, time: 0.247, loss: 1116.204590\n",
      "Train: step:  40550, time: 0.192, loss: 1916.662354\n",
      "Train: step:  40560, time: 0.218, loss: 974.494690\n",
      "Train: step:  40570, time: 0.191, loss: 2887.471191\n",
      "Train: step:  40580, time: 0.232, loss: 2251.894287\n",
      "Train: step:  40590, time: 0.187, loss: 1529.166016\n",
      "Train: step:  40600, time: 0.217, loss: 1165.752197\n",
      "Train: step:  40610, time: 0.187, loss: 2985.767578\n",
      "Train: step:  40620, time: 0.218, loss: 2381.278809\n",
      "Train: step:  40630, time: 0.227, loss: 3085.289795\n",
      "Train: step:  40640, time: 0.223, loss: 3171.518799\n",
      "Train: step:  40650, time: 0.190, loss: 3218.439697\n",
      "Train: step:  40660, time: 0.196, loss: 2058.138184\n",
      "Train: step:  40670, time: 0.223, loss: 1618.775879\n",
      "Train: step:  40680, time: 0.218, loss: 1441.833008\n",
      "Train: step:  40690, time: 0.212, loss: 1535.899536\n",
      "Train: step:  40700, time: 0.195, loss: 1925.340820\n",
      "Train: step:  40710, time: 0.215, loss: 1706.165771\n",
      "Train: step:  40720, time: 0.195, loss: 989.434082\n",
      "Train: step:  40730, time: 0.187, loss: 2714.332275\n",
      "Train: step:  40740, time: 0.189, loss: 3687.324951\n",
      "Train: step:  40750, time: 0.209, loss: 3033.107910\n",
      "Train: step:  40760, time: 0.200, loss: 3225.274658\n",
      "Train: step:  40770, time: 0.219, loss: 1696.419800\n",
      "Train: step:  40780, time: 0.200, loss: 2507.476074\n",
      "Train: step:  40790, time: 0.188, loss: 2265.980713\n",
      "Train: step:  40800, time: 0.226, loss: 1518.976318\n",
      "Train: step:  40810, time: 0.229, loss: 1899.182129\n",
      "Train: step:  40820, time: 0.186, loss: 1500.278198\n",
      "Train: step:  40830, time: 0.186, loss: 1987.960693\n",
      "Train: step:  40840, time: 0.230, loss: 1609.077271\n",
      "Train: step:  40850, time: 0.218, loss: 331.692688\n",
      "Train: step:  40860, time: 0.218, loss: 2044.561523\n",
      "Train: step:  40870, time: 0.183, loss: 965.893677\n",
      "Train: step:  40880, time: 0.233, loss: 1035.227051\n",
      "Train: step:  40890, time: 0.192, loss: 1716.722534\n",
      "Train: step:  40900, time: 0.196, loss: 1996.464722\n",
      "Train: step:  40910, time: 0.229, loss: 1392.745117\n",
      "Train: step:  40920, time: 0.226, loss: 1466.630981\n",
      "Train: step:  40930, time: 0.179, loss: 1402.251221\n",
      "Train: step:  40940, time: 0.236, loss: 778.957642\n",
      "Train: step:  40950, time: 0.208, loss: 657.421204\n",
      "Train: step:  40960, time: 0.194, loss: 2877.508545\n",
      "Train: step:  40970, time: 0.233, loss: 4512.533691\n",
      "Train: step:  40980, time: 0.191, loss: 1524.496948\n",
      "Train: step:  40990, time: 0.188, loss: 2670.701416\n",
      "Train: step:  41000, time: 0.216, loss: 2282.814209\n",
      "Train: step:  41010, time: 0.201, loss: 2145.754883\n",
      "Train: step:  41020, time: 0.199, loss: 1969.646729\n",
      "Train: step:  41030, time: 0.184, loss: 2975.434570\n",
      "Train: step:  41040, time: 0.230, loss: 2656.383301\n",
      "Train: step:  41050, time: 0.198, loss: 1863.643799\n",
      "Train: step:  41060, time: 0.193, loss: 1057.612549\n",
      "Train: step:  41070, time: 0.182, loss: 1777.782471\n",
      "Train: step:  41080, time: 0.189, loss: 549.501770\n",
      "Train: step:  41090, time: 0.191, loss: 2261.926514\n",
      "Train: step:  41100, time: 0.217, loss: 2880.580566\n",
      "Train: step:  41110, time: 0.191, loss: 2019.801514\n",
      "Train: step:  41120, time: 0.256, loss: 1466.088257\n",
      "Train: step:  41130, time: 0.216, loss: 2321.714111\n",
      "Train: step:  41140, time: 0.195, loss: 2618.208252\n",
      "Train: step:  41150, time: 0.218, loss: 2067.718018\n",
      "Train: step:  41160, time: 0.227, loss: 2080.951416\n",
      "Train: step:  41170, time: 0.196, loss: 801.818970\n",
      "Train: step:  41180, time: 0.190, loss: 1905.093506\n",
      "Train: step:  41190, time: 0.195, loss: 1501.758545\n",
      "Train: step:  41200, time: 0.194, loss: 1580.737793\n",
      "Train: step:  41210, time: 0.185, loss: 2775.849854\n",
      "Train: step:  41220, time: 0.214, loss: 2880.208008\n",
      "Train: step:  41230, time: 0.192, loss: 738.826538\n",
      "Train: step:  41240, time: 0.198, loss: 871.282776\n",
      "Train: step:  41250, time: 0.195, loss: 1485.548218\n",
      "Train: step:  41260, time: 0.190, loss: 2172.006836\n",
      "Train: step:  41270, time: 0.218, loss: 896.391663\n",
      "Train: step:  41280, time: 0.216, loss: 357.829498\n",
      "Train: step:  41290, time: 0.226, loss: 2307.977539\n",
      "Train: step:  41300, time: 0.187, loss: 2961.942139\n",
      "Train: step:  41310, time: 0.187, loss: 283.752716\n",
      "Train: step:  41320, time: 0.217, loss: 2028.757935\n",
      "Train: step:  41330, time: 0.222, loss: 2487.888672\n",
      "Train: step:  41340, time: 0.232, loss: 1578.785767\n",
      "Train: step:  41350, time: 0.194, loss: 1725.714478\n",
      "Train: step:  41360, time: 0.216, loss: 2413.159668\n",
      "Train: step:  41370, time: 0.184, loss: 1455.590454\n",
      "Train: step:  41380, time: 0.191, loss: 2044.083984\n",
      "Train: step:  41390, time: 0.182, loss: 4075.667236\n",
      "Train: step:  41400, time: 0.187, loss: 1302.684692\n",
      "Train: step:  41410, time: 0.190, loss: 3273.419189\n",
      "Train: step:  41420, time: 0.193, loss: 1896.980103\n",
      "Train: step:  41430, time: 0.226, loss: 927.603027\n",
      "Train: step:  41440, time: 0.230, loss: 2712.701660\n",
      "Train: step:  41450, time: 0.189, loss: 503.011871\n",
      "Train: step:  41460, time: 0.186, loss: 1414.246094\n",
      "Train: step:  41470, time: 0.189, loss: 1584.585205\n",
      "Train: step:  41480, time: 0.184, loss: 1998.708618\n",
      "Train: step:  41490, time: 0.228, loss: 1792.033936\n",
      "Train: step:  41500, time: 0.187, loss: 970.472900\n",
      "Train: step:  41510, time: 0.229, loss: 2981.614258\n",
      "Train: step:  41520, time: 0.216, loss: 2998.648438\n",
      "Train: step:  41530, time: 0.187, loss: 2154.755859\n",
      "Train: step:  41540, time: 0.193, loss: 2547.154785\n",
      "Train: step:  41550, time: 0.189, loss: 1923.735352\n",
      "Train: step:  41560, time: 0.192, loss: 2019.317871\n",
      "Train: step:  41570, time: 0.231, loss: 1924.749878\n",
      "Train: step:  41580, time: 0.228, loss: 3424.651367\n",
      "Train: step:  41590, time: 0.226, loss: 2031.066772\n",
      "Train: step:  41600, time: 0.182, loss: 1320.083496\n",
      "Train: step:  41610, time: 0.184, loss: 467.863708\n",
      "Train: step:  41620, time: 0.183, loss: 1083.564087\n",
      "Train: step:  41630, time: 0.205, loss: 839.035522\n",
      "Train: step:  41640, time: 0.181, loss: 2269.297852\n",
      "Train: step:  41650, time: 0.184, loss: 858.017395\n",
      "Train: step:  41660, time: 0.217, loss: 2249.914795\n",
      "Train: step:  41670, time: 0.215, loss: 938.185364\n",
      "Train: step:  41680, time: 0.200, loss: 565.351868\n",
      "Train: step:  41690, time: 0.222, loss: 1936.118774\n",
      "Train: step:  41700, time: 0.218, loss: 2923.454102\n",
      "Train: step:  41710, time: 0.226, loss: 342.649750\n",
      "Train: step:  41720, time: 0.188, loss: 1548.334473\n",
      "Train: step:  41730, time: 0.186, loss: 3336.729980\n",
      "Train: step:  41740, time: 0.219, loss: 1603.349365\n",
      "Train: step:  41750, time: 0.217, loss: 875.293091\n",
      "Train: step:  41760, time: 0.215, loss: 2658.636719\n",
      "Train: step:  41770, time: 0.230, loss: 1256.448853\n",
      "Train: step:  41780, time: 0.189, loss: 2262.768066\n",
      "Train: step:  41790, time: 0.184, loss: 2490.464355\n",
      "Train: step:  41800, time: 0.236, loss: 1030.713623\n",
      "Train: step:  41810, time: 0.224, loss: 2457.844238\n",
      "Train: step:  41820, time: 0.213, loss: 3087.790771\n",
      "Train: step:  41830, time: 0.233, loss: 1563.236328\n",
      "Train: step:  41840, time: 0.188, loss: 166.140747\n",
      "Train: step:  41850, time: 0.195, loss: 1900.634521\n",
      "Train: step:  41860, time: 0.192, loss: 2383.869873\n",
      "Train: step:  41870, time: 0.239, loss: 2677.237549\n",
      "Train: step:  41880, time: 0.233, loss: 4283.660645\n",
      "Train: step:  41890, time: 0.248, loss: 852.462402\n",
      "Train: step:  41900, time: 0.230, loss: 2710.036865\n",
      "Train: step:  41910, time: 0.185, loss: 2459.822998\n",
      "Train: step:  41920, time: 0.217, loss: 1813.229980\n",
      "Train: step:  41930, time: 0.197, loss: 1847.766113\n",
      "Train: step:  41940, time: 0.188, loss: 2428.996094\n",
      "Train: step:  41950, time: 0.192, loss: 1473.317139\n",
      "Train: step:  41960, time: 0.217, loss: 479.116577\n",
      "Train: step:  41970, time: 0.188, loss: 2644.578613\n",
      "Train: step:  41980, time: 0.190, loss: 1052.745728\n",
      "Train: step:  41990, time: 0.216, loss: 2729.252441\n",
      "Train: step:  42000, time: 0.229, loss: 1920.540283\n",
      "Train: step:  42010, time: 0.193, loss: 2606.721191\n",
      "Train: step:  42020, time: 0.219, loss: 1082.651978\n",
      "Train: step:  42030, time: 0.190, loss: 488.797791\n",
      "Train: step:  42040, time: 0.195, loss: 2455.271973\n",
      "Train: step:  42050, time: 0.188, loss: 2377.435059\n",
      "Train: step:  42060, time: 0.190, loss: 1589.815308\n",
      "Train: step:  42070, time: 0.211, loss: 2194.383057\n",
      "Train: step:  42080, time: 0.191, loss: 422.550812\n",
      "Train: step:  42090, time: 0.188, loss: 3391.536133\n",
      "Train: step:  42100, time: 0.215, loss: 1920.395508\n",
      "Train: step:  42110, time: 0.184, loss: 458.867493\n",
      "Train: step:  42120, time: 0.188, loss: 2284.427490\n",
      "Train: step:  42130, time: 0.207, loss: 3049.674072\n",
      "Train: step:  42140, time: 0.191, loss: 1838.668823\n",
      "Train: step:  42150, time: 0.227, loss: 2482.054688\n",
      "Train: step:  42160, time: 0.222, loss: 2645.279785\n",
      "Train: step:  42170, time: 0.193, loss: 2383.769287\n",
      "Train: step:  42180, time: 0.218, loss: 1748.513672\n",
      "Train: step:  42190, time: 0.193, loss: 1317.631348\n",
      "Train: step:  42200, time: 0.227, loss: 931.898376\n",
      "Train: step:  42210, time: 0.220, loss: 649.439514\n",
      "Train: step:  42220, time: 0.184, loss: 980.894714\n",
      "Train: step:  42230, time: 0.232, loss: 3567.244141\n",
      "Train: step:  42240, time: 0.188, loss: 703.087769\n",
      "Train: step:  42250, time: 0.217, loss: 1615.074585\n",
      "Train: step:  42260, time: 0.215, loss: 2043.931885\n",
      "Train: step:  42270, time: 0.229, loss: 4333.200195\n",
      "Train: step:  42280, time: 0.239, loss: 954.910400\n",
      "Train: step:  42290, time: 0.196, loss: 1412.348389\n",
      "Train: step:  42300, time: 0.248, loss: 3080.151123\n",
      "Train: step:  42310, time: 0.215, loss: 3477.649170\n",
      "Train: step:  42320, time: 0.191, loss: 3368.921631\n",
      "Train: step:  42330, time: 0.230, loss: 2508.019287\n",
      "Train: step:  42340, time: 0.271, loss: 2078.984131\n",
      "Train: step:  42350, time: 0.193, loss: 1269.335815\n",
      "Train: step:  42360, time: 0.228, loss: 843.496704\n",
      "Train: step:  42370, time: 0.185, loss: 1760.384399\n",
      "Train: step:  42380, time: 0.211, loss: 666.839905\n",
      "Train: step:  42390, time: 0.226, loss: 936.557251\n",
      "Train: step:  42400, time: 0.219, loss: 1707.969604\n",
      "Train: step:  42410, time: 0.185, loss: 1201.198364\n",
      "Train: step:  42420, time: 0.225, loss: 1402.077026\n",
      "Train: step:  42430, time: 0.231, loss: 3106.971680\n",
      "Train: step:  42440, time: 0.263, loss: 2283.128662\n",
      "Train: step:  42450, time: 0.233, loss: 2649.395020\n",
      "Train: step:  42460, time: 0.226, loss: 2654.513672\n",
      "Train: step:  42470, time: 0.212, loss: 2794.412598\n",
      "Train: step:  42480, time: 0.190, loss: 1107.740967\n",
      "Train: step:  42490, time: 0.200, loss: 2179.863525\n",
      "Train: step:  42500, time: 0.247, loss: 1617.404541\n",
      "Train: step:  42510, time: 0.227, loss: 794.506226\n",
      "Train: step:  42520, time: 0.194, loss: 2507.865723\n",
      "Train: step:  42530, time: 0.191, loss: 1643.199097\n",
      "Train: step:  42540, time: 0.222, loss: 447.168152\n",
      "Train: step:  42550, time: 0.216, loss: 738.294495\n",
      "Train: step:  42560, time: 0.193, loss: 1361.862305\n",
      "Train: step:  42570, time: 0.219, loss: 1748.051392\n",
      "Train: step:  42580, time: 0.191, loss: 1840.874756\n",
      "Train: step:  42590, time: 0.241, loss: 2987.952881\n",
      "Train: step:  42600, time: 0.186, loss: 2058.843018\n",
      "Train: step:  42610, time: 0.206, loss: 1061.450439\n",
      "Train: step:  42620, time: 0.189, loss: 4101.320801\n",
      "Train: step:  42630, time: 0.197, loss: 2749.249268\n",
      "Train: step:  42640, time: 0.202, loss: 2597.837891\n",
      "Train: step:  42650, time: 0.234, loss: 3841.375000\n",
      "Train: step:  42660, time: 0.195, loss: 1740.140625\n",
      "Train: step:  42670, time: 0.217, loss: 345.906860\n",
      "Train: step:  42680, time: 0.220, loss: 2738.210449\n",
      "Train: step:  42690, time: 0.192, loss: 4025.992188\n",
      "Train: step:  42700, time: 0.213, loss: 1613.464844\n",
      "Train: step:  42710, time: 0.232, loss: 2497.263428\n",
      "Train: step:  42720, time: 0.190, loss: 2983.960449\n",
      "Train: step:  42730, time: 0.209, loss: 2307.062988\n",
      "Train: step:  42740, time: 0.228, loss: 2214.032471\n",
      "Train: step:  42750, time: 0.220, loss: 2766.405762\n",
      "Train: step:  42760, time: 0.218, loss: 2105.854248\n",
      "Train: step:  42770, time: 0.196, loss: 2286.191895\n",
      "Train: step:  42780, time: 0.202, loss: 2641.007568\n",
      "Train: step:  42790, time: 0.191, loss: 3495.079346\n",
      "Train: step:  42800, time: 0.186, loss: 1440.384155\n",
      "Train: step:  42810, time: 0.193, loss: 1925.578491\n",
      "Train: step:  42820, time: 0.226, loss: 1565.678223\n",
      "Train: step:  42830, time: 0.234, loss: 1633.657837\n",
      "Train: step:  42840, time: 0.217, loss: 1033.702759\n",
      "Train: step:  42850, time: 0.192, loss: 1363.578857\n",
      "Train: step:  42860, time: 0.199, loss: 441.832062\n",
      "Train: step:  42870, time: 0.247, loss: 4411.784180\n",
      "Train: step:  42880, time: 0.220, loss: 1662.948364\n",
      "Train: step:  42890, time: 0.205, loss: 749.268982\n",
      "Train: step:  42900, time: 0.207, loss: 1277.324219\n",
      "Train: step:  42910, time: 0.188, loss: 1042.562988\n",
      "Train: step:  42920, time: 0.225, loss: 1597.832642\n",
      "Train: step:  42930, time: 0.193, loss: 3684.367920\n",
      "Train: step:  42940, time: 0.228, loss: 1741.826660\n",
      "Train: step:  42950, time: 0.183, loss: 2427.258057\n",
      "Train: step:  42960, time: 0.225, loss: 1567.298218\n",
      "Train: step:  42970, time: 0.229, loss: 436.516022\n",
      "Train: step:  42980, time: 0.203, loss: 1564.852173\n",
      "Train: step:  42990, time: 0.194, loss: 1396.700073\n",
      "Train: step:  43000, time: 0.184, loss: 637.782715\n",
      "Train: step:  43010, time: 0.218, loss: 4096.979492\n",
      "Train: step:  43020, time: 0.223, loss: 2652.723633\n",
      "Train: step:  43030, time: 0.185, loss: 2955.159180\n",
      "Train: step:  43040, time: 0.250, loss: 3332.048096\n",
      "Train: step:  43050, time: 0.231, loss: 966.360718\n",
      "Train: step:  43060, time: 0.255, loss: 2074.065674\n",
      "Train: step:  43070, time: 0.188, loss: 1739.836426\n",
      "Train: step:  43080, time: 0.187, loss: 2921.058838\n",
      "Train: step:  43090, time: 0.213, loss: 1825.348511\n",
      "Train: step:  43100, time: 0.192, loss: 3161.741943\n",
      "Train: step:  43110, time: 0.193, loss: 984.673889\n",
      "Train: step:  43120, time: 0.190, loss: 2543.775879\n",
      "Train: step:  43130, time: 0.189, loss: 2184.108398\n",
      "Train: step:  43140, time: 0.192, loss: 825.649292\n",
      "Train: step:  43150, time: 0.190, loss: 1147.646118\n",
      "Train: step:  43160, time: 0.192, loss: 1182.501465\n",
      "Train: step:  43170, time: 0.227, loss: 2455.366943\n",
      "Train: step:  43180, time: 0.245, loss: 1114.830444\n",
      "Train: step:  43190, time: 0.192, loss: 3269.052734\n",
      "Train: step:  43200, time: 0.230, loss: 923.935608\n",
      "Train: step:  43210, time: 0.216, loss: 3412.827881\n",
      "Train: step:  43220, time: 0.197, loss: 2704.967285\n",
      "Train: step:  43230, time: 0.194, loss: 2401.357666\n",
      "Train: step:  43240, time: 0.228, loss: 980.964172\n",
      "Train: step:  43250, time: 0.220, loss: 1618.728638\n",
      "Train: step:  43260, time: 0.232, loss: 1630.073853\n",
      "Train: step:  43270, time: 0.202, loss: 2203.461914\n",
      "Train: step:  43280, time: 0.216, loss: 1024.805176\n",
      "Train: step:  43290, time: 0.232, loss: 2624.707275\n",
      "Train: step:  43300, time: 0.228, loss: 1246.909058\n",
      "Train: step:  43310, time: 0.189, loss: 259.592865\n",
      "Train: step:  43320, time: 0.191, loss: 1786.034424\n",
      "Train: step:  43330, time: 0.184, loss: 2650.506104\n",
      "Train: step:  43340, time: 0.246, loss: 288.806244\n",
      "Train: step:  43350, time: 0.185, loss: 2583.063965\n",
      "Train: step:  43360, time: 0.228, loss: 1946.163086\n",
      "Train: step:  43370, time: 0.206, loss: 1136.478149\n",
      "Train: step:  43380, time: 0.188, loss: 765.272705\n",
      "Train: step:  43390, time: 0.187, loss: 2920.587891\n",
      "Train: step:  43400, time: 0.220, loss: 472.387024\n",
      "Train: step:  43410, time: 0.218, loss: 2578.510010\n",
      "Train: step:  43420, time: 0.213, loss: 615.328369\n",
      "Train: step:  43430, time: 0.194, loss: 443.950928\n",
      "Train: step:  43440, time: 0.187, loss: 1218.164429\n",
      "Train: step:  43450, time: 0.218, loss: 2286.026611\n",
      "Train: step:  43460, time: 0.199, loss: 1944.620239\n",
      "Train: step:  43470, time: 0.186, loss: 894.330505\n",
      "Train: step:  43480, time: 0.179, loss: 1589.866943\n",
      "Train: step:  43490, time: 0.186, loss: 1685.439209\n",
      "Train: step:  43500, time: 0.229, loss: 1076.863525\n",
      "Train: step:  43510, time: 0.204, loss: 2170.327637\n",
      "Train: step:  43520, time: 0.242, loss: 2126.273438\n",
      "Train: step:  43530, time: 0.189, loss: 662.576782\n",
      "Train: step:  43540, time: 0.231, loss: 3300.260010\n",
      "Train: step:  43550, time: 0.220, loss: 2502.422607\n",
      "Train: step:  43560, time: 0.184, loss: 3077.563721\n",
      "Train: step:  43570, time: 0.188, loss: 2573.165771\n",
      "Train: step:  43580, time: 0.191, loss: 2126.154297\n",
      "Train: step:  43590, time: 0.217, loss: 2615.881104\n",
      "Train: step:  43600, time: 0.196, loss: 436.874298\n",
      "Train: step:  43610, time: 0.207, loss: 1730.055176\n",
      "Train: step:  43620, time: 0.218, loss: 1226.495361\n",
      "Train: step:  43630, time: 0.181, loss: 1938.232300\n",
      "Train: step:  43640, time: 0.191, loss: 1266.933105\n",
      "Train: step:  43650, time: 0.182, loss: 662.120911\n",
      "Train: step:  43660, time: 0.182, loss: 1577.007446\n",
      "Train: step:  43670, time: 0.216, loss: 3304.503174\n",
      "Train: step:  43680, time: 0.185, loss: 1535.483521\n",
      "Train: step:  43690, time: 0.184, loss: 1183.311768\n",
      "Train: step:  43700, time: 0.184, loss: 2685.440918\n",
      "Train: step:  43710, time: 0.214, loss: 3064.042969\n",
      "Train: step:  43720, time: 0.222, loss: 2214.109375\n",
      "Train: step:  43730, time: 0.194, loss: 1719.783813\n",
      "Train: step:  43740, time: 0.190, loss: 1921.713379\n",
      "Train: step:  43750, time: 0.189, loss: 560.356812\n",
      "Train: step:  43760, time: 0.219, loss: 1696.136963\n",
      "Train: step:  43770, time: 0.220, loss: 1049.699829\n",
      "Train: step:  43780, time: 0.194, loss: 2144.945068\n",
      "Train: step:  43790, time: 0.225, loss: 2074.007812\n",
      "Train: step:  43800, time: 0.205, loss: 1026.417969\n",
      "Train: step:  43810, time: 0.229, loss: 1043.515137\n",
      "Train: step:  43820, time: 0.191, loss: 368.204529\n",
      "Train: step:  43830, time: 0.188, loss: 2222.144775\n",
      "Train: step:  43840, time: 0.236, loss: 2197.277588\n",
      "Train: step:  43850, time: 0.230, loss: 1384.626221\n",
      "Train: step:  43860, time: 0.213, loss: 2310.090820\n",
      "Train: step:  43870, time: 0.212, loss: 1403.753784\n",
      "Train: step:  43880, time: 0.191, loss: 4066.624023\n",
      "Train: step:  43890, time: 0.226, loss: 737.097839\n",
      "Train: step:  43900, time: 0.231, loss: 2275.774170\n",
      "Train: step:  43910, time: 0.187, loss: 1865.617432\n",
      "Train: step:  43920, time: 0.216, loss: 1828.158203\n",
      "Train: step:  43930, time: 0.227, loss: 469.696777\n",
      "Train: step:  43940, time: 0.200, loss: 1018.404236\n",
      "Train: step:  43950, time: 0.196, loss: 1295.875610\n",
      "Train: step:  43960, time: 0.194, loss: 2227.347412\n",
      "Train: step:  43970, time: 0.216, loss: 368.236572\n",
      "Train: step:  43980, time: 0.189, loss: 1873.755615\n",
      "Train: step:  43990, time: 0.198, loss: 1471.181763\n",
      "Train: step:  44000, time: 0.233, loss: 202.883331\n",
      "Train: step:  44010, time: 0.239, loss: 2455.784668\n",
      "Train: step:  44020, time: 0.217, loss: 2615.630615\n",
      "Train: step:  44030, time: 0.182, loss: 1912.317627\n",
      "Train: step:  44040, time: 0.226, loss: 2634.522461\n",
      "Train: step:  44050, time: 0.205, loss: 2978.167236\n",
      "Train: step:  44060, time: 0.216, loss: 1884.191406\n",
      "Train: step:  44070, time: 0.183, loss: 1697.647461\n",
      "Train: step:  44080, time: 0.192, loss: 1744.469482\n",
      "Train: step:  44090, time: 0.218, loss: 1491.312500\n",
      "Train: step:  44100, time: 0.239, loss: 615.156372\n",
      "Train: step:  44110, time: 0.216, loss: 1578.608521\n",
      "Train: step:  44120, time: 0.185, loss: 2159.693115\n",
      "Train: step:  44130, time: 0.218, loss: 624.060608\n",
      "Train: step:  44140, time: 0.184, loss: 2386.687988\n",
      "Train: step:  44150, time: 0.183, loss: 3371.676758\n",
      "Train: step:  44160, time: 0.214, loss: 1482.420044\n",
      "Train: step:  44170, time: 0.211, loss: 2102.378418\n",
      "Train: step:  44180, time: 0.214, loss: 1085.849243\n",
      "Train: step:  44190, time: 0.189, loss: 3340.987305\n",
      "Train: step:  44200, time: 0.230, loss: 1823.598389\n",
      "Train: step:  44210, time: 0.218, loss: 2267.595947\n",
      "Train: step:  44220, time: 0.215, loss: 1230.822144\n",
      "Train: step:  44230, time: 0.223, loss: 2957.831299\n",
      "Train: step:  44240, time: 0.217, loss: 1921.345947\n",
      "Train: step:  44250, time: 0.190, loss: 3141.334961\n",
      "Train: step:  44260, time: 0.214, loss: 1396.147461\n",
      "Train: step:  44270, time: 0.204, loss: 1539.700439\n",
      "Train: step:  44280, time: 0.217, loss: 1947.460327\n",
      "Train: step:  44290, time: 0.219, loss: 1353.095581\n",
      "Train: step:  44300, time: 0.180, loss: 1444.636963\n",
      "Train: step:  44310, time: 0.182, loss: 583.218750\n",
      "Train: step:  44320, time: 0.183, loss: 2941.111816\n",
      "Train: step:  44330, time: 0.185, loss: 706.940796\n",
      "Train: step:  44340, time: 0.212, loss: 316.079224\n",
      "Train: step:  44350, time: 0.232, loss: 2896.358887\n",
      "Train: step:  44360, time: 0.184, loss: 1609.037354\n",
      "Train: step:  44370, time: 0.186, loss: 963.602051\n",
      "Train: step:  44380, time: 0.203, loss: 2730.884033\n",
      "Train: step:  44390, time: 0.216, loss: 1334.331787\n",
      "Train: step:  44400, time: 0.218, loss: 2613.204590\n",
      "Train: step:  44410, time: 0.230, loss: 968.482056\n",
      "Train: step:  44420, time: 0.202, loss: 1576.617188\n",
      "Train: step:  44430, time: 0.217, loss: 1000.218445\n",
      "Train: step:  44440, time: 0.227, loss: 1994.145142\n",
      "Train: step:  44450, time: 0.209, loss: 2323.041260\n",
      "Train: step:  44460, time: 0.231, loss: 2516.269531\n",
      "Train: step:  44470, time: 0.193, loss: 575.597168\n",
      "Train: step:  44480, time: 0.194, loss: 1565.434082\n",
      "Train: step:  44490, time: 0.197, loss: 3312.703369\n",
      "Train: step:  44500, time: 0.213, loss: 306.653870\n",
      "Train: step:  44510, time: 0.203, loss: 2797.086914\n",
      "Train: step:  44520, time: 0.188, loss: 2016.566284\n",
      "Train: step:  44530, time: 0.218, loss: 1561.317627\n",
      "Train: step:  44540, time: 0.190, loss: 1824.135254\n",
      "Train: step:  44550, time: 0.191, loss: 573.801147\n",
      "Train: step:  44560, time: 0.192, loss: 1812.633423\n",
      "Train: step:  44570, time: 0.217, loss: 765.552612\n",
      "Train: step:  44580, time: 0.190, loss: 1635.469971\n",
      "Train: step:  44590, time: 0.190, loss: 1831.819946\n",
      "Train: step:  44600, time: 0.185, loss: 1296.107788\n",
      "Train: step:  44610, time: 0.194, loss: 2994.314453\n",
      "Train: step:  44620, time: 0.257, loss: 814.071167\n",
      "Train: step:  44630, time: 0.190, loss: 895.127625\n",
      "Train: step:  44640, time: 0.187, loss: 2003.439209\n",
      "Train: step:  44650, time: 0.193, loss: 2888.691895\n",
      "Train: step:  44660, time: 0.183, loss: 2287.869873\n",
      "Train: step:  44670, time: 0.187, loss: 1617.839966\n",
      "Train: step:  44680, time: 0.191, loss: 1160.506714\n",
      "Train: step:  44690, time: 0.214, loss: 1171.590576\n",
      "Train: step:  44700, time: 0.213, loss: 2107.426270\n",
      "Train: step:  44710, time: 0.227, loss: 2626.577881\n",
      "Train: step:  44720, time: 0.205, loss: 2197.160645\n",
      "Train: step:  44730, time: 0.219, loss: 1898.050049\n",
      "Train: step:  44740, time: 0.219, loss: 1386.109741\n",
      "Train: step:  44750, time: 0.188, loss: 2880.848389\n",
      "Train: step:  44760, time: 0.190, loss: 2950.728027\n",
      "Train: step:  44770, time: 0.187, loss: 667.282288\n",
      "Train: step:  44780, time: 0.189, loss: 757.683289\n",
      "Train: step:  44790, time: 0.229, loss: 1332.303467\n",
      "Train: step:  44800, time: 0.185, loss: 889.556152\n",
      "Train: step:  44810, time: 0.215, loss: 1854.727417\n",
      "Train: step:  44820, time: 0.188, loss: 1496.990112\n",
      "Train: step:  44830, time: 0.206, loss: 1770.967896\n",
      "Train: step:  44840, time: 0.184, loss: 2480.081055\n",
      "Train: step:  44850, time: 0.218, loss: 1418.251099\n",
      "Train: step:  44860, time: 0.217, loss: 1463.104614\n",
      "Train: step:  44870, time: 0.218, loss: 3903.223877\n",
      "Train: step:  44880, time: 0.186, loss: 898.174072\n",
      "Train: step:  44890, time: 0.194, loss: 1068.507568\n",
      "Train: step:  44900, time: 0.216, loss: 1052.056519\n",
      "Train: step:  44910, time: 0.185, loss: 530.914917\n",
      "Train: step:  44920, time: 0.203, loss: 391.688629\n",
      "Train: step:  44930, time: 0.217, loss: 2315.864990\n",
      "Train: step:  44940, time: 0.209, loss: 330.096039\n",
      "Train: step:  44950, time: 0.234, loss: 3722.183838\n",
      "Train: step:  44960, time: 0.213, loss: 429.492065\n",
      "Train: step:  44970, time: 0.234, loss: 3930.943359\n",
      "Train: step:  44980, time: 0.233, loss: 4904.900391\n",
      "Train: step:  44990, time: 0.187, loss: 909.506226\n",
      "Train: step:  45000, time: 0.217, loss: 2943.131836\n",
      "Train: step:  45010, time: 0.200, loss: 1880.359741\n",
      "Train: step:  45020, time: 0.232, loss: 1792.853149\n",
      "Train: step:  45030, time: 0.226, loss: 1568.113281\n",
      "Train: step:  45040, time: 0.195, loss: 1598.044312\n",
      "Train: step:  45050, time: 0.210, loss: 2355.680420\n",
      "Train: step:  45060, time: 0.189, loss: 1464.921143\n",
      "Train: step:  45070, time: 0.194, loss: 2449.269043\n",
      "Train: step:  45080, time: 0.188, loss: 3587.119141\n",
      "Train: step:  45090, time: 0.229, loss: 463.986908\n",
      "Train: step:  45100, time: 0.237, loss: 2298.030029\n",
      "Train: step:  45110, time: 0.214, loss: 582.048340\n",
      "Train: step:  45120, time: 0.189, loss: 2455.840820\n",
      "Train: step:  45130, time: 0.215, loss: 826.374329\n",
      "Train: step:  45140, time: 0.190, loss: 2768.324463\n",
      "Train: step:  45150, time: 0.215, loss: 2417.894287\n",
      "Train: step:  45160, time: 0.199, loss: 796.914001\n",
      "Train: step:  45170, time: 0.223, loss: 2581.410645\n",
      "Train: step:  45180, time: 0.191, loss: 2390.116943\n",
      "Train: step:  45190, time: 0.199, loss: 658.696777\n",
      "Train: step:  45200, time: 0.200, loss: 1330.728271\n",
      "Train: step:  45210, time: 0.196, loss: 1985.126221\n",
      "Train: step:  45220, time: 0.215, loss: 1178.653564\n",
      "Train: step:  45230, time: 0.230, loss: 3624.270020\n",
      "Train: step:  45240, time: 0.192, loss: 2200.464600\n",
      "Train: step:  45250, time: 0.187, loss: 1141.322388\n",
      "Train: step:  45260, time: 0.195, loss: 576.489258\n",
      "Train: step:  45270, time: 0.217, loss: 1989.167114\n",
      "Train: step:  45280, time: 0.240, loss: 2916.834473\n",
      "Train: step:  45290, time: 0.193, loss: 1546.471558\n",
      "Train: step:  45300, time: 0.232, loss: 895.607361\n",
      "Train: step:  45310, time: 0.200, loss: 1764.727783\n",
      "Train: step:  45320, time: 0.185, loss: 951.919250\n",
      "Train: step:  45330, time: 0.200, loss: 2338.096924\n",
      "Train: step:  45340, time: 0.216, loss: 1432.057373\n",
      "Train: step:  45350, time: 0.181, loss: 2272.063965\n",
      "Train: step:  45360, time: 0.202, loss: 1749.533081\n",
      "Train: step:  45370, time: 0.185, loss: 515.289673\n",
      "Train: step:  45380, time: 0.206, loss: 2167.253174\n",
      "Train: step:  45390, time: 0.235, loss: 2949.274170\n",
      "Train: step:  45400, time: 0.229, loss: 2019.798340\n",
      "Train: step:  45410, time: 0.216, loss: 2506.785400\n",
      "Train: step:  45420, time: 0.214, loss: 2678.527344\n",
      "Train: step:  45430, time: 0.191, loss: 2635.965820\n",
      "Train: step:  45440, time: 0.217, loss: 1011.090271\n",
      "Train: step:  45450, time: 0.182, loss: 677.532043\n",
      "Train: step:  45460, time: 0.228, loss: 857.681824\n",
      "Train: step:  45470, time: 0.194, loss: 2949.988770\n",
      "Train: step:  45480, time: 0.215, loss: 1901.200562\n",
      "Train: step:  45490, time: 0.208, loss: 2675.955322\n",
      "Train: step:  45500, time: 0.217, loss: 432.853210\n",
      "Train: step:  45510, time: 0.206, loss: 1134.200317\n",
      "Train: step:  45520, time: 0.225, loss: 2319.718018\n",
      "Train: step:  45530, time: 0.216, loss: 267.366608\n",
      "Train: step:  45540, time: 0.183, loss: 3247.279541\n",
      "Train: step:  45550, time: 0.227, loss: 2808.068848\n",
      "Train: step:  45560, time: 0.189, loss: 2434.440674\n",
      "Train: step:  45570, time: 0.230, loss: 1471.618530\n",
      "Train: step:  45580, time: 0.216, loss: 2503.935547\n",
      "Train: step:  45590, time: 0.191, loss: 350.577240\n",
      "Train: step:  45600, time: 0.189, loss: 3235.450684\n",
      "Train: step:  45610, time: 0.184, loss: 1192.274170\n",
      "Train: step:  45620, time: 0.219, loss: 628.418152\n",
      "Train: step:  45630, time: 0.194, loss: 2100.605225\n",
      "Train: step:  45640, time: 0.240, loss: 1698.161499\n",
      "Train: step:  45650, time: 0.189, loss: 1487.112305\n",
      "Train: step:  45660, time: 0.185, loss: 1521.571045\n",
      "Train: step:  45670, time: 0.181, loss: 955.173401\n",
      "Train: step:  45680, time: 0.241, loss: 1671.540771\n",
      "Train: step:  45690, time: 0.206, loss: 2683.035156\n",
      "Train: step:  45700, time: 0.189, loss: 3220.173340\n",
      "Train: step:  45710, time: 0.184, loss: 915.472595\n",
      "Train: step:  45720, time: 0.198, loss: 2257.744141\n",
      "Train: step:  45730, time: 0.218, loss: 1643.231201\n",
      "Train: step:  45740, time: 0.198, loss: 850.540527\n",
      "Train: step:  45750, time: 0.205, loss: 1087.822754\n",
      "Train: step:  45760, time: 0.191, loss: 2742.638184\n",
      "Train: step:  45770, time: 0.227, loss: 1405.171143\n",
      "Train: step:  45780, time: 0.196, loss: 669.609314\n",
      "Train: step:  45790, time: 0.222, loss: 727.466370\n",
      "Train: step:  45800, time: 0.194, loss: 2465.041016\n",
      "Train: step:  45810, time: 0.192, loss: 983.169006\n",
      "Train: step:  45820, time: 0.214, loss: 1143.604736\n",
      "Train: step:  45830, time: 0.221, loss: 784.452087\n",
      "Train: step:  45840, time: 0.217, loss: 1305.877441\n",
      "Train: step:  45850, time: 0.191, loss: 1167.153931\n",
      "Train: step:  45860, time: 0.196, loss: 4318.862305\n",
      "Train: step:  45870, time: 0.226, loss: 505.335236\n",
      "Train: step:  45880, time: 0.186, loss: 3898.759521\n",
      "Train: step:  45890, time: 0.231, loss: 3658.524414\n",
      "Train: step:  45900, time: 0.230, loss: 2221.216064\n",
      "Train: step:  45910, time: 0.234, loss: 405.052582\n",
      "Train: step:  45920, time: 0.186, loss: 692.671082\n",
      "Train: step:  45930, time: 0.186, loss: 696.246094\n",
      "Train: step:  45940, time: 0.188, loss: 1800.432495\n",
      "Train: step:  45950, time: 0.192, loss: 3303.367432\n",
      "Train: step:  45960, time: 0.199, loss: 2324.075195\n",
      "Train: step:  45970, time: 0.227, loss: 2487.036865\n",
      "Train: step:  45980, time: 0.217, loss: 3769.418701\n",
      "Train: step:  45990, time: 0.190, loss: 2055.401367\n",
      "Train: step:  46000, time: 0.195, loss: 1801.625488\n",
      "Train: step:  46010, time: 0.232, loss: 2699.434570\n",
      "Train: step:  46020, time: 0.214, loss: 1067.106079\n",
      "Train: step:  46030, time: 0.217, loss: 2391.564941\n",
      "Train: step:  46040, time: 0.224, loss: 187.091415\n",
      "Train: step:  46050, time: 0.182, loss: 1250.591553\n",
      "Train: step:  46060, time: 0.190, loss: 2162.507568\n",
      "Train: step:  46070, time: 0.217, loss: 2077.770264\n",
      "Train: step:  46080, time: 0.234, loss: 757.364075\n",
      "Train: step:  46090, time: 0.233, loss: 2466.311035\n",
      "Train: step:  46100, time: 0.226, loss: 3573.595215\n",
      "Train: step:  46110, time: 0.199, loss: 3040.971680\n",
      "Train: step:  46120, time: 0.221, loss: 393.079102\n",
      "Train: step:  46130, time: 0.223, loss: 2197.016113\n",
      "Train: step:  46140, time: 0.190, loss: 696.114136\n",
      "Train: step:  46150, time: 0.187, loss: 1574.860840\n",
      "Train: step:  46160, time: 0.183, loss: 1023.680298\n",
      "Train: step:  46170, time: 0.263, loss: 2019.515137\n",
      "Train: step:  46180, time: 0.255, loss: 749.690918\n",
      "Train: step:  46190, time: 0.217, loss: 2134.519287\n",
      "Train: step:  46200, time: 0.216, loss: 2980.018799\n",
      "Train: step:  46210, time: 0.182, loss: 520.647034\n",
      "Train: step:  46220, time: 0.194, loss: 1554.415283\n",
      "Train: step:  46230, time: 0.232, loss: 870.407959\n",
      "Train: step:  46240, time: 0.195, loss: 1083.952637\n",
      "Train: step:  46250, time: 0.226, loss: 587.478394\n",
      "Train: step:  46260, time: 0.209, loss: 1787.089478\n",
      "Train: step:  46270, time: 0.202, loss: 2803.318604\n",
      "Train: step:  46280, time: 0.206, loss: 4223.601074\n",
      "Train: step:  46290, time: 0.196, loss: 1821.046875\n",
      "Train: step:  46300, time: 0.193, loss: 1865.561157\n",
      "Train: step:  46310, time: 0.259, loss: 1634.494385\n",
      "Train: step:  46320, time: 0.219, loss: 1333.378052\n",
      "Train: step:  46330, time: 0.216, loss: 2714.135010\n",
      "Train: step:  46340, time: 0.203, loss: 1266.469116\n",
      "Train: step:  46350, time: 0.185, loss: 2124.320801\n",
      "Train: step:  46360, time: 0.201, loss: 1444.823730\n",
      "Train: step:  46370, time: 0.191, loss: 1751.853394\n",
      "Train: step:  46380, time: 0.214, loss: 1499.504028\n",
      "Train: step:  46390, time: 0.207, loss: 1718.065063\n",
      "Train: step:  46400, time: 0.233, loss: 961.649231\n",
      "Train: step:  46410, time: 0.216, loss: 2327.735352\n",
      "Train: step:  46420, time: 0.219, loss: 1810.179688\n",
      "Train: step:  46430, time: 0.212, loss: 270.059937\n",
      "Train: step:  46440, time: 0.229, loss: 2984.208740\n",
      "Train: step:  46450, time: 0.217, loss: 1184.886108\n",
      "Train: step:  46460, time: 0.197, loss: 3178.260498\n",
      "Train: step:  46470, time: 0.216, loss: 1727.846924\n",
      "Train: step:  46480, time: 0.190, loss: 2208.969971\n",
      "Train: step:  46490, time: 0.216, loss: 2870.379883\n",
      "Train: step:  46500, time: 0.197, loss: 1458.770142\n",
      "Train: step:  46510, time: 0.217, loss: 427.880707\n",
      "Train: step:  46520, time: 0.217, loss: 1762.283325\n",
      "Train: step:  46530, time: 0.186, loss: 1238.359741\n",
      "Train: step:  46540, time: 0.199, loss: 1745.406494\n",
      "Train: step:  46550, time: 0.219, loss: 523.755737\n",
      "Train: step:  46560, time: 0.226, loss: 2120.480225\n",
      "Train: step:  46570, time: 0.192, loss: 1164.052124\n",
      "Train: step:  46580, time: 0.245, loss: 2802.826660\n",
      "Train: step:  46590, time: 0.189, loss: 3049.212646\n",
      "Train: step:  46600, time: 0.201, loss: 1511.298218\n",
      "Train: step:  46610, time: 0.228, loss: 2482.178467\n",
      "Train: step:  46620, time: 0.191, loss: 747.015991\n",
      "Train: step:  46630, time: 0.226, loss: 1488.805908\n",
      "Train: step:  46640, time: 0.197, loss: 2133.207275\n",
      "Train: step:  46650, time: 0.183, loss: 1052.949951\n",
      "Train: step:  46660, time: 0.191, loss: 352.465729\n",
      "Train: step:  46670, time: 0.251, loss: 1148.365479\n",
      "Train: step:  46680, time: 0.233, loss: 795.363647\n",
      "Train: step:  46690, time: 0.207, loss: 1379.012329\n",
      "Train: step:  46700, time: 0.229, loss: 1547.138672\n",
      "Train: step:  46710, time: 0.218, loss: 1380.158936\n",
      "Train: step:  46720, time: 0.184, loss: 834.174988\n",
      "Train: step:  46730, time: 0.248, loss: 1871.144409\n",
      "Train: step:  46740, time: 0.234, loss: 1131.302612\n",
      "Train: step:  46750, time: 0.223, loss: 882.385559\n",
      "Train: step:  46760, time: 0.220, loss: 2415.687012\n",
      "Train: step:  46770, time: 0.228, loss: 1369.650269\n",
      "Train: step:  46780, time: 0.217, loss: 1843.540161\n",
      "Train: step:  46790, time: 0.192, loss: 1520.378540\n",
      "Train: step:  46800, time: 0.226, loss: 5831.389648\n",
      "Train: step:  46810, time: 0.217, loss: 1235.751709\n",
      "Train: step:  46820, time: 0.227, loss: 1341.935547\n",
      "Train: step:  46830, time: 0.199, loss: 1744.038574\n",
      "Train: step:  46840, time: 0.188, loss: 1431.916992\n",
      "Train: step:  46850, time: 0.183, loss: 538.138306\n",
      "Train: step:  46860, time: 0.225, loss: 330.635590\n",
      "Train: step:  46870, time: 0.214, loss: 2710.903320\n",
      "Train: step:  46880, time: 0.235, loss: 2841.413574\n",
      "Train: step:  46890, time: 0.194, loss: 2770.057129\n",
      "Train: step:  46900, time: 0.218, loss: 2199.674561\n",
      "Train: step:  46910, time: 0.213, loss: 624.576965\n",
      "Train: step:  46920, time: 0.230, loss: 1909.586792\n",
      "Train: step:  46930, time: 0.217, loss: 953.787048\n",
      "Train: step:  46940, time: 0.228, loss: 2690.331055\n",
      "Train: step:  46950, time: 0.183, loss: 644.378357\n",
      "Train: step:  46960, time: 0.183, loss: 1860.949097\n",
      "Train: step:  46970, time: 0.221, loss: 1442.728760\n",
      "Train: step:  46980, time: 0.214, loss: 2378.126465\n",
      "Train: step:  46990, time: 0.216, loss: 1120.759888\n",
      "Train: step:  47000, time: 0.222, loss: 2048.427734\n",
      "Train: step:  47010, time: 0.228, loss: 3197.507080\n",
      "Train: step:  47020, time: 0.181, loss: 2054.284912\n",
      "Train: step:  47030, time: 0.188, loss: 1151.901001\n",
      "Train: step:  47040, time: 0.187, loss: 2063.000977\n",
      "Train: step:  47050, time: 0.196, loss: 3188.296387\n",
      "Train: step:  47060, time: 0.201, loss: 1156.248291\n",
      "Train: step:  47070, time: 0.214, loss: 2947.684082\n",
      "Train: step:  47080, time: 0.217, loss: 2927.557129\n",
      "Train: step:  47090, time: 0.187, loss: 3108.635498\n",
      "Train: step:  47100, time: 0.190, loss: 2251.696289\n",
      "Train: step:  47110, time: 0.192, loss: 745.952209\n",
      "Train: step:  47120, time: 0.190, loss: 1403.441772\n",
      "Train: step:  47130, time: 0.215, loss: 3210.614502\n",
      "Train: step:  47140, time: 0.186, loss: 5307.521973\n",
      "Train: step:  47150, time: 0.203, loss: 1420.568726\n",
      "Train: step:  47160, time: 0.213, loss: 2907.559082\n",
      "Train: step:  47170, time: 0.221, loss: 3302.780273\n",
      "Train: step:  47180, time: 0.204, loss: 651.466370\n",
      "Train: step:  47190, time: 0.258, loss: 424.589081\n",
      "Train: step:  47200, time: 0.246, loss: 1068.103027\n",
      "Train: step:  47210, time: 0.225, loss: 2467.160400\n",
      "Train: step:  47220, time: 0.216, loss: 1851.877563\n",
      "Train: step:  47230, time: 0.202, loss: 2248.506836\n",
      "Train: step:  47240, time: 0.193, loss: 2367.565918\n",
      "Train: step:  47250, time: 0.189, loss: 1925.076660\n",
      "Train: step:  47260, time: 0.192, loss: 1713.750854\n",
      "Train: step:  47270, time: 0.210, loss: 2898.932861\n",
      "Train: step:  47280, time: 0.188, loss: 2416.381104\n",
      "Train: step:  47290, time: 0.189, loss: 2873.533691\n",
      "Train: step:  47300, time: 0.189, loss: 5259.531738\n",
      "Train: step:  47310, time: 0.216, loss: 1615.728638\n",
      "Train: step:  47320, time: 0.192, loss: 1970.763794\n",
      "Train: step:  47330, time: 0.195, loss: 2041.862915\n",
      "Train: step:  47340, time: 0.186, loss: 1200.499023\n",
      "Train: step:  47350, time: 0.191, loss: 2282.722656\n",
      "Train: step:  47360, time: 0.193, loss: 1062.001465\n",
      "Train: step:  47370, time: 0.218, loss: 1982.156372\n",
      "Train: step:  47380, time: 0.228, loss: 1977.416382\n",
      "Train: step:  47390, time: 0.215, loss: 2748.436523\n",
      "Train: step:  47400, time: 0.219, loss: 5044.307129\n",
      "Train: step:  47410, time: 0.233, loss: 537.053711\n",
      "Train: step:  47420, time: 0.218, loss: 1271.260132\n",
      "Train: step:  47430, time: 0.228, loss: 2777.860107\n",
      "Train: step:  47440, time: 0.216, loss: 3363.985107\n",
      "Train: step:  47450, time: 0.200, loss: 3357.420898\n",
      "Train: step:  47460, time: 0.224, loss: 553.201050\n",
      "Train: step:  47470, time: 0.238, loss: 1645.693604\n",
      "Train: step:  47480, time: 0.186, loss: 1152.275391\n",
      "Train: step:  47490, time: 0.194, loss: 2668.377930\n",
      "Train: step:  47500, time: 0.184, loss: 1692.426636\n",
      "Train: step:  47510, time: 0.228, loss: 3258.793457\n",
      "Train: step:  47520, time: 0.217, loss: 389.268951\n",
      "Train: step:  47530, time: 0.191, loss: 930.373169\n",
      "Train: step:  47540, time: 0.221, loss: 2781.523682\n",
      "Train: step:  47550, time: 0.193, loss: 3791.368652\n",
      "Train: step:  47560, time: 0.183, loss: 2741.956299\n",
      "Train: step:  47570, time: 0.201, loss: 1059.910767\n",
      "Train: step:  47580, time: 0.231, loss: 2333.638916\n",
      "Train: step:  47590, time: 0.194, loss: 558.051025\n",
      "Train: step:  47600, time: 0.198, loss: 2279.793945\n",
      "Train: step:  47610, time: 0.228, loss: 1649.034790\n",
      "Train: step:  47620, time: 0.222, loss: 1007.830627\n",
      "Train: step:  47630, time: 0.191, loss: 3603.891602\n",
      "Train: step:  47640, time: 0.184, loss: 1589.337769\n",
      "Train: step:  47650, time: 0.227, loss: 1736.897583\n",
      "Train: step:  47660, time: 0.232, loss: 668.473450\n",
      "Train: step:  47670, time: 0.229, loss: 287.465057\n",
      "Train: step:  47680, time: 0.221, loss: 1512.016968\n",
      "Train: step:  47690, time: 0.194, loss: 3362.429199\n",
      "Train: step:  47700, time: 0.189, loss: 930.127502\n",
      "Train: step:  47710, time: 0.219, loss: 569.747131\n",
      "Train: step:  47720, time: 0.192, loss: 541.497253\n",
      "Train: step:  47730, time: 0.190, loss: 1923.585083\n",
      "Train: step:  47740, time: 0.217, loss: 3635.749268\n",
      "Train: step:  47750, time: 0.217, loss: 2157.901611\n",
      "Train: step:  47760, time: 0.196, loss: 381.643768\n",
      "Train: step:  47770, time: 0.193, loss: 2319.184570\n",
      "Train: step:  47780, time: 0.225, loss: 1451.947632\n",
      "Train: step:  47790, time: 0.196, loss: 1711.343262\n",
      "Train: step:  47800, time: 0.191, loss: 564.365295\n",
      "Train: step:  47810, time: 0.240, loss: 866.118896\n",
      "Train: step:  47820, time: 0.189, loss: 498.936554\n",
      "Train: step:  47830, time: 0.189, loss: 1795.983032\n",
      "Train: step:  47840, time: 0.190, loss: 2106.947998\n",
      "Train: step:  47850, time: 0.195, loss: 2238.120117\n",
      "Train: step:  47860, time: 0.190, loss: 597.368896\n",
      "Train: step:  47870, time: 0.198, loss: 2306.907959\n",
      "Train: step:  47880, time: 0.219, loss: 1636.749878\n",
      "Train: step:  47890, time: 0.235, loss: 1567.100586\n",
      "Train: step:  47900, time: 0.193, loss: 1786.161133\n",
      "Train: step:  47910, time: 0.190, loss: 1562.409058\n",
      "Train: step:  47920, time: 0.218, loss: 622.838135\n",
      "Train: step:  47930, time: 0.244, loss: 1665.853271\n",
      "Train: step:  47940, time: 0.233, loss: 1301.942139\n",
      "Train: step:  47950, time: 0.187, loss: 738.193359\n",
      "Train: step:  47960, time: 0.248, loss: 1385.889038\n",
      "Train: step:  47970, time: 0.239, loss: 2029.839233\n",
      "Train: step:  47980, time: 0.231, loss: 1576.447754\n",
      "Train: step:  47990, time: 0.195, loss: 1344.920654\n",
      "Train: step:  48000, time: 0.191, loss: 950.448975\n",
      "Train: step:  48010, time: 0.217, loss: 2376.774902\n",
      "Train: step:  48020, time: 0.185, loss: 801.669617\n",
      "Train: step:  48030, time: 0.186, loss: 988.566772\n",
      "Train: step:  48040, time: 0.182, loss: 1934.230103\n",
      "Train: step:  48050, time: 0.249, loss: 1349.378906\n",
      "Train: step:  48060, time: 0.189, loss: 1947.830444\n",
      "Train: step:  48070, time: 0.229, loss: 1788.991211\n",
      "Train: step:  48080, time: 0.242, loss: 630.539856\n",
      "Train: step:  48090, time: 0.234, loss: 1231.556885\n",
      "Train: step:  48100, time: 0.217, loss: 330.452454\n",
      "Train: step:  48110, time: 0.189, loss: 1699.727051\n",
      "Train: step:  48120, time: 0.232, loss: 207.693619\n",
      "Train: step:  48130, time: 0.195, loss: 1048.591309\n",
      "Train: step:  48140, time: 0.194, loss: 1774.293579\n",
      "Train: step:  48150, time: 0.202, loss: 2646.637207\n",
      "Train: step:  48160, time: 0.217, loss: 1742.914307\n",
      "Train: step:  48170, time: 0.185, loss: 2752.595947\n",
      "Train: step:  48180, time: 0.217, loss: 2760.057861\n",
      "Train: step:  48190, time: 0.214, loss: 2259.050781\n",
      "Train: step:  48200, time: 0.230, loss: 3134.243164\n",
      "Train: step:  48210, time: 0.256, loss: 1308.953125\n",
      "Train: step:  48220, time: 0.220, loss: 502.931824\n",
      "Train: step:  48230, time: 0.189, loss: 1044.370239\n",
      "Train: step:  48240, time: 0.191, loss: 1997.009277\n",
      "Train: step:  48250, time: 0.188, loss: 2088.603516\n",
      "Train: step:  48260, time: 0.192, loss: 757.475098\n",
      "Train: step:  48270, time: 0.217, loss: 2043.656982\n",
      "Train: step:  48280, time: 0.205, loss: 3053.777832\n",
      "Train: step:  48290, time: 0.217, loss: 1451.881958\n",
      "Train: step:  48300, time: 0.227, loss: 578.590454\n",
      "Train: step:  48310, time: 0.218, loss: 3462.893799\n",
      "Train: step:  48320, time: 0.216, loss: 1343.855957\n",
      "Train: step:  48330, time: 0.219, loss: 2648.902588\n",
      "Train: step:  48340, time: 0.190, loss: 732.038696\n",
      "Train: step:  48350, time: 0.192, loss: 1087.675781\n",
      "Train: step:  48360, time: 0.219, loss: 2899.950439\n",
      "Train: step:  48370, time: 0.189, loss: 2794.746338\n",
      "Train: step:  48380, time: 0.241, loss: 2991.757324\n",
      "Train: step:  48390, time: 0.190, loss: 2689.143555\n",
      "Train: step:  48400, time: 0.190, loss: 1871.950928\n",
      "Train: step:  48410, time: 0.225, loss: 2906.172363\n",
      "Train: step:  48420, time: 0.233, loss: 988.946167\n",
      "Train: step:  48430, time: 0.198, loss: 2189.928711\n",
      "Train: step:  48440, time: 0.210, loss: 3555.863281\n",
      "Train: step:  48450, time: 0.188, loss: 1781.951050\n",
      "Train: step:  48460, time: 0.192, loss: 2371.339600\n",
      "Train: step:  48470, time: 0.214, loss: 1806.662109\n",
      "Train: step:  48480, time: 0.221, loss: 1369.408325\n",
      "Train: step:  48490, time: 0.231, loss: 3263.940186\n",
      "Train: step:  48500, time: 0.191, loss: 2410.872803\n",
      "Train: step:  48510, time: 0.221, loss: 2966.661377\n",
      "Train: step:  48520, time: 0.220, loss: 3718.006104\n",
      "Train: step:  48530, time: 0.196, loss: 2081.982910\n",
      "Train: step:  48540, time: 0.221, loss: 358.774139\n",
      "Train: step:  48550, time: 0.191, loss: 1446.676514\n",
      "Train: step:  48560, time: 0.218, loss: 2625.988281\n",
      "Train: step:  48570, time: 0.229, loss: 2764.343994\n",
      "Train: step:  48580, time: 0.249, loss: 1736.558594\n",
      "Train: step:  48590, time: 0.246, loss: 317.397217\n",
      "Train: step:  48600, time: 0.217, loss: 1376.596313\n",
      "Train: step:  48610, time: 0.192, loss: 1682.052490\n",
      "Train: step:  48620, time: 0.188, loss: 314.395569\n",
      "Train: step:  48630, time: 0.223, loss: 876.043457\n",
      "Train: step:  48640, time: 0.184, loss: 1480.255859\n",
      "Train: step:  48650, time: 0.182, loss: 2199.774414\n",
      "Train: step:  48660, time: 0.202, loss: 476.345856\n",
      "Train: step:  48670, time: 0.203, loss: 1079.213867\n",
      "Train: step:  48680, time: 0.185, loss: 2634.842773\n",
      "Train: step:  48690, time: 0.177, loss: 915.195129\n",
      "Train: step:  48700, time: 0.216, loss: 1064.335205\n",
      "Train: step:  48710, time: 0.219, loss: 862.288147\n",
      "Train: step:  48720, time: 0.214, loss: 2034.035889\n",
      "Train: step:  48730, time: 0.212, loss: 722.702271\n",
      "Train: step:  48740, time: 0.219, loss: 2680.481934\n",
      "Train: step:  48750, time: 0.188, loss: 2753.496826\n",
      "Train: step:  48760, time: 0.189, loss: 1989.003540\n",
      "Train: step:  48770, time: 0.197, loss: 3789.305176\n",
      "Train: step:  48780, time: 0.197, loss: 722.031860\n",
      "Train: step:  48790, time: 0.189, loss: 1964.970093\n",
      "Train: step:  48800, time: 0.187, loss: 849.581482\n",
      "Train: step:  48810, time: 0.187, loss: 1991.647827\n",
      "Train: step:  48820, time: 0.232, loss: 374.646179\n",
      "Train: step:  48830, time: 0.233, loss: 2738.533203\n",
      "Train: step:  48840, time: 0.216, loss: 3657.957031\n",
      "Train: step:  48850, time: 0.216, loss: 789.424072\n",
      "Train: step:  48860, time: 0.217, loss: 1778.775269\n",
      "Train: step:  48870, time: 0.188, loss: 592.564880\n",
      "Train: step:  48880, time: 0.188, loss: 4140.213867\n",
      "Train: step:  48890, time: 0.224, loss: 949.973816\n",
      "Train: step:  48900, time: 0.247, loss: 1198.451172\n",
      "Train: step:  48910, time: 0.187, loss: 938.176941\n",
      "Train: step:  48920, time: 0.183, loss: 1810.664307\n",
      "Train: step:  48930, time: 0.230, loss: 1183.230225\n",
      "Train: step:  48940, time: 0.251, loss: 1955.566162\n",
      "Train: step:  48950, time: 0.232, loss: 2202.330566\n",
      "Train: step:  48960, time: 0.214, loss: 2314.097168\n",
      "Train: step:  48970, time: 0.194, loss: 3094.224121\n",
      "Train: step:  48980, time: 0.200, loss: 2364.753906\n",
      "Train: step:  48990, time: 0.199, loss: 5473.194336\n",
      "Train: step:  49000, time: 0.186, loss: 1500.367798\n",
      "Train: step:  49010, time: 0.184, loss: 3301.617432\n",
      "Train: step:  49020, time: 0.214, loss: 2015.702881\n",
      "Train: step:  49030, time: 0.193, loss: 875.138733\n",
      "Train: step:  49040, time: 0.226, loss: 2683.543213\n",
      "Train: step:  49050, time: 0.217, loss: 1013.763916\n",
      "Train: step:  49060, time: 0.189, loss: 1886.092651\n",
      "Train: step:  49070, time: 0.185, loss: 1980.524658\n",
      "Train: step:  49080, time: 0.254, loss: 866.198853\n",
      "Train: step:  49090, time: 0.222, loss: 2538.221680\n",
      "Train: step:  49100, time: 0.197, loss: 1959.452271\n",
      "Train: step:  49110, time: 0.264, loss: 1724.716797\n",
      "Train: step:  49120, time: 0.188, loss: 2712.285645\n",
      "Train: step:  49130, time: 0.226, loss: 2291.299072\n",
      "Train: step:  49140, time: 0.238, loss: 1692.474121\n",
      "Train: step:  49150, time: 0.232, loss: 1636.605835\n",
      "Train: step:  49160, time: 0.218, loss: 1001.408081\n",
      "Train: step:  49170, time: 0.239, loss: 385.970306\n",
      "Train: step:  49180, time: 0.217, loss: 1409.312866\n",
      "Train: step:  49190, time: 0.193, loss: 1610.563354\n",
      "Train: step:  49200, time: 0.191, loss: 1179.237305\n",
      "Train: step:  49210, time: 0.182, loss: 1943.468628\n",
      "Train: step:  49220, time: 0.193, loss: 1044.704712\n",
      "Train: step:  49230, time: 0.205, loss: 759.769165\n",
      "Train: step:  49240, time: 0.187, loss: 2301.266357\n",
      "Train: step:  49250, time: 0.187, loss: 2119.020264\n",
      "Train: step:  49260, time: 0.193, loss: 1211.030640\n",
      "Train: step:  49270, time: 0.229, loss: 3845.288330\n",
      "Train: step:  49280, time: 0.195, loss: 515.089233\n",
      "Train: step:  49290, time: 0.188, loss: 931.622375\n",
      "Train: step:  49300, time: 0.218, loss: 3347.873291\n",
      "Train: step:  49310, time: 0.211, loss: 835.085632\n",
      "Train: step:  49320, time: 0.218, loss: 2560.742188\n",
      "Train: step:  49330, time: 0.195, loss: 1370.607056\n",
      "Train: step:  49340, time: 0.241, loss: 639.585876\n",
      "Train: step:  49350, time: 0.246, loss: 2866.062256\n",
      "Train: step:  49360, time: 0.217, loss: 1681.040649\n",
      "Train: step:  49370, time: 0.190, loss: 754.050903\n",
      "Train: step:  49380, time: 0.249, loss: 2767.283203\n",
      "Train: step:  49390, time: 0.195, loss: 4081.651611\n",
      "Train: step:  49400, time: 0.216, loss: 1074.985352\n",
      "Train: step:  49410, time: 0.183, loss: 609.256958\n",
      "Train: step:  49420, time: 0.186, loss: 763.720215\n",
      "Train: step:  49430, time: 0.200, loss: 2565.492432\n",
      "Train: step:  49440, time: 0.206, loss: 1715.667969\n",
      "Train: step:  49450, time: 0.217, loss: 1609.875488\n",
      "Train: step:  49460, time: 0.187, loss: 551.760254\n",
      "Train: step:  49470, time: 0.187, loss: 2258.092285\n",
      "Train: step:  49480, time: 0.229, loss: 1197.673950\n",
      "Train: step:  49490, time: 0.228, loss: 1157.366943\n",
      "Train: step:  49500, time: 0.218, loss: 3194.475830\n",
      "Train: step:  49510, time: 0.234, loss: 2953.122314\n",
      "Train: step:  49520, time: 0.206, loss: 2700.844971\n",
      "Train: step:  49530, time: 0.209, loss: 444.528656\n",
      "Train: step:  49540, time: 0.227, loss: 1825.053467\n",
      "Train: step:  49550, time: 0.195, loss: 1913.773560\n",
      "Train: step:  49560, time: 0.217, loss: 2851.131348\n",
      "Train: step:  49570, time: 0.194, loss: 2491.043701\n",
      "Train: step:  49580, time: 0.195, loss: 269.239105\n",
      "Train: step:  49590, time: 0.197, loss: 934.679810\n",
      "Train: step:  49600, time: 0.229, loss: 2494.280762\n",
      "Train: step:  49610, time: 0.197, loss: 1155.993408\n",
      "Train: step:  49620, time: 0.193, loss: 694.802002\n",
      "Train: step:  49630, time: 0.195, loss: 1801.251831\n",
      "Train: step:  49640, time: 0.187, loss: 2840.282715\n",
      "Train: step:  49650, time: 0.189, loss: 1151.045532\n",
      "Train: step:  49660, time: 0.190, loss: 602.793335\n",
      "Train: step:  49670, time: 0.187, loss: 888.446533\n",
      "Train: step:  49680, time: 0.225, loss: 1804.800537\n",
      "Train: step:  49690, time: 0.228, loss: 1174.335327\n",
      "Train: step:  49700, time: 0.202, loss: 1983.994263\n",
      "Train: step:  49710, time: 0.228, loss: 1042.534424\n",
      "Train: step:  49720, time: 0.216, loss: 2522.456543\n",
      "Train: step:  49730, time: 0.222, loss: 2270.336914\n",
      "Train: step:  49740, time: 0.183, loss: 1715.402832\n",
      "Train: step:  49750, time: 0.218, loss: 2237.500977\n",
      "Train: step:  49760, time: 0.228, loss: 1956.566772\n",
      "Train: step:  49770, time: 0.208, loss: 830.668579\n",
      "Train: step:  49780, time: 0.191, loss: 1564.062622\n",
      "Train: step:  49790, time: 0.228, loss: 551.987610\n",
      "Train: step:  49800, time: 0.229, loss: 530.764221\n",
      "Train: step:  49810, time: 0.200, loss: 1022.040527\n",
      "Train: step:  49820, time: 0.215, loss: 4914.595703\n",
      "Train: step:  49830, time: 0.185, loss: 1471.784424\n",
      "Train: step:  49840, time: 0.191, loss: 2837.532715\n",
      "Train: step:  49850, time: 0.227, loss: 2587.083496\n",
      "Train: step:  49860, time: 0.228, loss: 1816.906738\n",
      "Train: step:  49870, time: 0.211, loss: 2006.050781\n",
      "Train: step:  49880, time: 0.237, loss: 260.387726\n",
      "Train: step:  49890, time: 0.184, loss: 1875.754028\n",
      "Train: step:  49900, time: 0.202, loss: 2273.277588\n",
      "Train: step:  49910, time: 0.186, loss: 2912.763428\n",
      "Train: step:  49920, time: 0.195, loss: 2481.071289\n",
      "Train: step:  49930, time: 0.215, loss: 1235.341187\n",
      "Train: step:  49940, time: 0.270, loss: 2255.094727\n",
      "Train: step:  49950, time: 0.184, loss: 1179.883423\n",
      "Train: step:  49960, time: 0.224, loss: 1134.206543\n",
      "Train: step:  49970, time: 0.222, loss: 755.278198\n",
      "Train: step:  49980, time: 0.189, loss: 1774.295288\n",
      "Train: step:  49990, time: 0.196, loss: 1739.310547\n",
      "Train: step:  50000, time: 0.191, loss: 1777.628296\n",
      "Train: step:  50010, time: 0.260, loss: 2822.323975\n",
      "Train: step:  50020, time: 0.227, loss: 2284.124512\n",
      "Train: step:  50030, time: 0.188, loss: 3545.240234\n",
      "Train: step:  50040, time: 0.218, loss: 1711.549316\n",
      "Train: step:  50050, time: 0.216, loss: 1931.576538\n",
      "Train: step:  50060, time: 0.193, loss: 1696.814697\n",
      "Train: step:  50070, time: 0.230, loss: 1840.099121\n",
      "Train: step:  50080, time: 0.221, loss: 241.797714\n",
      "Train: step:  50090, time: 0.265, loss: 1362.167114\n",
      "Train: step:  50100, time: 0.186, loss: 1175.875854\n",
      "Train: step:  50110, time: 0.190, loss: 1850.089478\n",
      "Train: step:  50120, time: 0.215, loss: 1527.490356\n",
      "Train: step:  50130, time: 0.233, loss: 2539.064697\n",
      "Train: step:  50140, time: 0.216, loss: 2189.717529\n",
      "Train: step:  50150, time: 0.199, loss: 1576.815063\n",
      "Train: step:  50160, time: 0.216, loss: 1710.756226\n",
      "Train: step:  50170, time: 0.193, loss: 2563.046631\n",
      "Train: step:  50180, time: 0.210, loss: 1184.941162\n",
      "Train: step:  50190, time: 0.199, loss: 1372.277100\n",
      "Train: step:  50200, time: 0.231, loss: 442.693817\n",
      "Train: step:  50210, time: 0.192, loss: 2463.939209\n",
      "Train: step:  50220, time: 0.223, loss: 2240.334717\n",
      "Train: step:  50230, time: 0.205, loss: 1594.887817\n",
      "Train: step:  50240, time: 0.191, loss: 1216.218628\n",
      "Train: step:  50250, time: 0.213, loss: 4001.200928\n",
      "Train: step:  50260, time: 0.225, loss: 1474.337036\n",
      "Train: step:  50270, time: 0.227, loss: 275.586060\n",
      "Train: step:  50280, time: 0.216, loss: 1666.852295\n",
      "Train: step:  50290, time: 0.195, loss: 2181.927246\n",
      "Train: step:  50300, time: 0.192, loss: 1933.734375\n",
      "Train: step:  50310, time: 0.191, loss: 834.355591\n",
      "Train: step:  50320, time: 0.192, loss: 2766.659668\n",
      "Train: step:  50330, time: 0.208, loss: 1490.100342\n",
      "Train: step:  50340, time: 0.185, loss: 1482.933716\n",
      "Train: step:  50350, time: 0.186, loss: 2326.367432\n",
      "Train: step:  50360, time: 0.212, loss: 1639.067749\n",
      "Train: step:  50370, time: 0.224, loss: 1948.399170\n",
      "Train: step:  50380, time: 0.238, loss: 2255.481689\n",
      "Train: step:  50390, time: 0.184, loss: 1651.699829\n",
      "Train: step:  50400, time: 0.216, loss: 794.252502\n",
      "Train: step:  50410, time: 0.219, loss: 3387.407715\n",
      "Train: step:  50420, time: 0.216, loss: 2999.417725\n",
      "Train: step:  50430, time: 0.184, loss: 3718.459473\n",
      "Train: step:  50440, time: 0.223, loss: 988.908875\n",
      "Train: step:  50450, time: 0.222, loss: 2291.156250\n",
      "Train: step:  50460, time: 0.216, loss: 656.578491\n",
      "Train: step:  50470, time: 0.190, loss: 1892.738770\n",
      "Train: step:  50480, time: 0.187, loss: 1536.423706\n",
      "Train: step:  50490, time: 0.210, loss: 2186.215820\n",
      "Train: step:  50500, time: 0.244, loss: 2551.702881\n",
      "Train: step:  50510, time: 0.243, loss: 2403.901367\n",
      "Train: step:  50520, time: 0.227, loss: 1413.040039\n",
      "Train: step:  50530, time: 0.228, loss: 1126.638428\n",
      "Train: step:  50540, time: 0.219, loss: 4327.989746\n",
      "Train: step:  50550, time: 0.195, loss: 1325.119751\n",
      "Train: step:  50560, time: 0.185, loss: 1989.024658\n",
      "Train: step:  50570, time: 0.231, loss: 572.834167\n",
      "Train: step:  50580, time: 0.253, loss: 2021.722290\n",
      "Train: step:  50590, time: 0.215, loss: 1861.611816\n",
      "Train: step:  50600, time: 0.193, loss: 2106.809570\n",
      "Train: step:  50610, time: 0.218, loss: 2617.807129\n",
      "Train: step:  50620, time: 0.186, loss: 407.053040\n",
      "Train: step:  50630, time: 0.183, loss: 2566.755371\n",
      "Train: step:  50640, time: 0.217, loss: 1250.766724\n",
      "Train: step:  50650, time: 0.236, loss: 868.851685\n",
      "Train: step:  50660, time: 0.214, loss: 2195.632568\n",
      "Train: step:  50670, time: 0.222, loss: 3300.584961\n",
      "Train: step:  50680, time: 0.230, loss: 1117.895752\n",
      "Train: step:  50690, time: 0.232, loss: 1525.620972\n",
      "Train: step:  50700, time: 0.193, loss: 1242.800171\n",
      "Train: step:  50710, time: 0.216, loss: 1750.224487\n",
      "Train: step:  50720, time: 0.190, loss: 3618.938232\n",
      "Train: step:  50730, time: 0.202, loss: 2142.913574\n",
      "Train: step:  50740, time: 0.189, loss: 525.703552\n",
      "Train: step:  50750, time: 0.193, loss: 546.070862\n",
      "Train: step:  50760, time: 0.196, loss: 2194.159424\n",
      "Train: step:  50770, time: 0.195, loss: 841.548828\n",
      "Train: step:  50780, time: 0.235, loss: 4489.431152\n",
      "Train: step:  50790, time: 0.196, loss: 1838.364258\n",
      "Train: step:  50800, time: 0.190, loss: 673.974487\n",
      "Train: step:  50810, time: 0.225, loss: 1627.501587\n",
      "Train: step:  50820, time: 0.193, loss: 1012.483276\n",
      "Train: step:  50830, time: 0.190, loss: 2267.017090\n",
      "Train: step:  50840, time: 0.188, loss: 3635.730225\n",
      "Train: step:  50850, time: 0.215, loss: 787.494995\n",
      "Train: step:  50860, time: 0.189, loss: 2999.320557\n",
      "Train: step:  50870, time: 0.250, loss: 1281.468262\n",
      "Train: step:  50880, time: 0.191, loss: 1666.373779\n",
      "Train: step:  50890, time: 0.190, loss: 2786.842285\n",
      "Train: step:  50900, time: 0.183, loss: 2438.902588\n",
      "Train: step:  50910, time: 0.190, loss: 1977.591675\n",
      "Train: step:  50920, time: 0.208, loss: 1657.064453\n",
      "Train: step:  50930, time: 0.234, loss: 2053.383301\n",
      "Train: step:  50940, time: 0.229, loss: 4720.257324\n",
      "Train: step:  50950, time: 0.193, loss: 593.745789\n",
      "Train: step:  50960, time: 0.183, loss: 515.482910\n",
      "Train: step:  50970, time: 0.221, loss: 1389.373291\n",
      "Train: step:  50980, time: 0.224, loss: 519.815735\n",
      "Train: step:  50990, time: 0.221, loss: 1771.359009\n",
      "Train: step:  51000, time: 0.217, loss: 1792.105835\n",
      "Train: step:  51010, time: 0.184, loss: 2811.040771\n",
      "Train: step:  51020, time: 0.184, loss: 1466.012695\n",
      "Train: step:  51030, time: 0.216, loss: 2968.494385\n",
      "Train: step:  51040, time: 0.195, loss: 2321.289795\n",
      "Train: step:  51050, time: 0.224, loss: 1683.554321\n",
      "Train: step:  51060, time: 0.221, loss: 3350.205811\n",
      "Train: step:  51070, time: 0.188, loss: 665.810120\n",
      "Train: step:  51080, time: 0.217, loss: 1059.542725\n",
      "Train: step:  51090, time: 0.217, loss: 578.262756\n",
      "Train: step:  51100, time: 0.189, loss: 2314.994385\n",
      "Train: step:  51110, time: 0.195, loss: 2441.586670\n",
      "Train: step:  51120, time: 0.218, loss: 1140.340332\n",
      "Train: step:  51130, time: 0.231, loss: 1412.338501\n",
      "Train: step:  51140, time: 0.193, loss: 3646.702148\n",
      "Train: step:  51150, time: 0.217, loss: 708.677917\n",
      "Train: step:  51160, time: 0.229, loss: 1831.771240\n",
      "Train: step:  51170, time: 0.191, loss: 3177.502686\n",
      "Train: step:  51180, time: 0.190, loss: 1209.806030\n",
      "Train: step:  51190, time: 0.195, loss: 2229.229980\n",
      "Train: step:  51200, time: 0.194, loss: 4106.767578\n",
      "Train: step:  51210, time: 0.228, loss: 2130.156982\n",
      "Train: step:  51220, time: 0.216, loss: 2419.811279\n",
      "Train: step:  51230, time: 0.218, loss: 1273.198242\n",
      "Train: step:  51240, time: 0.215, loss: 1080.278442\n",
      "Train: step:  51250, time: 0.204, loss: 1126.798340\n",
      "Train: step:  51260, time: 0.223, loss: 1517.747803\n",
      "Train: step:  51270, time: 0.186, loss: 3030.531738\n",
      "Train: step:  51280, time: 0.253, loss: 1519.503662\n",
      "Train: step:  51290, time: 0.218, loss: 1764.744629\n",
      "Train: step:  51300, time: 0.187, loss: 2374.314453\n",
      "Train: step:  51310, time: 0.186, loss: 2166.300781\n",
      "Train: step:  51320, time: 0.219, loss: 1401.984619\n",
      "Train: step:  51330, time: 0.215, loss: 2972.318115\n",
      "Train: step:  51340, time: 0.241, loss: 1785.008057\n",
      "Train: step:  51350, time: 0.195, loss: 1721.648926\n",
      "Train: step:  51360, time: 0.198, loss: 3777.145752\n",
      "Train: step:  51370, time: 0.194, loss: 1127.451172\n",
      "Train: step:  51380, time: 0.227, loss: 1646.475220\n",
      "Train: step:  51390, time: 0.192, loss: 1457.507446\n",
      "Train: step:  51400, time: 0.193, loss: 1145.049438\n",
      "Train: step:  51410, time: 0.190, loss: 683.330627\n",
      "Train: step:  51420, time: 0.230, loss: 2323.157471\n",
      "Train: step:  51430, time: 0.186, loss: 2373.799072\n",
      "Train: step:  51440, time: 0.216, loss: 1229.055664\n",
      "Train: step:  51450, time: 0.228, loss: 1291.696411\n",
      "Train: step:  51460, time: 0.218, loss: 3251.838867\n",
      "Train: step:  51470, time: 0.200, loss: 1899.079712\n",
      "Train: step:  51480, time: 0.194, loss: 1359.864380\n",
      "Train: step:  51490, time: 0.193, loss: 2575.860840\n",
      "Train: step:  51500, time: 0.193, loss: 2674.597168\n",
      "Train: step:  51510, time: 0.192, loss: 2998.487305\n",
      "Train: step:  51520, time: 0.191, loss: 1873.207153\n",
      "Train: step:  51530, time: 0.227, loss: 1268.516479\n",
      "Train: step:  51540, time: 0.192, loss: 488.362946\n",
      "Train: step:  51550, time: 0.215, loss: 1646.882202\n",
      "Train: step:  51560, time: 0.220, loss: 1000.779114\n",
      "Train: step:  51570, time: 0.254, loss: 2034.132446\n",
      "Train: step:  51580, time: 0.191, loss: 1056.696655\n",
      "Train: step:  51590, time: 0.185, loss: 3557.073730\n",
      "Train: step:  51600, time: 0.196, loss: 662.130005\n",
      "Train: step:  51610, time: 0.183, loss: 522.887329\n",
      "Train: step:  51620, time: 0.215, loss: 1466.172363\n",
      "Train: step:  51630, time: 0.184, loss: 869.502930\n",
      "Train: step:  51640, time: 0.188, loss: 1599.209229\n",
      "Train: step:  51650, time: 0.228, loss: 2582.538330\n",
      "Train: step:  51660, time: 0.207, loss: 550.576233\n",
      "Train: step:  51670, time: 0.216, loss: 1775.290161\n",
      "Train: step:  51680, time: 0.216, loss: 1859.041626\n",
      "Train: step:  51690, time: 0.232, loss: 2011.661499\n",
      "Train: step:  51700, time: 0.231, loss: 2968.806152\n",
      "Train: step:  51710, time: 0.183, loss: 1378.511353\n",
      "Train: step:  51720, time: 0.217, loss: 2178.057373\n",
      "Train: step:  51730, time: 0.216, loss: 2514.079834\n",
      "Train: step:  51740, time: 0.227, loss: 2852.198975\n",
      "Train: step:  51750, time: 0.232, loss: 2463.673828\n",
      "Train: step:  51760, time: 0.194, loss: 1622.106934\n",
      "Train: step:  51770, time: 0.183, loss: 1100.850952\n",
      "Train: step:  51780, time: 0.187, loss: 2120.101562\n",
      "Train: step:  51790, time: 0.193, loss: 707.918457\n",
      "Train: step:  51800, time: 0.216, loss: 2291.820557\n",
      "Train: step:  51810, time: 0.212, loss: 3044.280273\n",
      "Train: step:  51820, time: 0.235, loss: 784.690796\n",
      "Train: step:  51830, time: 0.217, loss: 2725.468018\n",
      "Train: step:  51840, time: 0.192, loss: 2082.250732\n",
      "Train: step:  51850, time: 0.191, loss: 729.376099\n",
      "Train: step:  51860, time: 0.191, loss: 317.823456\n",
      "Train: step:  51870, time: 0.227, loss: 1269.831299\n",
      "Train: step:  51880, time: 0.228, loss: 1320.745850\n",
      "Train: step:  51890, time: 0.200, loss: 876.570251\n",
      "Train: step:  51900, time: 0.224, loss: 1203.311768\n",
      "Train: step:  51910, time: 0.231, loss: 877.167786\n",
      "Train: step:  51920, time: 0.193, loss: 1110.726807\n",
      "Train: step:  51930, time: 0.229, loss: 1101.692505\n",
      "Train: step:  51940, time: 0.217, loss: 1017.933655\n",
      "Train: step:  51950, time: 0.187, loss: 632.396362\n",
      "Train: step:  51960, time: 0.217, loss: 2243.049072\n",
      "Train: step:  51970, time: 0.234, loss: 447.429199\n",
      "Train: step:  51980, time: 0.238, loss: 4058.779541\n",
      "Train: step:  51990, time: 0.277, loss: 1524.598755\n",
      "Train: step:  52000, time: 0.239, loss: 1762.714233\n",
      "Train: step:  52010, time: 0.191, loss: 1817.635010\n",
      "Train: step:  52020, time: 0.202, loss: 1794.632324\n",
      "Train: step:  52030, time: 0.233, loss: 3514.210449\n",
      "Train: step:  52040, time: 0.253, loss: 1862.422607\n",
      "Train: step:  52050, time: 0.211, loss: 1946.928467\n",
      "Train: step:  52060, time: 0.219, loss: 337.154358\n",
      "Train: step:  52070, time: 0.209, loss: 1085.301392\n",
      "Train: step:  52080, time: 0.187, loss: 1482.410889\n",
      "Train: step:  52090, time: 0.190, loss: 792.862976\n",
      "Train: step:  52100, time: 0.191, loss: 1233.337646\n",
      "Train: step:  52110, time: 0.194, loss: 273.730011\n",
      "Train: step:  52120, time: 0.189, loss: 722.895630\n",
      "Train: step:  52130, time: 0.189, loss: 2226.822021\n",
      "Train: step:  52140, time: 0.187, loss: 2171.692871\n",
      "Train: step:  52150, time: 0.225, loss: 1191.735107\n",
      "Train: step:  52160, time: 0.234, loss: 1116.505981\n",
      "Train: step:  52170, time: 0.261, loss: 830.900940\n",
      "Train: step:  52180, time: 0.228, loss: 1684.007935\n",
      "Train: step:  52190, time: 0.218, loss: 257.520813\n",
      "Train: step:  52200, time: 0.188, loss: 1255.350830\n",
      "Train: step:  52210, time: 0.227, loss: 1350.271362\n",
      "Train: step:  52220, time: 0.199, loss: 3182.718750\n",
      "Train: step:  52230, time: 0.222, loss: 1854.671265\n",
      "Train: step:  52240, time: 0.198, loss: 1133.936279\n",
      "Train: step:  52250, time: 0.217, loss: 445.437500\n",
      "Train: step:  52260, time: 0.216, loss: 1068.789062\n",
      "Train: step:  52270, time: 0.184, loss: 1646.222412\n",
      "Train: step:  52280, time: 0.219, loss: 1955.022583\n",
      "Train: step:  52290, time: 0.186, loss: 765.266113\n",
      "Train: step:  52300, time: 0.183, loss: 240.013306\n",
      "Train: step:  52310, time: 0.186, loss: 3840.476074\n",
      "Train: step:  52320, time: 0.183, loss: 2491.800049\n",
      "Train: step:  52330, time: 0.239, loss: 1250.973022\n",
      "Train: step:  52340, time: 0.188, loss: 1729.291626\n",
      "Train: step:  52350, time: 0.188, loss: 1664.253296\n",
      "Train: step:  52360, time: 0.239, loss: 3609.498291\n",
      "Train: step:  52370, time: 0.240, loss: 276.073792\n",
      "Train: step:  52380, time: 0.197, loss: 946.985962\n",
      "Train: step:  52390, time: 0.188, loss: 2795.556396\n",
      "Train: step:  52400, time: 0.234, loss: 2176.544189\n",
      "Train: step:  52410, time: 0.215, loss: 1796.515625\n",
      "Train: step:  52420, time: 0.194, loss: 2376.356445\n",
      "Train: step:  52430, time: 0.212, loss: 1039.041870\n",
      "Train: step:  52440, time: 0.345, loss: 1056.884888\n",
      "Train: step:  52450, time: 0.207, loss: 1596.106445\n",
      "Train: step:  52460, time: 0.186, loss: 685.923279\n",
      "Train: step:  52470, time: 0.191, loss: 1141.517090\n",
      "Train: step:  52480, time: 0.217, loss: 1049.030396\n",
      "Train: step:  52490, time: 0.191, loss: 1140.437988\n",
      "Train: step:  52500, time: 0.225, loss: 3528.959961\n",
      "Train: step:  52510, time: 0.184, loss: 3532.020020\n",
      "Train: step:  52520, time: 0.217, loss: 2316.330322\n",
      "Train: step:  52530, time: 0.198, loss: 2248.295166\n",
      "Train: step:  52540, time: 0.217, loss: 1183.652344\n",
      "Train: step:  52550, time: 0.193, loss: 784.032959\n",
      "Train: step:  52560, time: 0.228, loss: 1337.347290\n",
      "Train: step:  52570, time: 0.190, loss: 1046.916260\n",
      "Train: step:  52580, time: 0.198, loss: 1363.199707\n",
      "Train: step:  52590, time: 0.227, loss: 982.959106\n",
      "Train: step:  52600, time: 0.217, loss: 862.069519\n",
      "Train: step:  52610, time: 0.220, loss: 1757.669434\n",
      "Train: step:  52620, time: 0.228, loss: 660.314514\n",
      "Train: step:  52630, time: 0.215, loss: 1009.808899\n",
      "Train: step:  52640, time: 0.217, loss: 823.758362\n",
      "Train: step:  52650, time: 0.196, loss: 1178.899048\n",
      "Train: step:  52660, time: 0.190, loss: 1645.097168\n",
      "Train: step:  52670, time: 0.217, loss: 1600.748413\n",
      "Train: step:  52680, time: 0.197, loss: 4455.520508\n",
      "Train: step:  52690, time: 0.232, loss: 3180.487305\n",
      "Train: step:  52700, time: 0.224, loss: 374.603455\n",
      "Train: step:  52710, time: 0.200, loss: 2153.199219\n",
      "Train: step:  52720, time: 0.227, loss: 1901.919067\n",
      "Train: step:  52730, time: 0.187, loss: 1902.091187\n",
      "Train: step:  52740, time: 0.218, loss: 2901.331543\n",
      "Train: step:  52750, time: 0.196, loss: 4084.256104\n",
      "Train: step:  52760, time: 0.227, loss: 1916.882935\n",
      "Train: step:  52770, time: 0.194, loss: 1042.315186\n",
      "Train: step:  52780, time: 0.216, loss: 3081.361328\n",
      "Train: step:  52790, time: 0.200, loss: 912.248108\n",
      "Train: step:  52800, time: 0.261, loss: 2774.019287\n",
      "Train: step:  52810, time: 0.216, loss: 581.167114\n",
      "Train: step:  52820, time: 0.190, loss: 2640.395264\n",
      "Train: step:  52830, time: 0.217, loss: 625.990356\n",
      "Train: step:  52840, time: 0.197, loss: 807.834656\n",
      "Train: step:  52850, time: 0.212, loss: 3295.418213\n",
      "Train: step:  52860, time: 0.185, loss: 1165.006958\n",
      "Train: step:  52870, time: 0.228, loss: 1284.930664\n",
      "Train: step:  52880, time: 0.202, loss: 2357.685791\n",
      "Train: step:  52890, time: 0.213, loss: 1949.981934\n",
      "Train: step:  52900, time: 0.219, loss: 711.929077\n",
      "Train: step:  52910, time: 0.227, loss: 1451.724243\n",
      "Train: step:  52920, time: 0.195, loss: 3134.011719\n",
      "Train: step:  52930, time: 0.226, loss: 4034.051758\n",
      "Train: step:  52940, time: 0.217, loss: 1622.768921\n",
      "Train: step:  52950, time: 0.207, loss: 316.162506\n",
      "Train: step:  52960, time: 0.218, loss: 1835.608276\n",
      "Train: step:  52970, time: 0.217, loss: 872.433289\n",
      "Train: step:  52980, time: 0.219, loss: 1720.157593\n",
      "Train: step:  52990, time: 0.184, loss: 2121.049805\n",
      "Train: step:  53000, time: 0.229, loss: 2992.547607\n",
      "Train: step:  53010, time: 0.214, loss: 1365.811646\n",
      "Train: step:  53020, time: 0.228, loss: 1931.729126\n",
      "Train: step:  53030, time: 0.187, loss: 1127.990723\n",
      "Train: step:  53040, time: 0.214, loss: 2297.682861\n",
      "Train: step:  53050, time: 0.211, loss: 2765.407715\n",
      "Train: step:  53060, time: 0.192, loss: 3303.240234\n",
      "Train: step:  53070, time: 0.231, loss: 934.603333\n",
      "Train: step:  53080, time: 0.186, loss: 1083.695923\n",
      "Train: step:  53090, time: 0.195, loss: 1157.479858\n",
      "Train: step:  53100, time: 0.229, loss: 1126.423096\n",
      "Train: step:  53110, time: 0.224, loss: 1572.748779\n",
      "Train: step:  53120, time: 0.206, loss: 657.371277\n",
      "Train: step:  53130, time: 0.219, loss: 3774.837158\n",
      "Train: step:  53140, time: 0.208, loss: 994.093567\n",
      "Train: step:  53150, time: 0.214, loss: 2270.265381\n",
      "Train: step:  53160, time: 0.183, loss: 1102.190674\n",
      "Train: step:  53170, time: 0.226, loss: 2033.651245\n",
      "Train: step:  53180, time: 0.213, loss: 2013.755981\n",
      "Train: step:  53190, time: 0.186, loss: 890.519348\n",
      "Train: step:  53200, time: 0.217, loss: 1100.860107\n",
      "Train: step:  53210, time: 0.229, loss: 2009.076904\n",
      "Train: step:  53220, time: 0.184, loss: 1480.468628\n",
      "Train: step:  53230, time: 0.190, loss: 3433.245605\n",
      "Train: step:  53240, time: 0.215, loss: 2793.902344\n",
      "Train: step:  53250, time: 0.227, loss: 888.789551\n",
      "Train: step:  53260, time: 0.194, loss: 2197.895996\n",
      "Train: step:  53270, time: 0.189, loss: 1536.409180\n",
      "Train: step:  53280, time: 0.194, loss: 678.873169\n",
      "Train: step:  53290, time: 0.199, loss: 3014.555420\n",
      "Train: step:  53300, time: 0.192, loss: 2089.521729\n",
      "Train: step:  53310, time: 0.209, loss: 1672.801636\n",
      "Train: step:  53320, time: 0.193, loss: 1036.915771\n",
      "Train: step:  53330, time: 0.228, loss: 1757.640381\n",
      "Train: step:  53340, time: 0.190, loss: 976.076294\n",
      "Train: step:  53350, time: 0.264, loss: 1040.590698\n",
      "Train: step:  53360, time: 0.227, loss: 1834.088135\n",
      "Train: step:  53370, time: 0.223, loss: 2528.859619\n",
      "Train: step:  53380, time: 0.235, loss: 3282.183594\n",
      "Train: step:  53390, time: 0.212, loss: 1744.404297\n",
      "Train: step:  53400, time: 0.224, loss: 2458.859619\n",
      "Train: step:  53410, time: 0.217, loss: 3397.705811\n",
      "Train: step:  53420, time: 0.216, loss: 714.484070\n",
      "Train: step:  53430, time: 0.243, loss: 2415.269287\n",
      "Train: step:  53440, time: 0.220, loss: 1461.405151\n",
      "Train: step:  53450, time: 0.210, loss: 532.045837\n",
      "Train: step:  53460, time: 0.222, loss: 1939.911621\n",
      "Train: step:  53470, time: 0.208, loss: 2320.634766\n",
      "Train: step:  53480, time: 0.191, loss: 1374.021484\n",
      "Train: step:  53490, time: 0.199, loss: 2344.643311\n",
      "Train: step:  53500, time: 0.217, loss: 1226.397827\n",
      "Train: step:  53510, time: 0.215, loss: 299.431976\n",
      "Train: step:  53520, time: 0.189, loss: 1657.645386\n",
      "Train: step:  53530, time: 0.197, loss: 2055.760498\n",
      "Train: step:  53540, time: 0.216, loss: 1302.376465\n",
      "Train: step:  53550, time: 0.197, loss: 1052.145630\n",
      "Train: step:  53560, time: 0.193, loss: 604.596313\n",
      "Train: step:  53570, time: 0.220, loss: 1979.170288\n",
      "Train: step:  53580, time: 0.186, loss: 444.349915\n",
      "Train: step:  53590, time: 0.188, loss: 1180.697388\n",
      "Train: step:  53600, time: 0.189, loss: 574.573547\n",
      "Train: step:  53610, time: 0.199, loss: 1151.519043\n",
      "Train: step:  53620, time: 0.182, loss: 1434.060913\n",
      "Train: step:  53630, time: 0.180, loss: 952.981689\n",
      "Train: step:  53640, time: 0.198, loss: 1692.495239\n",
      "Train: step:  53650, time: 0.232, loss: 2120.532227\n",
      "Train: step:  53660, time: 0.221, loss: 2840.855225\n",
      "Train: step:  53670, time: 0.193, loss: 1493.478271\n",
      "Train: step:  53680, time: 0.190, loss: 2337.870117\n",
      "Train: step:  53690, time: 0.229, loss: 1952.867798\n",
      "Train: step:  53700, time: 0.186, loss: 1475.010132\n",
      "Train: step:  53710, time: 0.215, loss: 1989.969727\n",
      "Train: step:  53720, time: 0.187, loss: 1989.399292\n",
      "Train: step:  53730, time: 0.188, loss: 2401.048340\n",
      "Train: step:  53740, time: 0.200, loss: 315.058990\n",
      "Train: step:  53750, time: 0.228, loss: 465.254730\n",
      "Train: step:  53760, time: 0.180, loss: 2489.339844\n",
      "Train: step:  53770, time: 0.218, loss: 640.416565\n",
      "Train: step:  53780, time: 0.187, loss: 2504.990234\n",
      "Train: step:  53790, time: 0.191, loss: 2015.481934\n",
      "Train: step:  53800, time: 0.194, loss: 1791.788818\n",
      "Train: step:  53810, time: 0.190, loss: 791.936523\n",
      "Train: step:  53820, time: 0.184, loss: 2272.107422\n",
      "Train: step:  53830, time: 0.197, loss: 1823.261963\n",
      "Train: step:  53840, time: 0.228, loss: 827.511475\n",
      "Train: step:  53850, time: 0.216, loss: 2026.881592\n",
      "Train: step:  53860, time: 0.191, loss: 1566.786621\n",
      "Train: step:  53870, time: 0.198, loss: 392.832001\n",
      "Train: step:  53880, time: 0.211, loss: 726.374329\n",
      "Train: step:  53890, time: 0.216, loss: 631.920532\n",
      "Train: step:  53900, time: 0.224, loss: 1154.104248\n",
      "Train: step:  53910, time: 0.251, loss: 451.200500\n",
      "Train: step:  53920, time: 0.211, loss: 2175.975586\n",
      "Train: step:  53930, time: 0.237, loss: 3113.939697\n",
      "Train: step:  53940, time: 0.216, loss: 1679.436401\n",
      "Train: step:  53950, time: 0.217, loss: 1910.373047\n",
      "Train: step:  53960, time: 0.193, loss: 690.944824\n",
      "Train: step:  53970, time: 0.220, loss: 2641.523438\n",
      "Train: step:  53980, time: 0.189, loss: 2268.346436\n",
      "Train: step:  53990, time: 0.227, loss: 2659.228516\n",
      "Train: step:  54000, time: 0.189, loss: 895.174866\n",
      "Train: step:  54010, time: 0.227, loss: 2001.245117\n",
      "Train: step:  54020, time: 0.197, loss: 2428.702148\n",
      "Train: step:  54030, time: 0.193, loss: 654.383484\n",
      "Train: step:  54040, time: 0.218, loss: 2724.932861\n",
      "Train: step:  54050, time: 0.230, loss: 1593.419434\n",
      "Train: step:  54060, time: 0.197, loss: 1039.539429\n",
      "Train: step:  54070, time: 0.191, loss: 1944.785034\n",
      "Train: step:  54080, time: 0.196, loss: 2129.981689\n",
      "Train: step:  54090, time: 0.224, loss: 1396.994385\n",
      "Train: step:  54100, time: 0.195, loss: 2913.565674\n",
      "Train: step:  54110, time: 0.192, loss: 2719.529053\n",
      "Train: step:  54120, time: 0.246, loss: 1151.595093\n",
      "Train: step:  54130, time: 0.230, loss: 1376.746704\n",
      "Train: step:  54140, time: 0.211, loss: 2112.367920\n",
      "Train: step:  54150, time: 0.197, loss: 2199.350586\n",
      "Train: step:  54160, time: 0.191, loss: 1294.999756\n",
      "Train: step:  54170, time: 0.180, loss: 1630.448975\n",
      "Train: step:  54180, time: 0.192, loss: 1914.184448\n",
      "Train: step:  54190, time: 0.191, loss: 1560.198730\n",
      "Train: step:  54200, time: 0.225, loss: 1636.087158\n",
      "Train: step:  54210, time: 0.218, loss: 1052.279175\n",
      "Train: step:  54220, time: 0.200, loss: 4130.066406\n",
      "Train: step:  54230, time: 0.178, loss: 1929.685425\n",
      "Train: step:  54240, time: 0.201, loss: 1182.276123\n",
      "Train: step:  54250, time: 0.262, loss: 2162.126465\n",
      "Train: step:  54260, time: 0.202, loss: 2033.457275\n",
      "Train: step:  54270, time: 0.227, loss: 1407.661865\n",
      "Train: step:  54280, time: 0.195, loss: 541.526306\n",
      "Train: step:  54290, time: 0.188, loss: 2122.494873\n",
      "Train: step:  54300, time: 0.204, loss: 1314.463867\n",
      "Train: step:  54310, time: 0.189, loss: 321.948303\n",
      "Train: step:  54320, time: 0.212, loss: 366.342590\n",
      "Train: step:  54330, time: 0.192, loss: 1236.602173\n",
      "Train: step:  54340, time: 0.218, loss: 1001.719788\n",
      "Train: step:  54350, time: 0.186, loss: 2968.803467\n",
      "Train: step:  54360, time: 0.219, loss: 1067.605591\n",
      "Train: step:  54370, time: 0.213, loss: 2464.761719\n",
      "Train: step:  54380, time: 0.217, loss: 3311.180908\n",
      "Train: step:  54390, time: 0.189, loss: 1815.685669\n",
      "Train: step:  54400, time: 0.186, loss: 1876.469360\n",
      "Train: step:  54410, time: 0.193, loss: 1943.801514\n",
      "Train: step:  54420, time: 0.205, loss: 561.289185\n",
      "Train: step:  54430, time: 0.219, loss: 983.167847\n",
      "Train: step:  54440, time: 0.197, loss: 1776.683105\n",
      "Train: step:  54450, time: 0.219, loss: 1572.359009\n",
      "Train: step:  54460, time: 0.193, loss: 3183.932373\n",
      "Train: step:  54470, time: 0.190, loss: 3362.785645\n",
      "Train: step:  54480, time: 0.230, loss: 2246.853271\n",
      "Train: step:  54490, time: 0.189, loss: 800.917236\n",
      "Train: step:  54500, time: 0.226, loss: 3353.395752\n",
      "Train: step:  54510, time: 0.217, loss: 755.537537\n",
      "Train: step:  54520, time: 0.231, loss: 1704.687988\n",
      "Train: step:  54530, time: 0.188, loss: 1292.831787\n",
      "Train: step:  54540, time: 0.228, loss: 2165.113037\n",
      "Train: step:  54550, time: 0.188, loss: 1146.779419\n",
      "Train: step:  54560, time: 0.218, loss: 1083.938599\n",
      "Train: step:  54570, time: 0.218, loss: 2860.856201\n",
      "Train: step:  54580, time: 0.191, loss: 3260.202148\n",
      "Train: step:  54590, time: 0.189, loss: 3200.488770\n",
      "Train: step:  54600, time: 0.218, loss: 418.696625\n",
      "Train: step:  54610, time: 0.198, loss: 2293.051514\n",
      "Train: step:  54620, time: 0.239, loss: 2600.143799\n",
      "Train: step:  54630, time: 0.191, loss: 2500.389893\n",
      "Train: step:  54640, time: 0.190, loss: 1867.016235\n",
      "Train: step:  54650, time: 0.235, loss: 1727.116333\n",
      "Train: step:  54660, time: 0.217, loss: 877.044861\n",
      "Train: step:  54670, time: 0.216, loss: 2011.197510\n",
      "Train: step:  54680, time: 0.218, loss: 2516.234619\n",
      "Train: step:  54690, time: 0.189, loss: 1993.223755\n",
      "Train: step:  54700, time: 0.234, loss: 3484.625244\n",
      "Train: step:  54710, time: 0.214, loss: 1471.253784\n",
      "Train: step:  54720, time: 0.208, loss: 1085.833740\n",
      "Train: step:  54730, time: 0.225, loss: 2399.024902\n",
      "Train: step:  54740, time: 0.210, loss: 1383.586182\n",
      "Train: step:  54750, time: 0.199, loss: 1207.783569\n",
      "Train: step:  54760, time: 0.228, loss: 2171.298828\n",
      "Train: step:  54770, time: 0.201, loss: 2120.913330\n",
      "Train: step:  54780, time: 0.227, loss: 1300.327515\n",
      "Train: step:  54790, time: 0.222, loss: 2658.075195\n",
      "Train: step:  54800, time: 0.217, loss: 1943.645752\n",
      "Train: step:  54810, time: 0.219, loss: 2054.988770\n",
      "Train: step:  54820, time: 0.224, loss: 796.793213\n",
      "Train: step:  54830, time: 0.215, loss: 2356.970459\n",
      "Train: step:  54840, time: 0.188, loss: 1526.902832\n",
      "Train: step:  54850, time: 0.193, loss: 1098.490845\n",
      "Train: step:  54860, time: 0.218, loss: 934.477112\n",
      "Train: step:  54870, time: 0.185, loss: 2667.272705\n",
      "Train: step:  54880, time: 0.230, loss: 648.577209\n",
      "Train: step:  54890, time: 0.228, loss: 1168.357422\n",
      "Train: step:  54900, time: 0.192, loss: 1824.479614\n",
      "Train: step:  54910, time: 0.229, loss: 1907.738037\n",
      "Train: step:  54920, time: 0.202, loss: 1621.258057\n",
      "Train: step:  54930, time: 0.187, loss: 492.329681\n",
      "Train: step:  54940, time: 0.218, loss: 2750.168213\n",
      "Train: step:  54950, time: 0.215, loss: 1104.453369\n",
      "Train: step:  54960, time: 0.216, loss: 2195.785645\n",
      "Train: step:  54970, time: 0.186, loss: 2901.320801\n",
      "Train: step:  54980, time: 0.192, loss: 975.842896\n",
      "Train: step:  54990, time: 0.230, loss: 2725.527588\n",
      "Train: step:  55000, time: 0.251, loss: 2134.373291\n",
      "Train: step:  55010, time: 0.219, loss: 1339.911499\n",
      "Train: step:  55020, time: 0.186, loss: 1223.728027\n",
      "Train: step:  55030, time: 0.221, loss: 1772.416504\n",
      "Train: step:  55040, time: 0.206, loss: 2314.881592\n",
      "Train: step:  55050, time: 0.198, loss: 898.713684\n",
      "Train: step:  55060, time: 0.189, loss: 1323.237793\n",
      "Train: step:  55070, time: 0.217, loss: 1580.228882\n",
      "Train: step:  55080, time: 0.195, loss: 1080.108643\n",
      "Train: step:  55090, time: 0.191, loss: 2381.333984\n",
      "Train: step:  55100, time: 0.193, loss: 1051.660889\n",
      "Train: step:  55110, time: 0.225, loss: 2028.726562\n",
      "Train: step:  55120, time: 0.226, loss: 889.253906\n",
      "Train: step:  55130, time: 0.185, loss: 1081.251465\n",
      "Train: step:  55140, time: 0.227, loss: 526.864197\n",
      "Train: step:  55150, time: 0.192, loss: 436.810730\n",
      "Train: step:  55160, time: 0.195, loss: 1515.567383\n",
      "Train: step:  55170, time: 0.239, loss: 1824.550659\n",
      "Train: step:  55180, time: 0.187, loss: 1532.953491\n",
      "Train: step:  55190, time: 0.188, loss: 3679.851318\n",
      "Train: step:  55200, time: 0.221, loss: 2263.571777\n",
      "Train: step:  55210, time: 0.193, loss: 3316.670654\n",
      "Train: step:  55220, time: 0.218, loss: 2435.840088\n",
      "Train: step:  55230, time: 0.190, loss: 428.024689\n",
      "Train: step:  55240, time: 0.221, loss: 2641.346924\n",
      "Train: step:  55250, time: 0.189, loss: 1471.077393\n",
      "Train: step:  55260, time: 0.220, loss: 2933.218750\n",
      "Train: step:  55270, time: 0.196, loss: 2357.767822\n",
      "Train: step:  55280, time: 0.219, loss: 1235.443237\n",
      "Train: step:  55290, time: 0.211, loss: 1213.270020\n",
      "Train: step:  55300, time: 0.200, loss: 1670.788086\n",
      "Train: step:  55310, time: 0.216, loss: 3559.517090\n",
      "Train: step:  55320, time: 0.189, loss: 1129.765137\n",
      "Train: step:  55330, time: 0.193, loss: 1950.603271\n",
      "Train: step:  55340, time: 0.221, loss: 1911.886963\n",
      "Train: step:  55350, time: 0.186, loss: 1045.829956\n",
      "Train: step:  55360, time: 0.217, loss: 909.250916\n",
      "Train: step:  55370, time: 0.218, loss: 881.964722\n",
      "Train: step:  55380, time: 0.194, loss: 1203.246216\n",
      "Train: step:  55390, time: 0.218, loss: 1552.640747\n",
      "Train: step:  55400, time: 0.227, loss: 2403.198730\n",
      "Train: step:  55410, time: 0.243, loss: 1443.543579\n",
      "Train: step:  55420, time: 0.245, loss: 1869.447388\n",
      "Train: step:  55430, time: 0.190, loss: 2207.373291\n",
      "Train: step:  55440, time: 0.219, loss: 3646.462158\n",
      "Train: step:  55450, time: 0.191, loss: 2970.048340\n",
      "Train: step:  55460, time: 0.216, loss: 783.437805\n",
      "Train: step:  55470, time: 0.185, loss: 873.177002\n",
      "Train: step:  55480, time: 0.196, loss: 1312.463135\n",
      "Train: step:  55490, time: 0.190, loss: 1771.015991\n",
      "Train: step:  55500, time: 0.209, loss: 2593.265137\n",
      "Train: step:  55510, time: 0.218, loss: 1741.881104\n",
      "Train: step:  55520, time: 0.221, loss: 2128.122559\n",
      "Train: step:  55530, time: 0.229, loss: 1121.328003\n",
      "Train: step:  55540, time: 0.217, loss: 3884.453125\n",
      "Train: step:  55550, time: 0.184, loss: 2174.471924\n",
      "Train: step:  55560, time: 0.207, loss: 1785.943726\n",
      "Train: step:  55570, time: 0.233, loss: 1039.047607\n",
      "Train: step:  55580, time: 0.217, loss: 2449.858398\n",
      "Train: step:  55590, time: 0.216, loss: 1734.882568\n",
      "Train: step:  55600, time: 0.230, loss: 2115.178467\n",
      "Train: step:  55610, time: 0.236, loss: 1833.033936\n",
      "Train: step:  55620, time: 0.218, loss: 2586.836182\n",
      "Train: step:  55630, time: 0.199, loss: 2061.135498\n",
      "Train: step:  55640, time: 0.189, loss: 658.421814\n",
      "Train: step:  55650, time: 0.198, loss: 1853.844116\n",
      "Train: step:  55660, time: 0.221, loss: 2999.217041\n",
      "Train: step:  55670, time: 0.187, loss: 644.915100\n",
      "Train: step:  55680, time: 0.184, loss: 1695.544434\n",
      "Train: step:  55690, time: 0.229, loss: 834.111877\n",
      "Train: step:  55700, time: 0.229, loss: 693.370422\n",
      "Train: step:  55710, time: 0.198, loss: 2228.858154\n",
      "Train: step:  55720, time: 0.229, loss: 3637.126465\n",
      "Train: step:  55730, time: 0.217, loss: 2353.923096\n",
      "Train: step:  55740, time: 0.216, loss: 1612.962402\n",
      "Train: step:  55750, time: 0.230, loss: 3078.153809\n",
      "Train: step:  55760, time: 0.190, loss: 664.063904\n",
      "Train: step:  55770, time: 0.188, loss: 2089.505615\n",
      "Train: step:  55780, time: 0.197, loss: 3174.947510\n",
      "Train: step:  55790, time: 0.196, loss: 2640.648926\n",
      "Train: step:  55800, time: 0.204, loss: 1274.963013\n",
      "Train: step:  55810, time: 0.186, loss: 1351.929565\n",
      "Train: step:  55820, time: 0.183, loss: 2508.053711\n",
      "Train: step:  55830, time: 0.189, loss: 3341.295166\n",
      "Train: step:  55840, time: 0.191, loss: 3428.446533\n",
      "Train: step:  55850, time: 0.242, loss: 1275.445557\n",
      "Train: step:  55860, time: 0.221, loss: 1365.113647\n",
      "Train: step:  55870, time: 0.215, loss: 2853.248291\n",
      "Train: step:  55880, time: 0.236, loss: 1709.038696\n",
      "Train: step:  55890, time: 0.224, loss: 3249.175537\n",
      "Train: step:  55900, time: 0.217, loss: 2458.427734\n",
      "Train: step:  55910, time: 0.260, loss: 1613.693848\n",
      "Train: step:  55920, time: 0.232, loss: 821.192078\n",
      "Train: step:  55930, time: 0.219, loss: 1614.760620\n",
      "Train: step:  55940, time: 0.234, loss: 1033.288574\n",
      "Train: step:  55950, time: 0.223, loss: 1524.763062\n",
      "Train: step:  55960, time: 0.217, loss: 2942.643555\n",
      "Train: step:  55970, time: 0.226, loss: 1379.422607\n",
      "Train: step:  55980, time: 0.227, loss: 2038.908936\n",
      "Train: step:  55990, time: 0.231, loss: 1328.394531\n",
      "Train: step:  56000, time: 0.193, loss: 3110.165527\n",
      "Train: step:  56010, time: 0.205, loss: 1560.953491\n",
      "Train: step:  56020, time: 0.218, loss: 2446.539795\n",
      "Train: step:  56030, time: 0.196, loss: 717.694885\n",
      "Train: step:  56040, time: 0.193, loss: 2380.120605\n",
      "Train: step:  56050, time: 0.231, loss: 567.710266\n",
      "Train: step:  56060, time: 0.220, loss: 1118.661621\n",
      "Train: step:  56070, time: 0.191, loss: 2505.171631\n",
      "Train: step:  56080, time: 0.224, loss: 3254.500244\n",
      "Train: step:  56090, time: 0.218, loss: 425.450256\n",
      "Train: step:  56100, time: 0.227, loss: 3184.882812\n",
      "Train: step:  56110, time: 0.180, loss: 914.341431\n",
      "Train: step:  56120, time: 0.216, loss: 590.607544\n",
      "Train: step:  56130, time: 0.228, loss: 2264.501953\n",
      "Train: step:  56140, time: 0.229, loss: 1327.029053\n",
      "Train: step:  56150, time: 0.231, loss: 2147.375488\n",
      "Train: step:  56160, time: 0.220, loss: 2716.535400\n",
      "Train: step:  56170, time: 0.189, loss: 2419.381836\n",
      "Train: step:  56180, time: 0.193, loss: 1260.294800\n",
      "Train: step:  56190, time: 0.187, loss: 2429.767822\n",
      "Train: step:  56200, time: 0.219, loss: 1650.492065\n",
      "Train: step:  56210, time: 0.191, loss: 2895.912109\n",
      "Train: step:  56220, time: 0.218, loss: 462.911896\n",
      "Train: step:  56230, time: 0.218, loss: 4949.545410\n",
      "Train: step:  56240, time: 0.230, loss: 1007.496277\n",
      "Train: step:  56250, time: 0.230, loss: 723.885803\n",
      "Train: step:  56260, time: 0.218, loss: 839.256165\n",
      "Train: step:  56270, time: 0.192, loss: 1452.735474\n",
      "Train: step:  56280, time: 0.214, loss: 4556.748047\n",
      "Train: step:  56290, time: 0.217, loss: 1327.022095\n",
      "Train: step:  56300, time: 0.188, loss: 1720.860962\n",
      "Train: step:  56310, time: 0.217, loss: 1822.096436\n",
      "Train: step:  56320, time: 0.224, loss: 2197.042480\n",
      "Train: step:  56330, time: 0.217, loss: 521.236694\n",
      "Train: step:  56340, time: 0.222, loss: 1773.136841\n",
      "Train: step:  56350, time: 0.216, loss: 1386.508179\n",
      "Train: step:  56360, time: 0.229, loss: 1368.114868\n",
      "Train: step:  56370, time: 0.234, loss: 1324.672729\n",
      "Train: step:  56380, time: 0.188, loss: 1764.153687\n",
      "Train: step:  56390, time: 0.189, loss: 1672.600830\n",
      "Train: step:  56400, time: 0.194, loss: 2860.475586\n",
      "Train: step:  56410, time: 0.191, loss: 476.345673\n",
      "Train: step:  56420, time: 0.208, loss: 647.594360\n",
      "Train: step:  56430, time: 0.196, loss: 2126.730225\n",
      "Train: step:  56440, time: 0.231, loss: 1558.913818\n",
      "Train: step:  56450, time: 0.216, loss: 2545.337646\n",
      "Train: step:  56460, time: 0.228, loss: 1093.277466\n",
      "Train: step:  56470, time: 0.240, loss: 2246.494385\n",
      "Train: step:  56480, time: 0.188, loss: 2403.478516\n",
      "Train: step:  56490, time: 0.193, loss: 2935.222168\n",
      "Train: step:  56500, time: 0.249, loss: 1612.816040\n",
      "Train: step:  56510, time: 0.191, loss: 2751.766113\n",
      "Train: step:  56520, time: 0.219, loss: 304.090393\n",
      "Train: step:  56530, time: 0.235, loss: 566.860046\n",
      "Train: step:  56540, time: 0.231, loss: 2424.336914\n",
      "Train: step:  56550, time: 0.189, loss: 2198.786865\n",
      "Train: step:  56560, time: 0.188, loss: 2289.782471\n",
      "Train: step:  56570, time: 0.188, loss: 2852.828857\n",
      "Train: step:  56580, time: 0.216, loss: 716.317871\n",
      "Train: step:  56590, time: 0.230, loss: 1827.465454\n",
      "Train: step:  56600, time: 0.188, loss: 561.140503\n",
      "Train: step:  56610, time: 0.214, loss: 2410.585938\n",
      "Train: step:  56620, time: 0.228, loss: 1484.915894\n",
      "Train: step:  56630, time: 0.190, loss: 1286.880249\n",
      "Train: step:  56640, time: 0.189, loss: 1598.550171\n",
      "Train: step:  56650, time: 0.210, loss: 4563.006836\n",
      "Train: step:  56660, time: 0.200, loss: 2977.899902\n",
      "Train: step:  56670, time: 0.195, loss: 986.504089\n",
      "Train: step:  56680, time: 0.204, loss: 1978.621094\n",
      "Train: step:  56690, time: 0.196, loss: 867.888550\n",
      "Train: step:  56700, time: 0.196, loss: 1881.589722\n",
      "Train: step:  56710, time: 0.233, loss: 709.359436\n",
      "Train: step:  56720, time: 0.188, loss: 366.997284\n",
      "Train: step:  56730, time: 0.208, loss: 1028.617432\n",
      "Train: step:  56740, time: 0.188, loss: 1909.601685\n",
      "Train: step:  56750, time: 0.216, loss: 3275.149414\n",
      "Train: step:  56760, time: 0.217, loss: 1868.428833\n",
      "Train: step:  56770, time: 0.225, loss: 392.422852\n",
      "Train: step:  56780, time: 0.214, loss: 467.372406\n",
      "Train: step:  56790, time: 0.228, loss: 1881.170044\n",
      "Train: step:  56800, time: 0.193, loss: 1976.298340\n",
      "Train: step:  56810, time: 0.197, loss: 2689.328369\n",
      "Train: step:  56820, time: 0.201, loss: 1929.380981\n",
      "Train: step:  56830, time: 0.185, loss: 1707.642578\n",
      "Train: step:  56840, time: 0.185, loss: 2419.350586\n",
      "Train: step:  56850, time: 0.193, loss: 2559.409180\n",
      "Train: step:  56860, time: 0.228, loss: 854.033081\n",
      "Train: step:  56870, time: 0.226, loss: 4249.772949\n",
      "Train: step:  56880, time: 0.201, loss: 4257.724121\n",
      "Train: step:  56890, time: 0.194, loss: 2529.563721\n",
      "Train: step:  56900, time: 0.188, loss: 618.480103\n",
      "Train: step:  56910, time: 0.193, loss: 1647.078491\n",
      "Train: step:  56920, time: 0.218, loss: 942.117493\n",
      "Train: step:  56930, time: 0.211, loss: 1311.500488\n",
      "Train: step:  56940, time: 0.193, loss: 1394.830200\n",
      "Train: step:  56950, time: 0.217, loss: 3775.580566\n",
      "Train: step:  56960, time: 0.253, loss: 3184.574463\n",
      "Train: step:  56970, time: 0.188, loss: 1770.557129\n",
      "Train: step:  56980, time: 0.233, loss: 500.136566\n",
      "Train: step:  56990, time: 0.219, loss: 1461.434326\n",
      "Train: step:  57000, time: 0.227, loss: 2171.107178\n",
      "Train: step:  57010, time: 0.180, loss: 2288.889160\n",
      "Train: step:  57020, time: 0.244, loss: 1517.514893\n",
      "Train: step:  57030, time: 0.188, loss: 2494.369873\n",
      "Train: step:  57040, time: 0.233, loss: 2165.980469\n",
      "Train: step:  57050, time: 0.201, loss: 1238.579956\n",
      "Train: step:  57060, time: 0.227, loss: 2581.203369\n",
      "Train: step:  57070, time: 0.191, loss: 1842.748779\n",
      "Train: step:  57080, time: 0.194, loss: 814.742126\n",
      "Train: step:  57090, time: 0.209, loss: 732.741638\n",
      "Train: step:  57100, time: 0.194, loss: 815.780029\n",
      "Train: step:  57110, time: 0.218, loss: 1102.406494\n",
      "Train: step:  57120, time: 0.189, loss: 1604.703003\n",
      "Train: step:  57130, time: 0.220, loss: 2070.321289\n",
      "Train: step:  57140, time: 0.197, loss: 3005.655518\n",
      "Train: step:  57150, time: 0.192, loss: 920.613708\n",
      "Train: step:  57160, time: 0.198, loss: 2983.976074\n",
      "Train: step:  57170, time: 0.217, loss: 2566.854736\n",
      "Train: step:  57180, time: 0.193, loss: 1347.269775\n",
      "Train: step:  57190, time: 0.213, loss: 3669.040771\n",
      "Train: step:  57200, time: 0.217, loss: 1080.865479\n",
      "Train: step:  57210, time: 0.229, loss: 997.343079\n",
      "Train: step:  57220, time: 0.198, loss: 1931.570557\n",
      "Train: step:  57230, time: 0.220, loss: 1661.216553\n",
      "Train: step:  57240, time: 0.219, loss: 2367.448486\n",
      "Train: step:  57250, time: 0.214, loss: 314.052582\n",
      "Train: step:  57260, time: 0.217, loss: 1074.657593\n",
      "Train: step:  57270, time: 0.197, loss: 3089.162842\n",
      "Train: step:  57280, time: 0.188, loss: 2209.427246\n",
      "Train: step:  57290, time: 0.216, loss: 1692.793457\n",
      "Train: step:  57300, time: 0.231, loss: 996.596863\n",
      "Train: step:  57310, time: 0.217, loss: 1435.268188\n",
      "Train: step:  57320, time: 0.188, loss: 754.046448\n",
      "Train: step:  57330, time: 0.213, loss: 1889.285889\n",
      "Train: step:  57340, time: 0.232, loss: 1437.545166\n",
      "Train: step:  57350, time: 0.189, loss: 2054.933350\n",
      "Train: step:  57360, time: 0.195, loss: 1932.297119\n",
      "Train: step:  57370, time: 0.190, loss: 2313.703369\n",
      "Train: step:  57380, time: 0.187, loss: 2316.236084\n",
      "Train: step:  57390, time: 0.185, loss: 1963.622070\n",
      "Train: step:  57400, time: 0.231, loss: 1541.223145\n",
      "Train: step:  57410, time: 0.239, loss: 803.073364\n",
      "Train: step:  57420, time: 0.193, loss: 675.499146\n",
      "Train: step:  57430, time: 0.227, loss: 3595.740723\n",
      "Train: step:  57440, time: 0.210, loss: 2171.823486\n",
      "Train: step:  57450, time: 0.219, loss: 1343.005249\n",
      "Train: step:  57460, time: 0.186, loss: 1770.212646\n",
      "Train: step:  57470, time: 0.227, loss: 1321.874268\n",
      "Train: step:  57480, time: 0.215, loss: 1759.304810\n",
      "Train: step:  57490, time: 0.211, loss: 1866.390747\n",
      "Train: step:  57500, time: 0.225, loss: 2310.146973\n",
      "Train: step:  57510, time: 0.207, loss: 2154.517578\n",
      "Train: step:  57520, time: 0.219, loss: 2466.993896\n",
      "Train: step:  57530, time: 0.219, loss: 1796.879761\n",
      "Train: step:  57540, time: 0.216, loss: 1279.417603\n",
      "Train: step:  57550, time: 0.195, loss: 2571.760986\n",
      "Train: step:  57560, time: 0.187, loss: 1145.740479\n",
      "Train: step:  57570, time: 0.224, loss: 3597.641846\n",
      "Train: step:  57580, time: 0.209, loss: 2351.337646\n",
      "Train: step:  57590, time: 0.189, loss: 2434.540283\n",
      "Train: step:  57600, time: 0.231, loss: 2366.389160\n",
      "Train: step:  57610, time: 0.188, loss: 2640.952881\n",
      "Train: step:  57620, time: 0.231, loss: 2626.314697\n",
      "Train: step:  57630, time: 0.193, loss: 2234.635986\n",
      "Train: step:  57640, time: 0.251, loss: 2655.623047\n",
      "Train: step:  57650, time: 0.232, loss: 517.825439\n",
      "Train: step:  57660, time: 0.216, loss: 2752.170898\n",
      "Train: step:  57670, time: 0.208, loss: 2274.357910\n",
      "Train: step:  57680, time: 0.216, loss: 1723.628906\n",
      "Train: step:  57690, time: 0.195, loss: 1415.471069\n",
      "Train: step:  57700, time: 0.233, loss: 2361.789062\n",
      "Train: step:  57710, time: 0.238, loss: 2727.457031\n",
      "Train: step:  57720, time: 0.216, loss: 2911.121826\n",
      "Train: step:  57730, time: 0.250, loss: 732.023804\n",
      "Train: step:  57740, time: 0.216, loss: 690.714111\n",
      "Train: step:  57750, time: 0.235, loss: 5498.032227\n",
      "Train: step:  57760, time: 0.220, loss: 1767.159546\n",
      "Train: step:  57770, time: 0.218, loss: 730.797607\n",
      "Train: step:  57780, time: 0.197, loss: 705.295410\n",
      "Train: step:  57790, time: 0.221, loss: 1685.658691\n",
      "Train: step:  57800, time: 0.217, loss: 4741.036133\n",
      "Train: step:  57810, time: 0.193, loss: 1865.306274\n",
      "Train: step:  57820, time: 0.232, loss: 1767.780151\n",
      "Train: step:  57830, time: 0.191, loss: 775.999634\n",
      "Train: step:  57840, time: 0.224, loss: 1030.090698\n",
      "Train: step:  57850, time: 0.192, loss: 1177.924927\n",
      "Train: step:  57860, time: 0.188, loss: 5885.988281\n",
      "Train: step:  57870, time: 0.192, loss: 2934.029541\n",
      "Train: step:  57880, time: 0.228, loss: 1974.931641\n",
      "Train: step:  57890, time: 0.258, loss: 2076.561523\n",
      "Train: step:  57900, time: 0.217, loss: 1691.331665\n",
      "Train: step:  57910, time: 0.188, loss: 1673.860229\n",
      "Train: step:  57920, time: 0.189, loss: 695.154785\n",
      "Train: step:  57930, time: 0.221, loss: 2627.477783\n",
      "Train: step:  57940, time: 0.216, loss: 298.658813\n",
      "Train: step:  57950, time: 0.185, loss: 2598.415039\n",
      "Train: step:  57960, time: 0.215, loss: 1346.472168\n",
      "Train: step:  57970, time: 0.214, loss: 948.398682\n",
      "Train: step:  57980, time: 0.184, loss: 2103.909912\n",
      "Train: step:  57990, time: 0.216, loss: 2929.241455\n",
      "Train: step:  58000, time: 0.188, loss: 2549.052734\n",
      "Train: step:  58010, time: 0.193, loss: 370.707428\n",
      "Train: step:  58020, time: 0.217, loss: 949.800049\n",
      "Train: step:  58030, time: 0.223, loss: 2008.219482\n",
      "Train: step:  58040, time: 0.235, loss: 1062.352661\n",
      "Train: step:  58050, time: 0.190, loss: 658.248596\n",
      "Train: step:  58060, time: 0.190, loss: 679.946716\n",
      "Train: step:  58070, time: 0.227, loss: 3034.043457\n",
      "Train: step:  58080, time: 0.198, loss: 2440.936279\n",
      "Train: step:  58090, time: 0.217, loss: 3572.991211\n",
      "Train: step:  58100, time: 0.232, loss: 2456.235107\n",
      "Train: step:  58110, time: 0.207, loss: 193.566513\n",
      "Train: step:  58120, time: 0.216, loss: 456.655487\n",
      "Train: step:  58130, time: 0.208, loss: 1295.532349\n",
      "Train: step:  58140, time: 0.197, loss: 975.418945\n",
      "Train: step:  58150, time: 0.248, loss: 2292.741211\n",
      "Train: step:  58160, time: 0.232, loss: 1653.955933\n",
      "Train: step:  58170, time: 0.191, loss: 1424.771240\n",
      "Train: step:  58180, time: 0.197, loss: 1489.392700\n",
      "Train: step:  58190, time: 0.222, loss: 1501.172119\n",
      "Train: step:  58200, time: 0.220, loss: 1458.358643\n",
      "Train: step:  58210, time: 0.221, loss: 317.109863\n",
      "Train: step:  58220, time: 0.193, loss: 3282.956299\n",
      "Train: step:  58230, time: 0.218, loss: 1750.041504\n",
      "Train: step:  58240, time: 0.198, loss: 920.029846\n",
      "Train: step:  58250, time: 0.189, loss: 1602.756714\n",
      "Train: step:  58260, time: 0.190, loss: 1173.701416\n",
      "Train: step:  58270, time: 0.222, loss: 2421.767090\n",
      "Train: step:  58280, time: 0.191, loss: 1108.616699\n",
      "Train: step:  58290, time: 0.229, loss: 2872.162354\n",
      "Train: step:  58300, time: 0.236, loss: 1974.878906\n",
      "Train: step:  58310, time: 0.216, loss: 1724.531860\n",
      "Train: step:  58320, time: 0.249, loss: 2296.324707\n",
      "Train: step:  58330, time: 0.231, loss: 150.637955\n",
      "Train: step:  58340, time: 0.184, loss: 1934.372681\n",
      "Train: step:  58350, time: 0.214, loss: 2035.043213\n",
      "Train: step:  58360, time: 0.217, loss: 1170.130249\n",
      "Train: step:  58370, time: 0.218, loss: 1572.589722\n",
      "Train: step:  58380, time: 0.189, loss: 3768.860107\n",
      "Train: step:  58390, time: 0.226, loss: 832.819885\n",
      "Train: step:  58400, time: 0.183, loss: 1747.712646\n",
      "Train: step:  58410, time: 0.184, loss: 1663.451904\n",
      "Train: step:  58420, time: 0.227, loss: 2530.510498\n",
      "Train: step:  58430, time: 0.183, loss: 1601.971924\n",
      "Train: step:  58440, time: 0.210, loss: 596.367859\n",
      "Train: step:  58450, time: 0.242, loss: 2225.907471\n",
      "Train: step:  58460, time: 0.187, loss: 328.295746\n",
      "Train: step:  58470, time: 0.231, loss: 1403.085083\n",
      "Train: step:  58480, time: 0.216, loss: 702.372681\n",
      "Train: step:  58490, time: 0.211, loss: 2415.035645\n",
      "Train: step:  58500, time: 0.222, loss: 3235.853760\n",
      "Train: step:  58510, time: 0.183, loss: 3478.560303\n",
      "Train: step:  58520, time: 0.190, loss: 759.279114\n",
      "Train: step:  58530, time: 0.223, loss: 3144.213867\n",
      "Train: step:  58540, time: 0.185, loss: 2385.813477\n",
      "Train: step:  58550, time: 0.211, loss: 3727.320312\n",
      "Train: step:  58560, time: 0.223, loss: 2520.815918\n",
      "Train: step:  58570, time: 0.189, loss: 609.796204\n",
      "Train: step:  58580, time: 0.201, loss: 2008.878052\n",
      "Train: step:  58590, time: 0.216, loss: 675.075256\n",
      "Train: step:  58600, time: 0.216, loss: 1600.353882\n",
      "Train: step:  58610, time: 0.196, loss: 1870.228394\n",
      "Train: step:  58620, time: 0.189, loss: 630.382080\n",
      "Train: step:  58630, time: 0.223, loss: 2249.581787\n",
      "Train: step:  58640, time: 0.190, loss: 2598.698486\n",
      "Train: step:  58650, time: 0.190, loss: 1272.769287\n",
      "Train: step:  58660, time: 0.221, loss: 2073.798584\n",
      "Train: step:  58670, time: 0.216, loss: 207.503876\n",
      "Train: step:  58680, time: 0.195, loss: 1807.483887\n",
      "Train: step:  58690, time: 0.209, loss: 281.782196\n",
      "Train: step:  58700, time: 0.205, loss: 5078.188965\n",
      "Train: step:  58710, time: 0.196, loss: 307.903931\n",
      "Train: step:  58720, time: 0.212, loss: 2110.787842\n",
      "Train: step:  58730, time: 0.189, loss: 1104.323853\n",
      "Train: step:  58740, time: 0.193, loss: 1155.144287\n",
      "Train: step:  58750, time: 0.219, loss: 2156.911865\n",
      "Train: step:  58760, time: 0.223, loss: 3805.161865\n",
      "Train: step:  58770, time: 0.253, loss: 370.768524\n",
      "Train: step:  58780, time: 0.233, loss: 2078.880615\n",
      "Train: step:  58790, time: 0.182, loss: 2850.972168\n",
      "Train: step:  58800, time: 0.220, loss: 2354.435059\n",
      "Train: step:  58810, time: 0.217, loss: 2126.762695\n",
      "Train: step:  58820, time: 0.198, loss: 2521.348145\n",
      "Train: step:  58830, time: 0.225, loss: 2039.264160\n",
      "Train: step:  58840, time: 0.228, loss: 1744.242798\n",
      "Train: step:  58850, time: 0.184, loss: 1990.146118\n",
      "Train: step:  58860, time: 0.190, loss: 1000.312561\n",
      "Train: step:  58870, time: 0.187, loss: 2340.291260\n",
      "Train: step:  58880, time: 0.215, loss: 596.082947\n",
      "Train: step:  58890, time: 0.183, loss: 2462.388916\n",
      "Train: step:  58900, time: 0.181, loss: 2381.963867\n",
      "Train: step:  58910, time: 0.191, loss: 634.235535\n",
      "Train: step:  58920, time: 0.182, loss: 2435.676514\n",
      "Train: step:  58930, time: 0.231, loss: 1956.921875\n",
      "Train: step:  58940, time: 0.218, loss: 1730.926147\n",
      "Train: step:  58950, time: 0.228, loss: 1846.618774\n",
      "Train: step:  58960, time: 0.223, loss: 758.723450\n",
      "Train: step:  58970, time: 0.229, loss: 3083.980957\n",
      "Train: step:  58980, time: 0.230, loss: 389.341248\n",
      "Train: step:  58990, time: 0.189, loss: 482.881683\n",
      "Train: step:  59000, time: 0.238, loss: 684.370361\n",
      "Train: step:  59010, time: 0.198, loss: 1374.126465\n",
      "Train: step:  59020, time: 0.187, loss: 580.959167\n",
      "Train: step:  59030, time: 0.242, loss: 1597.436646\n",
      "Train: step:  59040, time: 0.194, loss: 1140.590698\n",
      "Train: step:  59050, time: 0.212, loss: 3296.821533\n",
      "Train: step:  59060, time: 0.182, loss: 2656.867432\n",
      "Train: step:  59070, time: 0.183, loss: 2666.665771\n",
      "Train: step:  59080, time: 0.195, loss: 2161.378418\n",
      "Train: step:  59090, time: 0.218, loss: 2478.622314\n",
      "Train: step:  59100, time: 0.214, loss: 2877.647949\n",
      "Train: step:  59110, time: 0.183, loss: 2458.549316\n",
      "Train: step:  59120, time: 0.188, loss: 3655.683105\n",
      "Train: step:  59130, time: 0.208, loss: 1926.618652\n",
      "Train: step:  59140, time: 0.216, loss: 2791.105469\n",
      "Train: step:  59150, time: 0.229, loss: 1444.932617\n",
      "Train: step:  59160, time: 0.217, loss: 2979.764160\n",
      "Train: step:  59170, time: 0.214, loss: 1701.943848\n",
      "Train: step:  59180, time: 0.218, loss: 592.175842\n",
      "Train: step:  59190, time: 0.191, loss: 3036.673828\n",
      "Train: step:  59200, time: 0.227, loss: 2166.153320\n",
      "Train: step:  59210, time: 0.217, loss: 487.146179\n",
      "Train: step:  59220, time: 0.184, loss: 903.854919\n",
      "Train: step:  59230, time: 0.189, loss: 1589.723999\n",
      "Train: step:  59240, time: 0.216, loss: 1867.798706\n",
      "Train: step:  59250, time: 0.240, loss: 2073.793457\n",
      "Train: step:  59260, time: 0.242, loss: 935.427490\n",
      "Train: step:  59270, time: 0.260, loss: 3252.878662\n",
      "Train: step:  59280, time: 0.212, loss: 2710.654785\n",
      "Train: step:  59290, time: 0.236, loss: 3048.834717\n",
      "Train: step:  59300, time: 0.218, loss: 2600.217529\n",
      "Train: step:  59310, time: 0.231, loss: 360.306641\n",
      "Train: step:  59320, time: 0.209, loss: 1989.125122\n",
      "Train: step:  59330, time: 0.229, loss: 2734.505615\n",
      "Train: step:  59340, time: 0.187, loss: 1772.115723\n",
      "Train: step:  59350, time: 0.223, loss: 3626.239746\n",
      "Train: step:  59360, time: 0.188, loss: 3349.653564\n",
      "Train: step:  59370, time: 0.233, loss: 1833.928833\n",
      "Train: step:  59380, time: 0.222, loss: 1581.707520\n",
      "Train: step:  59390, time: 0.218, loss: 3162.174072\n",
      "Train: step:  59400, time: 0.217, loss: 1054.838745\n",
      "Train: step:  59410, time: 0.236, loss: 1392.013428\n",
      "Train: step:  59420, time: 0.229, loss: 776.297668\n",
      "Train: step:  59430, time: 0.215, loss: 1292.343140\n",
      "Train: step:  59440, time: 0.257, loss: 1986.802612\n",
      "Train: step:  59450, time: 0.230, loss: 964.924072\n",
      "Train: step:  59460, time: 0.226, loss: 2688.961426\n",
      "Train: step:  59470, time: 0.187, loss: 2275.458984\n",
      "Train: step:  59480, time: 0.189, loss: 848.618286\n",
      "Train: step:  59490, time: 0.190, loss: 1098.201050\n",
      "Train: step:  59500, time: 0.229, loss: 2927.591553\n",
      "Train: step:  59510, time: 0.216, loss: 2506.155762\n",
      "Train: step:  59520, time: 0.223, loss: 1103.527344\n",
      "Train: step:  59530, time: 0.233, loss: 2553.694336\n",
      "Train: step:  59540, time: 0.216, loss: 2184.775879\n",
      "Train: step:  59550, time: 0.191, loss: 2443.315430\n",
      "Train: step:  59560, time: 0.204, loss: 831.802673\n",
      "Train: step:  59570, time: 0.227, loss: 1935.080566\n",
      "Train: step:  59580, time: 0.217, loss: 665.438354\n",
      "Train: step:  59590, time: 0.188, loss: 770.390076\n",
      "Train: step:  59600, time: 0.212, loss: 1187.304565\n",
      "Train: step:  59610, time: 0.189, loss: 644.514404\n",
      "Train: step:  59620, time: 0.231, loss: 2131.490967\n",
      "Train: step:  59630, time: 0.195, loss: 2906.083496\n",
      "Train: step:  59640, time: 0.187, loss: 1969.598022\n",
      "Train: step:  59650, time: 0.217, loss: 1246.080078\n",
      "Train: step:  59660, time: 0.230, loss: 642.448792\n",
      "Train: step:  59670, time: 0.184, loss: 1367.083008\n",
      "Train: step:  59680, time: 0.194, loss: 723.869446\n",
      "Train: step:  59690, time: 0.188, loss: 1293.160400\n",
      "Train: step:  59700, time: 0.201, loss: 969.794922\n",
      "Train: step:  59710, time: 0.220, loss: 1181.647705\n",
      "Train: step:  59720, time: 0.231, loss: 2076.220703\n",
      "Train: step:  59730, time: 0.234, loss: 940.913330\n",
      "Train: step:  59740, time: 0.237, loss: 2870.566650\n",
      "Train: step:  59750, time: 0.181, loss: 1227.186890\n",
      "Train: step:  59760, time: 0.217, loss: 2191.973145\n",
      "Train: step:  59770, time: 0.217, loss: 1303.523682\n",
      "Train: step:  59780, time: 0.217, loss: 2227.524902\n",
      "Train: step:  59790, time: 0.222, loss: 914.962891\n",
      "Train: step:  59800, time: 0.186, loss: 3892.247559\n",
      "Train: step:  59810, time: 0.187, loss: 1847.710327\n",
      "Train: step:  59820, time: 0.242, loss: 1061.626831\n",
      "Train: step:  59830, time: 0.194, loss: 1079.748535\n",
      "Train: step:  59840, time: 0.221, loss: 2665.738525\n",
      "Train: step:  59850, time: 0.188, loss: 2670.600830\n",
      "Train: step:  59860, time: 0.203, loss: 2395.890869\n",
      "Train: step:  59870, time: 0.193, loss: 939.011353\n",
      "Train: step:  59880, time: 0.197, loss: 781.204956\n",
      "Train: step:  59890, time: 0.229, loss: 2558.172363\n",
      "Train: step:  59900, time: 0.227, loss: 2534.287354\n",
      "Train: step:  59910, time: 0.187, loss: 826.117554\n",
      "Train: step:  59920, time: 0.263, loss: 1618.743774\n",
      "Train: step:  59930, time: 0.310, loss: 2103.335938\n",
      "Train: step:  59940, time: 0.216, loss: 2909.264648\n",
      "Train: step:  59950, time: 0.189, loss: 605.472351\n",
      "Train: step:  59960, time: 0.218, loss: 1509.984253\n",
      "Train: step:  59970, time: 0.218, loss: 1514.497559\n",
      "Train: step:  59980, time: 0.249, loss: 877.492798\n",
      "Train: step:  59990, time: 0.210, loss: 2473.560547\n",
      "Train: step:  60000, time: 0.197, loss: 2258.944092\n",
      "Train: step:  60010, time: 0.228, loss: 2153.447754\n",
      "Train: step:  60020, time: 0.227, loss: 2580.726562\n",
      "Train: step:  60030, time: 0.227, loss: 1153.241211\n",
      "Train: step:  60040, time: 0.191, loss: 270.944977\n",
      "Train: step:  60050, time: 0.182, loss: 891.668579\n",
      "Train: step:  60060, time: 0.193, loss: 1268.375000\n",
      "Train: step:  60070, time: 0.219, loss: 2752.189453\n",
      "Train: step:  60080, time: 0.193, loss: 1892.827881\n",
      "Train: step:  60090, time: 0.217, loss: 819.014160\n",
      "Train: step:  60100, time: 0.210, loss: 3957.256592\n",
      "Train: step:  60110, time: 0.218, loss: 3120.201660\n",
      "Train: step:  60120, time: 0.217, loss: 584.018677\n",
      "Train: step:  60130, time: 0.193, loss: 2385.280273\n",
      "Train: step:  60140, time: 0.214, loss: 2361.435547\n",
      "Train: step:  60150, time: 0.254, loss: 3058.789307\n",
      "Train: step:  60160, time: 0.199, loss: 1575.612915\n",
      "Train: step:  60170, time: 0.183, loss: 922.611023\n",
      "Train: step:  60180, time: 0.247, loss: 2339.191650\n",
      "Train: step:  60190, time: 0.213, loss: 1770.628296\n",
      "Train: step:  60200, time: 0.209, loss: 2838.669922\n",
      "Train: step:  60210, time: 0.219, loss: 2380.957764\n",
      "Train: step:  60220, time: 0.232, loss: 3179.446533\n",
      "Train: step:  60230, time: 0.324, loss: 3961.490967\n",
      "Train: step:  60240, time: 0.192, loss: 2117.411621\n",
      "Train: step:  60250, time: 0.217, loss: 2140.421631\n",
      "Train: step:  60260, time: 0.190, loss: 1697.357300\n",
      "Train: step:  60270, time: 0.231, loss: 4165.458496\n",
      "Train: step:  60280, time: 0.230, loss: 478.492462\n",
      "Train: step:  60290, time: 0.194, loss: 2259.644043\n",
      "Train: step:  60300, time: 0.187, loss: 1025.358276\n",
      "Train: step:  60310, time: 0.185, loss: 377.811096\n",
      "Train: step:  60320, time: 0.190, loss: 961.426575\n",
      "Train: step:  60330, time: 0.241, loss: 1884.859009\n",
      "Train: step:  60340, time: 0.197, loss: 1714.991821\n",
      "Train: step:  60350, time: 0.187, loss: 2550.605469\n",
      "Train: step:  60360, time: 0.181, loss: 2637.601318\n",
      "Train: step:  60370, time: 0.231, loss: 1514.281128\n",
      "Train: step:  60380, time: 0.215, loss: 2794.522461\n",
      "Train: step:  60390, time: 0.197, loss: 2092.848877\n",
      "Train: step:  60400, time: 0.191, loss: 747.805115\n",
      "Train: step:  60410, time: 0.189, loss: 2658.508301\n",
      "Train: step:  60420, time: 0.195, loss: 2763.144775\n",
      "Train: step:  60430, time: 0.217, loss: 1700.535278\n",
      "Train: step:  60440, time: 0.231, loss: 2467.918945\n",
      "Train: step:  60450, time: 0.216, loss: 2837.260254\n",
      "Train: step:  60460, time: 0.189, loss: 2130.120850\n",
      "Train: step:  60470, time: 0.240, loss: 1185.205200\n",
      "Train: step:  60480, time: 0.199, loss: 367.669067\n",
      "Train: step:  60490, time: 0.204, loss: 2133.479004\n",
      "Train: step:  60500, time: 0.240, loss: 1004.305481\n",
      "Train: step:  60510, time: 0.191, loss: 2173.523193\n",
      "Train: step:  60520, time: 0.216, loss: 2344.894775\n",
      "Train: step:  60530, time: 0.189, loss: 986.017273\n",
      "Train: step:  60540, time: 0.215, loss: 1958.604492\n",
      "Train: step:  60550, time: 0.184, loss: 1481.448975\n",
      "Train: step:  60560, time: 0.230, loss: 1691.145508\n",
      "Train: step:  60570, time: 0.187, loss: 550.978394\n",
      "Train: step:  60580, time: 0.201, loss: 1950.528809\n",
      "Train: step:  60590, time: 0.219, loss: 2412.809570\n",
      "Train: step:  60600, time: 0.220, loss: 1491.809814\n",
      "Train: step:  60610, time: 0.217, loss: 1706.879883\n",
      "Train: step:  60620, time: 0.220, loss: 1534.016479\n",
      "Train: step:  60630, time: 0.192, loss: 1642.603516\n",
      "Train: step:  60640, time: 0.219, loss: 2951.290527\n",
      "Train: step:  60650, time: 0.192, loss: 2218.788574\n",
      "Train: step:  60660, time: 0.211, loss: 633.389404\n",
      "Train: step:  60670, time: 0.211, loss: 3227.875977\n",
      "Train: step:  60680, time: 0.219, loss: 2239.158203\n",
      "Train: step:  60690, time: 0.192, loss: 806.087646\n",
      "Train: step:  60700, time: 0.187, loss: 2978.470947\n",
      "Train: step:  60710, time: 0.220, loss: 3106.846436\n",
      "Train: step:  60720, time: 0.194, loss: 2349.621338\n",
      "Train: step:  60730, time: 0.230, loss: 1721.797241\n",
      "Train: step:  60740, time: 0.218, loss: 3540.224365\n",
      "Train: step:  60750, time: 0.215, loss: 1331.709351\n",
      "Train: step:  60760, time: 0.217, loss: 2041.784424\n",
      "Train: step:  60770, time: 0.225, loss: 1182.828125\n",
      "Train: step:  60780, time: 0.217, loss: 2903.045166\n",
      "Train: step:  60790, time: 0.183, loss: 1453.971191\n",
      "Train: step:  60800, time: 0.216, loss: 2868.350342\n",
      "Train: step:  60810, time: 0.219, loss: 1798.153198\n",
      "Train: step:  60820, time: 0.221, loss: 1496.799805\n",
      "Train: step:  60830, time: 0.216, loss: 683.571289\n",
      "Train: step:  60840, time: 0.189, loss: 2751.663086\n",
      "Train: step:  60850, time: 0.179, loss: 2028.224854\n",
      "Train: step:  60860, time: 0.185, loss: 1067.513306\n",
      "Train: step:  60870, time: 0.184, loss: 3370.421387\n",
      "Train: step:  60880, time: 0.199, loss: 3080.704346\n",
      "Train: step:  60890, time: 0.302, loss: 1313.059082\n",
      "Train: step:  60900, time: 0.216, loss: 1048.118530\n",
      "Train: step:  60910, time: 0.257, loss: 2994.492676\n",
      "Train: step:  60920, time: 0.194, loss: 3153.368408\n",
      "Train: step:  60930, time: 0.219, loss: 1596.854370\n",
      "Train: step:  60940, time: 0.191, loss: 1548.249756\n",
      "Train: step:  60950, time: 0.272, loss: 961.393066\n",
      "Train: step:  60960, time: 0.220, loss: 1187.132080\n",
      "Train: step:  60970, time: 0.211, loss: 523.220642\n",
      "Train: step:  60980, time: 0.183, loss: 1537.328003\n",
      "Train: step:  60990, time: 0.189, loss: 1959.917725\n",
      "Train: step:  61000, time: 0.194, loss: 1716.562500\n",
      "Train: step:  61010, time: 0.230, loss: 3438.614502\n",
      "Train: step:  61020, time: 0.217, loss: 1991.523315\n",
      "Train: step:  61030, time: 0.192, loss: 1157.038940\n",
      "Train: step:  61040, time: 0.223, loss: 1100.823486\n",
      "Train: step:  61050, time: 0.216, loss: 427.457458\n",
      "Train: step:  61060, time: 0.219, loss: 2931.578369\n",
      "Train: step:  61070, time: 0.216, loss: 2369.815674\n",
      "Train: step:  61080, time: 0.191, loss: 225.180817\n",
      "Train: step:  61090, time: 0.189, loss: 3741.958740\n",
      "Train: step:  61100, time: 0.193, loss: 796.333313\n",
      "Train: step:  61110, time: 0.196, loss: 2292.869873\n",
      "Train: step:  61120, time: 0.190, loss: 1139.825806\n",
      "Train: step:  61130, time: 0.197, loss: 1497.744141\n",
      "Train: step:  61140, time: 0.186, loss: 1598.613281\n",
      "Train: step:  61150, time: 0.217, loss: 1225.472412\n",
      "Train: step:  61160, time: 0.187, loss: 775.940186\n",
      "Train: step:  61170, time: 0.216, loss: 1546.563843\n",
      "Train: step:  61180, time: 0.218, loss: 2582.053955\n",
      "Train: step:  61190, time: 0.243, loss: 2660.797119\n",
      "Train: step:  61200, time: 0.191, loss: 655.412537\n",
      "Train: step:  61210, time: 0.213, loss: 1263.452515\n",
      "Train: step:  61220, time: 0.217, loss: 776.737427\n",
      "Train: step:  61230, time: 0.246, loss: 2191.120361\n",
      "Train: step:  61240, time: 0.215, loss: 819.654358\n",
      "Train: step:  61250, time: 0.188, loss: 2250.425537\n",
      "Train: step:  61260, time: 0.188, loss: 3354.410889\n",
      "Train: step:  61270, time: 0.185, loss: 4242.607910\n",
      "Train: step:  61280, time: 0.217, loss: 2506.208740\n",
      "Train: step:  61290, time: 0.186, loss: 3167.719482\n",
      "Train: step:  61300, time: 0.182, loss: 1687.277100\n",
      "Train: step:  61310, time: 0.186, loss: 1511.002686\n",
      "Train: step:  61320, time: 0.243, loss: 1824.331299\n",
      "Train: step:  61330, time: 0.228, loss: 2148.477783\n",
      "Train: step:  61340, time: 0.238, loss: 2567.908447\n",
      "Train: step:  61350, time: 0.182, loss: 1807.057617\n",
      "Train: step:  61360, time: 0.228, loss: 2961.110596\n",
      "Train: step:  61370, time: 0.238, loss: 2132.078857\n",
      "Train: step:  61380, time: 0.181, loss: 718.675049\n",
      "Train: step:  61390, time: 0.187, loss: 373.236877\n",
      "Train: step:  61400, time: 0.228, loss: 708.794861\n",
      "Train: step:  61410, time: 0.185, loss: 1511.620850\n",
      "Train: step:  61420, time: 0.225, loss: 672.927429\n",
      "Train: step:  61430, time: 0.220, loss: 815.433472\n",
      "Train: step:  61440, time: 0.219, loss: 3458.498291\n",
      "Train: step:  61450, time: 0.219, loss: 1358.237549\n",
      "Train: step:  61460, time: 0.217, loss: 3794.415283\n",
      "Train: step:  61470, time: 0.230, loss: 864.832764\n",
      "Train: step:  61480, time: 0.216, loss: 1183.406494\n",
      "Train: step:  61490, time: 0.192, loss: 597.878296\n",
      "Train: step:  61500, time: 0.204, loss: 1576.156494\n",
      "Train: step:  61510, time: 0.187, loss: 1181.859375\n",
      "Train: step:  61520, time: 0.184, loss: 1023.050171\n",
      "Train: step:  61530, time: 0.198, loss: 292.550171\n",
      "Train: step:  61540, time: 0.219, loss: 2117.433594\n",
      "Train: step:  61550, time: 0.227, loss: 1715.596924\n",
      "Train: step:  61560, time: 0.191, loss: 2558.894043\n",
      "Train: step:  61570, time: 0.193, loss: 3093.673096\n",
      "Train: step:  61580, time: 0.217, loss: 2573.371338\n",
      "Train: step:  61590, time: 0.231, loss: 2365.357666\n",
      "Train: step:  61600, time: 0.239, loss: 2790.019531\n",
      "Train: step:  61610, time: 0.196, loss: 2166.137695\n",
      "Train: step:  61620, time: 0.189, loss: 931.728394\n",
      "Train: step:  61630, time: 0.197, loss: 2939.437012\n",
      "Train: step:  61640, time: 0.230, loss: 1458.635132\n",
      "Train: step:  61650, time: 0.246, loss: 2287.690918\n",
      "Train: step:  61660, time: 0.221, loss: 1606.296509\n",
      "Train: step:  61670, time: 0.189, loss: 1075.051147\n",
      "Train: step:  61680, time: 0.189, loss: 2891.118896\n",
      "Train: step:  61690, time: 0.196, loss: 807.058044\n",
      "Train: step:  61700, time: 0.188, loss: 2130.270752\n",
      "Train: step:  61710, time: 0.236, loss: 1433.147095\n",
      "Train: step:  61720, time: 0.228, loss: 1537.195923\n",
      "Train: step:  61730, time: 0.190, loss: 2554.727783\n",
      "Train: step:  61740, time: 0.216, loss: 2940.007324\n",
      "Train: step:  61750, time: 0.226, loss: 3442.590332\n",
      "Train: step:  61760, time: 0.185, loss: 1089.635986\n",
      "Train: step:  61770, time: 0.230, loss: 2821.560791\n",
      "Train: step:  61780, time: 0.251, loss: 920.975037\n",
      "Train: step:  61790, time: 0.183, loss: 4390.371582\n",
      "Train: step:  61800, time: 0.218, loss: 399.988068\n",
      "Train: step:  61810, time: 0.201, loss: 420.117065\n",
      "Train: step:  61820, time: 0.221, loss: 660.084106\n",
      "Train: step:  61830, time: 0.221, loss: 1273.119385\n",
      "Train: step:  61840, time: 0.197, loss: 1367.440430\n",
      "Train: step:  61850, time: 0.217, loss: 2225.495850\n",
      "Train: step:  61860, time: 0.208, loss: 1023.458679\n",
      "Train: step:  61870, time: 0.226, loss: 3297.354248\n",
      "Train: step:  61880, time: 0.187, loss: 3398.076660\n",
      "Train: step:  61890, time: 0.231, loss: 454.620270\n",
      "Train: step:  61900, time: 0.219, loss: 2548.937500\n",
      "Train: step:  61910, time: 0.245, loss: 3623.087158\n",
      "Train: step:  61920, time: 0.184, loss: 1541.995605\n",
      "Train: step:  61930, time: 0.231, loss: 567.247070\n",
      "Train: step:  61940, time: 0.181, loss: 1445.796631\n",
      "Train: step:  61950, time: 0.188, loss: 2977.033691\n",
      "Train: step:  61960, time: 0.216, loss: 2420.055908\n",
      "Train: step:  61970, time: 0.229, loss: 924.028076\n",
      "Train: step:  61980, time: 0.233, loss: 2129.954590\n",
      "Train: step:  61990, time: 0.217, loss: 882.635010\n",
      "Train: step:  62000, time: 0.185, loss: 1082.779053\n",
      "Train: step:  62010, time: 0.229, loss: 481.729462\n",
      "Train: step:  62020, time: 0.237, loss: 4708.470215\n",
      "Train: step:  62030, time: 0.210, loss: 3002.028320\n",
      "Train: step:  62040, time: 0.200, loss: 2364.527832\n",
      "Train: step:  62050, time: 0.187, loss: 1627.674561\n",
      "Train: step:  62060, time: 0.216, loss: 2140.925537\n",
      "Train: step:  62070, time: 0.255, loss: 449.996490\n",
      "Train: step:  62080, time: 0.217, loss: 3395.123779\n",
      "Train: step:  62090, time: 0.219, loss: 1199.930664\n",
      "Train: step:  62100, time: 0.187, loss: 2055.793701\n",
      "Train: step:  62110, time: 0.207, loss: 1255.674194\n",
      "Train: step:  62120, time: 0.179, loss: 994.829224\n",
      "Train: step:  62130, time: 0.181, loss: 1737.135986\n",
      "Train: step:  62140, time: 0.182, loss: 1360.009888\n",
      "Train: step:  62150, time: 0.217, loss: 595.899780\n",
      "Train: step:  62160, time: 0.229, loss: 675.268005\n",
      "Train: step:  62170, time: 0.187, loss: 995.903625\n",
      "Train: step:  62180, time: 0.226, loss: 1605.302612\n",
      "Train: step:  62190, time: 0.217, loss: 378.092224\n",
      "Train: step:  62200, time: 0.194, loss: 3270.343506\n",
      "Train: step:  62210, time: 0.189, loss: 1982.014282\n",
      "Train: step:  62220, time: 0.191, loss: 2182.182861\n",
      "Train: step:  62230, time: 0.233, loss: 1268.113525\n",
      "Train: step:  62240, time: 0.185, loss: 2617.028076\n",
      "Train: step:  62250, time: 0.218, loss: 2326.707520\n",
      "Train: step:  62260, time: 0.241, loss: 2537.334473\n",
      "Train: step:  62270, time: 0.249, loss: 2042.715942\n",
      "Train: step:  62280, time: 0.186, loss: 1822.562744\n",
      "Train: step:  62290, time: 0.195, loss: 1549.618286\n",
      "Train: step:  62300, time: 0.199, loss: 2433.634277\n",
      "Train: step:  62310, time: 0.209, loss: 3903.043213\n",
      "Train: step:  62320, time: 0.189, loss: 2069.306885\n",
      "Train: step:  62330, time: 0.189, loss: 1642.979980\n",
      "Train: step:  62340, time: 0.190, loss: 1360.232666\n",
      "Train: step:  62350, time: 0.198, loss: 1473.115479\n",
      "Train: step:  62360, time: 0.231, loss: 2363.474121\n",
      "Train: step:  62370, time: 0.209, loss: 1696.690552\n",
      "Train: step:  62380, time: 0.190, loss: 856.100769\n",
      "Train: step:  62390, time: 0.238, loss: 3324.220459\n",
      "Train: step:  62400, time: 0.185, loss: 1026.284424\n",
      "Train: step:  62410, time: 0.248, loss: 1623.276611\n",
      "Train: step:  62420, time: 0.231, loss: 903.547852\n",
      "Train: step:  62430, time: 0.221, loss: 1515.457886\n",
      "Train: step:  62440, time: 0.187, loss: 1308.648804\n",
      "Train: step:  62450, time: 0.190, loss: 1779.100342\n",
      "Train: step:  62460, time: 0.190, loss: 2736.873535\n",
      "Train: step:  62470, time: 0.221, loss: 2489.417969\n",
      "Train: step:  62480, time: 0.191, loss: 2884.805908\n",
      "Train: step:  62490, time: 0.217, loss: 799.739441\n",
      "Train: step:  62500, time: 0.217, loss: 451.982361\n",
      "Train: step:  62510, time: 0.228, loss: 1515.609253\n",
      "Train: step:  62520, time: 0.196, loss: 2184.073975\n",
      "Train: step:  62530, time: 0.224, loss: 997.829651\n",
      "Train: step:  62540, time: 0.231, loss: 1781.648315\n",
      "Train: step:  62550, time: 0.227, loss: 1190.695312\n",
      "Train: step:  62560, time: 0.221, loss: 2098.722656\n",
      "Train: step:  62570, time: 0.188, loss: 1544.796875\n",
      "Train: step:  62580, time: 0.189, loss: 1611.568359\n",
      "Train: step:  62590, time: 0.216, loss: 1302.735596\n",
      "Train: step:  62600, time: 0.195, loss: 1503.676636\n",
      "Train: step:  62610, time: 0.195, loss: 2691.296631\n",
      "Train: step:  62620, time: 0.183, loss: 1404.757935\n",
      "Train: step:  62630, time: 0.232, loss: 1003.889832\n",
      "Train: step:  62640, time: 0.189, loss: 971.979126\n",
      "Train: step:  62650, time: 0.189, loss: 1292.571045\n",
      "Train: step:  62660, time: 0.196, loss: 1812.737671\n",
      "Train: step:  62670, time: 0.194, loss: 1269.423706\n",
      "Train: step:  62680, time: 0.248, loss: 825.870422\n",
      "Train: step:  62690, time: 0.228, loss: 2329.780273\n",
      "Train: step:  62700, time: 0.218, loss: 205.345749\n",
      "Train: step:  62710, time: 0.218, loss: 417.940338\n",
      "Train: step:  62720, time: 0.228, loss: 772.095764\n",
      "Train: step:  62730, time: 0.193, loss: 2110.908447\n",
      "Train: step:  62740, time: 0.189, loss: 1760.249756\n",
      "Train: step:  62750, time: 0.186, loss: 2299.842529\n",
      "Train: step:  62760, time: 0.214, loss: 1666.374023\n",
      "Train: step:  62770, time: 0.227, loss: 2702.423828\n",
      "Train: step:  62780, time: 0.186, loss: 1173.589355\n",
      "Train: step:  62790, time: 0.247, loss: 1727.784302\n",
      "Train: step:  62800, time: 0.215, loss: 2333.740234\n",
      "Train: step:  62810, time: 0.231, loss: 1339.416382\n",
      "Train: step:  62820, time: 0.189, loss: 1674.246338\n",
      "Train: step:  62830, time: 0.186, loss: 676.508789\n",
      "Train: step:  62840, time: 0.190, loss: 2643.952393\n",
      "Train: step:  62850, time: 0.187, loss: 373.742645\n",
      "Train: step:  62860, time: 0.213, loss: 1432.398682\n",
      "Train: step:  62870, time: 0.215, loss: 1817.291870\n",
      "Train: step:  62880, time: 0.221, loss: 2031.193970\n",
      "Train: step:  62890, time: 0.227, loss: 1288.133789\n",
      "Train: step:  62900, time: 0.221, loss: 1681.991821\n",
      "Train: step:  62910, time: 0.195, loss: 2546.282471\n",
      "Train: step:  62920, time: 0.200, loss: 1278.097290\n",
      "Train: step:  62930, time: 0.229, loss: 3006.485596\n",
      "Train: step:  62940, time: 0.189, loss: 3182.681885\n",
      "Train: step:  62950, time: 0.215, loss: 2596.364502\n",
      "Train: step:  62960, time: 0.231, loss: 2003.723267\n",
      "Train: step:  62970, time: 0.219, loss: 1839.631714\n",
      "Train: step:  62980, time: 0.217, loss: 1060.043579\n",
      "Train: step:  62990, time: 0.184, loss: 1605.860840\n",
      "Train: step:  63000, time: 0.192, loss: 1015.229553\n",
      "Train: step:  63010, time: 0.192, loss: 723.910217\n",
      "Train: step:  63020, time: 0.190, loss: 752.706482\n",
      "Train: step:  63030, time: 0.215, loss: 514.304443\n",
      "Train: step:  63040, time: 0.185, loss: 2605.894287\n",
      "Train: step:  63050, time: 0.185, loss: 1374.331421\n",
      "Train: step:  63060, time: 0.215, loss: 1983.522583\n",
      "Train: step:  63070, time: 0.194, loss: 3172.341797\n",
      "Train: step:  63080, time: 0.182, loss: 1938.428223\n",
      "Train: step:  63090, time: 0.188, loss: 3290.258301\n",
      "Train: step:  63100, time: 0.221, loss: 1764.843994\n",
      "Train: step:  63110, time: 0.255, loss: 1608.852295\n",
      "Train: step:  63120, time: 0.188, loss: 2293.257080\n",
      "Train: step:  63130, time: 0.229, loss: 1124.391602\n",
      "Train: step:  63140, time: 0.215, loss: 1830.905518\n",
      "Train: step:  63150, time: 0.228, loss: 1704.290039\n",
      "Train: step:  63160, time: 0.220, loss: 1485.310425\n",
      "Train: step:  63170, time: 0.245, loss: 841.196777\n",
      "Train: step:  63180, time: 0.213, loss: 1402.839111\n",
      "Train: step:  63190, time: 0.232, loss: 2492.119385\n",
      "Train: step:  63200, time: 0.219, loss: 2228.491455\n",
      "Train: step:  63210, time: 0.211, loss: 1941.291870\n",
      "Train: step:  63220, time: 0.227, loss: 984.634094\n",
      "Train: step:  63230, time: 0.192, loss: 2708.854980\n",
      "Train: step:  63240, time: 0.229, loss: 2029.577393\n",
      "Train: step:  63250, time: 0.187, loss: 586.082031\n",
      "Train: step:  63260, time: 0.220, loss: 2425.539307\n",
      "Train: step:  63270, time: 0.217, loss: 1601.389282\n",
      "Train: step:  63280, time: 0.218, loss: 486.675262\n",
      "Train: step:  63290, time: 0.229, loss: 1384.453247\n",
      "Train: step:  63300, time: 0.228, loss: 2638.375732\n",
      "Train: step:  63310, time: 0.201, loss: 856.672791\n",
      "Train: step:  63320, time: 0.242, loss: 1786.613525\n",
      "Train: step:  63330, time: 0.229, loss: 1553.735596\n",
      "Train: step:  63340, time: 0.190, loss: 2303.625488\n",
      "Train: step:  63350, time: 0.191, loss: 727.726257\n",
      "Train: step:  63360, time: 0.200, loss: 1502.962158\n",
      "Train: step:  63370, time: 0.229, loss: 1223.255859\n",
      "Train: step:  63380, time: 0.222, loss: 1417.114624\n",
      "Train: step:  63390, time: 0.183, loss: 1943.953247\n",
      "Train: step:  63400, time: 0.225, loss: 1264.763794\n",
      "Train: step:  63410, time: 0.183, loss: 757.662537\n",
      "Train: step:  63420, time: 0.193, loss: 703.199951\n",
      "Train: step:  63430, time: 0.234, loss: 1621.371704\n",
      "Train: step:  63440, time: 0.190, loss: 767.486572\n",
      "Train: step:  63450, time: 0.188, loss: 1113.288452\n",
      "Train: step:  63460, time: 0.214, loss: 1235.931030\n",
      "Train: step:  63470, time: 0.190, loss: 471.027954\n",
      "Train: step:  63480, time: 0.229, loss: 2868.343750\n",
      "Train: step:  63490, time: 0.210, loss: 1121.575195\n",
      "Train: step:  63500, time: 0.198, loss: 2246.365234\n",
      "Train: step:  63510, time: 0.229, loss: 3583.482422\n",
      "Train: step:  63520, time: 0.216, loss: 1009.430054\n",
      "Train: step:  63530, time: 0.225, loss: 3224.296143\n",
      "Train: step:  63540, time: 0.201, loss: 1952.510986\n",
      "Train: step:  63550, time: 0.199, loss: 2199.885742\n",
      "Train: step:  63560, time: 0.215, loss: 2576.072998\n",
      "Train: step:  63570, time: 0.189, loss: 915.016235\n",
      "Train: step:  63580, time: 0.227, loss: 1113.011353\n",
      "Train: step:  63590, time: 0.225, loss: 744.836060\n",
      "Train: step:  63600, time: 0.203, loss: 1169.011108\n",
      "Train: step:  63610, time: 0.189, loss: 4327.846191\n",
      "Train: step:  63620, time: 0.202, loss: 2900.561035\n",
      "Train: step:  63630, time: 0.192, loss: 2350.826416\n",
      "Train: step:  63640, time: 0.191, loss: 1609.838135\n",
      "Train: step:  63650, time: 0.315, loss: 208.350647\n",
      "Train: step:  63660, time: 0.216, loss: 3416.289307\n",
      "Train: step:  63670, time: 0.224, loss: 2997.667480\n",
      "Train: step:  63680, time: 0.230, loss: 783.943298\n",
      "Train: step:  63690, time: 0.217, loss: 2222.898438\n",
      "Train: step:  63700, time: 0.220, loss: 603.664856\n",
      "Train: step:  63710, time: 0.217, loss: 2315.001465\n",
      "Train: step:  63720, time: 0.224, loss: 2178.241943\n",
      "Train: step:  63730, time: 0.209, loss: 710.982117\n",
      "Train: step:  63740, time: 0.221, loss: 2788.970459\n",
      "Train: step:  63750, time: 0.192, loss: 906.988159\n",
      "Train: step:  63760, time: 0.217, loss: 2172.130615\n",
      "Train: step:  63770, time: 0.187, loss: 624.287903\n",
      "Train: step:  63780, time: 0.183, loss: 747.952148\n",
      "Train: step:  63790, time: 0.227, loss: 2408.114502\n",
      "Train: step:  63800, time: 0.216, loss: 547.071716\n",
      "Train: step:  63810, time: 0.207, loss: 297.783356\n",
      "Train: step:  63820, time: 0.184, loss: 1590.924438\n",
      "Train: step:  63830, time: 0.229, loss: 658.817139\n",
      "Train: step:  63840, time: 0.231, loss: 1480.572266\n",
      "Train: step:  63850, time: 0.185, loss: 4125.794922\n",
      "Train: step:  63860, time: 0.183, loss: 3086.814453\n",
      "Train: step:  63870, time: 0.226, loss: 2513.242432\n",
      "Train: step:  63880, time: 0.186, loss: 1868.478760\n",
      "Train: step:  63890, time: 0.220, loss: 546.158020\n",
      "Train: step:  63900, time: 0.184, loss: 448.518677\n",
      "Train: step:  63910, time: 0.187, loss: 1112.732056\n",
      "Train: step:  63920, time: 0.178, loss: 3426.427490\n",
      "Train: step:  63930, time: 0.237, loss: 2870.259277\n",
      "Train: step:  63940, time: 0.191, loss: 3312.765137\n",
      "Train: step:  63950, time: 0.228, loss: 309.978821\n",
      "Train: step:  63960, time: 0.208, loss: 1530.346924\n",
      "Train: step:  63970, time: 0.185, loss: 3219.426025\n",
      "Train: step:  63980, time: 0.215, loss: 2674.882324\n",
      "Train: step:  63990, time: 0.232, loss: 677.293640\n",
      "Train: step:  64000, time: 0.183, loss: 791.522888\n",
      "Train: step:  64010, time: 0.217, loss: 1747.026001\n",
      "Train: step:  64020, time: 0.181, loss: 2002.289185\n",
      "Train: step:  64030, time: 0.227, loss: 3064.279785\n",
      "Train: step:  64040, time: 0.224, loss: 1441.694214\n",
      "Train: step:  64050, time: 0.187, loss: 2379.408936\n",
      "Train: step:  64060, time: 0.216, loss: 913.186584\n",
      "Train: step:  64070, time: 0.188, loss: 1617.449219\n",
      "Train: step:  64080, time: 0.229, loss: 2125.390381\n",
      "Train: step:  64090, time: 0.238, loss: 996.480957\n",
      "Train: step:  64100, time: 0.215, loss: 2908.453857\n",
      "Train: step:  64110, time: 0.213, loss: 1837.451782\n",
      "Train: step:  64120, time: 0.185, loss: 780.709534\n",
      "Train: step:  64130, time: 0.219, loss: 1736.336426\n",
      "Train: step:  64140, time: 0.231, loss: 1591.285034\n",
      "Train: step:  64150, time: 0.216, loss: 3296.188477\n",
      "Train: step:  64160, time: 0.218, loss: 2359.628906\n",
      "Train: step:  64170, time: 0.184, loss: 2791.202881\n",
      "Train: step:  64180, time: 0.221, loss: 1121.270752\n",
      "Train: step:  64190, time: 0.185, loss: 2643.318604\n",
      "Train: step:  64200, time: 0.183, loss: 2706.116699\n",
      "Train: step:  64210, time: 0.183, loss: 1897.686768\n",
      "Train: step:  64220, time: 0.238, loss: 842.250732\n",
      "Train: step:  64230, time: 0.190, loss: 2220.543701\n",
      "Train: step:  64240, time: 0.200, loss: 1904.741943\n",
      "Train: step:  64250, time: 0.189, loss: 2559.793457\n",
      "Train: step:  64260, time: 0.228, loss: 1471.206421\n",
      "Train: step:  64270, time: 0.217, loss: 2149.102539\n",
      "Train: step:  64280, time: 0.194, loss: 2844.701660\n",
      "Train: step:  64290, time: 0.191, loss: 843.283203\n",
      "Train: step:  64300, time: 0.193, loss: 1285.307861\n",
      "Train: step:  64310, time: 0.215, loss: 1176.389893\n",
      "Train: step:  64320, time: 0.196, loss: 2387.593018\n",
      "Train: step:  64330, time: 0.198, loss: 1331.454834\n",
      "Train: step:  64340, time: 0.193, loss: 1574.422241\n",
      "Train: step:  64350, time: 0.215, loss: 312.346283\n",
      "Train: step:  64360, time: 0.193, loss: 666.548950\n",
      "Train: step:  64370, time: 0.191, loss: 304.507782\n",
      "Train: step:  64380, time: 0.226, loss: 2285.541748\n",
      "Train: step:  64390, time: 0.188, loss: 1475.308105\n",
      "Train: step:  64400, time: 0.226, loss: 1004.098877\n",
      "Train: step:  64410, time: 0.216, loss: 2567.967285\n",
      "Train: step:  64420, time: 0.215, loss: 567.634216\n",
      "Train: step:  64430, time: 0.218, loss: 1668.059448\n",
      "Train: step:  64440, time: 0.190, loss: 1854.442993\n",
      "Train: step:  64450, time: 0.235, loss: 1681.247314\n",
      "Train: step:  64460, time: 0.228, loss: 2710.186523\n",
      "Train: step:  64470, time: 0.217, loss: 2399.072266\n",
      "Train: step:  64480, time: 0.217, loss: 1895.861328\n",
      "Train: step:  64490, time: 0.190, loss: 1070.007812\n",
      "Train: step:  64500, time: 0.216, loss: 1852.986694\n",
      "Train: step:  64510, time: 0.182, loss: 1687.789185\n",
      "Train: step:  64520, time: 0.193, loss: 603.755249\n",
      "Train: step:  64530, time: 0.229, loss: 2356.375732\n",
      "Train: step:  64540, time: 0.227, loss: 806.318359\n",
      "Train: step:  64550, time: 0.189, loss: 2740.003418\n",
      "Train: step:  64560, time: 0.189, loss: 2103.952393\n",
      "Train: step:  64570, time: 0.191, loss: 2801.341064\n",
      "Train: step:  64580, time: 0.188, loss: 961.557190\n",
      "Train: step:  64590, time: 0.229, loss: 1847.348267\n",
      "Train: step:  64600, time: 0.217, loss: 1075.496216\n",
      "Train: step:  64610, time: 0.186, loss: 943.106506\n",
      "Train: step:  64620, time: 0.216, loss: 3483.427979\n",
      "Train: step:  64630, time: 0.191, loss: 1789.333252\n",
      "Train: step:  64640, time: 0.190, loss: 803.025879\n",
      "Train: step:  64650, time: 0.193, loss: 486.728729\n",
      "Train: step:  64660, time: 0.217, loss: 1896.069702\n",
      "Train: step:  64670, time: 0.191, loss: 1835.679199\n",
      "Train: step:  64680, time: 0.233, loss: 855.451843\n",
      "Train: step:  64690, time: 0.217, loss: 1721.969360\n",
      "Train: step:  64700, time: 0.226, loss: 1029.441162\n",
      "Train: step:  64710, time: 0.187, loss: 3460.418213\n",
      "Train: step:  64720, time: 0.200, loss: 1671.075195\n",
      "Train: step:  64730, time: 0.198, loss: 1699.374023\n",
      "Train: step:  64740, time: 0.190, loss: 323.632263\n",
      "Train: step:  64750, time: 0.219, loss: 3418.039307\n",
      "Train: step:  64760, time: 0.223, loss: 2160.062256\n",
      "Train: step:  64770, time: 0.226, loss: 459.434784\n",
      "Train: step:  64780, time: 0.184, loss: 3714.443848\n",
      "Train: step:  64790, time: 0.227, loss: 1339.217285\n",
      "Train: step:  64800, time: 0.209, loss: 1324.771729\n",
      "Train: step:  64810, time: 0.230, loss: 2438.442383\n",
      "Train: step:  64820, time: 0.231, loss: 1256.087891\n",
      "Train: step:  64830, time: 0.198, loss: 587.955078\n",
      "Train: step:  64840, time: 0.187, loss: 2383.279541\n",
      "Train: step:  64850, time: 0.216, loss: 3344.644531\n",
      "Train: step:  64860, time: 0.233, loss: 2663.911133\n",
      "Train: step:  64870, time: 0.191, loss: 1442.571533\n",
      "Train: step:  64880, time: 0.192, loss: 2076.123779\n",
      "Train: step:  64890, time: 0.185, loss: 689.692871\n",
      "Train: step:  64900, time: 0.190, loss: 338.232117\n",
      "Train: step:  64910, time: 0.200, loss: 2976.951416\n",
      "Train: step:  64920, time: 0.186, loss: 719.558899\n",
      "Train: step:  64930, time: 0.186, loss: 654.683899\n",
      "Train: step:  64940, time: 0.197, loss: 1122.355957\n",
      "Train: step:  64950, time: 0.189, loss: 1352.412964\n",
      "Train: step:  64960, time: 0.183, loss: 794.584290\n",
      "Train: step:  64970, time: 0.184, loss: 703.820618\n",
      "Train: step:  64980, time: 0.188, loss: 1729.508667\n",
      "Train: step:  64990, time: 0.229, loss: 2122.385010\n",
      "Train: step:  65000, time: 0.192, loss: 3337.686279\n",
      "Train: step:  65010, time: 0.217, loss: 1758.930542\n",
      "Train: step:  65020, time: 0.191, loss: 1401.580688\n",
      "Train: step:  65030, time: 0.190, loss: 1127.919312\n",
      "Train: step:  65040, time: 0.185, loss: 2110.900635\n",
      "Train: step:  65050, time: 0.230, loss: 2696.166748\n",
      "Train: step:  65060, time: 0.192, loss: 801.482483\n",
      "Train: step:  65070, time: 0.230, loss: 1667.110962\n",
      "Train: step:  65080, time: 0.182, loss: 1504.158691\n",
      "Train: step:  65090, time: 0.218, loss: 1962.402466\n",
      "Train: step:  65100, time: 0.192, loss: 1966.565918\n",
      "Train: step:  65110, time: 0.217, loss: 3311.332275\n",
      "Train: step:  65120, time: 0.226, loss: 1328.369751\n",
      "Train: step:  65130, time: 0.217, loss: 4084.099121\n",
      "Train: step:  65140, time: 0.216, loss: 1169.576172\n",
      "Train: step:  65150, time: 0.189, loss: 1105.966797\n",
      "Train: step:  65160, time: 0.186, loss: 634.067261\n",
      "Train: step:  65170, time: 0.181, loss: 4373.250977\n",
      "Train: step:  65180, time: 0.203, loss: 3380.429688\n",
      "Train: step:  65190, time: 0.226, loss: 1680.340942\n",
      "Train: step:  65200, time: 0.260, loss: 2474.714600\n",
      "Train: step:  65210, time: 0.218, loss: 835.902527\n",
      "Train: step:  65220, time: 0.217, loss: 1200.511719\n",
      "Train: step:  65230, time: 0.189, loss: 834.574890\n",
      "Train: step:  65240, time: 0.182, loss: 2071.151611\n",
      "Train: step:  65250, time: 0.208, loss: 936.059631\n",
      "Train: step:  65260, time: 0.186, loss: 1345.581299\n",
      "Train: step:  65270, time: 0.227, loss: 2028.426147\n",
      "Train: step:  65280, time: 0.187, loss: 1753.022705\n",
      "Train: step:  65290, time: 0.216, loss: 839.689819\n",
      "Train: step:  65300, time: 0.205, loss: 761.099976\n",
      "Train: step:  65310, time: 0.186, loss: 1070.003052\n",
      "Train: step:  65320, time: 0.190, loss: 2677.808350\n",
      "Train: step:  65330, time: 0.273, loss: 503.881287\n",
      "Train: step:  65340, time: 0.213, loss: 261.901672\n",
      "Train: step:  65350, time: 0.217, loss: 1430.868408\n",
      "Train: step:  65360, time: 0.228, loss: 3014.022705\n",
      "Train: step:  65370, time: 0.221, loss: 2251.195557\n",
      "Train: step:  65380, time: 0.217, loss: 736.768982\n",
      "Train: step:  65390, time: 0.186, loss: 2388.691162\n",
      "Train: step:  65400, time: 0.185, loss: 1866.392822\n",
      "Train: step:  65410, time: 0.184, loss: 832.878723\n",
      "Train: step:  65420, time: 0.193, loss: 1384.706787\n",
      "Train: step:  65430, time: 0.229, loss: 884.775513\n",
      "Train: step:  65440, time: 0.195, loss: 1431.607666\n",
      "Train: step:  65450, time: 0.190, loss: 3144.603760\n",
      "Train: step:  65460, time: 0.246, loss: 2516.412842\n",
      "Train: step:  65470, time: 0.225, loss: 1371.148438\n",
      "Train: step:  65480, time: 0.218, loss: 1431.892578\n",
      "Train: step:  65490, time: 0.215, loss: 1136.680298\n",
      "Train: step:  65500, time: 0.227, loss: 2471.109863\n",
      "Train: step:  65510, time: 0.198, loss: 1205.436768\n",
      "Train: step:  65520, time: 0.185, loss: 777.332092\n",
      "Train: step:  65530, time: 0.197, loss: 1300.480347\n",
      "Train: step:  65540, time: 0.187, loss: 3284.956299\n",
      "Train: step:  65550, time: 0.187, loss: 1392.556519\n",
      "Train: step:  65560, time: 0.210, loss: 1752.994507\n",
      "Train: step:  65570, time: 0.180, loss: 3446.057373\n",
      "Train: step:  65580, time: 0.231, loss: 1498.166016\n",
      "Train: step:  65590, time: 0.196, loss: 1243.779663\n",
      "Train: step:  65600, time: 0.216, loss: 2306.907471\n",
      "Train: step:  65610, time: 0.186, loss: 1323.905640\n",
      "Train: step:  65620, time: 0.235, loss: 1923.379883\n",
      "Train: step:  65630, time: 0.220, loss: 2355.453369\n",
      "Train: step:  65640, time: 0.186, loss: 675.045715\n",
      "Train: step:  65650, time: 0.224, loss: 788.579529\n",
      "Train: step:  65660, time: 0.225, loss: 2987.010742\n",
      "Train: step:  65670, time: 0.240, loss: 3292.264893\n",
      "Train: step:  65680, time: 0.182, loss: 960.776184\n",
      "Train: step:  65690, time: 0.216, loss: 1469.647095\n",
      "Train: step:  65700, time: 0.235, loss: 1840.906128\n",
      "Train: step:  65710, time: 0.189, loss: 1528.388306\n",
      "Train: step:  65720, time: 0.227, loss: 1524.360352\n",
      "Train: step:  65730, time: 0.187, loss: 882.610535\n",
      "Train: step:  65740, time: 0.215, loss: 785.097717\n",
      "Train: step:  65750, time: 0.233, loss: 1637.242554\n",
      "Train: step:  65760, time: 0.198, loss: 953.403381\n",
      "Train: step:  65770, time: 0.194, loss: 2114.091553\n",
      "Train: step:  65780, time: 0.187, loss: 2122.571777\n",
      "Train: step:  65790, time: 0.216, loss: 1839.018921\n",
      "Train: step:  65800, time: 0.219, loss: 1717.698242\n",
      "Train: step:  65810, time: 0.199, loss: 1932.659058\n",
      "Train: step:  65820, time: 0.209, loss: 1691.672852\n",
      "Train: step:  65830, time: 0.192, loss: 2081.411133\n",
      "Train: step:  65840, time: 0.232, loss: 1631.806152\n",
      "Train: step:  65850, time: 0.190, loss: 4470.347168\n",
      "Train: step:  65860, time: 0.198, loss: 1445.915527\n",
      "Train: step:  65870, time: 0.193, loss: 1599.821777\n",
      "Train: step:  65880, time: 0.186, loss: 3547.660400\n",
      "Train: step:  65890, time: 0.191, loss: 477.289764\n",
      "Train: step:  65900, time: 0.187, loss: 1589.268555\n",
      "Train: step:  65910, time: 0.190, loss: 1864.881348\n",
      "Train: step:  65920, time: 0.184, loss: 1866.545410\n",
      "Train: step:  65930, time: 0.184, loss: 1399.763672\n",
      "Train: step:  65940, time: 0.190, loss: 1733.977051\n",
      "Train: step:  65950, time: 0.183, loss: 862.992859\n",
      "Train: step:  65960, time: 0.183, loss: 3162.996338\n",
      "Train: step:  65970, time: 0.223, loss: 660.463867\n",
      "Train: step:  65980, time: 0.229, loss: 1091.576904\n",
      "Train: step:  65990, time: 0.195, loss: 3365.593262\n",
      "Train: step:  66000, time: 0.183, loss: 2035.653564\n",
      "Train: step:  66010, time: 0.182, loss: 2307.880859\n",
      "Train: step:  66020, time: 0.217, loss: 1644.130737\n",
      "Train: step:  66030, time: 0.188, loss: 1832.221313\n",
      "Train: step:  66040, time: 0.182, loss: 756.623352\n",
      "Train: step:  66050, time: 0.193, loss: 985.679016\n",
      "Train: step:  66060, time: 0.229, loss: 991.889526\n",
      "Train: step:  66070, time: 0.190, loss: 2238.578613\n",
      "Train: step:  66080, time: 0.190, loss: 1641.912720\n",
      "Train: step:  66090, time: 0.185, loss: 3345.673340\n",
      "Train: step:  66100, time: 0.217, loss: 1725.764893\n",
      "Train: step:  66110, time: 0.205, loss: 648.377930\n",
      "Train: step:  66120, time: 0.185, loss: 3385.753906\n",
      "Train: step:  66130, time: 0.183, loss: 2213.483154\n",
      "Train: step:  66140, time: 0.188, loss: 2742.672119\n",
      "Train: step:  66150, time: 0.191, loss: 923.667419\n",
      "Train: step:  66160, time: 0.193, loss: 2126.095947\n",
      "Train: step:  66170, time: 0.185, loss: 1041.034058\n",
      "Train: step:  66180, time: 0.198, loss: 1436.375977\n",
      "Train: step:  66190, time: 0.183, loss: 1355.034790\n",
      "Train: step:  66200, time: 0.182, loss: 760.629944\n",
      "Train: step:  66210, time: 0.191, loss: 1066.211792\n",
      "Train: step:  66220, time: 0.188, loss: 1179.775146\n",
      "Train: step:  66230, time: 0.192, loss: 622.488037\n",
      "Train: step:  66240, time: 0.184, loss: 1130.177124\n",
      "Train: step:  66250, time: 0.187, loss: 2617.221680\n",
      "Train: step:  66260, time: 0.236, loss: 1870.058472\n",
      "Train: step:  66270, time: 0.195, loss: 2321.345215\n",
      "Train: step:  66280, time: 0.206, loss: 1430.229126\n",
      "Train: step:  66290, time: 0.188, loss: 2979.347168\n",
      "Train: step:  66300, time: 0.187, loss: 1722.143677\n",
      "Train: step:  66310, time: 0.184, loss: 2048.730225\n",
      "Train: step:  66320, time: 0.215, loss: 1970.159302\n",
      "Train: step:  66330, time: 0.191, loss: 1341.776611\n",
      "Train: step:  66340, time: 0.230, loss: 2454.139893\n",
      "Train: step:  66350, time: 0.182, loss: 1616.852905\n",
      "Train: step:  66360, time: 0.187, loss: 1322.607544\n",
      "Train: step:  66370, time: 0.188, loss: 1597.431885\n",
      "Train: step:  66380, time: 0.230, loss: 1618.140625\n",
      "Train: step:  66390, time: 0.185, loss: 1489.439087\n",
      "Train: step:  66400, time: 0.187, loss: 1670.565308\n",
      "Train: step:  66410, time: 0.185, loss: 1676.452026\n",
      "Train: step:  66420, time: 0.184, loss: 320.832886\n",
      "Train: step:  66430, time: 0.193, loss: 1241.556641\n",
      "Train: step:  66440, time: 0.185, loss: 1989.833984\n",
      "Train: step:  66450, time: 0.182, loss: 1481.196411\n",
      "Train: step:  66460, time: 0.230, loss: 2980.687744\n",
      "Train: step:  66470, time: 0.182, loss: 3258.510742\n",
      "Train: step:  66480, time: 0.218, loss: 5106.765625\n",
      "Train: step:  66490, time: 0.187, loss: 1216.946167\n",
      "Train: step:  66500, time: 0.211, loss: 664.336487\n",
      "Train: step:  66510, time: 0.219, loss: 2194.479004\n",
      "Train: step:  66520, time: 0.215, loss: 2213.495605\n",
      "Train: step:  66530, time: 0.232, loss: 2666.485840\n",
      "Train: step:  66540, time: 0.193, loss: 2589.133545\n",
      "Train: step:  66550, time: 0.216, loss: 1298.670776\n",
      "Train: step:  66560, time: 0.194, loss: 2023.630859\n",
      "Train: step:  66570, time: 0.213, loss: 482.828400\n",
      "Train: step:  66580, time: 0.186, loss: 4681.880859\n",
      "Train: step:  66590, time: 0.212, loss: 381.581329\n",
      "Train: step:  66600, time: 0.226, loss: 1915.517578\n",
      "Train: step:  66610, time: 0.184, loss: 2275.261475\n",
      "Train: step:  66620, time: 0.205, loss: 1878.608765\n",
      "Train: step:  66630, time: 0.188, loss: 2377.038818\n",
      "Train: step:  66640, time: 0.252, loss: 787.972412\n",
      "Train: step:  66650, time: 0.191, loss: 617.349976\n",
      "Train: step:  66660, time: 0.189, loss: 2813.463867\n",
      "Train: step:  66670, time: 0.190, loss: 308.253754\n",
      "Train: step:  66680, time: 0.188, loss: 1958.553589\n",
      "Train: step:  66690, time: 0.184, loss: 2927.643799\n",
      "Train: step:  66700, time: 0.186, loss: 2474.545410\n",
      "Train: step:  66710, time: 0.185, loss: 3575.168701\n",
      "Train: step:  66720, time: 0.186, loss: 1232.670288\n",
      "Train: step:  66730, time: 0.188, loss: 2220.049316\n",
      "Train: step:  66740, time: 0.196, loss: 2087.770996\n",
      "Train: step:  66750, time: 0.184, loss: 2064.581787\n",
      "Train: step:  66760, time: 0.190, loss: 2065.796387\n",
      "Train: step:  66770, time: 0.182, loss: 1553.907593\n",
      "Train: step:  66780, time: 0.188, loss: 1276.835327\n",
      "Train: step:  66790, time: 0.189, loss: 2492.083252\n",
      "Train: step:  66800, time: 0.190, loss: 778.109680\n",
      "Train: step:  66810, time: 0.215, loss: 2217.641113\n",
      "Train: step:  66820, time: 0.196, loss: 1445.181519\n",
      "Train: step:  66830, time: 0.189, loss: 1872.927490\n",
      "Train: step:  66840, time: 0.189, loss: 786.428345\n",
      "Train: step:  66850, time: 0.184, loss: 989.844666\n",
      "Train: step:  66860, time: 0.200, loss: 1846.443237\n",
      "Train: step:  66870, time: 0.190, loss: 2361.013916\n",
      "Train: step:  66880, time: 0.217, loss: 2394.049805\n",
      "Train: step:  66890, time: 0.180, loss: 438.668427\n",
      "Train: step:  66900, time: 0.194, loss: 792.503113\n",
      "Train: step:  66910, time: 0.228, loss: 2400.055664\n",
      "Train: step:  66920, time: 0.192, loss: 1624.462280\n",
      "Train: step:  66930, time: 0.187, loss: 896.100891\n",
      "Train: step:  66940, time: 0.192, loss: 2114.428955\n",
      "Train: step:  66950, time: 0.215, loss: 427.847992\n",
      "Train: step:  66960, time: 0.192, loss: 1969.160645\n",
      "Train: step:  66970, time: 0.184, loss: 1982.993286\n",
      "Train: step:  66980, time: 0.182, loss: 2438.189453\n",
      "Train: step:  66990, time: 0.192, loss: 358.987854\n",
      "Train: step:  67000, time: 0.191, loss: 2022.736450\n",
      "Train: step:  67010, time: 0.188, loss: 2936.637207\n",
      "Train: step:  67020, time: 0.262, loss: 1669.727783\n",
      "Train: step:  67030, time: 0.192, loss: 1253.636597\n",
      "Train: step:  67040, time: 0.200, loss: 459.129150\n",
      "Train: step:  67050, time: 0.188, loss: 1393.229126\n",
      "Train: step:  67060, time: 0.184, loss: 1908.185425\n",
      "Train: step:  67070, time: 0.184, loss: 2202.118164\n",
      "Train: step:  67080, time: 0.188, loss: 786.128723\n",
      "Train: step:  67090, time: 0.215, loss: 2973.333740\n",
      "Train: step:  67100, time: 0.188, loss: 509.855652\n",
      "Train: step:  67110, time: 0.233, loss: 1950.840698\n",
      "Train: step:  67120, time: 0.187, loss: 3385.514648\n",
      "Train: step:  67130, time: 0.235, loss: 477.344330\n",
      "Train: step:  67140, time: 0.185, loss: 2567.107178\n",
      "Train: step:  67150, time: 0.226, loss: 2185.408203\n",
      "Train: step:  67160, time: 0.216, loss: 1727.931763\n",
      "Train: step:  67170, time: 0.216, loss: 3791.855469\n",
      "Train: step:  67180, time: 0.197, loss: 2645.782959\n",
      "Train: step:  67190, time: 0.227, loss: 1983.387939\n",
      "Train: step:  67200, time: 0.214, loss: 3376.023682\n",
      "Train: step:  67210, time: 0.186, loss: 2097.758057\n",
      "Train: step:  67220, time: 0.245, loss: 1288.573730\n",
      "Train: step:  67230, time: 0.182, loss: 2343.755859\n",
      "Train: step:  67240, time: 0.201, loss: 1533.337891\n",
      "Train: step:  67250, time: 0.198, loss: 696.515137\n",
      "Train: step:  67260, time: 0.224, loss: 1984.014038\n",
      "Train: step:  67270, time: 0.192, loss: 2783.669678\n",
      "Train: step:  67280, time: 0.189, loss: 1657.026489\n",
      "Train: step:  67290, time: 0.191, loss: 880.744385\n",
      "Train: step:  67300, time: 0.183, loss: 1351.991333\n",
      "Train: step:  67310, time: 0.188, loss: 3018.808350\n",
      "Train: step:  67320, time: 0.185, loss: 2358.922363\n",
      "Train: step:  67330, time: 0.186, loss: 3089.274658\n",
      "Train: step:  67340, time: 0.183, loss: 2181.455566\n",
      "Train: step:  67350, time: 0.183, loss: 1258.880127\n",
      "Train: step:  67360, time: 0.190, loss: 729.209229\n",
      "Train: step:  67370, time: 0.188, loss: 2188.046875\n",
      "Train: step:  67380, time: 0.186, loss: 2788.181885\n",
      "Train: step:  67390, time: 0.184, loss: 450.693481\n",
      "Train: step:  67400, time: 0.184, loss: 1328.726318\n",
      "Train: step:  67410, time: 0.190, loss: 3527.958252\n",
      "Train: step:  67420, time: 0.186, loss: 1316.791016\n",
      "Train: step:  67430, time: 0.183, loss: 1546.638794\n",
      "Train: step:  67440, time: 0.181, loss: 2068.110840\n",
      "Train: step:  67450, time: 0.185, loss: 1425.802979\n",
      "Train: step:  67460, time: 0.186, loss: 1955.967163\n",
      "Train: step:  67470, time: 0.193, loss: 2450.042969\n",
      "Train: step:  67480, time: 0.222, loss: 797.567505\n",
      "Train: step:  67490, time: 0.227, loss: 1528.954834\n",
      "Train: step:  67500, time: 0.182, loss: 1797.720459\n"
     ]
    }
   ],
   "source": [
    "!./train.py --year='2017' --epochs=10 --save_count=1 --checkpoint_num=4 --display_count=10000 --checkpoint_path=checkpoints2 --checkpoint_loadpath=checkpoints2 --imgpath='/datasets/COCO-2017' --batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
